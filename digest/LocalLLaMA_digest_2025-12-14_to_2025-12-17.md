# r/LocalLLaMA Reading Digest

**Period:** 2025-12-14 to 2025-12-17
**Posts Summarized:** 28
**Total Posts Analyzed:** 28

---

## 1. [I'm strong enough to admit that this bugs the hell out of me](https://reddit.com/r/LocalLLaMA/comments/1pnfaqo/im_strong_enough_to_admit_that_this_bugs_the_hell/)

**Author:** u/ForsookComparison | **Upvotes:** 1651 | **Comments:** 338 | **Date:** 2025-12-15

**Summary:** The Reddit post discusses a user's frustration with a specific issue, which has garnered significant attention with 1651 upvotes and 338 comments. The discussion includes humorous and technical responses, highlighting community engagement and diverse perspectives. Key points include the post's popularity, the mix of humorous and technical comments, and the active community engagement. The discussion is marked by a mix of technical advice and humorous banter, with the community engaging actively.

---

## 2. [New Google model incoming!!!](https://reddit.com/r/LocalLLaMA/comments/1pn37mw/new_google_model_incoming/)

**Author:** u/R46H4V | **Upvotes:** 1234 | **Comments:** 258 | **Date:** 2025-12-15

**Summary:** The Reddit post discusses anticipation and speculation around an upcoming new Google model, with users expressing various hopes and expectations.

**Key Points:**
- The post links to a tweet and Google's Hugging Face page, hinting at a new model release.
- Users hope the new model is not similar to Gemma3-Math and speculate it could be Gemma 4.
- There is a desire for a multi-modal model to replace existing options like gpt-oss-120b and 20b.
- The community is excited and hopeful about the new model's potential.

**Discussion Highlights:** The discussion highlights a mix of excitement and specific expectations, with users hoping for significant improvements and multi-modal capabilities in the new Google model.

---

## 3. [Aaaand... is gone...](https://reddit.com/r/LocalLLaMA/comments/1pmungj/aaaand_is_gone/)

**Author:** u/HumanDrone8721 | **Upvotes:** 902 | **Comments:** 189 | **Date:** 2025-12-14

**Summary:** The Reddit post discusses the discontinuation or scarcity of SATA drives, sparking a conversation about storage solutions and their implications.

**Key Points:**
- The post title suggests the disappearance of something significant.
- Comments indicate a focus on storage drives, particularly SATA drives.
- Users discuss the impact of this change, with some seeing it as a non-issue.
- One user mentions buying additional storage in response.
- The discussion includes both humorous and serious takes on the topic.

**Discussion Highlights:** The discussion highlights a mix of concern and indifference regarding the discontinuation of SATA drives, with users sharing their personal responses and opinions on the matter.

---

## 4. [NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model!](https://reddit.com/r/LocalLLaMA/comments/1pn8upp/nvidia_releases_nemotron_3_nano_a_new_30b_hybrid/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 826 | **Comments:** 174 | **Date:** 2025-12-15

**Summary:** NVIDIA has released Nemotron 3 Nano, a 30B hybrid reasoning model with a 1M context window, claiming best-in-class performance for SWE-Bench, reasoning, and chat. The model is available in GGUF format and has been noted for its exceptional speed.

**Key Points:**
- Nemotron 3 Nano is a 30B hybrid reasoning model with a 1M context window.
- It claims best-in-class performance for SWE-Bench, reasoning, and chat.
- The model is available in GGUF format on Hugging Face.
- It is part of a family of MoE models with three sizes.
- Users report exceptional speed (110 t/s).

**Discussion Highlights:** The community discussed the model's speed, clarified its place in the MoE model family, and expressed surprise at the 'nano' designation for a 30B model.

---

## 5. [8x Radeon 7900 XTX Build for Longer Context Local Inference - Performance Results &amp; Build Details](https://reddit.com/r/LocalLLaMA/comments/1pogwb6/8x_radeon_7900_xtx_build_for_longer_context_local/)

**Author:** u/Beautiful_Trust_8151 | **Upvotes:** 525 | **Comments:** 159 | **Date:** 2025-12-16

**Summary:** The post details a custom multi-GPU setup using 8x AMD Radeon 7900 XTX cards for local AI inference, achieving 192 GB VRAM and stable performance with a 131072-token context window. The build cost around $6-7k and offers flexibility and long-context capability for specific work use cases.

**Key Points:**
- 8x AMD Radeon 7900 XTX GPUs provide 192 GB VRAM for local AI inference
- Performance testing shows stable results with up to 131072-token context window
- Build cost is around $6-7k, offering flexibility and customizability
- System consumes about 900 watts during prompt processing and inferencing
- Discussion highlights appreciation for the build and interest in further performance tests

**Discussion Highlights:** The discussion highlights appreciation for the build's capabilities and cost-effectiveness compared to professional alternatives. Users expressed interest in further performance tests and noted the build's potential for future AI applications.

---

## 6. [It was Ilya who "closed" OpenAI](https://reddit.com/r/LocalLLaMA/comments/1pnxekt/it_was_ilya_who_closed_openai/)

**Author:** u/licuphand | **Upvotes:** 484 | **Comments:** 218 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses Ilya's role in 'closing' OpenAI, highlighting leadership struggles and distrust in AI governance.

**Key Points:**
- Distrust in companies handling AI
- Leadership conflicts among Elon, Ilya, and Sam
- Historical context of oversight in AI governance

**Discussion Highlights:** The discussion emphasizes leadership dynamics and the broader implications of AI governance, with a consensus on the challenges of oversight.

---

## 7. [Meta announced a new SAM Audio Model for audio editing that can segment sound from complex audio mixtures using text, visual, and time span prompts.](https://reddit.com/r/LocalLLaMA/comments/1po7i0c/meta_announced_a_new_sam_audio_model_for_audio/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 448 | **Comments:** 67 | **Date:** 2025-12-16

**Summary:** Meta announced a new SAM Audio Model that can segment sound from complex audio mixtures using text, visual, and time span prompts, transforming audio processing.

**Key Points:**
- SAM Audio Model can isolate any sound from complex audio mixtures using text, visual, and time span prompts.
- Potential applications include isolating and subtracting unwanted sounds in Microsoft Teams meetings.
- The model's ability to pick specific sounds from complex audio is highly praised.
- Model sizes and specifications are available in the provided image link.
- Questions about its effectiveness on music instruments were raised.

**Discussion Highlights:** The discussion highlights the potential applications of the SAM Audio Model, such as improving audio quality in virtual meetings by isolating unwanted sounds. Users also expressed interest in the model's effectiveness on music instruments and its ability to pick specific sounds from complex audio mixtures.

---

## 8. [They're finally here (Radeon 9700)](https://reddit.com/r/LocalLLaMA/comments/1pnd5uf/theyre_finally_here_radeon_9700/)

**Author:** u/Zeikos | **Upvotes:** 358 | **Comments:** 67 | **Date:** 2025-12-15

**Summary:** The post announces the arrival of Radeon 9700 GPUs, sparking community interest and requests for benchmarks. Users express nostalgia and enthusiasm for testing the new hardware.

**Key Points:**
- Community eagerly awaits benchmarks for the new Radeon 9700 GPUs
- Nostalgia expressed over the Radeon 9700 name from the 2000s
- Specific requests for inference, training, noise, and heat benchmarks
- Users plan to test the GPUs during the holidays
- Humorous mention of 'time to first smokey smelling' as a benchmark

**Discussion Highlights:** The discussion is dominated by enthusiasm for benchmarking the new GPUs, with users requesting specific performance metrics and sharing nostalgic sentiments about the Radeon 9700 name. There's a consensus on the need for comprehensive testing, including inference, training, noise, and heat levels.

---

## 9. [NVIDIA Nemotron 3 Nano 30B A3B released](https://reddit.com/r/LocalLLaMA/comments/1pn8h5h/nvidia_nemotron_3_nano_30b_a3b_released/)

**Author:** u/rerri | **Upvotes:** 273 | **Comments:** 78 | **Date:** 2025-12-15

**Summary:** NVIDIA has released the Nemotron 3 Nano 30B A3B model, featuring a hybrid Mamba-Transformer architecture with 31.6B parameters and exceptional inference efficiency. The model is fully open and designed for high throughput and low latency.

**Key Points:**
- Hybrid Mamba-Transformer MoE architecture with 31.6B total parameters
- Up to 4x faster inference than Nemotron Nano 2 and 3.3x faster than leading models in its size category
- 1M-token context window and best-in-class reasoning accuracy
- Fully open with open weights, datasets, training recipes, and framework
- Discussion includes Llama.cpp PR, Unsloth quant recommendations, and concerns about synthetic data training

**Discussion Highlights:** The discussion highlights include a Llama.cpp PR for integration, questions about optimal Unsloth quant settings for specific hardware, concerns about the uncanny valley effect due to synthetic data training, and mixed reviews on the model's performance despite its speed.

---

## 10. [XiaomiMiMo/MiMo-V2-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1po3bn4/xiaomimimomimov2flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 222 | **Comments:** 45 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses MiMo-V2-Flash, a Mixture-of-Experts (MoE) language model with 309B total parameters and 15B active parameters, designed for high-speed reasoning and agentic workflows. Users highlight its impressive performance on multilingual SWE tasks and discuss its technical specifications and potential applications.

**Key Points:**
- MiMo-V2-Flash is a Mixture-of-Experts (MoE) language model with 309B total parameters and 15B active parameters.
- Designed for high-speed reasoning and agentic workflows.
- Performs exceptionally well on multilingual SWE tasks, surpassing models like Sonnet 4.5 and Gemini 3.
- Users discuss the feasibility of running the model on specific hardware configurations.
- The model's weights have been released, making it accessible for further research and applications.

**Discussion Highlights:** Users express excitement about the model's performance and accessibility. There is some skepticism about the reported performance metrics, and discussions include technical details about running the model on specific hardware setups.

---

## 11. [Alibaba Open-Sources CosyVoice 3, a New TTS Model](https://reddit.com/r/LocalLLaMA/comments/1pnusp9/alibaba_opensources_cosyvoice_3_a_new_tts_model/)

**Author:** u/nekofneko | **Upvotes:** 211 | **Comments:** 30 | **Date:** 2025-12-16

**Summary:** Alibaba has open-sourced CosyVoice 3, a new TTS model with advanced features like multi-lingual support, high naturalness, and low latency. The model supports various languages, dialects, and emotions, and includes features like pronunciation inpainting and text normalization.

**Key Points:**
- Supports 9 common languages and 18+ Chinese dialects/accents
- Achieves state-of-the-art performance in content consistency and naturalness
- Features like pronunciation inpainting, text normalization, and bi-streaming
- Supports various instructions such as emotions, speed, and volume
- Discussion highlights include comparisons with other models like Chatterbox and Microsoft VibeVoice

**Discussion Highlights:** Users are comparing CosyVoice 3 with other models like Chatterbox and Microsoft VibeVoice. There is interest in a potential 1.5B model release and positive feedback on the model's capabilities.

---

## 12. [Qwen3 Next speed optimization has been merged into llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1pnz9xu/qwen3_next_speed_optimization_has_been_merged/)

**Author:** u/jacek2023 | **Upvotes:** 204 | **Comments:** 24 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses the recent speed optimization for Qwen3 Next in llama.cpp, highlighting significant performance improvements across different hardware configurations.

**Key Points:**
- Speed optimization for Qwen3 Next has been merged into llama.cpp
- Performance on M1 64GB improved from 12 t/s to 18 t/s
- Win11 + RTX5090 + vulkan setup achieves 37.x t/s without CUDA
- UD-Q2_K_XL setup can reach 100+ t/s without CPU offloading
- Qwen3-30B achieves around 58 t/s on the same M1 64GB setup

**Discussion Highlights:** Users report substantial speed improvements, with specific metrics provided for different hardware setups. The consensus is that the optimization significantly enhances performance, making Qwen3 Next more efficient.

---

## 13. [32GB Mi50's were getting so expensive that I ended up buying a 32GB w6800 for about the same price instead](https://reddit.com/r/LocalLLaMA/comments/1pob44f/32gb_mi50s_were_getting_so_expensive_that_i_ended/)

**Author:** u/EmPips | **Upvotes:** 200 | **Comments:** 41 | **Date:** 2025-12-16

**Summary:** The author opted for a 32GB w6800 GPU instead of a 32GB Mi50 due to similar pricing, highlighting the convenience and cooling performance of the w6800. The discussion includes a comparison chart and suggests considering the AMD Radeon™ AI PRO R9700 as a potentially better alternative.

**Key Points:**
- Author chose 32GB w6800 over 32GB Mi50 due to similar pricing
- w6800 offers convenience and effective blower-style cooling
- Pros/cons comparison chart provided for decision-making
- AMD Radeon™ AI PRO R9700 suggested as a better alternative
- Discussion includes pricing and performance considerations

**Discussion Highlights:** The discussion primarily focuses on the practical aspects of the w6800, such as its cooling performance and ease of installation. There is also a suggestion to consider the AMD Radeon™ AI PRO R9700 for better performance and software support, indicating a consensus on exploring better alternatives within a similar price range.

---

## 14. [llama.cpp: Automation for GPU layers, tensor split, tensor overrides, and context size (with MoE optimizations)](https://reddit.com/r/LocalLLaMA/comments/1pn2e1c/llamacpp_automation_for_gpu_layers_tensor_split/)

**Author:** u/Remove_Ayys | **Upvotes:** 183 | **Comments:** 58 | **Date:** 2025-12-15

**Summary:** The post discusses the implementation of automation for GPU layers, tensor split, tensor overrides, and context size in llama.cpp, aiming to improve usability and performance, especially for MoE models. The author highlights the challenges of manual memory control and the benefits of the new automated approach.

**Key Points:**
- CPU + GPU hybrid inference is a core feature of llama.cpp.
- Manual memory control via parameters like --n-gpu-layers and --tensor-split is suboptimal.
- Automation for memory allocation has been implemented to improve usability and performance.
- The new functionality prioritizes dense tensors for better MoE performance.
- The implementation is generic and works for any ggml backend supporting CPU + GPU hybrid inference.

**Discussion Highlights:** The discussion highlights positive feedback on the implementation, with users appreciating the convenience and suggesting further improvements like caching to reduce fitting time. Some users also mentioned additional tools and scripts for optimizing tensor splits.

---

## 15. [status of Nemotron 3 Nano support in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1pnc045/status_of_nemotron_3_nano_support_in_llamacpp/)

**Author:** u/jacek2023 | **Upvotes:** 175 | **Comments:** 30 | **Date:** 2025-12-15

**Summary:** The Reddit post discusses the status of Nemotron 3 Nano support in llama.cpp, highlighting a GitHub pull request and community reactions. The discussion emphasizes the importance of collaboration between organizations and the llama.cpp project for new model architectures.

**Key Points:**
- Nemotron 3 Nano support is being added to llama.cpp via a GitHub pull request.
- The model sizes (Q4_K_M and Q4_K_XL) are noted to be around 24GB, which is a point of discussion.
- Community members appreciate Nvidia's approach and encourage other labs to follow suit.
- There is a consensus that organizations releasing new models should work with llama.cpp for early support.

**Discussion Highlights:** The community generally supports the collaboration between Nvidia and llama.cpp, viewing it as a positive step for the ecosystem. There is a call for other organizations to adopt similar practices to ensure broader compatibility and support.

---

## 16. [Nemotron 3 Nano 30B is Amazing! (TLDR)](https://reddit.com/r/LocalLLaMA/comments/1pocsdy/nemotron_3_nano_30b_is_amazing_tldr/)

**Author:** u/DonkeyBonked | **Upvotes:** 174 | **Comments:** 78 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses the author's experience with Nemotron 3 Nano 30B, highlighting its token efficiency and performance on their hardware setup. The author compares it favorably to other models like Devstral 2 Small 24B and Qwen models, noting its ability to handle large contexts efficiently.

**Key Points:**
- Nemotron 3 Nano 30B shows impressive token efficiency and performance on the author's hardware setup.
- The model fits 256k tokens in VRAM and can handle up to 1M context with spillover.
- Performance comparisons show Nemotron 3 Nano 30B outperforming Devstral 2 Small 24B and Qwen models in certain tasks.
- The author uses a unique hardware setup with an RTX 5000 and an RTX 3090 eGPU.
- Discussion highlights include performance metrics and use cases where Nemotron 3 Nano 30B excels.

**Discussion Highlights:** The discussion highlights the model's performance metrics, such as high tokens per second and low repetition issues. Users also inquire about specific use cases and standardized tests for comparing models.

---

## 17. [Understanding the new router mode in llama cpp server](https://reddit.com/r/LocalLLaMA/comments/1pmc7lk/understanding_the_new_router_mode_in_llama_cpp/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 159 | **Comments:** 44 | **Date:** 2025-12-14

**Summary:** Router mode in llama cpp server allows managing multiple AI models simultaneously without restarting the server, similar to Ollama's functionality. It enables dynamic loading/unloading of models and routing requests to the appropriate model, saving memory and simplifying model switching.

**Key Points:**
- Router mode enables managing multiple AI models in a single server process
- Eliminates the need to restart the server when switching or loading models
- Allows dynamic loading/unloading of models on demand
- Routes requests to the appropriate model internally
- Useful for testing multiple GGUF models, building local APIs, and dynamic model switching

**Discussion Highlights:** Users expressed interest in the new functionality, with some comparing it to existing tools like llama-swap. Key discussion points included model memory management, concurrent model loading, and VRAM management for multi-GPU setups. Some users requested more detailed explanations and comparisons with existing solutions.

---

## 18. [GLM-4.5V, GLM-4.6V and GLM_4.6V-Flash are now supported by llama.cpp (GGUFs)](https://reddit.com/r/LocalLLaMA/comments/1po18y9/glm45v_glm46v_and_glm_46vflash_are_now_supported/)

**Author:** u/jacek2023 | **Upvotes:** 156 | **Comments:** 33 | **Date:** 2025-12-16

**Summary:** The Reddit post announces that GLM-4.5V, GLM-4.6V, and GLM_4.6V-Flash are now supported by llama.cpp with GGUFs, which is seen as a significant update by the community.

**Key Points:**
- Support for GLM-4.5V, GLM-4.6V, and GLM_4.6V-Flash has been added to llama.cpp.
- The update is celebrated as a great Christmas gift by the community.
- There is a question about whether the GGUFs support vision, with some users reporting issues.
- Comparisons between Qwen3-VL-4B and GLM_4.6V are being discussed.

**Discussion Highlights:** The community is excited about the new support for GLM models in llama.cpp, though there are some concerns and questions about vision support and compatibility with existing setups.

---

## 19. [New budget local AI rig](https://reddit.com/r/LocalLLaMA/comments/1pnllux/new_budget_local_ai_rig/)

**Author:** u/vucamille | **Upvotes:** 150 | **Comments:** 38 | **Date:** 2025-12-15

**Summary:** The user built a budget AI rig using a Qiyida X99 motherboard, Xeon E5 2680 V4 CPU, and two MI50 16GB GPUs for around $650. The system works well with ROCm 7.0.2 and can handle basic inference tasks, with plans for future upgrades.

**Key Points:**
- Budget build with Xeon E5 2680 V4 and dual MI50 16GB GPUs
- Total cost around $650, with PSU being the most expensive component
- ROCm 7.0.2 works well for multi-GPU inference tasks
- Community praises the cost-effectiveness and expandability of the system
- User plans to add brackets and decorations, and may upgrade to 32GB GPUs later

**Discussion Highlights:** The community highlights the system's cost-effectiveness and expandability, with praise for its performance in handling AI tasks. Some users request benchmarks, while others share their own experiences and offer encouragement.

---

## 20. [Finally managed to run Qwen-2.5-7B on a 4GB GTX 1050 without CPU offloading (Surgical Memory Alignment)](https://reddit.com/r/LocalLLaMA/comments/1po97ad/finally_managed_to_run_qwen257b_on_a_4gb_gtx_1050/)

**Author:** u/HuseyinKama | **Upvotes:** 138 | **Comments:** 43 | **Date:** 2025-12-16

**Summary:** The post describes a method called 'Surgical Memory Alignment' to run Qwen-2.5-7B on a 4GB GTX 1050 without CPU offloading, saving VRAM and improving speed. The author open-sourced the solution as QKV Core.

**Key Points:**
- Standard GGUF quantization tools add unnecessary padding, causing OOM errors on low-end GPUs.
- Surgical Alignment trims and realigns memory blocks to fit llama.cpp's block boundaries, saving ~44MB per model.
- The method improved I/O load times by ~34% using Numba-accelerated kernels.
- The solution is open-sourced as QKV Core, targeting users with 4GB/6GB GPUs.
- Community feedback includes praise, technical scrutiny, and suggestions for further optimization.

**Discussion Highlights:** The community praised the innovation but also raised concerns about the code's effectiveness and suggested further optimizations. Some users highlighted the importance of memory efficiency in low-end hardware.

---

## 21. [8 Million Users' AI Conversations Sold for Profit by "Privacy" Extensions | Koi Blog](https://reddit.com/r/LocalLLaMA/comments/1poal2a/8_million_users_ai_conversations_sold_for_profit/)

**Author:** u/ManThigh | **Upvotes:** 136 | **Comments:** 37 | **Date:** 2025-12-16

**Summary:** The Reddit post highlights privacy concerns regarding browser extensions selling AI conversation data of millions of users for profit, emphasizing the importance of using local models and auditing extensions.

**Key Points:**
- Browser extensions like Urban VPN Proxy and 1ClickVPN Proxy sold user AI conversation data.
- Over 6 million users were affected by these privacy breaches.
- The community advocates for using local models to avoid such privacy issues.
- There is a call to punish companies that buy and exploit user data.
- Users express pride in their local setups and caution against browser-based interfaces.

**Discussion Highlights:** The discussion consensus emphasizes the importance of privacy, the risks of browser extensions, and the benefits of local AI models. Users express strong disapproval of companies profiting from user data and advocate for stricter penalties.

---

## 22. [To Mistral and other lab employees: please test with community tools BEFORE releasing models](https://reddit.com/r/LocalLLaMA/comments/1pmgm2x/to_mistral_and_other_lab_employees_please_test/)

**Author:** u/dtdisapointingresult | **Upvotes:** 136 | **Comments:** 71 | **Date:** 2025-12-14

**Summary:** The Reddit post criticizes Mistral for releasing Devstral 2 without thorough testing with community tools, leading to issues like benchmark discrepancies and repetition loops. The author emphasizes the importance of testing with local tools to maintain reputation and user trust. Key points include: Devstral 2 release faced issues like benchmark discrepancies and repetition loops; lack of testing with community tools before release hurt Mistral's reputation; the author stresses the importance of testing with local tools for open model adoption; community feedback highlights mixed experiences with the model across different tools; the discussion underscores the value of tech geeks' recommendations in driving adoption. The discussion reveals a mix of experiences with Devstral 2, with some users reporting positive outcomes while others encounter issues. There is a consensus on the need for better testing and documentation to ensure smooth integration with community tools.

---

## 23. [I may have over-quantized this little guy.](https://reddit.com/r/LocalLLaMA/comments/1pnz80z/i_may_have_overquantized_this_little_guy/)

**Author:** u/AllergicToTeeth | **Upvotes:** 135 | **Comments:** 27 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses the quantization of a model, with comments highlighting technical aspects like system prompts and quantization levels, along with humorous references to AI advancements.

**Key Points:**
- Quantization of a model is the main topic
- System prompts are important for some models
- Q0 quantization level is mentioned
- Humorous references to GPT versions are made

**Discussion Highlights:** The community engages in technical discussion about model quantization and makes light-hearted jokes about AI advancements.

---

## 24. [Qwen3-Next-80B-A3B-Thinking-GGUF has just been released on HuggingFace](https://reddit.com/r/LocalLLaMA/comments/1pmo0dn/qwen3next80ba3bthinkinggguf_has_just_been/)

**Author:** u/LegacyRemaster | **Upvotes:** 124 | **Comments:** 54 | **Date:** 2025-12-14

**Summary:** The post announces the release of Qwen3-Next-80B-A3B-Thinking-GGUF on HuggingFace, highlighting its impressive performance in a Tetris task. Users discuss its capabilities, release timing, and technical aspects.

**Key Points:**
- Qwen3-Next-80B-A3B-Thinking-GGUF model released on HuggingFace
- Model excels in Tetris task within a single HTML file
- Users compare it favorably to Devstral
- Discussion includes release timing and technical queries
- Mixed reactions on release date and model capabilities

**Discussion Highlights:** Users express amazement at the model's performance, debate the release timing, and discuss technical aspects like native tool calling support. Some question the novelty of the release, while others highlight its potential for agentic coding tasks.

---

## 25. [I was bored](https://reddit.com/r/LocalLLaMA/comments/1po8yt0/i_was_bored/)

**Author:** u/MyLovelyAngelKirino | **Upvotes:** 119 | **Comments:** 59 | **Date:** 2025-12-16

**Summary:** The author, who is unemployed, built a high-performance computer setup with excess hardware, including 3x 3090 GPUs, 512GB RAM, and an Epyc 7663 56-core processor. The post garnered attention and humorous comments from the community.

**Key Points:**
- Author built a powerful computer setup due to unemployment and excess hardware
- Setup includes 3x 3090 GPUs, 512GB RAM, and an Epyc 7663 56-core processor
- Post received 119 upvotes and 59 comments
- Top comments express admiration and humor
- Community engagement highlights interest in high-performance hardware

**Discussion Highlights:** The discussion highlights admiration for the author's setup and humorous reactions, with one comment jokingly asking how to magically acquire such hardware and another playfully referencing a character named Felix.

---

## 26. [How to do a RTX Pro 6000 build right](https://reddit.com/r/LocalLLaMA/comments/1pn6ijr/how_to_do_a_rtx_pro_6000_build_right/)

**Author:** u/GPTrack_dot_ai | **Upvotes:** 114 | **Comments:** 174 | **Date:** 2025-12-15

**Summary:** The post discusses building a high-performance system using 8x Nvidia RTX Pro 6000 GPUs with integrated 400G networking, highlighting the need for a compatible switch, CPU, RAM, and storage. The setup is described as ready-to-use with minimal configuration required.

**Key Points:**
- The RTX Pro 6000 lacks NVlink, so Nvidia integrated high-speed networking directly into each GPU.
- The system requires 8 PCIe slots, each with a 400G networking connection.
- Key components include dual Intel Xeon CPUs, up to 32x 6400 RDIMM or 8000 MRDIMM, and a 6000W TDP.
- The build is praised for its performance and readiness, though its cost is noted as high.
- Users expressed awe at the specifications, comparing it to luxury items like Ferraris and private jets.

**Discussion Highlights:** The discussion highlights the impressive specifications and performance of the RTX Pro 6000 build, with users expressing admiration and humor about its cost and capabilities. The consensus is that this setup is ideal for high-performance computing tasks requiring multiple GPUs.

---

## 27. [Alibaba Tongyi Open Sources Two Audio Models: Fun-CosyVoice 3.0 (TTS) and Fun-ASR-Nano-2512 (ASR)](https://reddit.com/r/LocalLLaMA/comments/1pn7c3f/alibaba_tongyi_open_sources_two_audio_models/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 109 | **Comments:** 24 | **Date:** 2025-12-15

**Summary:** Alibaba Tongyi has open-sourced two audio models: Fun-CosyVoice 3.0 (TTS) and Fun-ASR-Nano-2512 (ASR). These models are lightweight, support local deployment, and offer features like zero-shot voice cloning.

**Key Points:**
- Fun-ASR-Nano is a lightweight ASR model with lower inference cost and support for local deployment.
- Fun-CosyVoice3 is a TTS model with zero-shot voice cloning capabilities.
- Both models are open-sourced and ready for local deployment and custom fine-tuning.
- The community appreciates the release and sees it as a positive step in audio model development.
- There is a separate page for FunAudioLLM models on Hugging Face.

**Discussion Highlights:** The community is generally positive about the release, with some users highlighting the potential to compete with Nvidia's Parakeet and the availability of models on Hugging Face. Some users expressed specific use cases and excitement about the release.

---

## 28. [Bolmo-the first family of competitive fully open byte-level language models (LMs) at the 1B and 7B parameter scales.](https://reddit.com/r/LocalLLaMA/comments/1pndzy7/bolmothe_first_family_of_competitive_fully_open/)

**Author:** u/BreakfastFriendly728 | **Upvotes:** 103 | **Comments:** 23 | **Date:** 2025-12-15

**Summary:** The post introduces Bolmo, a family of competitive fully open byte-level language models at the 1B and 7B parameter scales, developed by AllenAI. It includes links to the model collection, GitHub repository, and a research paper. The discussion highlights excitement about open-sourcing byte-level models and potential future developments like omnimodal capabilities.

**Key Points:**
- Bolmo is a family of fully open byte-level language models at 1B and 7B parameter scales.
- Byte-level language models process text using UTF-8 bytes instead of traditional subword tokenization.
- The community is excited about the potential of byte-level models and their future applications.
- Suggestions for future developments include making the models omnimodal.
- There is interest in the availability of GGUF format for these models.

**Discussion Highlights:** The discussion reflects enthusiasm for the open-sourcing of byte-level models, with users expressing interest in their potential advantages and future developments such as omnimodal capabilities. There is also a request for the models to be available in GGUF format.

---

