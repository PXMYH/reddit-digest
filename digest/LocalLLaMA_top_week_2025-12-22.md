# r/LocalLLaMA Reading Digest

**Period:** 2025-12-22 to 2025-12-22
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [1 year later and people are still speedrunning NanoGPT. Last time this was posted the WR was 8.2 min. Its now 127.7 sec.](https://reddit.com/r/LocalLLaMA/comments/1psh1w2/1_year_later_and_people_are_still_speedrunning/)

**Author:** u/jd_3d | **Upvotes:** 207 | **Comments:** 22 | **Date:** 2025-12-21

**Summary:** The Reddit post discusses the progress in speedrunning NanoGPT training, highlighting a significant reduction in training time from 45 minutes to 127.7 seconds. The discussion includes insights from users about their experiences and achievements in training the model efficiently.

**Key Points:**
- NanoGPT training time has significantly improved from 45 minutes to 127.7 seconds.
- Users share their experiences and achievements in training the model efficiently.
- There is interest in understanding the specific improvements and techniques used to achieve these results.
- The discussion highlights the rapid progress in algorithmic speed improvements.
- Users express curiosity about the rules and details of LLM speedrunning.

**Discussion Highlights:** The discussion highlights the rapid progress in algorithmic speed improvements and the curiosity among users about the specific techniques and rules involved in LLM speedrunning. There is a consensus on the impressive achievements in reducing training times and a desire to learn more about the methods used.

---

## 2. [It ain’t much, but proud of my 2x3090 + a spare 3060 for support](https://reddit.com/r/LocalLLaMA/comments/1pse7w6/it_aint_much_but_proud_of_my_2x3090_a_spare_3060/)

**Author:** u/liviuberechet | **Upvotes:** 120 | **Comments:** 54 | **Date:** 2025-12-21

**Summary:** The user shares their hardware setup featuring 2x3090 GPUs and a spare 3060, expressing pride in their build despite its tight fit. They mention their positive experience with Qwen3-Next-80b and ongoing struggles with Clint in VS Code.

**Key Points:**
- User has a high-end setup with 2x3090 GPUs and a spare 3060.
- The build is praised by commenters as being top-tier for enthusiasts.
- User is satisfied with Qwen3-Next-80b but faces issues with Clint in VS Code.
- Comments highlight the rarity and power of the setup, with some expressing envy.
- Concerns about heat management are raised in the discussion.

**Discussion Highlights:** The discussion is largely positive, with commenters praising the user's hardware setup as being among the top 1% of enthusiasts. Some users express envy, while others raise concerns about potential heat issues. The consensus is that the build is impressive and powerful.

---

## 3. [llama.cpp appreciation post](https://reddit.com/r/LocalLLaMA/comments/1psbx2q/llamacpp_appreciation_post/)

**Author:** u/hackiv | **Upvotes:** 1525 | **Comments:** 147 | **Date:** 2025-12-21

**Summary:** The Reddit post appreciates llama.cpp for its performance and frequent updates, highlighting its superiority over other tools like Ollama and LM Studio. Users share their positive experiences and performance metrics.

**Key Points:**
- llama.cpp is praised for its frequent updates and features.
- Users report significant performance improvements with llama.cpp (e.g., 23t/s on specific hardware).
- Some users mention switching from Ollama to llama.cpp due to its advantages.
- The community appreciates the contributions of llama.cpp developers.

**Discussion Highlights:** The discussion highlights the performance benefits of llama.cpp, with users sharing their experiences and metrics. There is a consensus on the superiority of llama.cpp over other tools, and appreciation for the developers' contributions.

---

## 4. [Dataset quality is not improving much](https://reddit.com/r/LocalLLaMA/comments/1ps6w96/dataset_quality_is_not_improving_much/)

**Author:** u/rekriux | **Upvotes:** 181 | **Comments:** 33 | **Date:** 2025-12-21

**Summary:** The Reddit post discusses the lack of significant improvements in dataset quality for AI models, highlighting a few notable datasets and expressing concern over the stagnation in dataset innovation. The author mentions specific datasets like Tulu, smoltalk2, and Hermes 3, and points out the challenges in accessing and creating high-quality datasets. Key points include the lack of breakthroughs in dataset creation, the importance of dataset quality, and the reluctance of companies to invest in manual data curation. The discussion highlights the challenges in dataset creation and the need for more research and innovation in this area.

---

## 5. [How big do we think Gemini 3 flash is](https://reddit.com/r/LocalLLaMA/comments/1pruoy7/how_big_do_we_think_gemini_3_flash_is/)

**Author:** u/davikrehalt | **Upvotes:** 126 | **Comments:** 111 | **Date:** 2025-12-20

**Summary:** The Reddit post discusses speculation about the size of Gemini 3 Flash, with users estimating it to be around 1.2T parameters or 600B+ with a small expert size. The discussion highlights the potential for running such models on devices like MacBooks with sufficient memory.

**Key Points:**
- Gemini 3 Flash is speculated to be a 1.2T parameter model or around 600B+ with small expert size.
- The model size is relevant for understanding the feasibility of running advanced models on local devices like MacBooks.
- Users express curiosity about updated local LLMs like Gemma and discuss the potential for local deployment.
- There is a call for Google to provide official information about the model size.

**Discussion Highlights:** The discussion features a range of estimates for Gemini 3 Flash's size, from 1.2T parameters to 600B+, with users emphasizing the importance of model size for local deployment. There is also speculation about the future of local LLMs and a desire for more transparency from Google.

---

## 6. [Xiaomi’s MiMo-V2-Flash (309B model) jumping straight to the big leagues](https://reddit.com/r/LocalLLaMA/comments/1prjzoh/xiaomis_mimov2flash_309b_model_jumping_straight/)

**Author:** u/98Saman | **Upvotes:** 415 | **Comments:** 89 | **Date:** 2025-12-20

**Summary:** The Reddit post discusses Xiaomi's MiMo-V2-Flash (309B model), highlighting its impressive performance and community interest. The model is noted for its efficiency and speed, drawing comparisons to other advanced models.

**Key Points:**
- Xiaomi's MiMo-V2-Flash (309B model) is gaining attention for its performance.
- Community interest in open weights and GGUF availability.
- Performance comparisons suggest it benchmarks similarly to DS 3.2 with fewer parameters and higher speed.
- Discussion includes skepticism about the Artificial Analysis Index's accuracy.

**Discussion Highlights:** The community shows strong interest in the model's open weight status and performance metrics. There is consensus on its impressive benchmarking results and speed, though some skepticism about certain evaluation metrics.

---

## 7. [A Raspberry Pi + eGPU isn't as dumb as I thought](https://reddit.com/r/LocalLLaMA/comments/1prh5jp/a_raspberry_pi_egpu_isnt_as_dumb_as_i_thought/)

**Author:** u/geerlingguy | **Upvotes:** 141 | **Comments:** 21 | **Date:** 2025-12-20

**Summary:** The post discusses benchmarks comparing a Raspberry Pi CM5 with an eGPU to a high-end PC, showing minimal performance differences for larger models and even better performance for some Nvidia cards. The discussion highlights cost considerations and the feasibility of using a Raspberry Pi for AI tasks.

**Key Points:**
- Performance delta between Raspberry Pi + eGPU and high-end PC is less than 5% for larger models
- Raspberry Pi was faster for some Nvidia cards with llama 2 13B
- AMD cards showed significant performance issues, possibly due to driver problems
- Cost of the GPU is a major consideration in the discussion
- Feasibility of using Raspberry Pi for AI tasks like llamacpp or ComfyUI is questioned

**Discussion Highlights:** The discussion focuses on the cost-effectiveness and feasibility of using a Raspberry Pi with an eGPU for AI tasks. Users are curious about multi-GPU setups and the potential of Raspberry Pi as a cheap AI rig. There is also interest in specific hardware like the dolphin ICS card and PCIe 4.0 switches.

---

## 8. [Of course it works, in case you are wondering... and it's quite faster.](https://reddit.com/r/LocalLLaMA/comments/1prcu0t/of_course_it_works_in_case_you_are_wondering_and/)

**Author:** u/JLeonsarmiento | **Upvotes:** 229 | **Comments:** 57 | **Date:** 2025-12-20

**Summary:** The Reddit post highlights the performance of Qwen's agent, noting its speed compared to other models. The discussion focuses on comparisons with dense models and the benefits of open-source competition.

**Key Points:**
- Qwen's agent is noted for its speed
- Comparison with a dense 24B model
- Open-source competition is beneficial
- Performance advantages of MoE models

**Discussion Highlights:** The discussion emphasizes the speed of Qwen's agent compared to larger dense models, with some users questioning the basis of comparison and others highlighting the benefits of open-source development.

---

## 9. [Open source LLM tooling is getting eaten by big tech](https://reddit.com/r/LocalLLaMA/comments/1pragtf/open_source_llm_tooling_is_getting_eaten_by_big/)

**Author:** u/Inevitable_Wear_9107 | **Upvotes:** 339 | **Comments:** 128 | **Date:** 2025-12-20

**Summary:** The Reddit post discusses the rapid evolution and consolidation of open-source LLM tooling by big tech companies, highlighting the decline of older projects and the shift towards ecosystem-driven tooling.

**Key Points:**
- Rapid replacement of open-source LLM projects within short timeframes.
- Decline of older tools like TensorFlow and the short median project age in the space.
- Shift towards ecosystem-driven tooling by big tech companies like NVIDIA, Google, and OpenAI.
- Challenges faced by open-source projects in attracting resources and maintaining operations.
- The role of big tech in shaping the LLM tooling ecosystem.

**Discussion Highlights:** The discussion highlights the challenges faced by open-source projects in maintaining operations and attracting resources, with a consensus that big tech companies are increasingly shaping the LLM tooling ecosystem.

---

## 10. [Just pushed M2.1 through a 3D particle system. Insane！](https://reddit.com/r/LocalLLaMA/comments/1pr54as/just_pushed_m21_through_a_3d_particle_system/)

**Author:** u/srtng | **Upvotes:** 153 | **Comments:** 40 | **Date:** 2025-12-19

**Summary:** The Reddit post discusses the impressive performance of MiniMax M2.1 in an interactive 3D particle system, with the author expressing excitement about its capabilities and hinting at an upcoming release. The community shares positive feedback and comparisons to other models.

**Key Points:**
- MiniMax M2.1 demonstrates strong performance in a 3D particle system.
- The model is compared favorably to other advanced models like Sonnet4.5.
- Users express enthusiasm for M2.1's speed and efficiency, even on lower-end hardware.
- The release of M2.1 is anticipated soon.
- M2 is praised as a top local model of 2025, with good performance on CPUs.

**Discussion Highlights:** The discussion highlights a consensus on M2.1's impressive performance and efficiency, with users sharing their positive experiences and comparisons to other models. There is anticipation for the official release of M2.1.

---

## 11. [Key Highlights of NVIDIA’s New Open-Source Vision-to-Action Model: NitroGen](https://reddit.com/r/LocalLLaMA/comments/1pr48qm/key_highlights_of_nvidias_new_opensource/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 338 | **Comments:** 71 | **Date:** 2025-12-19

**Summary:** NVIDIA's NitroGen is an open-source vision-to-action model designed to play video games directly from raw frames using imitation learning. It works best with gamepad-controlled games and leverages a vision transformer and diffusion matching transformer for action generation.

**Key Points:**
- NitroGen is a unified vision-to-action model for playing video games from raw frames.
- It is trained through large-scale imitation learning on human gameplay videos.
- The model is most effective on gamepad-controlled games like action, platformer, and racing games.
- It uses a pre-trained vision transformer (SigLip2) and a diffusion matching transformer (DiT) for action generation.
- Potential applications include making couch-coop games playable alone, though concerns about bots in online games were raised.

**Discussion Highlights:** The discussion highlights both positive and negative aspects of NitroGen. While some users appreciate its potential for enabling solo play in couch-coop games, others express concerns about increased bots in online games. There is also curiosity about the use of a diffusion transformer and its necessity for denoising outputs.

---

## 12. [Japan's Rakuten is going to release a 700B open weight model in Spring 2026](https://reddit.com/r/LocalLLaMA/comments/1pr20el/japans_rakuten_is_going_to_release_a_700b_open/)

**Author:** u/Ok_Warning2146 | **Upvotes:** 262 | **Comments:** 45 | **Date:** 2025-12-19

**Summary:** Rakuten plans to release a 700B open weight model in Spring 2026, which could serve as an alternative to Chinese models and prompt US companies to release larger models.

**Key Points:**
- Rakuten's 700B model release is scheduled for Spring 2026
- The model aims to be an alternative to Chinese models and encourage US companies to release larger models
- Users are anticipating a 0.4 quantized version to fit 24GB VRAM
- There is speculation about whether the model is a fine-tune of Deepseek V3
- The release timeline of 6 months is considered long in the fast-moving AI space

**Discussion Highlights:** The community is eagerly awaiting the model, with discussions focusing on technical specifications, potential applications, and comparisons to existing models. There is also humor and speculation about the model's capabilities and origins.

---

## 13. [Devstral 2 (with Mistral's Vibe) vs Sonnet 4.5 (Claude Code) on SWE-bench: 37.6% vs 39.8% (within statistical error)](https://reddit.com/r/LocalLLaMA/comments/1pqy2bq/devstral_2_with_mistrals_vibe_vs_sonnet_45_claude/)

**Author:** u/Constant_Branch282 | **Upvotes:** 135 | **Comments:** 85 | **Date:** 2025-12-19

**Summary:** The post compares the performance of Devstral 2 (Mistral's Vibe) and Sonnet 4.5 (Claude Code) on the SWE-bench, showing that Devstral 2 matches Anthropic's best model within statistical error. Devstral 2 is also faster and can be run locally.

**Key Points:**
- Devstral 2 and Sonnet 4.5 perform similarly on SWE-bench, with results within statistical error.
- Devstral 2 is faster (296s vs 357s) and can be run locally on hardware like the Strix Halo.
- About 40% of test cases showed inconsistent outcomes across runs, highlighting variance in results.
- Devstral 2 is praised for its performance in agentic coding tasks.
- Some users report mixed experiences with Devstral 2, depending on the programming language used.

**Discussion Highlights:** The discussion highlights praise for Mistral's models, particularly Devstral 2, for their performance and cost-effectiveness. Some users note variability in performance depending on the programming language, and there is a consensus that open-weight models like Devstral 2 are becoming competitive with proprietary models.

---

## 14. [FlashHead: Up to 50% faster token generation on top of other techniques like quantization](https://reddit.com/r/LocalLLaMA/comments/1pqui9l/flashhead_up_to_50_faster_token_generation_on_top/)

**Author:** u/Any_Frame9721 | **Upvotes:** 195 | **Comments:** 62 | **Date:** 2025-12-19

**Summary:** FlashHead is an architectural innovation for small language models (SLMs) that offers up to 50% faster token generation on top of techniques like quantization. It replaces the expensive language model head with a more efficient layer using information retrieval, maintaining perfect accuracy compared to baseline models. The technology is available via pip installation and integrates with vLLM.

**Key Points:**
- FlashHead provides up to 50% faster token generation on top of quantization.
- It is a drop-in replacement for the language model head, maintaining perfect accuracy.
- Benchmark results show significant speedups, especially when combined with quantization (e.g., 3.73× speedup with W4A16).
- The technology is available via pip installation and integrates with vLLM.
- The startup behind FlashHead also offers a free Edge AI Hub for running models on mobile devices.

**Discussion Highlights:** The discussion highlights interest in scalability to larger models, compatibility with Mixture of Experts (MoE), potential for llama.cpp support, and applications in reinforcement learning (RL). Users are curious about the technical details and broader applicability of FlashHead.

---

## 15. [Career Advice in AI — Notes from an Andrew Ng Lecture](https://reddit.com/r/LocalLLaMA/comments/1pqpj29/career_advice_in_ai_notes_from_an_andrew_ng/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 350 | **Comments:** 54 | **Date:** 2025-12-19

**Summary:** Andrew Ng emphasizes that now is the best time to build a career in AI, highlighting the rapid progress in the field and the importance of staying updated with the latest coding tools. He also stresses the value of product management skills, surrounding oneself with the right people, and focusing on building projects to gain practical experience.

**Key Points:**
- AI career opportunities are expanding rapidly with accelerating progress.
- Staying updated with the latest AI coding tools is crucial for productivity.
- Product management skills are becoming increasingly important in AI development.
- Building a strong network and choosing the right team are key to success.
- Practical experience through building projects is highly valuable.

**Discussion Highlights:** The discussion highlights a mix of enthusiasm and skepticism about AI careers. Some users agree with the importance of staying updated with tools and the value of hard work, while others express concerns about job security and the practical challenges of working in AI.

---

## 16. [Chinese researchers unveil "LightGen": An all-optical chip that outperforms Nvidia’s A100 by 100x](https://reddit.com/r/LocalLLaMA/comments/1pqoldt/chinese_researchers_unveil_lightgen_an_alloptical/)

**Author:** u/entsnack | **Upvotes:** 207 | **Comments:** 61 | **Date:** 2025-12-19

**Summary:** Chinese researchers from top-tier labs (SJTU and Tsinghua) have unveiled 'LightGen', an all-optical chip claimed to outperform Nvidia's A100 by 100x. The announcement has sparked skepticism and discussion about its practical limitations and potential impact.

**Key Points:**
- Research from top-tier labs (SJTU and Tsinghua)
- Chip limited to linear operations like matrix multiplications
- Skepticism about practicality and maturity of the technology
- Comparisons to overhyped tech announcements
- Community interest in competitive advancements in computing hardware

**Discussion Highlights:** The community expresses skepticism about the chip's capabilities, particularly its limitations in handling nonlinear operations and its analog nature. There is also interest in the potential for technological competition and advancements in computing hardware.

---

## 17. [Qwen released Qwen-Image-Layered on Hugging face.](https://reddit.com/r/LocalLLaMA/comments/1pqoi6i/qwen_released_qwenimagelayered_on_hugging_face/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 618 | **Comments:** 70 | **Date:** 2025-12-19

**Summary:** Qwen has released Qwen-Image-Layered on Hugging Face, featuring Photoshop-grade layering with physically isolated RGBA layers, prompt-controlled structure, and infinite decomposition capabilities.

**Key Points:**
- Photoshop-grade layering with true native editability
- Physically isolated RGBA layers
- Prompt-controlled structure for specifying layers
- Infinite decomposition for detailed layering
- Core model is 40GB unquantized

**Discussion Highlights:** The community is excited about the release, with discussions focusing on RAM/VRAM requirements and appreciation for Qwen's continuous innovation.

---

## 18. [GLM 4.7 is Coming?](https://reddit.com/r/LocalLLaMA/comments/1pqn0vq/glm_47_is_coming/)

**Author:** u/InternationalAsk1490 | **Upvotes:** 264 | **Comments:** 43 | **Date:** 2025-12-19

**Summary:** The Reddit post discusses the potential release of GLM 4.7, with users expressing anticipation and disappointment over the removal of GLM 4.6-air. The community hopes for a Christmas release.

**Key Points:**
- Anticipation for GLM 4.7 release
- Disappointment over removal of GLM 4.6-air
- Hope for a Christmas release
- Community engagement with 264 upvotes and 43 comments

**Discussion Highlights:** The discussion highlights a mix of anticipation and disappointment, with users eagerly awaiting the new release and expressing their hopes for it to arrive as a Christmas present.

---

## 19. [Realist meme of the year!](https://reddit.com/r/LocalLLaMA/comments/1pqegcr/realist_meme_of_the_year/)

**Author:** u/Slight_Tone_2188 | **Upvotes:** 1936 | **Comments:** 121 | **Date:** 2025-12-19

**Summary:** The Reddit post titled 'Realist meme of the year!' gained significant attention with 1936 upvotes and 121 comments. The discussion revolves around various topics including the need for a cure for cancer, humorous suggestions like downloading more RAM, and critiques of AI companies and hardware manufacturers.

**Key Points:**
- The post received a special flair for its contribution.
- A prominent comment highlights the urgency for a cure for cancer.
- Humorous suggestions like downloading more RAM were made.
- Criticism was directed towards AI companies and hardware manufacturers.
- The discussion includes a mix of serious and light-hearted comments.

**Discussion Highlights:** The discussion highlights a mix of serious concerns, such as the need for medical advancements, and humorous or satirical comments. There is also a notable critique of the tech industry, particularly AI companies and hardware manufacturers, for their role in current technological limitations.

---

## 20. [Jake (formerly of LTT) demonstrate's Exo's RDMA-over-Thunderbolt on four Mac Studios](https://reddit.com/r/LocalLLaMA/comments/1pq5k6e/jake_formerly_of_ltt_demonstrates_exos/)

**Author:** u/Competitive_Travel16 | **Upvotes:** 190 | **Comments:** 138 | **Date:** 2025-12-18

**Summary:** Jake, formerly of Linus Tech Tips (LTT), demonstrated Exo's RDMA-over-Thunderbolt on four Mac Studios. The post, which is a link with no text content, sparked discussions about PR timing and Jake's departure from LTT.

**Key Points:**
- Jake demonstrated Exo's RDMA-over-Thunderbolt on four Mac Studios.
- The post is a link with no text content.
- Discussion includes comments about PR timing and Jake's departure from LTT.
- A user expressed a wish for llama.cpp to adapt RDMA, mentioning affordable Mellanox ConnectX-3 cards.

**Discussion Highlights:** The discussion highlights include speculation about PR timing due to a similar video posted by Jeff Geerling, curiosity about Jake's departure from LTT, and a desire for llama.cpp to support RDMA, with mentions of affordable hardware options.

---

## 21. [192GB VRAM 8x 3090s + 512GB DDR4 RAM AMA](https://reddit.com/r/LocalLLaMA/comments/1pq2uvi/192gb_vram_8x_3090s_512gb_ddr4_ram_ama/)

**Author:** u/Sero_x | **Upvotes:** 138 | **Comments:** 159 | **Date:** 2025-12-18

**Summary:** A user built a high-end system with 8x 3090 GPUs and 512GB RAM, concluding they need even more VRAM for their workloads. The community discussed VRAM limitations and potential solutions like partial offload.

**Key Points:**
- User started with 4x 3090s, expanded to 8x 3090s, and still feels VRAM is insufficient
- Community members shared similar experiences with VRAM constraints
- Suggestions included partial offload as an alternative to adding more VRAM
- Cost and scalability of VRAM were discussed as challenges

**Discussion Highlights:** The discussion highlighted a consensus on VRAM being a bottleneck for large-scale workloads, with some users suggesting partial offload as a cost-effective solution. The community also acknowledged the high cost of expanding VRAM further.

---

## 22. [Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster](https://reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/)

**Author:** u/geerlingguy | **Upvotes:** 534 | **Comments:** 142 | **Date:** 2025-12-18

**Summary:** The post discusses performance testing of Kimi K2 on a cluster of 4x Mac Studios, highlighting the use of RDMA Tensor settings and the challenges in benchmarking. The author, u/geerlingguy, mentions ongoing testing and the lack of straightforward benchmarking tools like llama-bench in Exo.

**Key Points:**
- Performance testing of Kimi K2 on a 4x Mac Studio cluster with RDMA Tensor settings.
- Challenges in benchmarking due to the lack of tools like llama-bench in Exo.
- Ongoing testing and debugging of RDMA support.
- Mention of upcoming Apple Silicon ultra chips with MATMUL instructions for potential performance improvements.
- Positive community feedback and appreciation for the testing efforts.

**Discussion Highlights:** The discussion highlights the community's interest in the performance improvements and the anticipation of new Apple Silicon ultra chips. There is also appreciation for the author's efforts in testing and sharing the results.

---

## 23. [Exo 1.0 is finally out](https://reddit.com/r/LocalLLaMA/comments/1pq2rx7/exo_10_is_finally_out/)

**Author:** u/No_Conversation9561 | **Upvotes:** 148 | **Comments:** 46 | **Date:** 2025-12-18

**Summary:** Exo 1.0 has been released and is available for download. The live demo showed promising performance, but there are questions about its cost-effectiveness compared to equivalent GPU setups.

**Key Points:**
- Exo 1.0 is now available for download from exolabs.net
- Live demo confirmed good performance (25 tok/s)
- Cost concerns raised: setup costs $20k, equivalent to a high-end GPU
- Performance with large context (100k) is a point of interest
- GitHub repository is available for further exploration

**Discussion Highlights:** The discussion highlights a consensus on the promising performance of Exo 1.0 but raises questions about its cost-effectiveness compared to GPUs. There is also interest in its performance with larger context sizes.

---

## 24. [T5Gemma 2: The next generation of encoder-decoder models](https://reddit.com/r/LocalLLaMA/comments/1ppzhtq/t5gemma_2_the_next_generation_of_encoderdecoder/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 217 | **Comments:** 33 | **Date:** 2025-12-18

**Summary:** T5Gemma 2 is a new generation of encoder-decoder models based on Gemma 3, offering multilingual and multimodal capabilities with open weights for three sizes (270M, 1B, and 4B). These models feature tied embeddings, merged attention, and support for up to 140 languages, making them versatile for various tasks.

**Key Points:**
- T5Gemma 2 models are multilingual and multimodal, handling text and image inputs.
- They feature tied embeddings and merged attention mechanisms for efficiency.
- The models support up to 140 languages and have a context window of 128K tokens.
- Community interest includes requests for GGUF format and anticipation for larger models like Gemma 4.

**Discussion Highlights:** The community shows excitement about the return of encoder-decoder models and their potential for multimodal translation tasks. There is also anticipation for larger models and requests for additional formats like GGUF.

---

## 25. [Google's Gemma models family](https://reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/)

**Author:** u/jacek2023 | **Upvotes:** 489 | **Comments:** 119 | **Date:** 2025-12-18

**Summary:** The Reddit post discusses Google's Gemma models family, focusing on FunctionGemma, a model intended for fine-tuning in function-calling tasks. The community shows enthusiasm and anticipation for new developments.

**Key Points:**
- FunctionGemma is designed for fine-tuning in function-calling tasks
- Hints at new Gemma models being released
- Community shows strong enthusiasm for Google's developments

**Discussion Highlights:** The discussion highlights the community's positive reception of FunctionGemma and anticipation for new Gemma models, with some users joking about the rapid realization of community predictions.

---

## 26. [MiraTTS: High quality and fast TTS model](https://reddit.com/r/LocalLLaMA/comments/1pper90/miratts_high_quality_and_fast_tts_model/)

**Author:** u/SplitNice1982 | **Upvotes:** 137 | **Comments:** 60 | **Date:** 2025-12-17

**Summary:** MiraTTS is a high-quality, fast TTS model that generates realistic 48khz speech at 100x realtime, optimized for efficiency and low latency. It supports multilingual versions and is memory-efficient, working with GPUs as low as 6GB VRAM.

**Key Points:**
- Generates speech at 100x realtime with high quality and clarity
- Memory efficient, works with 6GB VRAM GPUs
- Low latency, as low as 150ms
- Supports multilingual versions and is in progress for multispeaker support
- Optimized using Lmdeploy and FlashSR for audio enhancement

**Discussion Highlights:** The discussion highlights include inquiries about multilingual support, voice cloning, and comparisons with other TTS models like KaniTTS. Users also expressed appreciation for the work and shared their experiences with the model.

---

## 27. [AMA with the Meta researchers behind SAM 3 + SAM 3D + SAM Audio](https://reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/)

**Author:** u/AIatMeta | **Upvotes:** 142 | **Comments:** 77 | **Date:** 2025-12-17

**Summary:** The post announces an AMA with Meta researchers behind SAM 3, SAM 3D, and SAM Audio, highlighting their capabilities and providing links to learn more. The discussion includes questions about voice separation, model architecture, and specific use cases like stem creation and Apple Silicon support.

**Key Points:**
- Introduction of SAM 3, SAM 3D, and SAM Audio by Meta researchers
- AMA session to discuss capabilities and applications of these models
- Discussion on voice separation, model architecture, and specific use cases
- Questions about stem creation and Apple Silicon support
- Links provided for further learning and a playground for testing the models

**Discussion Highlights:** The discussion highlights user interest in practical applications like voice separation, model limitations, and technical support for specific hardware. Users also inquired about the architecture similarities and capabilities for tasks like stem creation.

---

## 28. [Nvidia plans heavy cuts to GPU supply in early 2026](https://reddit.com/r/LocalLLaMA/comments/1pp8vo4/nvidia_plans_heavy_cuts_to_gpu_supply_in_early/)

**Author:** u/HumanDrone8721 | **Upvotes:** 350 | **Comments:** 175 | **Date:** 2025-12-17

**Summary:** Nvidia plans to significantly reduce GPU supply in early 2026, which could impact gaming PC builds and market competition. The move is seen as part of a broader trend of supply cuts in the tech industry.

**Key Points:**
- Nvidia's GPU supply cuts in early 2026
- Potential impact on gaming PC builds
- Broader industry trend of supply reductions
- Concerns about market competition and innovation
- Criticism of corporate focus on stock buybacks over growth

**Discussion Highlights:** The discussion highlights concerns about the impact on gaming PC builds, with users noting similar cuts by Micron and Samsung. There is a consensus that this could lead to increased competition and criticism of corporate priorities.

---

## 29. [Hey, LocalLLaMa. We need to talk...](https://reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/)

**Author:** u/Eisenstein | **Upvotes:** 415 | **Comments:** 135 | **Date:** 2025-12-17

**Summary:** The post highlights the importance of community engagement and support for contributors in r/LocalLLaMA, urging users to provide feedback and upvotes to encourage continued sharing of projects.

**Key Points:**
- The author calls for more engagement with smaller posts and constructive feedback.
- The post emphasizes the need for upvotes and recognition to sustain open-source contributions.
- Top comments reveal mixed reactions, with some users agreeing and others criticizing low-quality projects.
- The discussion underscores the tension between encouraging contributions and maintaining quality standards.

**Discussion Highlights:** The discussion reflects a divide in the community, with some users supporting the call for engagement and others expressing frustration with the quality of certain projects, particularly those perceived as AI-generated or overly ambitious.

---

## 30. [Nemotron was post-trained to assume humans have reasoning, but they never use it](https://reddit.com/r/LocalLLaMA/comments/1pp2rtn/nemotron_was_posttrained_to_assume_humans_have/)

**Author:** u/RetiredApostle | **Upvotes:** 167 | **Comments:** 20 | **Date:** 2025-12-17

**Summary:** The Reddit post discusses Nemotron's post-training assumption that humans have reasoning capabilities, though they may not use them. The discussion includes interpretations of this assumption and technical details about data processing and schema requirements.

**Key Points:**
- Nemotron was post-trained to assume humans have reasoning capabilities.
- The assumption may be a placeholder or requirement for data processing.
- The Arrow format and Hugging Face datasets require shared schemas, influencing the reasoning_content property.
- The assumption might be related to Python type safety in data processing.
- Some comments humorously reference 'userlm-thinking' as a potential leak.

**Discussion Highlights:** The discussion highlights technical aspects of data processing and schema requirements, with some users interpreting the assumption as a placeholder or a result of technical constraints. There is no clear consensus, but the technical explanations are well-received.

---

## 31. [Drummer's Cydonia and Magidonia 24B v4.3 - The best pair of Cydonia for RP yet!](https://reddit.com/r/LocalLLaMA/comments/1pp2j60/drummers_cydonia_and_magidonia_24b_v43_the_best/)

**Author:** u/TheLocalDrummer | **Upvotes:** 138 | **Comments:** 20 | **Date:** 2025-12-17

**Summary:** The post announces the release of Drummer's Cydonia and Magidonia 24B v4.3 models, which are praised as the best pair for role-playing yet. The author expresses gratitude to patrons for their support and shares links to the models on Hugging Face. Key points include the release of the models, their high praise for role-playing, the author's gratitude to patrons, links to the models, and community feedback highlighting the excellence of Magidonia 4.3. The discussion highlights show community appreciation and positive feedback on the models, with some users mentioning specific use cases and additional resources.

---

## 32. [Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds.](https://reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/)

**Author:** u/themixtergames | **Upvotes:** 1180 | **Comments:** 135 | **Date:** 2025-12-17

**Summary:** Apple has introduced SHARP, a model capable of generating photorealistic 3D Gaussian representations from a single image in seconds. The model is showcased with examples rendered in real-time on Apple Vision Pro and generated on a MacBook Pro M1 Max.

**Key Points:**
- SHARP generates 3D Gaussian representations from a single image in seconds.
- Examples were rendered in real-time on Apple Vision Pro.
- Scenes were generated in 5–10 seconds on a MacBook Pro M1 Max.
- The model is CUDA GPU-dependent for rendering trajectories.
- Community interest includes potential applications and performance on different content types.

**Discussion Highlights:** The community showed significant interest in the model's capabilities, with discussions ranging from its performance on different types of content to comparisons with fictional technologies like Cyberpunk's braindance. There was also a notable comment about the model's dependency on CUDA GPU for rendering trajectories.

---

## 33. [LangChain and LlamaIndex are in "steep decline" according to new ecosystem report. Anyone else quietly ditching agent frameworks?](https://reddit.com/r/LocalLLaMA/comments/1pox733/langchain_and_llamaindex_are_in_steep_decline/)

**Author:** u/Exact-Literature-395 | **Upvotes:** 206 | **Comments:** 58 | **Date:** 2025-12-17

**Summary:** The Reddit post discusses the decline of LangChain and LlamaIndex frameworks, citing reduced community activity and investment. Users share experiences of simplifying their codebases by removing these frameworks and calling APIs directly, questioning the necessity of such tools with improved base models.

**Key Points:**
- LangChain and LlamaIndex are listed as 'steepest declining' projects by community activity.
- Users report simplifying codebases and improving debugging by removing these frameworks.
- Criticism of LangChain includes bloated features, poor security/performance, and non-pythonic design.
- LlamaIndex maintainer acknowledges the shift but highlights the frameworks' initial ease of integration.
- Discussion suggests a potential shift away from agent frameworks as base models improve.

**Discussion Highlights:** The discussion highlights a consensus that LangChain and similar frameworks may be becoming obsolete as base models improve. Users express frustration with the complexity and lack of performance in these frameworks, preferring direct API calls. However, there is acknowledgment of the frameworks' initial utility in easing integration.

---

## 34. [anthropic blog on code execution for agents. 98.7% token reduction sounds promising for local setups](https://reddit.com/r/LocalLLaMA/comments/1powhy6/anthropic_blog_on_code_execution_for_agents_987/)

**Author:** u/Zestyclose_Ring1123 | **Upvotes:** 136 | **Comments:** 33 | **Date:** 2025-12-17

**Summary:** Anthropic's blog discusses a code execution approach for agents that significantly reduces token usage, making it promising for local setups. The method involves model-generated code to orchestrate tools on demand, enhancing privacy and efficiency.

**Key Points:**
- Massive token reduction (e.g., 150k to 2k tokens) through code execution instead of direct tool calls.
- Privacy benefits as sensitive data flows directly between tools without entering model context.
- Sandboxing is a main challenge for running model-generated code locally.
- Similar patterns exist in other projects like smolagents and Cloudflare's 'code mode'.
- Alternative approaches include generating a DAG of steps to reduce sandboxing needs.

**Discussion Highlights:** The discussion highlights existing implementations like smolagents and alternative methods such as DAG generation to mitigate sandboxing issues. Some users express skepticism about Anthropic's originality, while others share their own experiments with similar patterns.

---

## 35. [Peak LLM Wars: Xiaomi Blocks Kimi Employees on Twitter](https://reddit.com/r/LocalLLaMA/comments/1pow797/peak_llm_wars_xiaomi_blocks_kimi_employees_on/)

**Author:** u/nekofneko | **Upvotes:** 135 | **Comments:** 30 | **Date:** 2025-12-17

**Summary:** The Reddit post discusses a conflict between Xiaomi and Kimi employees on Twitter, highlighting the ongoing 'LLM wars' with included images and comments on the wild nature of such conflicts.

**Key Points:**
- The post includes images and a comment about the wild nature of LLM wars.
- Top comments mention a meme format, potential involvement of former DeepSeek members in Xiaomi, and comparisons to other tech industry conflicts.
- The discussion also draws parallels to other social media dramas.

**Discussion Highlights:** The comments focus on the dynamics of the conflict, potential insider information, and broader industry comparisons.

---

## 36. [Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model](https://reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 1171 | **Comments:** 127 | **Date:** 2025-12-17

**Summary:** Microsoft's TRELLIS 2-4B is an open-source image-to-3D model with 4 billion parameters, converting single images into 3D assets. The model uses Flow-Matching Transformers with Sparse Voxel based 3D VAE. Community feedback is mixed, with some users praising its quality while others find it lacking in practical applications.

**Key Points:**
- Model Type: Flow-Matching Transformers with Sparse Voxel based 3D VAE
- Parameters: 4 Billion
- Input: Single Image, Output: 3D Asset
- Mixed community feedback on practical usability
- Some users report good results with sample images

**Discussion Highlights:** The community discussion highlights mixed reactions, with some users finding the model excellent for sample images, while others criticize its practical usability. There is also a suggestion to improve the model by allowing a series of images as input.

---

## 37. [QwenLong-L1.5: Revolutionizing Long-Context AI](https://reddit.com/r/LocalLLaMA/comments/1pokpha/qwenlongl15_revolutionizing_longcontext_ai/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 214 | **Comments:** 28 | **Date:** 2025-12-16

**Summary:** QwenLong-L1.5 is a new AI model that achieves state-of-the-art long-context reasoning with novel data synthesis, stabilized RL, and memory management for contexts up to 4M tokens. It is available on HuggingFace and has garnered significant attention for its capabilities.

**Key Points:**
- Achieves SOTA long-context reasoning with up to 4M tokens
- Uses novel data synthesis and stabilized RL
- Available on HuggingFace
- Integration into llama.cpp may require additional work
- Specific query template is recommended for optimal use

**Discussion Highlights:** The discussion highlights the model's significant capabilities and potential challenges in integration. Users appreciate the model's performance but note the need for specific query templates and potential improvements in visualization.

---

## 38. [8x Radeon 7900 XTX Build for Longer Context Local Inference - Performance Results &amp; Build Details](https://reddit.com/r/LocalLLaMA/comments/1pogwb6/8x_radeon_7900_xtx_build_for_longer_context_local/)

**Author:** u/Beautiful_Trust_8151 | **Upvotes:** 736 | **Comments:** 218 | **Date:** 2025-12-16

**Summary:** The post details a custom multi-GPU setup using 8x AMD Radeon 7900 XTX cards for local AI inference, highlighting performance metrics and build specifics. The author shares their experience with the system's stability and performance, noting its advantages for long-context inference despite not being the cheapest or most plug-and-play solution.

**Key Points:**
- The system uses 8x AMD Radeon 7900 XTX cards with a total of 192 GB VRAM, paired with an Intel Core i7-14700F and 192 GB system RAM.
- Performance tests show 437 tokens per second for prompt processing and 27 tokens per second for generation with an empty context, dropping to 200 and 16 tokens per second respectively at 19k tokens.
- The build cost is around $6-7k, offering a budget-friendly alternative to professional-grade GPUs like the RTX Pro 6000.
- The setup is praised for its upgradability, customizability, and long-context capability.
- The community appreciates the build as a notable example of early AI era hardware experimentation.

**Discussion Highlights:** The discussion highlights the impressive nature of the build, comparing it to historical technological milestones. Users appreciate the cost-effectiveness and performance of the setup, with some suggesting further tests with other models.

---

## 39. [Nemotron 3 Nano 30B is Amazing! (TLDR)](https://reddit.com/r/LocalLLaMA/comments/1pocsdy/nemotron_3_nano_30b_is_amazing_tldr/)

**Author:** u/DonkeyBonked | **Upvotes:** 206 | **Comments:** 148 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses the user's experience with Nemotron 3 Nano 30B, highlighting its token efficiency and performance on their hardware setup. The user compares it favorably to other models like Devstral 2 Small 24B and Qwen models, noting its ability to handle large context sizes efficiently.

**Key Points:**
- Nemotron 3 Nano 30B shows impressive token efficiency, fitting 256k tokens in VRAM and handling up to 1M context with spillover.
- The model performs well on the user's hardware setup, which includes an RTX 5000 and an RTX 3090 eGPU.
- Comparisons with other models like Devstral 2 Small 24B and Qwen models show Nemotron 3 Nano 30B's superior performance in certain tasks.
- Users in the comments discuss the model's speed, performance, and open-source nature, with some preferring Qwen models for specific use cases.
- The model's ability to generate functional code and follow instructions is highlighted in the discussion.

**Discussion Highlights:** The discussion highlights the model's speed and efficiency, with users noting its performance in coding tasks and its open-source nature. Some users prefer Qwen models for specific use cases, indicating a consensus that while Nemotron 3 Nano 30B is impressive, other models may excel in certain areas.

---

## 40. [32GB Mi50's were getting so expensive that I ended up buying a 32GB w6800 for about the same price instead](https://reddit.com/r/LocalLLaMA/comments/1pob44f/32gb_mi50s_were_getting_so_expensive_that_i_ended/)

**Author:** u/EmPips | **Upvotes:** 236 | **Comments:** 42 | **Date:** 2025-12-16

**Summary:** The author opted for a 32GB w6800 GPU instead of a 32GB Mi50 due to similar pricing and better convenience. The decision was based on a pros/cons analysis, highlighting the w6800's ease of installation and cooling efficiency.

**Key Points:**
- Author chose 32GB w6800 over 32GB Mi50 due to similar pricing
- Pros of w6800 include convenience and effective cooling
- Alternative suggestions include AMD Radeon AI PRO R9700 and Zotac 3090
- Price comparison and value were key discussion points
- Community feedback provided additional options and considerations

**Discussion Highlights:** The discussion highlighted the author's cost-effective choice and convenience factors of the w6800. Alternatives like the AMD Radeon AI PRO R9700 and Zotac 3090 were suggested, with community members emphasizing price-to-performance ratios and software support.

---

## 41. [8 Million Users' AI Conversations Sold for Profit by "Privacy" Extensions | Koi Blog](https://reddit.com/r/LocalLLaMA/comments/1poal2a/8_million_users_ai_conversations_sold_for_profit/)

**Author:** u/ManThigh | **Upvotes:** 157 | **Comments:** 47 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses privacy concerns regarding browser extensions selling AI conversation data of millions of users for profit, highlighting the importance of using local models and auditing extensions.

**Key Points:**
- Browser extensions like Urban VPN Proxy and 1ClickVPN Proxy sold user AI conversation data.
- Over 6 million users were affected by these extensions.
- The community emphasizes the importance of local AI setups and auditing extensions.
- There is a strong sentiment against companies buying and selling user data.

**Discussion Highlights:** The discussion highlights a consensus on the need for privacy measures, such as using local AI models and being cautious with browser extensions. Users express strong disapproval of companies profiting from user data.

---

## 42. [Finally managed to run Qwen-2.5-7B on a 4GB GTX 1050 without CPU offloading (Surgical Memory Alignment)](https://reddit.com/r/LocalLLaMA/comments/1po97ad/finally_managed_to_run_qwen257b_on_a_4gb_gtx_1050/)

**Author:** u/HuseyinKama | **Upvotes:** 150 | **Comments:** 49 | **Date:** 2025-12-16

**Summary:** The post discusses a method called 'Surgical Memory Alignment' to run Qwen-2.5-7B on a 4GB GTX 1050 without CPU offloading, saving VRAM and improving speed. The author open-sourced the solution as QKV Core.

**Key Points:**
- Standard GGUF quantization tools add padding that wastes memory, causing OOM errors on low-end GPUs.
- Surgical Alignment trims and realigns memory blocks to save VRAM and improve performance.
- The method saved 44MB of VRAM and improved I/O load times by ~34%.
- The solution is open-sourced as QKV Core for others with low-end GPUs.
- Discussion includes praise for the work and skepticism about the code quality.

**Discussion Highlights:** The discussion includes praise for the author's expertise and the potential benefits of the solution, as well as skepticism about the code quality and the actual gains achieved.

---

## 43. [I was bored](https://reddit.com/r/LocalLLaMA/comments/1po8yt0/i_was_bored/)

**Author:** u/MyLovelyAngelKirino | **Upvotes:** 134 | **Comments:** 74 | **Date:** 2025-12-16

**Summary:** The author, who is unemployed, built a high-performance computer setup with excess hardware, including 3x 3090 GPUs, 512GB RAM, and an Epyc 7663 56-core processor. The post sparked humorous and envious reactions from the community.

**Key Points:**
- Author built a powerful computer setup due to unemployment and excess hardware
- Hardware includes 3x 3090 GPUs, 512GB RAM, and an Epyc 7663 56-core processor
- Community reactions include humor, envy, and requests for more details
- Discussion highlights the neatness of the setup and curiosity about water-cooling components

**Discussion Highlights:** The community reacted with a mix of humor and envy, praising the setup's neatness and expressing curiosity about specific hardware details like water-cooling components. Some users joked about the author's ability to acquire such hardware while unemployed.

---

## 44. [Meta announced a new SAM Audio Model for audio editing that can segment sound from complex audio mixtures using text, visual, and time span prompts.](https://reddit.com/r/LocalLLaMA/comments/1po7i0c/meta_announced_a_new_sam_audio_model_for_audio/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 519 | **Comments:** 86 | **Date:** 2025-12-16

**Summary:** Meta announced a new SAM Audio Model that can segment sound from complex audio mixtures using text, visual, and time span prompts, transforming audio processing.

**Key Points:**
- SAM Audio Model can isolate any sound from complex audio mixtures using text, visual, and time span prompts.
- Potential applications include isolating and subtracting unwanted noises in Microsoft Teams meetings.
- The model's ability to pick specific sounds from complex audio is highly praised.
- Model sizes and specifications are available in the provided image link.
- Questions about its effectiveness on music instruments were raised.

**Discussion Highlights:** The discussion highlights the potential applications of the SAM Audio Model, such as noise isolation in meetings, and praises its advanced capabilities. There is also interest in its effectiveness on music instruments.

---

## 45. [Allen Institute for AI introduces Molmo 2](https://reddit.com/r/LocalLLaMA/comments/1po78bl/allen_institute_for_ai_introduces_molmo_2/)

**Author:** u/Agitated_Camel1886 | **Upvotes:** 245 | **Comments:** 22 | **Date:** 2025-12-16

**Summary:** Allen Institute for AI has introduced Molmo 2, an 8B model capable of advanced video analysis tasks like Video QA, counting, pointing, and dense captioning. The community is impressed by its capabilities and the public availability of datasets.

**Key Points:**
- Molmo 2 is an 8B model from Allen Institute for AI.
- It excels in video analysis tasks such as Video QA, counting, pointing, and dense captioning.
- The model and datasets are publicly available on HuggingFace.
- An AMA session was held to discuss Olmo 3 and Molmo 2.
- The community appreciates the public release of datasets for advancements in the field.

**Discussion Highlights:** The community is highly impressed with Molmo 2's capabilities, especially given its size. There is enthusiasm about the public availability of datasets, which fosters further advancements. Some users expressed excitement and curiosity about the model's performance and requirements.

---

## 46. [XiaomiMiMo/MiMo-V2-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1po3bn4/xiaomimimomimov2flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 245 | **Comments:** 59 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses MiMo-V2-Flash, a Mixture-of-Experts (MoE) language model with 309B total parameters and 15B active parameters, designed for high-speed reasoning and agentic workflows. The model's performance on the SWE-Bench is notably strong, surpassing larger models like Sonnet 4.5 and Gemini 3 on multilingual tasks. Users are discussing its capabilities, potential larger versions, and hardware requirements for running it.

**Key Points:**
- MiMo-V2-Flash is a MoE language model with 309B total parameters and 15B active parameters.
- It excels in high-speed reasoning and agentic workflows.
- The model's SWE-Bench performance is impressively strong for its size, outperforming larger models.
- Users are inquiring about larger versions and hardware requirements for running the model.
- The weights for the model have been released, making it accessible for further exploration.

**Discussion Highlights:** The discussion highlights the model's strong performance on the SWE-Bench, with users expressing surprise at its capabilities relative to its size. There is interest in potential larger versions and the feasibility of running the model on specific hardware configurations. The release of the model's weights is also noted as a positive development.

---

## 47. [GLM-4.5V, GLM-4.6V and GLM_4.6V-Flash are now supported by llama.cpp (GGUFs)](https://reddit.com/r/LocalLLaMA/comments/1po18y9/glm45v_glm46v_and_glm_46vflash_are_now_supported/)

**Author:** u/jacek2023 | **Upvotes:** 170 | **Comments:** 34 | **Date:** 2025-12-16

**Summary:** The post announces that GLM-4.5V, GLM-4.6V, and GLM_4.6V-Flash are now supported by llama.cpp with GGUFs, which is seen as a significant update by the community.

**Key Points:**
- Support for GLM-4.5V, GLM-4.6V, and GLM_4.6V-Flash has been added to llama.cpp.
- The update is considered a valuable Christmas gift by the community.
- There are questions about whether the GGUFs support vision capabilities.
- Some users have faced challenges setting up the new models.
- Comparisons between Qwen3-VL-4B and GLM_4.6V are being discussed.

**Discussion Highlights:** The community is excited about the new support for GLM models in llama.cpp, with some users expressing gratitude and others discussing technical challenges and comparisons with other models like Qwen3-VL-4B.

---

## 48. [Qwen3 Next speed optimization has been merged into llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1pnz9xu/qwen3_next_speed_optimization_has_been_merged/)

**Author:** u/jacek2023 | **Upvotes:** 216 | **Comments:** 25 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses the recent speed optimization for Qwen3 Next in llama.cpp, highlighting significant performance improvements across different hardware configurations.

**Key Points:**
- Speed optimization for Qwen3 Next has been merged into llama.cpp
- Performance on M1 64GB improved from 12 t/s to 18 t/s
- Qwen3-30B achieves around 58 t/s on the same hardware
- Win11 + RTX5090 + vulkan setup achieves 37.x t/s without CUDA
- Over 100 t/s possible with UD-Q2_K_XL without CPU offloading

**Discussion Highlights:** Users report significant performance gains, with specific metrics provided for different hardware setups. The consensus is that the optimization is substantial and beneficial for various configurations.

---

## 49. [I may have over-quantized this little guy.](https://reddit.com/r/LocalLLaMA/comments/1pnz80z/i_may_have_overquantized_this_little_guy/)

**Author:** u/AllergicToTeeth | **Upvotes:** 141 | **Comments:** 35 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses the quantization of an AI model, with comments highlighting technical aspects like system prompts and quantization levels, along with humorous references to AI advancements.

**Key Points:**
- Quantization of a model is the main topic
- System prompts are important for model behavior
- Quantization level Q0 is mentioned
- Humorous references to GPT versions are made

**Discussion Highlights:** The community engages in technical discussion about model quantization and performance, with some light-hearted comments about AI advancements.

---

## 50. [Alibaba Open-Sources CosyVoice 3, a New TTS Model](https://reddit.com/r/LocalLLaMA/comments/1pnusp9/alibaba_opensources_cosyvoice_3_a_new_tts_model/)

**Author:** u/nekofneko | **Upvotes:** 217 | **Comments:** 32 | **Date:** 2025-12-16

**Summary:** Alibaba has open-sourced CosyVoice 3, a new TTS model with advanced features like multi-lingual support, high naturalness, and low latency. The model supports various languages, dialects, and instructions, making it suitable for production use.

**Key Points:**
- Supports 9 common languages and 18+ Chinese dialects/accents
- Achieves state-of-the-art performance in content consistency and naturalness
- Features bi-streaming with latency as low as 150ms
- Supports pronunciation inpainting and text normalization
- Community discussion compares it favorably to other models like Chatterbox and Microsoft VibeVoice

**Discussion Highlights:** The community is excited about CosyVoice 3, with discussions focusing on comparisons to other TTS models like Chatterbox and Microsoft VibeVoice. Users are particularly interested in its voice cloning capabilities and performance improvements.

---

