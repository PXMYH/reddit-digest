# r/LocalLLaMA Reading Digest

**Period:** 2025-12-11 to 2025-12-14
**Posts Summarized:** 10
**Total Posts Analyzed:** 16

---

## 1. [Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/)

**Author:** u/Nunki08 | **Upvotes:** 1259 | **Comments:** 148 | **Date:** 2025-12-12

**Summary:** An NVIDIA employee accidentally uploaded the parent folder of their upcoming model on Hugging Face, sparking a discussion about the potential leak of sensitive information.

**Key Points:**
- Accidental upload of NVIDIA's upcoming model files on Hugging Face
- Concerns about potential takedown of the leaked content
- Mentions of specific model details like 'Nano' and '30B-A3B'
- Positive feedback on the Nemotron lineup
- Urgent calls to save the content before censorship

**Discussion Highlights:** The discussion highlights a mix of excitement about the potential leak and concerns about censorship, with users urging others to save the content before it gets taken down.

---

## 2. [Training an LLM only on 1800s London texts - 90GB dataset](https://reddit.com/r/LocalLLaMA/comments/1pkpsee/training_an_llm_only_on_1800s_london_texts_90gb/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 649 | **Comments:** 70 | **Date:** 2025-12-12

**Summary:** The post discusses the TimeCapsuleLLM project, which involves training an LLM on a 90GB dataset of 1800-1875 London texts. The author has conducted a bias report and trained a small evaluation model to assess the dataset before scaling up.

**Key Points:**
- The dataset consists of 90GB with 135,000 documents from the 1800-1875 period.
- A bias report covering temporal, gender/pronoun, and geographic bias has been generated.
- A small evaluation model (300M parameters) was trained on a 15GB subset to evaluate the dataset.
- The example output shows the model's response to a prompt about Charles Dickens.
- The project aims to study the biases inherent in historical texts.

**Discussion Highlights:** The discussion highlights appreciation for the project's thoroughness and methodology. Key points include the omission of post-1800 reprints, suggestions for using Mixture of Experts (MoE) for better compute efficiency, and general enthusiasm for the project's progress and potential impact.

---

## 3. [The new monster-server](https://reddit.com/r/LocalLLaMA/comments/1pl0ojb/the_new_monsterserver/)

**Author:** u/eribob | **Upvotes:** 565 | **Comments:** 114 | **Date:** 2025-12-12

**Summary:** The Reddit post details a user's upgraded 'monster-server' built for running local LLMs, featuring a Ryzen 3950x CPU, 128GB RAM, and three GPUs (2x RTX 3090 and 1x RTX 4090). The server is used for research, coding, and as an alternative to search engines, with notable mentions of its storage and network capabilities.

**Key Points:**
- The server uses a Ryzen 3950x CPU and 128GB RAM, with three GPUs (2x RTX 3090 and 1x RTX 4090).
- The RTX 4090 is powered via an M.2 to Oculink to PCIe adapter with a second PSU.
- The user has 10GB fiber internet for $50/month and uses the server for running LLMs like GPT-OSS-120B.
- Discussion highlights include nostalgia for early 2000s overclocking forums and questions about the user's location and PSU setup.
- Some users noted potential inefficiencies in the 3-GPU setup compared to 2 or 4 GPUs due to parallel processing limitations.

**Discussion Highlights:** The discussion includes nostalgia for early 2000s tech forums, curiosity about the user's location and internet setup, and technical feedback on the GPU configuration's efficiency.

---

## 4. [OpenAI's flagship model, ChatGPT-5.2 Thinking, ranks  most censored AI on Sansa benchmark.](https://reddit.com/r/LocalLLaMA/comments/1plnuqu/openais_flagship_model_chatgpt52_thinking_ranks/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 551 | **Comments:** 99 | **Date:** 2025-12-13

**Summary:** The Reddit post discusses OpenAI's ChatGPT-5.2 Thinking model being ranked as the most censored AI on the Sansa benchmark, with users expressing concerns about its performance and censorship levels compared to previous models.

**Key Points:**
- ChatGPT-5.2 is ranked as the most censored AI on the Sansa benchmark.
- Users report performance issues, particularly with follow-up questions and research tasks.
- The model denies more clinical note evaluations compared to previous versions.
- Comparisons with other models like Grok and Gemini highlight differences in censorship levels.
- User experiences suggest a decline in functionality compared to ChatGPT-5.1.

**Discussion Highlights:** The discussion highlights user dissatisfaction with ChatGPT-5.2's performance and increased censorship, with comparisons to other models indicating varying levels of censorship and functionality.

---

## 5. [8x RTX Pro 6000 server complete](https://reddit.com/r/LocalLLaMA/comments/1plwgun/8x_rtx_pro_6000_server_complete/)

**Author:** u/koushd | **Upvotes:** 522 | **Comments:** 239 | **Date:** 2025-12-13

**Summary:** The author details their journey upgrading a GPU server, culminating in a setup with 8x RTX Pro 6000 GPUs, a Threadripper PRO 9955WX CPU, and 384 GB RAM, totaling 768 GB VRAM. They faced challenges with heat, power, and hardware compatibility, eventually resolving issues with a larger case and server platform.

**Key Points:**
- The server features 8x RTX Pro 6000 GPUs (4 Workstation, 4 Max-Q), a Threadripper PRO 9955WX CPU, and 384 GB RAM, providing 768 GB VRAM.
- The author faced issues with overheating, hardware compatibility, and power consumption during upgrades.
- Solutions included upgrading to a larger case, using a server platform, and managing power distribution.
- The post received significant engagement, with comments highlighting the impressive setup and some concerns about the hardware setup.

**Discussion Highlights:** The discussion includes praise for the impressive setup, concerns about the hardware configuration, and anecdotes about power supply issues. The top comment humorously compares the setup to a 'Porsche in a trailer park,' while others share experiences with similar hardware challenges.

---

## 6. [New in llama.cpp: Live Model Switching](https://reddit.com/r/LocalLLaMA/comments/1pk0ubn/new_in_llamacpp_live_model_switching/)

**Author:** u/paf1138 | **Upvotes:** 457 | **Comments:** 83 | **Date:** 2025-12-11

**Summary:** The Reddit post announces a new feature in llama.cpp called 'Live Model Switching,' which has gained significant attention with 457 upvotes and 83 comments. The community is excited about the progress and the potential to replace other tools like ollama.

**Key Points:**
- Introduction of 'Live Model Switching' feature in llama.cpp
- High engagement with 457 upvotes and 83 comments
- Community appreciation for recent UX improvements
- Potential to replace other tools like ollama
- Special recognition for the contributor with a flair

**Discussion Highlights:** The community is highly engaged and positive about the new feature, with comments highlighting the progress in UX improvements and the potential to switch from other tools. There is also recognition for the contributor's efforts.

---

## 7. [This is how open ai is advertising them selfs on redditâ€¦. They are doomed](https://reddit.com/r/LocalLLaMA/comments/1plcrg8/this_is_how_open_ai_is_advertising_them_selfs_on/)

**Author:** u/ThinkExtension2328 | **Upvotes:** 230 | **Comments:** 77 | **Date:** 2025-12-12

**Summary:** The Reddit post criticizes OpenAI's advertising strategy, highlighting a shift from promoting advanced AI to using astrology ads, which is seen as a decline in their approach.

**Key Points:**
- OpenAI's advertising strategy is criticized for shifting from advanced AI to astrology ads.
- The post suggests that OpenAI's focus on normies rather than programmers is a misstep.
- Comments highlight the irony of OpenAI's shift from warning about open models to using astrology ads.
- There is a consensus that OpenAI's new advertising approach is less effective and potentially damaging to their reputation.

**Discussion Highlights:** The discussion highlights a consensus that OpenAI's shift in advertising strategy is seen as a decline in their approach, with many users criticizing the move towards astrology ads and suggesting it is less effective than targeting programmers.

---

## 8. [Agentic Local AI on CPU = Mistral Vibe + Granite-4-h-1b](https://reddit.com/r/LocalLLaMA/comments/1pkdkjo/agentic_local_ai_on_cpu_mistral_vibe_granite4h1b/)

**Author:** u/PotentialFunny7143 | **Upvotes:** 228 | **Comments:** 40 | **Date:** 2025-12-11

**Summary:** The post discusses running local AI models on CPU, highlighting Mistral Vibe and Granite-4-h-1b as viable options. Users are interested in performance metrics, hardware requirements, and comparisons with other models.

**Key Points:**
- Mistral Vibe and Granite-4-h-1b are mentioned as effective local AI models for CPU.
- Users are curious about performance comparisons with other models like Cline.
- Hardware stats, tokens per second, and RAM/CPU consumption are key discussion points.
- Questions about the upper boundary capabilities of these models are raised.
- Comparisons with open-source alternatives are also discussed.

**Discussion Highlights:** The discussion focuses on practical aspects of running local AI models on CPU, with users sharing interest in performance metrics, hardware requirements, and model capabilities. There is a consensus on the importance of understanding the trade-offs between different models and hardware configurations.

---

## 9. [Olmo 3.1 32B Think &amp; Instruct: New Additions to the Olmo Model Family](https://reddit.com/r/LocalLLaMA/comments/1pky5u4/olmo_31_32b_think_instruct_new_additions_to_the/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 176 | **Comments:** 22 | **Date:** 2025-12-12

**Summary:** The post introduces Olmo 3.1 32B Think and Instruct models, two new 32-billion-parameter models optimized for deep reasoning and instruction following, respectively. The Think model excels in multi-step reasoning and code generation, while the Instruct model focuses on conversational fluency and tool-use capabilities.

**Key Points:**
- Olmo 3.1 32B Think and Instruct models are the newest additions to the Olmo family.
- The Think model is optimized for deep reasoning, math, logic, and code generation.
- The Instruct model is optimized for instruction following, conversational fluency, and tool-use capabilities.
- The models are fully open-source and have received positive feedback from the community.
- The community anticipates further developments, such as Mixture of Experts (MOE) models.

**Discussion Highlights:** The community is enthusiastic about the new models, praising their open-source nature and improvements. There is anticipation for future developments like MOE models, and some users highlighted the educational value of the accompanying paper.

---

## 10. [Mistral 3 Large is DeepSeek V3!?](https://reddit.com/r/LocalLLaMA/comments/1plpc6h/mistral_3_large_is_deepseek_v3/)

**Author:** u/seraschka | **Upvotes:** 161 | **Comments:** 32 | **Date:** 2025-12-13

**Summary:** The post discusses the architectural similarities between Mistral 3 Large and DeepSeek V3, noting that they share nearly identical sizes and architectures, with minor differences in expert configurations. The community highlights the open-source spirit and the adoption of DeepSeek V3's architecture by multiple models.

**Key Points:**
- Mistral 3 Large and DeepSeek V3 have almost identical sizes (671B vs 673B).
- Mistral 3 Large uses the same architecture as DeepSeek V3 but with adjusted expert configurations.
- The community notes that other models like Kimi K2 and Gigachat also use the DeepSeek V3 architecture.
- Mistral likely trained their model from scratch due to using their own tokenizer.
- The discussion highlights the benefits of open-source collaboration and innovation.

**Discussion Highlights:** The community generally appreciates the open-source spirit, with some noting that adopting proven architectures like DeepSeek V3 is practical. There is consensus that while Mistral 3 Large uses a similar architecture, it brings its own innovations, such as multimodal capabilities.

---

