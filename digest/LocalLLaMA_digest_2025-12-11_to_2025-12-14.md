# r/LocalLLaMA Reading Digest

**Period:** 2025-12-11 to 2025-12-14
**Posts Summarized:** 6
**Total Posts Analyzed:** 16

---

## 1. [Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/)

**Author:** u/Nunki08 | **Upvotes:** 1246 | **Comments:** 149 | **Date:** 2025-12-12

**Summary:** An NVIDIA employee accidentally uploaded the parent folder of their upcoming model on Hugging Face, sparking a discussion about the potential leak of sensitive information and the urgency to save the data before it gets taken down.

**Key Points:**
- NVIDIA's upcoming model files were accidentally uploaded on Hugging Face.
- The community is concerned about the potential removal of the leaked data.
- There is interest in the Nemotron lineup and other promising projects mentioned.
- Users are encouraged to save the data before it gets censored.

**Discussion Highlights:** The discussion highlights a sense of urgency to preserve the leaked data, with users expressing interest in the Nemotron lineup and other projects. There is a consensus on the importance of saving the information before it gets taken down.

---

## 2. [Training an LLM only on 1800s London texts - 90GB dataset](https://reddit.com/r/LocalLLaMA/comments/1pkpsee/training_an_llm_only_on_1800s_london_texts_90gb/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 623 | **Comments:** 68 | **Date:** 2025-12-12

**Summary:** The post discusses an open-source project called TimeCapsuleLLM, which involves training LLMs using only 1800-1875 London texts. The author has compiled a 90GB dataset with 135,000 documents and conducted a bias report and evaluation using a smaller subset.

**Key Points:**
- The project focuses on training LLMs on historical texts from 1800-1875 London.
- A 90GB dataset with 135,000 documents has been compiled for training.
- A bias report and evaluation model have been created to assess the dataset.
- The community appreciates the detailed and methodical approach to the project.
- Discussions include suggestions for using Mixture of Experts (MoE) for better efficiency.

**Discussion Highlights:** The community shows strong support for the project, with discussions focusing on the methodology, dataset composition, and suggestions for improving training efficiency. There is a consensus on the importance of such historical-focused LLM projects.

---

## 3. [The new monster-server](https://reddit.com/r/LocalLLaMA/comments/1pl0ojb/the_new_monsterserver/)

**Author:** u/eribob | **Upvotes:** 541 | **Comments:** 113 | **Date:** 2025-12-12

**Summary:** The Reddit post details a user's upgraded 'monster-server' setup, featuring a Ryzen 3950x CPU, 128GB RAM, and multiple GPUs including RTX 3090s and an RTX 4090. The server is used for running local LLMs like GPT-OSS-120B and other tasks. The post has garnered significant attention with 541 upvotes and 113 comments.

**Key Points:**
- The server is built on a Ryzen 3950x CPU and X570 Taichi motherboard, with 128GB RAM and multiple GPUs including RTX 3090s and an RTX 4090.
- The user runs local LLMs like GPT-OSS-120B, achieving over 100 tokens per second.
- The setup includes 10GB fiber internet, enterprise-grade storage, and creative use of M2 to PCIe adapters.
- Community reactions include nostalgia for early 2000s overclocking forums and discussions on GPU setup efficiency.
- Some comments highlight potential heat management issues and inquire about the second PSU setup.

**Discussion Highlights:** The discussion includes nostalgic comments about early 2000s overclocking forums, questions about the user's location for affordable 10GB internet, and technical discussions on GPU setup efficiency and heat management. The community appreciates the detailed setup and shares both admiration and concerns about the hardware configuration.

---

## 4. [OpenAI's flagship model, ChatGPT-5.2 Thinking, ranks  most censored AI on Sansa benchmark.](https://reddit.com/r/LocalLLaMA/comments/1plnuqu/openais_flagship_model_chatgpt52_thinking_ranks/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 467 | **Comments:** 83 | **Date:** 2025-12-13

**Summary:** The Reddit post discusses OpenAI's ChatGPT-5.2 model being ranked as the most censored AI on the Sansa benchmark, with users reporting issues in follow-up questions, research capabilities, and processing clinical notes.

**Key Points:**
- ChatGPT-5.2 is ranked as the most censored AI on the Sansa benchmark.
- Users report issues with follow-up questions and research capabilities compared to previous versions.
- Difficulties in processing made-up clinical notes for evaluation.
- Questions about the testing criteria for the Sansa benchmark.
- Observations about Gemini being less censored than other models.

**Discussion Highlights:** Users express concerns about the model's performance and censorship levels, with comparisons to previous versions and other AI models like Gemini.

---

## 5. [New in llama.cpp: Live Model Switching](https://reddit.com/r/LocalLLaMA/comments/1pk0ubn/new_in_llamacpp_live_model_switching/)

**Author:** u/paf1138 | **Upvotes:** 457 | **Comments:** 83 | **Date:** 2025-12-11

**Summary:** The Reddit post announces a new feature in llama.cpp called 'Live Model Switching'. The community response is positive, with comments highlighting its popularity and improvements in user experience.

**Key Points:**
- New feature: Live Model Switching in llama.cpp
- Post is popular with 457 upvotes and 83 comments
- Community appreciates UX improvements
- Users prefer llama.cpp over alternatives like ollama

**Discussion Highlights:** The discussion is generally positive, with users expressing excitement about the new feature and its potential to replace other tools like ollama. The community also acknowledges recent UX improvements in llama.cpp.

---

## 6. [Mistral’s Vibe CLI now supports a 200K token context window (previously 100K)](https://reddit.com/r/LocalLLaMA/comments/1pjw7rj/mistrals_vibe_cli_now_supports_a_200k_token/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 437 | **Comments:** 36 | **Date:** 2025-12-11

**Summary:** Mistral’s Vibe CLI has doubled its context window from 100K to 200K tokens, a change achieved with a simple configuration update. The community has reacted positively, with discussions highlighting both the technical simplicity and the practical implications of larger context windows.

**Key Points:**
- Context window increased from 100K to 200K tokens
- Change implemented via a single-line config update
- Community engagement includes Discord feature and special flair for the contributor
- Discussions note potential struggles with context beyond 100K tokens
- Difference between supporting and effectively using large context windows highlighted

**Discussion Highlights:** The community praised the simplicity of the update (a single config line change) and discussed the practical challenges of utilizing large context windows effectively. Some users noted that while the feature is impressive, actual usefulness may vary depending on the use case.

---

