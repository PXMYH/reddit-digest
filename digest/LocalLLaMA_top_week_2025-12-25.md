# r/LocalLLaMA Reading Digest

**Period:** 2025-12-25 to 2025-12-25
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [GLM 4.7 has now taken #2 on Website Arena](https://reddit.com/r/LocalLLaMA/comments/1pv8dbb/glm_47_has_now_taken_2_on_website_arena/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 211 | **Comments:** 64 | **Date:** 2025-12-25

**Summary:** GLM 4.7 has risen to #2 on Website Arena, ranking just behind Gemini 3 Pro Preview and surpassing other models like Claude 4.5 Opus. It is noted for its strong performance in text generation, particularly in role-play scenarios.

**Key Points:**
- GLM 4.7 is #1 among open weight models and ranks #2 overall on Website Arena.
- It has made a significant jump from its previous ranking (GLM 4.6).
- Users discuss its performance compared to models like Claude 4.5 Opus and GPT 5.2.
- Some users express skepticism about the rankings, while others confirm its effectiveness in real-world usage.
- The model is praised for its role-play capabilities.

**Discussion Highlights:** The discussion highlights a mix of skepticism and praise for GLM 4.7. Some users question its ranking above models like Claude 4.5 Opus, while others confirm its strong performance in practical use cases, especially in text generation and role-play scenarios. The consensus suggests that GLM 4.7 is a competitive model, though opinions on its superiority vary.

---

## 2. [Thoughts ?](https://reddit.com/r/LocalLLaMA/comments/1pv5shc/thoughts/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 152 | **Comments:** 21 | **Date:** 2025-12-24

**Summary:** The Reddit post discusses the progress of local LLMs (Large Language Models) catching up to closed-source models, with users sharing their experiences and opinions on the matter. Key points include the competitiveness of top local LLMs, surprise at their rapid advancement, comparability to closed-source models in specific use cases, and cost-effectiveness. The discussion highlights a consensus that local LLMs are making significant strides and are becoming viable alternatives, though closed-source models still excel in more complex tasks.

---

## 3. [FYI GLM 4.7 is way more censored than 4.6.](https://reddit.com/r/LocalLLaMA/comments/1pv2wwm/fyi_glm_47_is_way_more_censored_than_46/)

**Author:** u/bigman11 | **Upvotes:** 134 | **Comments:** 47 | **Date:** 2025-12-24

**Summary:** The Reddit post discusses the increased censorship in GLM 4.7 compared to 4.6, noting that 4.6 was better for adult writing. Users share mixed experiences, with some noting creative writing quality issues in 4.7.

**Key Points:**
- GLM 4.7 is more censored than 4.6
- 4.6 was better for adult writing
- Some users report creative writing quality issues in 4.7
- Local versions may not have the same censorship issues
- Mixed user experiences with censorship and performance

**Discussion Highlights:** Users generally agree that GLM 4.7 has increased censorship and some creative writing quality issues compared to 4.6. However, experiences vary, with some users not noticing significant censorship issues. The consensus suggests that 4.6 may be preferable for certain use cases like creative writing.

---

## 4. [All of the major open weight labs have shifted to large params general models instead of smaller, more focused models. By this time next year, there won’t be much “local” about this sub unless the paradigm shifts to smaller models good at specific domains.](https://reddit.com/r/LocalLLaMA/comments/1pv2cnz/all_of_the_major_open_weight_labs_have_shifted_to/)

**Author:** u/LocoMod | **Upvotes:** 189 | **Comments:** 214 | **Date:** 2025-12-24

**Summary:** The post discusses a shift in open weight labs towards larger, general models, making it difficult for local users to run them without significant hardware. The author advocates for a return to smaller, domain-specific models that can be run locally with limited resources.

**Key Points:**
- Open weight labs are shifting to larger models that require more resources, making local execution difficult.
- Users are resorting to lower quantization levels (Q3 and below), which impacts performance.
- The author suggests a focus on smaller, domain-specific models (e.g., coding, creative writing, math) that can fit within 16-32GB of VRAM.
- Recent releases like Mistral's 14B models and Qwen3's smaller models are noted as exceptions.
- The discussion highlights a tension between the goals of open weight models and local usability.

**Discussion Highlights:** The discussion reflects a consensus that while larger models are becoming the norm, there is still a demand for smaller, efficient models that can be run locally. Some users point to recent releases as examples of viable smaller models, while others express frustration at the reliance on well-funded labs.

---

## 5. [Exclusive: Nvidia buying AI chip startup Groq's assets for about $20 billion in largest deal on record](https://reddit.com/r/LocalLLaMA/comments/1puyq9r/exclusive_nvidia_buying_ai_chip_startup_groqs/)

**Author:** u/fallingdowndizzyvr | **Upvotes:** 612 | **Comments:** 136 | **Date:** 2025-12-24

**Summary:** Nvidia is acquiring AI chip startup Groq's assets for approximately $20 billion, marking the largest deal on record. The acquisition has sparked discussions about market competition and consolidation in the AI industry.

**Key Points:**
- Nvidia is buying Groq's assets for about $20 billion
- The deal is the largest on record
- Discussions highlight concerns about market consolidation
- Some commenters question Groq's valuation
- The acquisition is seen as an 'acquihire' to bypass regulatory hurdles

**Discussion Highlights:** The discussion reflects mixed sentiments, with some viewing the acquisition as beneficial for market competition, while others express concerns about further consolidation in the AI industry. There is also skepticism about Groq's valuation and the nature of the deal as an 'acquihire'.

---

## 6. [We asked OSS-120B and GLM 4.6 to play 1,408 Civilization V games from the Stone Age into the future. Here's what we found.](https://reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/)

**Author:** u/vox-deorum | **Upvotes:** 558 | **Comments:** 126 | **Date:** 2025-12-24

**Summary:** Researchers used open-source LLMs (GPT-OSS-120B and GLM-4.6) to play 1,408 full games of Civilization V, finding that LLMs can survive full games and develop distinct playstyles. The LLMs performed slightly better in best scores but worse in win rates compared to baseline AI. Key points include: LLMs can survive full Civilization V games with a hybrid approach; OSS-120B favored a warmonger playstyle, while GLM-4.6 was more balanced; Both models preferred the Order ideology over Freedom; Cost per game was approximately $0.86 for OSS-120B; The study involved 2,207 games in total, with 919 baseline games. The community expressed excitement about the potential for LLMs to play Civilization V, with comments highlighting interest in playing against local models and integrating LLMs into multiplayer games. Some users also inquired about the impact of model size on performance.

---

## 7. [Hmm all reference to open-sourcing has been removed for Minimax M2.1...](https://reddit.com/r/LocalLLaMA/comments/1pullo0/hmm_all_reference_to_opensourcing_has_been/)

**Author:** u/Responsible_Fig_1271 | **Upvotes:** 232 | **Comments:** 83 | **Date:** 2025-12-24

**Summary:** The Reddit post discusses the removal of open-sourcing references for Minimax M2.1, suggesting a potential shift to an API-only model, which has sparked community concern and speculation.

**Key Points:**
- Open-sourcing references for Minimax M2.1 have been removed
- Possible shift to API-only model
- Community concern and speculation about the change
- Mixed reactions with some users expressing disappointment and others remaining optimistic

**Discussion Highlights:** The discussion includes speculation about financial troubles, references to past goodwill from Minimax, and a statement from the head of research indicating that open-sourcing is still planned for Christmas.

---

## 8. [The current state of sparse-MoE's for agentic coding work (Opinion)](https://reddit.com/r/LocalLLaMA/comments/1puglt8/the_current_state_of_sparsemoes_for_agentic/)

**Author:** u/ForsookComparison | **Upvotes:** 256 | **Comments:** 78 | **Date:** 2025-12-24

**Summary:** The Reddit post discusses the current state of sparse-MoE's for agentic coding work, with a focus on model evaluations and comparisons. The discussion highlights varying opinions on model performance and specific use cases.

**Key Points:**
- Evaluation methods for sparse-MoE's are a topic of discussion.
- GPT-OSS-120B is noted for its performance but has limitations in long context tasks.
- Qwen3-Next 80B is mentioned as a potential superior model.
- K2 Thinking is highlighted for its capabilities in certain contexts.

**Discussion Highlights:** The discussion includes debates on model evaluations, with some users disagreeing on the effectiveness of certain models. There is a consensus that GPT-OSS-120B has limitations in long context tasks, while other models like Qwen3-Next 80B and K2 Thinking are noted for their strengths.

---

## 9. [New 1B parameter open-source coding model getting 76% on HumanEval [shameless but proud self-plug]](https://reddit.com/r/LocalLLaMA/comments/1puf614/new_1b_parameter_opensource_coding_model_getting/)

**Author:** u/More_Article9837 | **Upvotes:** 266 | **Comments:** 37 | **Date:** 2025-12-23

**Summary:** The post introduces Maincoder-1B, a 1B-parameter open-source coding model achieving 76% on HumanEval, designed for low-latency and low-cost inference. It is released under Apache 2.0 and is suitable for small, self-contained coding tasks.

**Key Points:**
- Maincoder-1B achieves 76% on HumanEval, a high score for its size.
- Designed for low-latency and low-cost inference, suitable for constrained hardware.
- Useful for interactive tools, local/offline coding, and batch refactors.
- Limited to a 2048 token context window and best for small tasks.
- Released under Apache 2.0 license.

**Discussion Highlights:** Users highlighted the model's potential for custom-built IDEs or NeoVim extensions, and its suitability for simple, quick coding tasks. There was also interest in the model's availability in GGUF format.

---

## 10. [Thoughts on DGX Spark as a macOS Companion: Two Months Later](https://reddit.com/r/LocalLLaMA/comments/1pu7pfi/thoughts_on_dgx_spark_as_a_macos_companion_two/)

**Author:** u/PropellerheadViJ | **Upvotes:** 143 | **Comments:** 51 | **Date:** 2025-12-23

**Summary:** The author shares their experience using the NVIDIA DGX Spark alongside their Mac for two months, highlighting its role as a CUDA companion for ML research on macOS. They discuss the device's limitations in memory bandwidth but emphasize its practicality for R&D and experiments. The post also touches on the challenges of dependency management outside x86 environments.

**Key Points:**
- DGX Spark serves as a CUDA companion for Mac users, addressing the lack of CUDA support on macOS.
- The device has lower memory bandwidth compared to alternatives like RTX 4090 and M4 Ultra, but is sufficient for R&D and experiments.
- Dependency management and software constraints are significant challenges when working outside x86 environments.
- Some users suggest renting CUDA-access systems as a cost-effective alternative to purchasing a DGX Spark.
- The Spark is seen as a development platform for those who cannot access cloud systems.

**Discussion Highlights:** The discussion highlights the practicality of the DGX Spark for specific use cases, such as R&D and experiments, while acknowledging its limitations in memory bandwidth. There is a consensus on the challenges of dependency management outside x86 environments, with some users suggesting cost-effective alternatives like renting CUDA-access systems.

---

## 11. [Uncensored Qwen3-Next-80B-Thinking (Chinese political censorship removed)](https://reddit.com/r/LocalLLaMA/comments/1pu5bob/uncensored_qwen3next80bthinking_chinese_political/)

**Author:** u/ikergarcia1996 | **Upvotes:** 141 | **Comments:** 43 | **Date:** 2025-12-23

**Summary:** Multiverse Computing has released an uncensored version of Qwen3-Next-80B-Thinking, removing Chinese political censorship while maintaining balanced and objective answers. The model uses steering vectors to disable refusals only for Chinese sensitive topics, ensuring robustness against jailbreaks and preserving performance on non-sensitive topics.

**Key Points:**
- The model removes Chinese political censorship while providing balanced, objective answers.
- It uses steering vectors to disable refusals only for Chinese sensitive topics, avoiding broad unsafe behavior.
- The model remains robust against jailbreaks and maintains performance on non-sensitive topics.
- No architecture changes or extra layers were added; it is a drop-in replacement for the original Qwen-Next model.
- The approach avoids hand-crafted data or new knowledge injection, relying on existing model knowledge.

**Discussion Highlights:** The discussion highlights mixed reactions, with some users appreciating the removal of censorship and others expressing concerns about the limited scope of uncensoring. Some users questioned the practical use of political questions, while others focused on the model's capabilities beyond censorship, such as coding or other tasks.

---

## 12. [Saw this on local marketplace, must be from a fellow r/LocalLLaMA here](https://reddit.com/r/LocalLLaMA/comments/1pu1uq6/saw_this_on_local_marketplace_must_be_from_a/)

**Author:** u/bobaburger | **Upvotes:** 182 | **Comments:** 59 | **Date:** 2025-12-23

**Summary:** A Reddit post in r/LocalLLaMA discusses a marketplace listing likely related to local AI hardware, with users speculating about the hardware inside and its value.

**Key Points:**
- Users speculate the device could be a 1B model on a Pi or a debranded Beelink SER5.
- Cost-effectiveness is questioned, with suggestions that upgrading a PC might be better.
- Humorous comparisons are made, like 'lawyer in a box' and references to Silicon Valley.
- The post is a link with no text content, sparking discussion in the comments.

**Discussion Highlights:** The discussion centers around hardware speculation, with a consensus that the device may not be worth the cost compared to upgrading a PC. Humorous comments add levity to the technical discussion.

---

## 13. [Qwen released Qwen-Image-Edit-2511 — a major upgrade over 2509](https://reddit.com/r/LocalLLaMA/comments/1pty4l1/qwen_released_qwenimageedit2511_a_major_upgrade/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 225 | **Comments:** 31 | **Date:** 2025-12-23

**Summary:** Qwen has released Qwen-Image-Edit-2511, a significant upgrade over its predecessor, featuring improved multi-person consistency, built-in LoRAs, enhanced industrial design generation, reduced image drift, and better geometric reasoning. The community has responded positively, with notable comments highlighting its timely release and practical applications.

**Key Points:**
- Stronger multi-person consistency for group photos and complex scenes
- Built-in popular community LoRAs requiring no extra tuning
- Enhanced industrial and product design generation capabilities
- Reduced image drift with improved character and identity consistency
- Improved geometric reasoning, including construction lines and structural edits

**Discussion Highlights:** The community has shown enthusiasm for the release, with comments noting its timely arrival around Christmas and the availability of additional tools like a lighting LoRA for faster inference. There is also curiosity about the hardware requirements for running the model.

---

## 14. [AMA With Z.AI, The Lab Behind GLM-4.7](https://reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/)

**Author:** u/zixuanlimit | **Upvotes:** 552 | **Comments:** 391 | **Date:** 2025-12-23

**Summary:** The post announces an AMA session with Z.AI, the research lab behind GLM-4.7, featuring several team members. The session ran from 8 AM to 11 AM PST, with follow-ups over the next 48 hours.

**Key Points:**
- AMA session with Z.AI team members
- Duration: 8 AM - 11 AM PST with 48-hour follow-up
- Top comments include questions about future releases, censorship concerns, training challenges, and creative writing instruction sets
- High engagement with 552 upvotes and 391 comments

**Discussion Highlights:** The discussion highlights include questions about future releases (e.g., 'when Air?'), concerns over potential censorship, inquiries about training challenges, and interest in creative writing instruction sets.

---

## 15. [How to run the GLM-4.7 model locally on your own device (guide)](https://reddit.com/r/LocalLLaMA/comments/1ptttcm/how_to_run_the_glm47_model_locally_on_your_own/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 164 | **Comments:** 45 | **Date:** 2025-12-23

**Summary:** The post discusses how to run the GLM-4.7 model locally, highlighting its improved performance and reduced size through quantization. It also mentions the model's achievements on various benchmarks.

**Key Points:**
- GLM-4.7 is Z.ai’s latest model with stronger coding, agent, and chat performance.
- It achieves SOTA performance on SWE-bench (73.8%), SWE-bench Multilingual (66.7%), and Terminal Bench 2.0 (41.0%).
- The full 355B parameter model requires 400GB of disk space, but the Unsloth Dynamic 2-bit GGUF reduces it to 134GB.
- Top comments question the trade-offs of quantization and the practicality of running the model locally.

**Discussion Highlights:** The discussion highlights concerns about the impact of quantization on model performance and the practical challenges of running the model locally, such as speed and resource requirements.

---

## 16. [Unsloth GLM-4.7 GGUF](https://reddit.com/r/LocalLLaMA/comments/1ptk5fs/unsloth_glm47_gguf/)

**Author:** u/Wooden-Deer-1276 | **Upvotes:** 212 | **Comments:** 39 | **Date:** 2025-12-22

**Summary:** The post announces the release of GLM-4.7 GGUF model by Unsloth, with various quantizations being uploaded. The community is actively discussing the model's capabilities and requirements.

**Key Points:**
- GLM-4.7 GGUF model released by Unsloth
- Multiple quantizations (e.g., Q8, Q4) are being uploaded, with some still in progress
- Community interest in model performance for tasks like coding
- Hardware requirements discussed (e.g., Q2 requires 131GB)
- Active community engagement with 212 upvotes and 39 comments

**Discussion Highlights:** The community shows strong interest in the model's performance, particularly for coding tasks. There is discussion about hardware requirements for different quantizations, with some users sharing their system specifications. The overall sentiment is positive, with appreciation for the rapid development and release.

---

## 17. [DGX Spark: an unpopular opinion](https://reddit.com/r/LocalLLaMA/comments/1ptdtmz/dgx_spark_an_unpopular_opinion/)

**Author:** u/emdblc | **Upvotes:** 710 | **Comments:** 214 | **Date:** 2025-12-22

**Summary:** The author, a doctoral student in data science, shares their positive experience with the DGX Spark, highlighting its benefits for small research groups with limited resources. Despite not being as fast as high-end GPUs like the H100, the Spark's all-in-one design and large memory capacity enable their group to compete in research.

**Key Points:**
- DGX Spark enables small research groups to prototype and train foundation models.
- It provides a significant amount of memory in an all-in-one design.
- The Spark is not faster than high-end GPUs like the H100 but is valuable for its accessibility and memory capacity.
- The author's use case aligns with the intended target demographic for the Spark.
- Community discussion acknowledges the Spark's utility for specific use cases despite its limitations.

**Discussion Highlights:** The discussion highlights a consensus that the DGX Spark is particularly useful for small research groups with limited resources, as intended by its design. While it may not match the performance of high-end GPUs, its accessibility and memory capacity are highly valued.

---

## 18. [GLM-4.7 GGUF is here!](https://reddit.com/r/LocalLLaMA/comments/1ptb4jj/glm47_gguf_is_here/)

**Author:** u/KvAk_AKPlaysYT | **Upvotes:** 181 | **Comments:** 23 | **Date:** 2025-12-22

**Summary:** The post announces the release of GLM-4.7 GGUF, a large model currently undergoing quantization. The author shares a Hugging Face link for the model and mentions the ongoing process.

**Key Points:**
- GLM-4.7 GGUF model has been released and is available on Hugging Face.
- The model is still being quantized, indicating it is a work in progress.
- Community members express interest in optimized versions (e.g., 'Air version') and humorously comment on hardware limitations.
- Some users request specific quantized versions like Q1 reap pruned.

**Discussion Highlights:** The discussion includes a mix of technical interest, requests for optimized model versions, and humorous comments about hardware constraints. The community appears engaged and eager for accessible versions of the model.

---

## 19. [GLM 4.7 released!](https://reddit.com/r/LocalLLaMA/comments/1pt5jfn/glm_47_released/)

**Author:** u/ResearchCrafty1804 | **Upvotes:** 319 | **Comments:** 90 | **Date:** 2025-12-22

**Summary:** GLM-4.7 has been released with significant improvements in coding, complex reasoning, and tool usage, setting new open-source SOTA standards. It also enhances performance in chat, creative writing, and role-play scenarios. Weights and technical details are available on Hugging Face and the Z.ai blog.

**Key Points:**
- GLM-4.7 surpasses GLM-4.6 with substantial improvements in coding, complex reasoning, and tool usage.
- It sets new open-source SOTA standards and boosts performance in chat, creative writing, and role-play scenarios.
- Users are eagerly awaiting the Unsloth UD_Q2_K_XL quant for testing.
- GLM-4.7 introduces features like Interleaved Thinking, Preserved Thinking, and Turn-level Thinking.
- Comparisons with other models like Gemini 3.0 and GPT 5.0 are discussed.

**Discussion Highlights:** The discussion highlights enthusiasm for the new release, with users praising its performance and features. Some users compare it favorably to other models like Gemini 3.0, while others note that it still falls short of models like GPT 5.0. The introduction of new thinking features and the availability of weights are particularly noted.

---

## 20. [GLM 4.7 is out on HF!](https://reddit.com/r/LocalLLaMA/comments/1pt5heq/glm_47_is_out_on_hf/)

**Author:** u/KvAk_AKPlaysYT | **Upvotes:** 585 | **Comments:** 124 | **Date:** 2025-12-22

**Summary:** The Reddit post announces the release of GLM 4.7 on Hugging Face, garnering significant attention with 585 upvotes and 124 comments. The community discusses its features and compares it to other models.

**Key Points:**
- GLM 4.7 is now available on Hugging Face
- The post received 585 upvotes and 124 comments
- Community highlights include comparisons to other models like Minimax and Gemma 4
- Discussion mentions improvements in speed and performance
- Special recognition given to the post author for their contribution

**Discussion Highlights:** The discussion is generally positive, with users appreciating the release and comparing it favorably to other models. Some users express excitement about the improvements in speed and performance, while others note the absence of certain expected features like Gemma 4.

---

## 21. [I made Soprano-80M: Stream ultra-realistic TTS in &lt;15ms, up to 2000x realtime, and &lt;1 GB VRAM, released under Apache 2.0!](https://reddit.com/r/LocalLLaMA/comments/1pt3sco/i_made_soprano80m_stream_ultrarealistic_tts_in/)

**Author:** u/eugenekwek | **Upvotes:** 616 | **Comments:** 99 | **Date:** 2025-12-22

**Summary:** Eugene introduced Soprano-80M, a state-of-the-art TTS model optimized for ultra-low latency and high-speed audio generation, achieving <15ms latency and up to 2000x realtime performance. The model uses a 32 kHz sample rate and a vocoder-based decoder for superior audio quality and speed. It can generate a 10-hour audiobook in under 20 seconds, making it ideal for voice chatbots and long-form speech applications.

**Key Points:**
- Soprano-80M achieves <15ms latency and up to 2000x realtime performance.
- Uses a 32 kHz sample rate for clearer audio and a vocoder-based decoder for faster generation.
- Can generate a 10-hour audiobook in under 20 seconds.
- Users report extremely fast performance with minimal GPU usage initially.
- Questions raised about hardware requirements and finetuning code availability.

**Discussion Highlights:** Users confirmed the model's speed and efficiency, with one noting minimal GPU usage followed by rapid generation. There was interest in the finetuning code and questions about the hardware used to achieve the reported performance metrics.

---

## 22. [GLM-4.7 Scores 42% on Humanities Last Exam?!](https://reddit.com/r/LocalLLaMA/comments/1pt27mo/glm47_scores_42_on_humanities_last_exam/)

**Author:** u/domlincog | **Upvotes:** 168 | **Comments:** 86 | **Date:** 2025-12-22

**Summary:** The Reddit post discusses GLM-4.7's performance, scoring 42% on the Humanities Last Exam (HLE), which is considered significant. The discussion highlights the model's pricing and its performance compared to other models like Sonnet 4.5. Key points include GLM-4.7's score on HLE, its pricing plan of $28.8 for a year, surpassing Sonnet 4.5 in the livebench benchmark, anticipation for its availability on Open Router, and a noted typo in the post title. The discussion highlights the significance of GLM-4.7's performance on the HLE and its competitive pricing, with users excited about its benchmark performance and eagerly awaiting its availability on Open Router.

---

## 23. [NVIDIA made a beginner's guide to fine-tuning LLMs with Unsloth!](https://reddit.com/r/LocalLLaMA/comments/1pt18x4/nvidia_made_a_beginners_guide_to_finetuning_llms/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 494 | **Comments:** 36 | **Date:** 2025-12-22

**Summary:** NVIDIA released a beginner's guide to fine-tuning LLMs using Unsloth, covering training methods, use-cases, and local training options on DGX Spark and RTX GPUs.

**Key Points:**
- Training methods: LoRA, FFT, RL
- Guidance on when and why to fine-tune LLMs
- Details on data and VRAM requirements
- Instructions for local training on DGX Spark and RTX GPUs
- Community appreciation for open-source models and tools

**Discussion Highlights:** The community expressed appreciation for NVIDIA's open-source contributions but also raised concerns about corporate responsibility. Some users inquired about AMD GPU compatibility, while others praised the collaboration and requested mirrors for accessibility.

---

## 24. [Jan-v2-VL-Max: A 30B multimodal model outperforming Gemini 2.5 Pro and DeepSeek R1 on execution-focused benchmarks](https://reddit.com/r/LocalLLaMA/comments/1psw818/janv2vlmax_a_30b_multimodal_model_outperforming/)

**Author:** u/Delicious_Focus3465 | **Upvotes:** 135 | **Comments:** 25 | **Date:** 2025-12-22

**Summary:** Jan-v2-VL-Max, a 30B multimodal model, outperforms Gemini 2.5 Pro and DeepSeek R1 on execution-focused benchmarks. It is built on Qwen3-VL-30B-A3B-Thinking and is available for testing on Jan's public interface.

**Key Points:**
- Jan-v2-VL-Max is a 30B multimodal model designed for long-horizon execution.
- It outperforms DeepSeek R1 and Gemini 2.5 Pro on the Illusion of Diminishing Returns benchmark.
- The model is available on Jan's public interface and can be run locally using vLLM.
- It is released under the Apache-2.0 license.
- The community has shown positive feedback and interest in the model.

**Discussion Highlights:** The community has expressed enthusiasm and appreciation for the release, with some users sharing benchmark results and others asking about the implementation details of the model.

---

## 25. [GLM 4.7 IS COMING!!!](https://reddit.com/r/LocalLLaMA/comments/1psuy8g/glm_47_is_coming/)

**Author:** u/External_Mood4719 | **Upvotes:** 185 | **Comments:** 49 | **Date:** 2025-12-22

**Summary:** Zhipu is releasing GLM-4.7, their latest model with enhanced coding capabilities and tool orchestration, now in Early Access Beta for long-term supporters. The beta aims to gather feedback on real-world development scenarios to improve the model's performance.

**Key Points:**
- GLM-4.7 features enhanced coding capabilities, long-range task planning, and tool orchestration optimized for Agentic Coding scenarios.
- Early Access Beta is open for long-term supporters to provide feedback on real-world development scenarios.
- The beta period runs from December 22, 2025, until the official release.
- Feedback channels include direct group feedback for API errors and a topic-based system for discussing unexpected results.
- The early access form is currently only available for Chinese users.

**Discussion Highlights:** The discussion includes a mix of excitement about the release, questions about availability and accessibility, and a focus on coding capabilities. Some users expressed interest in the model's coding features and potential future releases.

---

## 26. [MiniMax M2.1 is a straight up beast at UI/UX design. Just saw this demo...](https://reddit.com/r/LocalLLaMA/comments/1pstuyv/minimax_m21_is_a_straight_up_beast_at_uiux_design/)

**Author:** u/BlackRice_hmz | **Upvotes:** 135 | **Comments:** 37 | **Date:** 2025-12-22

**Summary:** The Reddit post highlights MiniMax M2.1's impressive UI/UX design capabilities, with users expressing excitement about its potential. The discussion includes requests for model weights, comparisons with other models like Gemini 3, and some skepticism about the authenticity of the hype.

**Key Points:**
- MiniMax M2.1 demonstrates strong UI/UX design skills.
- Users are eager to access the model weights for local use.
- Comparisons are made with Gemini 3, particularly in frontend design and quick information retrieval.
- Some users express fatigue with marketing materials and skepticism about the hype.
- Existing users of M2 express excitement for the M2.1 update.

**Discussion Highlights:** The discussion reveals a mix of enthusiasm and skepticism. Users are particularly interested in the model's design capabilities and practical applications, while others caution against overhyping the model based on marketing materials.

---

## 27. [major open-source releases this year](https://reddit.com/r/LocalLLaMA/comments/1pstlas/major_opensource_releases_this_year/)

**Author:** u/sahilypatel | **Upvotes:** 653 | **Comments:** 98 | **Date:** 2025-12-22

**Summary:** The Reddit post highlights major open-source releases this year, with comments discussing China's dominance in open-source, high expectations for DeepSeek's future performance, and Mistral's performance in small models.

**Key Points:**
- China is dominating the open-source space
- High expectations for DeepSeek's future performance
- Discussion on Mistral's performance in small models

**Discussion Highlights:** The discussion highlights China's strong presence in open-source development, anticipation for DeepSeek's upcoming releases, and a debate on Mistral's effectiveness in smaller model sizes.

---

## 28. [Got me a 32GB RTX 4080 Super](https://reddit.com/r/LocalLLaMA/comments/1pstaoo/got_me_a_32gb_rtx_4080_super/)

**Author:** u/Spooknik | **Upvotes:** 189 | **Comments:** 59 | **Date:** 2025-12-22

**Summary:** User purchased a modified RTX 4080 Super with 32GB VRAM for $1200, finding it cost-effective for AI tasks like Diffusion models. The card performed well with no issues after a month of use.

**Key Points:**
- Modified RTX 4080 Super with 32GB VRAM bought for $1200, half the price of an RTX 5090.
- Card works with stock Nvidia drivers and has good build quality.
- User finds it suitable for AI tasks like Diffusion models.
- Discussion highlights frustration with GPU memory segmentation and curiosity about VRAM setup.
- Some commenters note the price is at cost and question the source of the modified card.

**Discussion Highlights:** The discussion revolves around the cost-effectiveness of the modified GPU, frustration with artificial memory segmentation by manufacturers, and technical curiosity about the VRAM configuration. Some users express interest in the source of such modified cards.

---

## 29. [1 year later and people are still speedrunning NanoGPT. Last time this was posted the WR was 8.2 min. Its now 127.7 sec.](https://reddit.com/r/LocalLLaMA/comments/1psh1w2/1_year_later_and_people_are_still_speedrunning/)

**Author:** u/jd_3d | **Upvotes:** 219 | **Comments:** 23 | **Date:** 2025-12-21

**Summary:** The Reddit post discusses the significant progress in speedrunning NanoGPT training times, highlighting a reduction from the original 45 minutes to a new record of 127.7 seconds. The community is impressed by these improvements and seeks to understand the underlying techniques.

**Key Points:**
- NanoGPT training time has drastically reduced from 45 minutes to 127.7 seconds.
- The community is interested in learning about the specific improvements and techniques used.
- Users share their own experiences and achievements in training times.
- There is a discussion about the broader implications of these speed improvements in the field.

**Discussion Highlights:** The discussion highlights the rapid advancements in algorithmic speed improvements and the community's enthusiasm for understanding and replicating these results. Users share their own training experiences and express interest in learning more about the techniques used to achieve such significant speedups.

---

## 30. [llama.cpp appreciation post](https://reddit.com/r/LocalLLaMA/comments/1psbx2q/llamacpp_appreciation_post/)

**Author:** u/hackiv | **Upvotes:** 1607 | **Comments:** 152 | **Date:** 2025-12-21

**Summary:** The Reddit post appreciates llama.cpp for its performance and frequent updates, highlighting its superiority over other tools like Ollama in terms of speed and features.

**Key Points:**
- llama.cpp is praised for its frequent updates and numerous features
- Users report significant performance improvements, such as achieving 23t/s on specific hardware
- The community appreciates the open-source nature and contributions of llama.cpp
- Some users mention switching from Ollama to llama.cpp due to its advantages

**Discussion Highlights:** The discussion highlights the community's admiration for llama.cpp's performance and features, with users sharing their positive experiences and performance metrics.

---

## 31. [Dataset quality is not improving much](https://reddit.com/r/LocalLLaMA/comments/1ps6w96/dataset_quality_is_not_improving_much/)

**Author:** u/rekriux | **Upvotes:** 184 | **Comments:** 32 | **Date:** 2025-12-21

**Summary:** The Reddit post discusses the lack of significant improvements in dataset quality for AI models, highlighting a few notable datasets like Tulu, smoltakl, and Hermes 3. The author expresses concern over the stagnation in dataset innovation and mentions challenges in accessing some datasets, such as those from NVIDIA. Key points include the identification of top datasets, the perceived lack of breakthroughs, restricted access to some datasets, and the importance of high-quality datasets. The discussion emphasizes the importance of high-quality datasets and the challenges in their creation and accessibility, with a consensus that data synthesis is a critical but costly process often kept proprietary by companies.

---

## 32. [How big do we think Gemini 3 flash is](https://reddit.com/r/LocalLLaMA/comments/1pruoy7/how_big_do_we_think_gemini_3_flash_is/)

**Author:** u/davikrehalt | **Upvotes:** 130 | **Comments:** 111 | **Date:** 2025-12-20

**Summary:** The Reddit post discusses speculation about the size of Gemini 3 Flash, with users estimating it could be around 1.2T parameters or 600B+ with a small expert size. The discussion focuses on whether such a model could fit in memory on devices like a 128GB MacBook.

**Key Points:**
- Gemini 3 Flash is speculated to be a 1.2T parameter model or around 600B+ with small expert size.
- Users are curious about its potential to run on local hardware like 128GB MacBooks.
- There is uncertainty about whether Google will provide updated local models like Gemma.
- Some users express frustration at the lack of official information from Google.

**Discussion Highlights:** The discussion highlights a range of opinions on the size of Gemini 3 Flash, with estimates varying from 100B to 1.2T parameters. There is a consensus that the model is likely very large, but users are divided on its feasibility for local hardware. The lack of official information from Google is a recurring theme.

---

## 33. [Xiaomi’s MiMo-V2-Flash (309B model) jumping straight to the big leagues](https://reddit.com/r/LocalLLaMA/comments/1prjzoh/xiaomis_mimov2flash_309b_model_jumping_straight/)

**Author:** u/98Saman | **Upvotes:** 429 | **Comments:** 97 | **Date:** 2025-12-20

**Summary:** The Reddit post discusses Xiaomi's MiMo-V2-Flash (309B model), highlighting its impressive performance and efficiency compared to other models. The discussion includes comparisons with models like DS 3.2 and questions about the availability of open weights.

**Key Points:**
- MiMo-V2-Flash (309B model) is noted for its high performance and efficiency.
- Comparisons are made with other models like DS 3.2, suggesting MiMo-V2-Flash performs similarly with fewer parameters.
- Questions are raised about the availability of open weights and GGUF format.
- The Artificial Analysis Index is criticized for not accurately reflecting model performance.

**Discussion Highlights:** The discussion highlights the model's impressive benchmarks and efficiency, with some users questioning the reliability of certain performance indices and expressing interest in the availability of open weights.

---

## 34. [A Raspberry Pi + eGPU isn't as dumb as I thought](https://reddit.com/r/LocalLLaMA/comments/1prh5jp/a_raspberry_pi_egpu_isnt_as_dumb_as_i_thought/)

**Author:** u/geerlingguy | **Upvotes:** 137 | **Comments:** 22 | **Date:** 2025-12-20

**Summary:** The post discusses benchmarks comparing a Raspberry Pi CM5 with an eGPU to a high-end PC, showing minimal performance differences for larger models and even better performance for some Nvidia cards. The discussion highlights cost considerations and the feasibility of using a Raspberry Pi for AI tasks.

**Key Points:**
- Performance delta between Raspberry Pi CM5 with eGPU and a high-end PC is less than 5% for larger models.
- Raspberry Pi was faster for some Nvidia cards, but significantly slower for AMD cards, possibly due to driver issues.
- Benchmark data is publicly available on GitHub.
- Discussion focuses on cost-effectiveness and feasibility of using Raspberry Pi for AI tasks.
- Inquiries about multi-GPU setups and specific hardware configurations.

**Discussion Highlights:** The discussion highlights the cost-effectiveness and feasibility of using a Raspberry Pi with an eGPU for AI tasks, with users expressing interest in multi-GPU setups and specific hardware recommendations.

---

## 35. [Of course it works, in case you are wondering... and it's quite faster.](https://reddit.com/r/LocalLLaMA/comments/1prcu0t/of_course_it_works_in_case_you_are_wondering_and/)

**Author:** u/JLeonsarmiento | **Upvotes:** 238 | **Comments:** 59 | **Date:** 2025-12-20

**Summary:** The post highlights the efficiency of a model or tool, emphasizing its speed and functionality. The discussion revolves around comparisons with other models and the benefits of using specific agents.

**Key Points:**
- The post suggests a model or tool is faster and effective
- Comments mention Qwen and its agent as alternatives
- Discussion includes comparisons with other models like a dense 24B model
- The efficiency of a 3B MoE model is noted
- Competition in open-source models is highlighted

**Discussion Highlights:** The discussion focuses on the advantages of using specific agents like Qwen's, comparisons with other models, and the competitive landscape of open-source models.

---

## 36. [Open source LLM tooling is getting eaten by big tech](https://reddit.com/r/LocalLLaMA/comments/1pragtf/open_source_llm_tooling_is_getting_eaten_by_big/)

**Author:** u/Inevitable_Wear_9107 | **Upvotes:** 345 | **Comments:** 130 | **Date:** 2025-12-20

**Summary:** The Reddit post discusses the rapid evolution and consolidation of open-source LLM tooling by big tech companies, highlighting the shift from independent projects to ecosystem-driven tools. Key points include the rapid replacement of open-source projects by big tech solutions, the high turnover rate with a median project age of 30 months, and the integration of tools with proprietary hardware and services. The discussion highlights challenges faced by open-source projects in attracting resources and maintaining operations, acknowledging the role of big tech in driving innovation and capturing market share.

---

## 37. [Just pushed M2.1 through a 3D particle system. Insane！](https://reddit.com/r/LocalLLaMA/comments/1pr54as/just_pushed_m21_through_a_3d_particle_system/)

**Author:** u/srtng | **Upvotes:** 154 | **Comments:** 40 | **Date:** 2025-12-19

**Summary:** The Reddit post discusses the impressive performance of MiniMax M2.1 in an interactive 3D particle system, with the author expressing excitement about its capabilities and hinting at an upcoming release. The community shares positive feedback and comparisons to other models. Key points include: MiniMax M2.1 demonstrates strong performance in a 3D particle system, the model is compared favorably to other advanced models like Sonnet4.5, M2.1 is anticipated to be released soon, users report smooth performance even on lower-end hardware with appropriate quantization, and the community expresses enthusiasm and high regard for the M2 series. The discussion highlights the community's excitement about M2.1's performance and upcoming release, with users sharing positive experiences with the M2 series, noting its efficiency and capability even on less powerful hardware. There is a consensus that M2.1 is a significant advancement in local models.

---

## 38. [Key Highlights of NVIDIA’s New Open-Source Vision-to-Action Model: NitroGen](https://reddit.com/r/LocalLLaMA/comments/1pr48qm/key_highlights_of_nvidias_new_opensource/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 343 | **Comments:** 74 | **Date:** 2025-12-19

**Summary:** NVIDIA's NitroGen is an open-source vision-to-action model designed to play video games directly from raw frames using imitation learning. It works best with gamepad-controlled games and uses a vision transformer and diffusion matching transformer to generate actions.

**Key Points:**
- NitroGen is a unified vision-to-action model for playing video games from raw frames.
- It is trained through large-scale imitation learning on human gameplay videos.
- Effective for gamepad-controlled games but less so for mouse/keyboard games.
- Uses SigLip2 for processing RGB frames and a diffusion transformer for action generation.
- Potential applications include enabling solo play in couch-coop games.

**Discussion Highlights:** The discussion highlights both positive and negative aspects of NitroGen, with users noting potential uses like enabling solo play in couch-coop games, while also expressing concerns about increased bots in online games. There is also curiosity about the use of a diffusion transformer and its necessity.

---

## 39. [Japan's Rakuten is going to release a 700B open weight model in Spring 2026](https://reddit.com/r/LocalLLaMA/comments/1pr20el/japans_rakuten_is_going_to_release_a_700b_open/)

**Author:** u/Ok_Warning2146 | **Upvotes:** 268 | **Comments:** 45 | **Date:** 2025-12-19

**Summary:** Rakuten plans to release a 700B open weight model in Spring 2026, which could serve as an alternative to Chinese models and prompt US companies to release larger models.

**Key Points:**
- Rakuten's 700B model is expected to be released in Spring 2026.
- The model aims to be an alternative to Chinese models and encourage US companies to release larger models.
- Users are anticipating a quantized version to fit within 24GB VRAM.
- There is skepticism about the model's originality, with some suggesting it might be a fine-tune of Deepseek V3.
- The release timeline of 6 months is considered long in the rapidly evolving AI space.

**Discussion Highlights:** The discussion highlights anticipation for a quantized version of the model, skepticism about its originality, and comments on the lengthy release timeline.

---

## 40. [Devstral 2 (with Mistral's Vibe) vs Sonnet 4.5 (Claude Code) on SWE-bench: 37.6% vs 39.8% (within statistical error)](https://reddit.com/r/LocalLLaMA/comments/1pqy2bq/devstral_2_with_mistrals_vibe_vs_sonnet_45_claude/)

**Author:** u/Constant_Branch282 | **Upvotes:** 136 | **Comments:** 86 | **Date:** 2025-12-19

**Summary:** The post compares Devstral 2 (Mistral's Vibe) and Sonnet 4.5 (Claude Code) on the SWE-bench-verified-mini benchmark, showing that Devstral 2 performs comparably to Sonnet 4.5 within statistical error margins. Devstral 2 was also faster in execution time. The discussion highlights user experiences and opinions on the models' performance and usability.

**Key Points:**
- Devstral 2 and Sonnet 4.5 performed similarly on SWE-bench-verified-mini, with results within statistical error.
- Devstral 2 was faster, with a mean execution time of 296s compared to Claude's 357s.
- About 40% of test cases showed inconsistency across runs, indicating variability in outcomes.
- Users in the discussion praised Mistral's models for agentic coding and noted Devstral 2's availability for free on the API.
- Some users reported mixed experiences with Devstral 2, depending on the programming language used.

**Discussion Highlights:** The discussion highlights a general consensus that Mistral's models, including Devstral 2, are strong contenders in the coding domain. Users appreciated the performance and accessibility of Devstral 2, though some noted variability in performance depending on the use case. There was also a discussion about the implications of open-weight models matching proprietary models in benchmarks.

---

## 41. [FlashHead: Up to 50% faster token generation on top of other techniques like quantization](https://reddit.com/r/LocalLLaMA/comments/1pqui9l/flashhead_up_to_50_faster_token_generation_on_top/)

**Author:** u/Any_Frame9721 | **Upvotes:** 199 | **Comments:** 63 | **Date:** 2025-12-19

**Summary:** FlashHead is an architectural innovation for small language models (SLMs) that offers up to 50% faster token generation on top of techniques like quantization. It replaces the expensive language model head with a more efficient layer using information retrieval, maintaining perfect accuracy compared to baseline models.

**Key Points:**
- FlashHead provides significant speed improvements (up to 50%) in token generation for SLMs.
- It is a drop-in replacement for the language model head, compatible with quantization techniques.
- Benchmark results show substantial speedups, especially when combined with quantization (e.g., 3.73× speedup with W4A16).
- The technology is designed to be user-friendly with vLLM integration and is available via pip install.
- The discussion highlights interest in scalability to larger models, compatibility with MoE, and potential for llama.cpp support.

**Discussion Highlights:** The community shows strong interest in FlashHead's scalability to larger models, its compatibility with other architectures like MoE, and potential integration with tools like llama.cpp. There is also enthusiasm for European AI innovation and requests for more technical details.

---

## 42. [Career Advice in AI — Notes from an Andrew Ng Lecture](https://reddit.com/r/LocalLLaMA/comments/1pqpj29/career_advice_in_ai_notes_from_an_andrew_ng/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 349 | **Comments:** 55 | **Date:** 2025-12-19

**Summary:** Andrew Ng emphasizes that now is the best time to build a career in AI, highlighting the rapid progress in the field and the importance of staying updated with coding tools. He also stresses the value of product management skills, surrounding oneself with the right people, and focusing on building projects to gain practical experience.

**Key Points:**
- AI career opportunities are rapidly expanding with accelerating progress.
- Staying updated with the latest AI coding tools is crucial for productivity.
- Product management skills are becoming a bottleneck in AI development.
- Success is influenced by the people you surround yourself with.
- Practical experience through building projects is highly valuable.

**Discussion Highlights:** The discussion highlights a mix of agreement and skepticism. Some users emphasize the importance of staying current with tools and the value of social skills, while others express concerns about job security and the practical limitations of AI in real-world applications.

---

## 43. [Chinese researchers unveil "LightGen": An all-optical chip that outperforms Nvidia’s A100 by 100x](https://reddit.com/r/LocalLLaMA/comments/1pqoldt/chinese_researchers_unveil_lightgen_an_alloptical/)

**Author:** u/entsnack | **Upvotes:** 213 | **Comments:** 59 | **Date:** 2025-12-19

**Summary:** Chinese researchers from SJTU and Tsinghua have unveiled 'LightGen', an all-optical chip claimed to outperform Nvidia’s A100 by 100x. The announcement has sparked skepticism and discussions about its practical limitations and potential impact.

**Key Points:**
- Research from top-tier labs (SJTU and Tsinghua)
- Chip limited to linear math operations like matrix multiplications
- Skepticism about practicality and maturity of the technology
- Comparisons to overhyped tech announcements
- Community interest in competitive advancements in computing hardware

**Discussion Highlights:** The community is skeptical about the claims, citing limitations in nonlinear operations and the analog nature of the chip, while also expressing interest in technological competition.

---

## 44. [Qwen released Qwen-Image-Layered on Hugging face.](https://reddit.com/r/LocalLLaMA/comments/1pqoi6i/qwen_released_qwenimagelayered_on_hugging_face/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 635 | **Comments:** 70 | **Date:** 2025-12-19

**Summary:** Qwen has released Qwen-Image-Layered on Hugging Face, featuring advanced image layering capabilities with Photoshop-grade quality, physically isolated RGBA layers, and infinite decomposition for detailed editing.

**Key Points:**
- Photoshop-grade layering with true native editability
- Physically isolated RGBA layers
- Prompt-controlled structure for specifying layers
- Infinite decomposition for detailed editing
- Core model size is 40GB unquantized

**Discussion Highlights:** The community is excited about the release, with some expressing concerns about the RAM/VRAM requirements and the large model size. Overall, the release is seen as a significant advancement in image editing capabilities.

---

## 45. [GLM 4.7 is Coming?](https://reddit.com/r/LocalLLaMA/comments/1pqn0vq/glm_47_is_coming/)

**Author:** u/InternationalAsk1490 | **Upvotes:** 266 | **Comments:** 43 | **Date:** 2025-12-19

**Summary:** The Reddit post discusses the potential release of GLM 4.7, with users expressing anticipation and disappointment over the removal of GLM 4.6-air. The community hopes for a Christmas release.

**Key Points:**
- Potential release of GLM 4.7
- Disappointment over removal of GLM 4.6-air
- Community anticipation for a Christmas release
- Mixed sentiments about the timeline and availability

**Discussion Highlights:** Users are eagerly awaiting GLM 4.7, with some expressing disappointment over the removal of GLM 4.6-air. There is a hopeful sentiment for a Christmas release, indicating strong community interest and engagement.

---

## 46. [Realist meme of the year!](https://reddit.com/r/LocalLLaMA/comments/1pqegcr/realist_meme_of_the_year/)

**Author:** u/Slight_Tone_2188 | **Upvotes:** 2004 | **Comments:** 124 | **Date:** 2025-12-19

**Summary:** The Reddit post titled 'Realist meme of the year!' is a link post with no text content, sparking a discussion with 124 comments. The top comments humorously reference a cure for cancer, suggest downloading more RAM, and discuss corporate responsibility in hardware production.

**Key Points:**
- The post is a link with no text content
- Top comment humorously mentions a cure for cancer
- Another comment suggests downloading more RAM
- Discussion includes corporate responsibility in hardware production
- Post received 2004 upvotes and 124 comments

**Discussion Highlights:** The discussion highlights a mix of humor and serious commentary on technology constraints and societal expectations, with some users pointing fingers at hardware manufacturers for current limitations.

---

## 47. [Jake (formerly of LTT) demonstrate's Exo's RDMA-over-Thunderbolt on four Mac Studios](https://reddit.com/r/LocalLLaMA/comments/1pq5k6e/jake_formerly_of_ltt_demonstrates_exos/)

**Author:** u/Competitive_Travel16 | **Upvotes:** 190 | **Comments:** 138 | **Date:** 2025-12-18

**Summary:** Jake, formerly of Linus Tech Tips, demonstrated Exo's RDMA-over-Thunderbolt on four Mac Studios. The post, which is a link with no text content, sparked discussions about potential PR timing, Jake's departure from LTT, and the affordability of Mellanox ConnectX-3 cards for RDMA applications.

**Key Points:**
- Jake demonstrated Exo's RDMA-over-Thunderbolt on four Mac Studios
- The post is a link with no text content
- Discussion includes potential PR timing and Jake's departure from LTT
- Mellanox ConnectX-3 cards are affordable for RDMA applications

**Discussion Highlights:** The discussion highlights the affordability of Mellanox ConnectX-3 cards and their potential use in RDMA applications, with some users expressing interest in adapting RDMA for llama.cpp.

---

## 48. [192GB VRAM 8x 3090s + 512GB DDR4 RAM AMA](https://reddit.com/r/LocalLLaMA/comments/1pq2uvi/192gb_vram_8x_3090s_512gb_ddr4_ram_ama/)

**Author:** u/Sero_x | **Upvotes:** 136 | **Comments:** 161 | **Date:** 2025-12-18

**Summary:** A user built a high-end system with 8x 3090 GPUs and 512GB RAM, concluding they need even more VRAM. The community discussed the challenges and alternatives like partial offloading.

**Key Points:**
- User started with 4x 3090s, expanded to 8x 3090s, and still feels VRAM is insufficient
- Community members shared similar experiences with VRAM limitations
- Suggestions included partial offloading as an alternative to adding more VRAM
- Cost and scalability of such builds were discussed

**Discussion Highlights:** The discussion highlighted a consensus on VRAM limitations for large models like Llama 405B, with some suggesting partial offloading as a practical solution. The cost and technical challenges of scaling such systems were also noted.

---

## 49. [Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster](https://reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/)

**Author:** u/geerlingguy | **Upvotes:** 549 | **Comments:** 143 | **Date:** 2025-12-18

**Summary:** The post discusses performance testing of Kimi K2 on a cluster of 4 Mac Studios, highlighting the use of RDMA Tensor settings and the challenges in benchmarking due to lack of tools like llama-bench.

**Key Points:**
- Testing Kimi K2 on a 4x Mac Studio cluster with RDMA Tensor settings
- Challenges in benchmarking due to lack of tools like llama-bench
- Mention of upcoming Apple Silicon ultra chips with MATMUL instructions
- Positive community feedback and appreciation for the testing efforts
- Additional data and context provided in linked sources

**Discussion Highlights:** The discussion highlights the technical challenges and community interest in the performance testing. There is consensus on the potential improvements with upcoming Apple Silicon ultra chips and appreciation for the detailed testing efforts.

---

## 50. [Exo 1.0 is finally out](https://reddit.com/r/LocalLLaMA/comments/1pq2rx7/exo_10_is_finally_out/)

**Author:** u/No_Conversation9561 | **Upvotes:** 151 | **Comments:** 51 | **Date:** 2025-12-18

**Summary:** Exo 1.0 has been released and is available for download from exolabs.net. The release includes a live demo and a GitHub repository for further exploration.

**Key Points:**
- Exo 1.0 is now available for download
- Live demo was well-received with good TPS
- The setup costs around $20k, raising questions about its cost-effectiveness
- GitHub repository is available for further exploration
- Performance with large context sizes is a topic of discussion

**Discussion Highlights:** The discussion highlights include positive feedback on the live demo's performance, questions about the cost-effectiveness of the setup, and interest in the GitHub repository. There is also a focus on performance metrics, particularly with large context sizes.

---

