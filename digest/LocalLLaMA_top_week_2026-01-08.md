# r/LocalLLaMA Reading Digest

**Period:** 2026-01-08 to 2026-01-08
**Posts Summarized:** 36
**Total Posts Analyzed:** 36

---

## 1. [Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt](https://reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/)

**Author:** u/Prior-Arm-6705 | **Upvotes:** 238 | **Comments:** 42 | **Date:** 2026-01-08

**Summary:** A Reddit user created a compilation video of every instance Jensen Huang said 'AI' during the NVIDIA CES 2025 keynote, totaling 121 times. The process involved using open-source tools to download, parse, and edit the video locally. The top comments humorously suggested that the compilation could serve as a summary of the keynote. Some users joked about the frequency of 'AI' mentions and its impact on pricing, while others referenced similar repetitive presentations in tech history.

---

## 2. [16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/)

**Author:** u/ai-infos | **Upvotes:** 400 | **Comments:** 216 | **Date:** 2026-01-07

**Summary:** The post discusses running Deepseek v3.2 on 16 AMD MI50 32GB GPUs, achieving 10 tokens per second (output) and 2000 tokens per second (input) with a context length of 69,000. The setup aims for cost-effective hardware and highlights power draw and future plans for scaling.

**Key Points:**
- Deepseek v3.2 runs on 16 AMD MI50 GPUs with 10 t/s output and 2000 t/s input.
- Power draw is 550W idle and 2400W peak during inference.
- Future plans include testing 32 AMD MI50 GPUs for Kimi K2 Thinking.
- The setup is open-source and aims for cost-effective local AGI solutions.
- Discussion highlights include power efficiency and hardware scalability.

**Discussion Highlights:** The community appreciates the cost-effective approach and power efficiency, with some users expressing interest in noise levels and home setup feasibility.

---

## 3. [DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.](https://reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/)

**Author:** u/Nunki08 | **Upvotes:** 588 | **Comments:** 51 | **Date:** 2026-01-07

**Summary:** The Reddit post discusses the recent update to DeepSeek-R1's paper, which expanded from 22 pages to 86 pages, adding significant detail. The discussion includes speculation about new architectures and the potential impact of linear attention research. Key points include the paper's expansion, potential new architectures, and community interest in architectural improvements at different scales.

---

## 4. [Don't put off hardware purchases: GPUs, SSDs, and RAM are going to skyrocket in price soon](https://reddit.com/r/LocalLLaMA/comments/1q694ic/dont_put_off_hardware_purchases_gpus_ssds_and_ram/)

**Author:** u/Eisenstein | **Upvotes:** 233 | **Comments:** 210 | **Date:** 2026-01-07

**Summary:** The post warns about imminent price hikes for GPUs, SSDs, and RAM due to supply shortages and increased demand, with specific examples like NVIDIA's RTX 5090 potentially reaching $5,000. Users in the comments express frustration and reluctance to purchase at inflated prices.

**Key Points:**
- GPU prices are expected to rise significantly, with NVIDIA's RTX 5090 potentially reaching $5,000.
- NAND flash prices increased by 20% in November, leading to higher SSD costs.
- DRAM prices are projected to surge by 55-60% in Q1 2026, affecting both conventional and server memory.
- Consoles may face delays due to component shortages.
- Users are hesitant to buy hardware at inflated prices, with some planning to wait 3-4 years.

**Discussion Highlights:** The discussion reflects a consensus of frustration and reluctance among users to purchase hardware at the anticipated high prices. Many commenters express intentions to delay purchases or avoid buying altogether, citing current prices as already too high. Some users humorously plead with their existing hardware to last longer.

---

## 5. [NousResearch/NousCoder-14B · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1q61wpv/nousresearchnouscoder14b_hugging_face/)

**Author:** u/jacek2023 | **Upvotes:** 155 | **Comments:** 43 | **Date:** 2026-01-06

**Summary:** NousResearch introduced NousCoder-14B, a competitive programming model based on Qwen3-14B, achieving a 7.08% improvement in Pass@1 accuracy on LiveCodeBench v6. The model was trained on 24k coding problems using 48 B200s over four days.

**Key Points:**
- NousCoder-14B is a post-trained model based on Qwen3-14B.
- Achieved 67.87% Pass@1 accuracy, a 7.08% improvement over Qwen3-14B.
- Trained on 24k verifiable coding problems using 48 B200s over four days.
- Community reactions include engagement, skepticism about overfitting, and concerns about language support.
- Anticipation and mixed reactions from the community regarding the model's performance.

**Discussion Highlights:** The community showed engagement with the post, including skepticism about potential overfitting to the test suite and concerns about the model's language support. There was also anticipation and mixed reactions regarding the model's performance.

---

## 6. [Razer is demonstrating a “AI accelerator” box with a Wormhole n150 processor from Tenstorrent at CES](https://reddit.com/r/LocalLLaMA/comments/1q617ug/razer_is_demonstrating_a_ai_accelerator_box_with/)

**Author:** u/Hasuto | **Upvotes:** 116 | **Comments:** 38 | **Date:** 2026-01-06

**Summary:** Razer is showcasing an AI accelerator box featuring Tenstorrent's Wormhole n150 processor at CES. The hardware, which includes 12GB memory and is priced at $1000, is seen as a proof of concept by the community, with mixed reactions about its practicality.

**Key Points:**
- Razer's AI accelerator box uses Tenstorrent's Wormhole n150 processor.
- The hardware comes with 12GB memory and is priced at $1000.
- Community views the product as a proof of concept.
- Mixed reactions about the product's practicality and future viability.
- Tenstorrent's new Blackhole part is mentioned as an upcoming improvement.

**Discussion Highlights:** The community consensus is that the product is a proof of concept, with some users expressing skepticism about its practicality and long-term usefulness. There is also humor about the high cost relative to the specifications.

---

## 7. [Unsloth-MLX - Fine-tune LLMs on your Mac (same API as Unsloth)](https://reddit.com/r/LocalLLaMA/comments/1q5mh84/unslothmlx_finetune_llms_on_your_mac_same_api_as/)

**Author:** u/A-Rahim | **Upvotes:** 134 | **Comments:** 25 | **Date:** 2026-01-06

**Summary:** Unsloth-MLX is a library that enables fine-tuning LLMs on Macs with Apple Silicon, offering a local development bridge to cloud-based Unsloth with the same API. It aims to reduce cloud GPU costs during experimentation by leveraging Mac's unified memory.

**Key Points:**
- Unsloth-MLX brings Unsloth's fine-tuning experience to Apple Silicon Macs.
- It allows prototyping locally on Macs and scaling to cloud GPUs without code changes.
- The project is not affiliated with Unsloth AI or Apple and is a personal initiative.
- Discussion includes concerns about branding and mentions of alternative PRs for Unsloth.
- The goal is code portability, not performance superiority over Unsloth.

**Discussion Highlights:** The discussion highlights concerns about the use of Unsloth's branding in the project name, with some users pointing to an ongoing PR in the Unsloth repository that might address similar needs. There are also mentions of alternative models and a general sentiment of experimentation and feedback.

---

## 8. [A 30B Qwen Model Walks Into a Raspberry Pi… and Runs in Real Time](https://reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/)

**Author:** u/ali_byteshape | **Upvotes:** 480 | **Comments:** 75 | **Date:** 2026-01-06

**Summary:** The post discusses running a 30B Qwen model on a Raspberry Pi 5 with real-time performance, achieving 8.03 TPS at 2.70 BPW while retaining 94.18% of BF16 quality. The optimization focuses on fitting the model within memory constraints and then optimizing for tokens per second (TPS) without sacrificing quality. Key points include the model's performance on different hardware, the quirks of GPU vs. CPU behavior, and community feedback on testing and configurations. The discussion highlights community engagement in testing various setups and interest in combining the model with other solutions.

---

## 9. [Liquid AI released LFM2.5 1.2B Instruct](https://reddit.com/r/LocalLLaMA/comments/1q5f1jz/liquid_ai_released_lfm25_12b_instruct/)

**Author:** u/KaroYadgar | **Upvotes:** 102 | **Comments:** 29 | **Date:** 2026-01-06

**Summary:** Liquid AI released LFM2.5, a 1.2B parameter model optimized for on-device applications, featuring improved quality, lower latency, and expanded modality support. The model builds on a hybrid architecture with scaled pretraining and enhanced reinforcement learning.

**Key Points:**
- LFM2.5 is designed for reliable on-device agentic applications with higher quality and lower latency.
- The model features a hybrid architecture, scaled pretraining from 10T to 28T tokens, and expanded reinforcement learning.
- Users express enthusiasm for running the model on personal devices and curiosity about its performance improvements.
- Discussion includes inquiries about use cases for tiny models and comparisons with previous versions like LFM2-8B-A1B.

**Discussion Highlights:** Users are excited about the model's potential for local use, with some requesting benchmarks to compare improvements over previous versions. There is also interest in learning about practical applications for small models and hopes for better instruction-following capabilities.

---

## 10. [Supertonic2: Lightning Fast, On-Device, Multilingual TTS](https://reddit.com/r/LocalLLaMA/comments/1q5e010/supertonic2_lightning_fast_ondevice_multilingual/)

**Author:** u/ANLGBOY | **Upvotes:** 191 | **Comments:** 40 | **Date:** 2026-01-06

**Summary:** Supertonic2 is a lightweight, multilingual TTS model with 5 supported languages, designed for speed and on-device use. It offers commercial licensing and flexible deployment options.

**Key Points:**
- Supports 5 languages: Korean, Spanish, French, Portuguese, and English
- Lightning fast with minimal footprint (66M parameters)
- On-device TTS for privacy and zero latency
- Open-weight model with commercial use allowed
- User feedback highlights high quality and demand for additional languages

**Discussion Highlights:** Users praised the model's quality and speed but expressed concerns about the OpenRAIL-M license. There is demand for additional languages like German, Russian, and Arabic.

---

## 11. [Performance improvements in llama.cpp over time](https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/)

**Author:** u/jacek2023 | **Upvotes:** 644 | **Comments:** 78 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses performance improvements in llama.cpp over time, with a focus on NVIDIA GPU optimizations and recent advancements in token generation speed.

**Key Points:**
- Performance gains in llama.cpp are highlighted
- NVIDIA GPUs are mentioned as a key factor in these improvements
- Recent advancements have brought llama.cpp close to the performance of other optimized implementations
- The post was featured on Discord, indicating community interest

**Discussion Highlights:** The discussion highlights significant performance gains in llama.cpp, particularly for NVIDIA GPUs, and notes that the project has made impressive progress in token generation speed, approaching the performance of other optimized implementations like ik_llama.cpp.

---

## 12. [Liquid Ai released LFM2.5, family of tiny on-device foundation models.](https://reddit.com/r/LocalLLaMA/comments/1q5a0if/liquid_ai_released_lfm25_family_of_tiny_ondevice/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 298 | **Comments:** 52 | **Date:** 2026-01-05

**Summary:** Liquid AI released LFM2.5, a family of tiny on-device foundation models designed for reliable agentic applications. The models feature higher quality, lower latency, and broader modality support in the ~1B parameter class, with five open-weight model instances available.

**Key Points:**
- LFM2.5 builds on a device-optimized hybrid architecture with scaled pretraining from 10T to 28T tokens.
- Five model instances include general-purpose instruct, Japanese-optimized chat, vision-language, native audio-language, and base checkpoints.
- Discussion highlights include comparisons with Qwen3-0.6B, performance observations, and suggestions for native FP8 or FP4 training.
- Users noted the model's speed but criticized its instruction-following capabilities for special formats.
- Some users expressed a desire for larger models from Liquid AI.

**Discussion Highlights:** The discussion included comparisons with other models like Qwen3-0.6B, observations on performance and speed, and suggestions for improvements such as native FP8 or FP4 training. Users generally appreciated the model's capabilities but noted areas for improvement, particularly in instruction following and model size.

---

## 13. [I just saw Intel embrace local LLM inference in their CES presentation](https://reddit.com/r/LocalLLaMA/comments/1q52miw/i_just_saw_intel_embrace_local_llm_inference_in/)

**Author:** u/Mundane-Light6394 | **Upvotes:** 142 | **Comments:** 71 | **Date:** 2026-01-05

**Summary:** Intel emphasized the importance of local LLM inference during their CES presentation, highlighting benefits like user privacy, control, model responsiveness, and avoiding cloud bottlenecks. This contrasts with Nvidia's cloud-first strategy and suggests a potential resurgence in local inference technology.

**Key Points:**
- Intel's focus on local inference for privacy, control, and responsiveness
- Potential resurgence of local inference despite Nvidia's cloud-first approach
- Mention of Intel Arc Pro B50 GPU as a cost-effective option for local inference
- Discussion on the future of local LLM inference and hardware efficiency
- Hope for Intel to support unified memory technologies like CXL

**Discussion Highlights:** The discussion highlights a positive reception towards Intel's focus on local inference, with users expressing hope for more efficient and affordable hardware. There is a consensus that local inference is not dead and may become more prominent in the future.

---

## 14. [Rubin uplifts from CES conference going on now](https://reddit.com/r/LocalLLaMA/comments/1q502bi/rubin_uplifts_from_ces_conference_going_on_now/)

**Author:** u/mr_zerolith | **Upvotes:** 220 | **Comments:** 94 | **Date:** 2026-01-05

**Summary:** The Reddit post discusses the Rubin uplifts announced at the CES conference, highlighting their performance and cost implications. Users express excitement but also note the lack of consumer-focused announcements.

**Key Points:**
- Rubin uplifts announced at CES with significant performance gains
- Cost implications discussed, with potential high prices per unit
- Memory bandwidth improvements noted as impressive
- Lack of consumer-focused announcements criticized
- Power requirements and performance per watt gains debated

**Discussion Highlights:** The discussion highlights excitement about the performance gains and memory bandwidth improvements of the Rubin uplifts. However, there is criticism about the lack of consumer-focused announcements and concerns about the high power requirements and cost implications.

---

## 15. [For the first time in 5 years, Nvidia will not announce any new GPUs at CES — company quashes RTX 50 Super rumors as AI expected to take center stage](https://reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/)

**Author:** u/FullstackSensei | **Upvotes:** 616 | **Comments:** 195 | **Date:** 2026-01-05

**Summary:** Nvidia will not announce new GPUs at CES, focusing instead on AI. The post discusses limited supply of new GPUs, potential re-release of older models, and rising hardware prices, expressing concerns about future upgrades.

**Key Points:**
- Nvidia will not announce new GPUs at CES, shifting focus to AI.
- Limited supply of RTX 50 series GPUs and potential re-release of RTX 3060.
- Rising prices of DDR5 RAM and storage, making upgrades expensive.
- Concerns about future hardware upgrades due to high costs and limited availability.
- Discussion highlights corporate greed and the need for alternative solutions.

**Discussion Highlights:** The discussion highlights frustration with corporate greed, the high cost of hardware, and the lack of new GPU announcements. Users express concerns about the future of local computing and the need for alternative solutions, such as increased competition from other manufacturers.

---

## 16. [[Release] EchoChamber - Add AI-Generated Audience Reactions to Your SillyTavern Stories &amp; Conversations](https://reddit.com/r/LocalLLaMA/comments/1q4tken/release_echochamber_add_aigenerated_audience/)

**Author:** u/mattjb | **Upvotes:** 103 | **Comments:** 27 | **Date:** 2026-01-05

**Summary:** EchoChamber is a new SillyTavern extension that adds real-time AI-generated audience reactions to stories and conversations, offering various chat styles and customization options.

**Key Points:**
- 10+ built-in chat styles including Discord, Twitter, and MST3K-style commentary
- Flexible backend supporting various local models and APIs
- Customizable styles and theme integration
- Mixed reactions from users, ranging from amusement to concern
- Available for installation via GitHub

**Discussion Highlights:** Users expressed a mix of amusement, concern, and excitement about the extension, with comments highlighting its potential for enhancing role-playing experiences.

---

## 17. [llama.cpp performance breakthrough for multi-GPU setups](https://reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/)

**Author:** u/Holiday-Injury-9397 | **Upvotes:** 549 | **Comments:** 173 | **Date:** 2026-01-05

**Summary:** The ik_llama.cpp project achieved a significant performance breakthrough for multi-GPU setups, delivering a 3x to 4x speed improvement in local LLM inference, making it cost-effective to use multiple low-cost GPUs instead of high-end enterprise cards.

**Key Points:**
- ik_llama.cpp introduces a new execution mode (split mode graph) for maximum utilization of multiple GPUs.
- Performance improvement ranges from 3x to 4x, making it a game-changer for cost-effective GPU setups.
- Even on single GPU or CPU-only setups, ik_llama.cpp shows consistent 2x prompt processing speed improvements.
- The breakthrough allows harnessing collective power of multiple low-cost GPUs in homelabs, server rooms, or the cloud.
- Details and discussions are available on GitHub and other community platforms.

**Discussion Highlights:** The community highlights the significant performance gains, with some users reporting 2x speed improvements even on single GPU or CPU-only setups. There is consensus on the cost-effectiveness and potential of using multiple low-cost GPUs. Some users also discuss specific hardware setups and potential bottlenecks.

---

## 18. [Falcon H1R 7B, a new reasoning model with 256k context window by the Technology Innovation Institute (TII) in Abu Dhabi](https://reddit.com/r/LocalLLaMA/comments/1q4jnq0/falcon_h1r_7b_a_new_reasoning_model_with_256k/)

**Author:** u/Nunki08 | **Upvotes:** 123 | **Comments:** 26 | **Date:** 2026-01-05

**Summary:** The post introduces Falcon H1R 7B, a new reasoning model with a 256k context window developed by the Technology Innovation Institute (TII) in Abu Dhabi. The model shows impressive benchmarks but raises concerns about real-world usage and the need for new benchmarks.

**Key Points:**
- Falcon H1R 7B is a new reasoning model with a 256k context window by TII in Abu Dhabi
- The model shows impressive benchmarks but may not translate well to real-world usage
- There is a call for new, private benchmarks and more agentic benchmarks
- The model is noted for its efficiency and potential to be sub-32B SOTA if agentic capabilities are included
- Some users express skepticism about overfitted models and benchmarking practices

**Discussion Highlights:** The discussion highlights a mix of enthusiasm for the model's efficiency and skepticism about its real-world performance. Users call for more comprehensive benchmarks and express fatigue with overfitted models. There is also interest in seeing more agentic benchmarks to better evaluate the model's capabilities.

---

## 19. [What do we think about Gorgon Point (Ryzen AI 9 HX 470)?](https://reddit.com/r/LocalLLaMA/comments/1q4jc99/what_do_we_think_about_gorgon_point_ryzen_ai_9_hx/)

**Author:** u/Everlier | **Upvotes:** 142 | **Comments:** 45 | **Date:** 2026-01-05

**Summary:** The Reddit post discusses the new Gorgon Point (Ryzen AI 9 HX 470) APU, highlighting its support for high-speed memory and potential improvements over previous models, but also noting challenges in accessing the necessary chips. The discussion includes mixed opinions on its significance and future expectations.

**Key Points:**
- Gorgon Point supports DDR5-6400 and LPDDR5X-8533, improving performance for some models.
- Manufacturers face challenges in accessing the required chips.
- Gorgon Point is a mid-cycle refresh, not a replacement for Strix Halo.
- Mixed opinions on its significance compared to other models like Ryzen AI Max 395.
- Criticism of the rapid pace of technological updates and expectations for future improvements.

**Discussion Highlights:** The discussion highlights a mix of optimism and skepticism about the Gorgon Point APU. While some users appreciate the potential performance improvements, others express concerns about chip accessibility and the rapid pace of technological updates. There is also a consensus that Gorgon Point is a mid-cycle refresh, with expectations for more significant updates in the future.

---

## 20. [I built a visual AI workflow tool that runs entirely in your browser - Ollama, LM Studio, llama.cpp and Most cloud API's all work out of the box. Agents/Websearch/TTS/Etc.](https://reddit.com/r/LocalLLaMA/comments/1q4f0tm/i_built_a_visual_ai_workflow_tool_that_runs/)

**Author:** u/l33t-Mt | **Upvotes:** 153 | **Comments:** 57 | **Date:** 2026-01-05

**Summary:** The post introduces EmergentFlow, a visual AI workflow tool that runs entirely in the browser, supporting various local and cloud-based AI models. It offers a free tier with unlimited use of local models and a Pro tier for additional features. Key points include its support for Ollama, LM Studio, llama.cpp, and various cloud APIs, with an optional desktop runner for CORS issues. The discussion highlights comparisons with existing tools like n8n and Flowise, with users questioning the unique value proposition of EmergentFlow and expressing concerns about the integration of cloud API keys in a tool aimed at local LLM usage.

---

## 21. [Introducing Adaptive-P: A New Sampler for Creative Text Generation (llama.cpp PR)](https://reddit.com/r/LocalLLaMA/comments/1q42wtt/introducing_adaptivep_a_new_sampler_for_creative/)

**Author:** u/DragPretend7554 | **Upvotes:** 119 | **Comments:** 27 | **Date:** 2026-01-04

**Summary:** Adaptive-P is a new sampling method for creative text generation that addresses repetitive patterns by targeting a probability range and using a feedback loop to maintain diversity. It has been merged into Kobold.cpp and is in staging for SillyTavern.

**Key Points:**
- Adaptive-P targets a probability range to encourage diverse token selection.
- It uses an exponential moving average for adaptive targeting.
- The method breaks repetitive high-confidence chains.
- It is already integrated into Kobold.cpp and in staging for SillyTavern.
- Users report improved word diversity and logic preservation.

**Discussion Highlights:** Users generally praise Adaptive-P for its effectiveness in creative tasks and its versatility in targeting different probability ranges. There is consensus on its utility and ongoing integration into various platforms.

---

## 22. [GLM-Image model from Z.ai is coming](https://reddit.com/r/LocalLLaMA/comments/1q41bw1/glmimage_model_from_zai_is_coming/)

**Author:** u/Ravencloud007 | **Upvotes:** 315 | **Comments:** 58 | **Date:** 2026-01-04

**Summary:** The Reddit post announces the upcoming GLM-Image model from Z.ai, generating significant interest and discussion in the community. Users are excited about its potential, with some humorously speculating about its size and requirements.

**Key Points:**
- GLM-Image model from Z.ai is being introduced
- Community shows strong interest and excitement
- Speculation about model size and computational requirements
- Z.ai's image model is currently a community favorite
- Desire for a balance between model size, ease of fine-tuning, and quality

**Discussion Highlights:** The discussion highlights a strong community interest in the new GLM-Image model, with users expressing excitement and humor about its potential size and requirements. There is a consensus that Z.ai's current image model is highly regarded, and users are looking forward to advancements that balance model size, ease of use, and quality.

---

## 23. [MultiverseComputingCAI/HyperNova-60B · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1q3p9oz/multiversecomputingcaihypernova60b_hugging_face/)

**Author:** u/jacek2023 | **Upvotes:** 126 | **Comments:** 58 | **Date:** 2026-01-04

**Summary:** The Reddit post discusses HyperNova 60B, a model based on the gpt-oss-120b architecture with 59B parameters, 4.8B active parameters, and MXFP4 quantization. It supports configurable reasoning effort and requires less than 40GB of GPU memory. The discussion includes user experiences with hardware compatibility and performance metrics.

**Key Points:**
- HyperNova 60B is based on the gpt-oss-120b architecture with 59B parameters and 4.8B active parameters.
- It uses MXFP4 quantization and supports configurable reasoning effort (low, medium, high).
- The model requires less than 40GB of GPU memory.
- Users report successful deployment on 3090 + 5060 ti with 40GB total VRAM and performance metrics of around 3k prefill / 100 token generation.
- There is interest in the novel compression technology used, with requests for more information or papers.

**Discussion Highlights:** The discussion highlights user experiences with hardware compatibility, performance metrics, and interest in the novel compression technology used in HyperNova 60B. Users share their setups and performance results, and there is a request for more information about the compression technology.

---

## 24. [Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched](https://reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/)

**Author:** u/ubrtnk | **Upvotes:** 378 | **Comments:** 191 | **Date:** 2026-01-03

**Summary:** The post discusses challenges faced by local LLMs in processing extreme breaking news events, such as the US attacking Venezuela, where models initially classified the event as a hoax despite credible sources. The author shares experiences with different LLM models and their varying responses to the news.

**Key Points:**
- Local LLMs struggled to accept extreme breaking news as real, often classifying it as a hoax.
- Different LLM models (Qwen Research, Spark 4.0, GPT-OSS:120B) had varying responses to the same event.
- Models required explicit credible sources to acknowledge the reality of the event.
- The post highlights the bias and limitations of LLMs in processing unfamiliar geopolitical events.
- Discussion consensus suggests LLMs tend to be skeptical of extreme or unlikely events.

**Discussion Highlights:** The discussion highlights a consensus that LLMs often struggle with extreme or unfamiliar events, showing a tendency to classify them as hoaxes. Users shared similar experiences and noted the bias in LLM responses to geopolitical events.

---

## 25. [Llama.cpp running on Android with Snapdragon 888 and 8GB of ram. Compiled/Built on device. [Guide/Tutorial]](https://reddit.com/r/LocalLLaMA/comments/1q2wvsj/llamacpp_running_on_android_with_snapdragon_888/)

**Author:** u/hackiv | **Upvotes:** 132 | **Comments:** 31 | **Date:** 2026-01-03

**Summary:** The post provides a guide on running Llama.cpp on Android devices with Snapdragon 888 and 8GB RAM using Termux. It includes steps for installation, model download, and server setup.

**Key Points:**
- Uses Termux for installation and setup
- Requires cmake and additional packages like git and libandroid-spawn
- Models are downloaded from HuggingFace in quantized 4-bit format
- Server runs locally on the device at localhost:8080
- Models are cached for future use without re-downloading

**Discussion Highlights:** The discussion highlights curiosity about performance metrics (tokens/sec) and hardware utilization (CPU/NPU/GPU). Users expressed surprise at Llama.cpp's compatibility with ARM architecture and shared additional setup tips.

---

## 26. [ElevenLabs is killing my budget. What are the best "hidden gem" alternatives for documentary style TTS?](https://reddit.com/r/LocalLLaMA/comments/1q2sfwx/elevenlabs_is_killing_my_budget_what_are_the_best/)

**Author:** u/Ancient_Routine8576 | **Upvotes:** 231 | **Comments:** 125 | **Date:** 2026-01-03

**Summary:** The Reddit post discusses alternatives to ElevenLabs for documentary-style TTS, focusing on cost-effective and high-quality options. The author seeks recommendations for tools with a dark, authoritative tone, either paid or local solutions. The discussion highlights several local TTS options and their pros and cons.

**Key Points:**
- Author seeks cost-effective alternatives to ElevenLabs for documentary-style TTS.
- Preferences include a dark, authoritative tone and either cheaper paid options or high-quality local solutions.
- Mentioned tools include Fish Audio, OpenAI TTS API wrappers, Soprano, Kokoro, VibeVoice, XTTS v2, F5 TTS, and Echo-TTS.
- Community recommendations emphasize local TTS options with varying stability and quality.
- Google's upcoming voice synthesis technology is noted as a potential game-changer.

**Discussion Highlights:** The discussion primarily focuses on local TTS solutions like Soprano, Kokoro, VibeVoice, XTTS v2, and F5 TTS, with VibeVoice being highlighted for its ease of use. Echo-TTS is mentioned but has a 30-second limitation. The community also notes the instability of some models like VibeVoice Large. Additionally, there is anticipation for Google's advanced voice synthesis technology.

---

## 27. [Don't sleep on granite 4 small if you got an 8+32+ system](https://reddit.com/r/LocalLLaMA/comments/1q2s3hp/dont_sleep_on_granite_4_small_if_you_got_an_832/)

**Author:** u/Zestyclose-Shift710 | **Upvotes:** 118 | **Comments:** 46 | **Date:** 2026-01-03

**Summary:** The post discusses optimizing a ThinkPad P15 with 32GB RAM and an 8GB Quadro GPU to run large language models efficiently. By using a Mixture of Experts (MoE) model and keeping experts in CPU, the user achieved high context lengths and usable generation speeds, particularly with the Granite 4.0 Small model.

**Key Points:**
- Using a MoE model and keeping experts in CPU frees up VRAM for larger context lengths.
- Granite 4.0 Small (32B total / 9B activated) maintains high speed (~7 tkps) even with large context (~50.5k tokens).
- The setup allows for efficient use of hardware resources, making it feasible to run large models on mid-range hardware.
- Discussion includes comparisons with other models like Qwen 30B A3B and mentions of performance optimizations.

**Discussion Highlights:** The discussion highlights comparisons with other models like Qwen 30B A3B, mentions of performance optimizations using specific parameters, and notes on issues like constant cache rebuilding in Vulkan inference. There is also a mention of using Jan, a FOSS alternative to LM Studio, for managing the models.

---

## 28. [GLM-4.7-REAP-50-W4A16: 50% Expert-Pruned + INT4 Quantized GLM-4 (179B params, ~92GB)](https://reddit.com/r/LocalLLaMA/comments/1q2pons/glm47reap50w4a16_50_expertpruned_int4_quantized/)

**Author:** u/Maxious | **Upvotes:** 180 | **Comments:** 72 | **Date:** 2026-01-03

**Summary:** The post introduces GLM-4.7-REAP-50-W4A16, a 50% expert-pruned and INT4 quantized version of GLM-4 with 179B parameters (~92GB). The discussion focuses on calibration details, benchmarking, and comparisons with other models. Key points include the model's specifications, concerns about expert activation during calibration, questions about the calibration tasks, and interest in benchmark results. The discussion highlights the importance of providing calibration details for expert activation and the interest in benchmarking and comparisons with other models.

---

## 29. [Built a fully local AI assistant with long-term memory, tool orchestration, and a 3D UI (runs on a GTX 1650)](https://reddit.com/r/LocalLLaMA/comments/1q2onpg/built_a_fully_local_ai_assistant_with_longterm/)

**Author:** u/atif_dev | **Upvotes:** 106 | **Comments:** 40 | **Date:** 2026-01-03

**Summary:** The Reddit post describes ATOM, a fully local AI assistant with features like long-term memory, tool orchestration, and a 3D UI, running on a GTX 1650. It includes components such as a local LLM, ChromaDB for memory, and a React-based UI. The project is experimental and focuses on exploring local AI systems.

**Key Points:**
- ATOM is a fully local AI assistant with long-term memory and tool orchestration.
- It runs on a GTX 1650, showcasing performance tradeoffs.
- The UI is built with React and React Three Fiber, visualizing tool usage as orbiting planets.
- The project is experimental and not a product, focusing on local AI systems.
- Discussion highlights include praise for the setup and suggestions for alternative tools like llama.cpp.

**Discussion Highlights:** The discussion highlights praise for the coherent setup and suggestions for alternative tools like llama.cpp. There is also curiosity about the choice of edge/piper over kokoro and interest in long-term memory performance.

---

## 30. [What is the smartest uncensored nsfw LLM you can run with 20GB VRAM and 24GB RAM](https://reddit.com/r/LocalLLaMA/comments/1q2o033/what_is_the_smartest_uncensored_nsfw_llm_you_can/)

**Author:** u/Death_12_35_taken | **Upvotes:** 192 | **Comments:** 76 | **Date:** 2026-01-03

**Summary:** The post seeks recommendations for an uncensored, smart, and fast LLM that can run locally with 20GB VRAM and 24GB RAM. Users suggest models like Dolphin-Mistral-24B-Venice-Edition and others from the UGI-Leaderboard.

**Key Points:**
- User seeks an uncensored, smart, and fast LLM for local use with specific hardware constraints.
- Dolphin-Mistral-24B-Venice-Edition is recommended as a suitable model.
- UGI-Leaderboard is suggested as a resource for finding appropriate models.
- Additional models like Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated are mentioned.
- Users also inquire about models suitable for 70B parameters.

**Discussion Highlights:** The discussion highlights Dolphin-Mistral-24B-Venice-Edition as a top recommendation, with additional suggestions pointing to the UGI-Leaderboard for further options. The consensus leans towards models that balance performance and hardware compatibility.

---

## 31. [How is Cloud Inference so cheap](https://reddit.com/r/LocalLLaMA/comments/1q2jwsn/how_is_cloud_inference_so_cheap/)

**Author:** u/VolkoTheWorst | **Upvotes:** 106 | **Comments:** 106 | **Date:** 2026-01-02

**Summary:** The Reddit post discusses how cloud inference companies manage to offer services at low costs despite high GPU and electricity expenses. The discussion highlights strategies like batching, scaling, and quantization, but also questions the profitability of these companies.

**Key Points:**
- Batching allows one GPU to serve hundreds of users simultaneously.
- Profitability is uncertain, with many companies operating at a loss.
- Efficiency is achieved through scale, batching, and model quantization.
- Some companies may be relying on investor funding and future profitability projections.

**Discussion Highlights:** The discussion suggests that while techniques like batching and scaling improve efficiency, many cloud inference providers may not yet be profitable and are competing for market dominance.

---

## 32. [LeCun Says Llama 4 results "were fudged a little bit"](https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/)

**Author:** u/MrPecunius | **Upvotes:** 360 | **Comments:** 89 | **Date:** 2026-01-02

**Summary:** Yann LeCun confirmed that Llama 4 benchmarks were manipulated, and Meta's AI division faced significant restructuring, leading to departures and lack of follow-up on promised models. The community expressed disappointment and shared additional resources.

**Key Points:**
- LeCun confirms Llama 4 benchmark manipulation
- Meta's AI division restructured, leading to departures
- Community disappointment over lack of progress in Llama models
- Shared resources and discussions on organizational failures

**Discussion Highlights:** The community expressed disappointment over Meta's handling of Llama 4 and shared additional resources. There was a consensus on the missed opportunity for open-source AI advancement.

---

## 33. [Most optimal vram/performance per price and advice for Shenzhen GPU market](https://reddit.com/r/LocalLLaMA/comments/1q1w1qj/most_optimal_vramperformance_per_price_and_advice/)

**Author:** u/notafakename10 | **Upvotes:** 266 | **Comments:** 65 | **Date:** 2026-01-02

**Summary:** The post discusses finding the most optimal GPU setup with high VRAM (48GB-96GB) for local models and occasional PyTorch training within a $1500-3000 budget in the Shenzhen market. The discussion highlights various GPU options and their value propositions. Key points include the user's need for 48GB-96GB VRAM GPUs, budget range, and options like MI100, 4090D 48GB, and A100 40GB. The discussion highlights MI100 as the best value for performance per dollar without CUDA, while 4090D 48GB is recommended for CUDA support. Cooling and power requirements are emphasized as critical factors.

---

## 34. [Getting ready to train in Intel arc](https://reddit.com/r/LocalLLaMA/comments/1q1p5q5/getting_ready_to_train_in_intel_arc/)

**Author:** u/hasanismail_ | **Upvotes:** 307 | **Comments:** 92 | **Date:** 2026-01-01

**Summary:** The user is preparing to train models on Intel Arc GPUs and shares their excitement, while also addressing concerns about GPU shortages. The community provides advice on setup and tools.

**Key Points:**
- User is waiting for PCIe risers to start training on Intel Arc GPUs
- User clarifies they are not causing a GPU shortage
- Community suggests using Ubuntu 24.04 and mentions support for Intel Arc in Unsloth
- Advice to join OpenArc Discord for setup assistance
- Discussion about the feasibility of training on PCIe setup vs renting N*H100 from Vast

**Discussion Highlights:** The community is supportive and provides practical advice on setup and tools. There is a consensus on using Ubuntu 24.04 and leveraging Unsloth for Intel Arc support. Some users question the feasibility of training on a PCIe setup compared to renting more powerful GPUs.

---

## 35. [TIL you can allocate 128 GB of unified memory to normal AMD iGPUs on Linux via GTT](https://reddit.com/r/LocalLLaMA/comments/1q1lgb7/til_you_can_allocate_128_gb_of_unified_memory_to/)

**Author:** u/1ncehost | **Upvotes:** 176 | **Comments:** 30 | **Date:** 2026-01-01

**Summary:** The post discusses using Graphics Translation Table (GTT) on Linux to allocate up to 128 GB of system memory as VRAM for AMD iGPUs, which is useful for development and hybrid CPU/GPU architectures. The author shares their experience using this feature for parallel kernel development and profiling.

**Key Points:**
- GTT allows dynamic allocation of up to 128 GB of system memory as VRAM for AMD iGPUs on Linux.
- Useful for development and profiling, but not ideal for inference due to slow iGPU performance.
- Enables hybrid CPU/GPU architectures and big memory AMD GPU kernel development.
- Users share experiences with older Ryzen processors and Strix Halo for background tasks.
- Alternative methods like using Nvidia GPUs with llama.cpp for kv cache are mentioned.

**Discussion Highlights:** The discussion highlights practical use cases for GTT, such as background LLM tasks and hybrid architectures. Users share their experiences with different hardware setups and alternative methods for similar tasks.

---

## 36. [IQuestCoder - new 40B dense coding model](https://reddit.com/r/LocalLLaMA/comments/1q1986x/iquestcoder_new_40b_dense_coding_model/)

**Author:** u/ilintar | **Upvotes:** 185 | **Comments:** 37 | **Date:** 2026-01-01

**Summary:** The post introduces IQuestCoder, a new 40B dense coding model based on Llama architecture, which has been adapted to GGUF and is claimed to be state-of-the-art. The author has made it compatible with Llama.cpp and shared it on Hugging Face.

**Key Points:**
- IQuestCoder is a 40B dense coding model with Llama architecture.
- The model has been adapted to GGUF and works with Llama.cpp.
- The Loop version of the model requires adaptation due to its new architecture.
- The model has shown promising performance in tasks like zero-shot Snake game and embedded Rust concepts.
- There is some skepticism about the lack of transparency regarding the architecture used.

**Discussion Highlights:** The discussion highlights include appreciation for the GGUF adaptation, skepticism about the model's architecture transparency, and positive feedback on its performance in various coding tasks. Some users are testing the model against other AI models for real-world coding problems.

---

