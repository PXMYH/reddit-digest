# r/LocalLLaMA Reading Digest

**Period:** 2026-01-15 to 2026-01-15
**Posts Summarized:** 35
**Total Posts Analyzed:** 35

---

## 1. [Which are the top LLMs under 8B right now?](https://reddit.com/r/LocalLLaMA/comments/1qcl543/which_are_the_top_llms_under_8b_right_now/)

**Author:** u/Additional_Secret_75 | **Upvotes:** 171 | **Comments:** 101 | **Date:** 2026-01-14

**Summary:** The post discusses the best local LLMs under 8B for general chat, research, and coding, with a focus on models that are not overly censored and run efficiently on limited VRAM. Users share their experiences and recommendations for top-performing models in this category.

**Key Points:**
- Users are seeking recommendations for local LLMs under 8B that balance performance and efficiency.
- Qwen3 4B and Qwen3 8B are highlighted for their capabilities, though some find the 8B version underwhelming.
- Gemma 3n E4B is praised for its reasoning and multimodal abilities, including vision and audio understanding.
- Other models like Nanbeige 3B are mentioned as alternatives.
- The discussion emphasizes the importance of models that run well without requiring excessive VRAM.

**Discussion Highlights:** The discussion highlights a consensus around Qwen3 and Gemma 3n E4B as strong contenders in the under 8B category, with users valuing models that offer good performance without high resource demands. The conversation also reflects the diversity of opinions and experiences with different models.

---

## 2. [GLM-Image is released!](https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/)

**Author:** u/foldl-li | **Upvotes:** 580 | **Comments:** 81 | **Date:** 2026-01-13

**Summary:** GLM-Image is a new image generation model with a hybrid autoregressive + diffusion decoder architecture. It excels in text-rendering and knowledge-intensive tasks while maintaining high-fidelity detail generation. The model supports various image-to-image tasks and has been released under an MIT license.

**Key Points:**
- Hybrid autoregressive + diffusion decoder architecture
- Excels in text-rendering and knowledge-intensive generation
- Supports image editing, style transfer, and multi-subject consistency
- Released under MIT license
- Model size: 13GB diffusion model + 20GB text encoder

**Discussion Highlights:** The community appreciates the MIT license and the model's capabilities. There is excitement about its performance compared to other models and its potential for various image tasks. Some users are waiting for optimized versions for easier use.

---

## 3. [Soprano TTS training code released: Create your own 2000x realtime on-device text-to-speech model with Soprano-Factory!](https://reddit.com/r/LocalLLaMA/comments/1qc5nml/soprano_tts_training_code_released_create_your/)

**Author:** u/eugenekwek | **Upvotes:** 307 | **Comments:** 33 | **Date:** 2026-01-13

**Summary:** The post introduces Soprano-Factory, a tool for training custom text-to-speech models with high performance metrics (up to 2000x realtime on GPU) and low latency (15 ms). It provides resources for training and encoding, allowing users to add new voices, styles, and languages.

**Key Points:**
- Soprano-Factory allows training custom TTS models with user's own data and hardware
- Performance metrics include up to 2000x realtime on GPU and 15 ms latency
- Users can add new voices, styles, and languages to Soprano
- The repository is concise (600 lines of code) and customizable
- Users express interest in features like pauses and calm reading in TTS models

**Discussion Highlights:** Users appreciate the lightweight and fast nature of Soprano, with requests for additional features like pauses and calm reading. Overall sentiment is positive, with users looking forward to further developments.

---

## 4. [My wishes for 2026](https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/)

**Author:** u/jacek2023 | **Upvotes:** 611 | **Comments:** 179 | **Date:** 2026-01-13

**Summary:** The Reddit post discusses predictions for 2026, focusing on the possibility of affordable GPUs with more than 32GB of memory. The community engages in a mix of hopeful and skeptical comments about this prospect.

**Key Points:**
- The post asks which predictions for 2026 are likely to happen first
- A top comment humorously doubts the possibility of affordable GPUs with >32GB
- Another comment references 'Qwen 4' and 'Mistral' as potential developments
- The community shows a mix of optimism and skepticism about technological advancements in 2026

**Discussion Highlights:** The discussion highlights a mix of humor and skepticism regarding the feasibility of affordable high-memory GPUs in 2026. Some users reference specific models like 'Qwen 4' and 'Mistral' as potential advancements, while others express doubt about the likelihood of such developments.

---

## 5. [kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop—no GPU required](https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/)

**Author:** u/Nunki08 | **Upvotes:** 378 | **Comments:** 79 | **Date:** 2026-01-13

**Summary:** Kyutai introduced Pocket TTS, a 100M-parameter text-to-speech model with high-quality voice cloning that runs on a laptop without requiring a GPU. The model is open-source with resources available on GitHub, Hugging Face, and arXiv.

**Key Points:**
- Pocket TTS is a lightweight (100M parameters) TTS model with high-quality voice cloning.
- It runs on a laptop without needing a GPU.
- Resources include a blog post, GitHub repository, Hugging Face model card, arXiv paper, and social media link.
- Users are interested in fine-tuning for different languages.
- Memory usage can balloon during generation, reaching up to 32 GB in some cases.

**Discussion Highlights:** The community shows interest in multilingual support and raises concerns about memory usage during generation. Some users suggest that smaller models may not be worth the effort compared to existing solutions.

---

## 6. [baichuan-inc/Baichuan-M3-235B · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qbjbrf/baichuanincbaichuanm3235b_hugging_face/)

**Author:** u/jacek2023 | **Upvotes:** 120 | **Comments:** 33 | **Date:** 2026-01-12

**Summary:** Baichuan-M3 is a new medical-enhanced language model by Baichuan AI that surpasses GPT-5.2 in medical benchmarks, focusing on clinical decision-making and low hallucination rates. The community is excited about its capabilities and potential use cases.

**Key Points:**
- Baichuan-M3 outperforms GPT-5.2 in medical benchmarks
- Focuses on clinical decision-making and low hallucination rates
- Community is excited about its capabilities
- Users discuss hardware requirements and potential use cases
- Interest in adding vision capabilities

**Discussion Highlights:** The community is positive about the model's advancements, with some humor about hardware needs and interest in vision capabilities.

---

## 7. [How do people even afford these expensive graphic cards...?...](https://reddit.com/r/LocalLLaMA/comments/1qb1w7a/how_do_people_even_afford_these_expensive_graphic/)

**Author:** u/boisheep | **Upvotes:** 104 | **Comments:** 265 | **Date:** 2026-01-12

**Summary:** The Reddit post discusses the financial and technical challenges of using high-end GPUs for ML/LLM tasks, highlighting the limitations of a single RTX 3090 and the high cost of more powerful cards. The comments emphasize that such expenses are often justified as business costs or by individuals with significant disposable income.

**Key Points:**
- The author struggles with the performance of an RTX 3090 for ML/LLM tasks, especially with diffusion models and LLMs.
- Upgrading to more powerful GPUs is expensive, with costs reaching up to $10,000.
- The benefits of dual GPUs are limited for certain tasks like diffusion models.
- High-end GPUs are often considered business expenses rather than personal leisure purchases.
- Some individuals invest in expensive GPUs despite the lack of financial justification.

**Discussion Highlights:** The discussion highlights that while high-end GPUs are costly, they are often seen as necessary business expenses. Some users acknowledge that such investments may not always make financial sense but are made due to personal interest or disposable income.

---

## 8. [GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/)

**Author:** u/TKGaming_11 | **Upvotes:** 329 | **Comments:** 76 | **Date:** 2026-01-12

**Summary:** The Reddit post highlights a GitHub project by DeepSeek-AI called Engram, which introduces a novel approach to memory in large language models using scalable lookup and n-gram embeddings. The community praises the originality and technical depth of the work.

**Key Points:**
- Engram introduces a new axis of sparsity for LLMs via conditional memory and scalable lookup.
- The approach uses n-gram embeddings, complementing traditional MoE methods with O(1) lookup.
- DeepSeek's work is noted for its originality and technical rigor, as highlighted by the community.
- The method may draw parallels to biological memory processes, as suggested by some commenters.

**Discussion Highlights:** The discussion emphasizes the technical novelty of Engram, particularly its use of n-gram embeddings and scalable lookup. The community consensus is highly positive, with praise for DeepSeek's consistent innovation and the potential biological plausibility of the approach.

---

## 9. [We fine-tuned a 4B Text2SQL model that matches a 685B teacher - query your CSV data in plain English, locally](https://reddit.com/r/LocalLLaMA/comments/1qaz4je/we_finetuned_a_4b_text2sql_model_that_matches_a/)

**Author:** u/party-horse | **Upvotes:** 169 | **Comments:** 34 | **Date:** 2026-01-12

**Summary:** A 4B parameter Text2SQL model was fine-tuned to match the accuracy of a 685B model, enabling local execution for converting plain English queries to SQL. The model runs locally, ensuring data privacy and fast responses, with performance metrics showing 80% LLM-as-a-judge accuracy and 60% exact match accuracy.

**Key Points:**
- 4B model matches 685B model accuracy in Text2SQL tasks
- Runs locally with fast response times (<2 seconds)
- Supports complex queries including JOINs and aggregations
- Community questions about SQL dialect, linting errors, and licensing
- Performance verified using LLM-as-a-judge methodology

**Discussion Highlights:** The community raised questions about the SQL dialect (SQLite-compatible), linting error rates, licensing, and the use of LLM-as-a-judge for verification. Some users noted the complexity of the examples and the need for careful review to spot errors.

---

## 10. [[Release] Eva-4B: Specialized Financial Evasion Detection (Based on Qwen3-4B). Outperforms GPT-5.2 on domain benchmarks.](https://reddit.com/r/LocalLLaMA/comments/1qautxm/release_eva4b_specialized_financial_evasion/)

**Author:** u/Awkward_Run_9982 | **Upvotes:** 178 | **Comments:** 35 | **Date:** 2026-01-12

**Summary:** Eva-4B is a specialized 4B parameter model designed to detect evasive answers in corporate earnings calls, outperforming GPT-5.2 on domain benchmarks with 81.3% accuracy. It is efficient and cost-effective for local or production use.

**Key Points:**
- Eva-4B classifies answers into 'direct', 'intermediate', or 'fully_evasive' categories using the Rasiah framework.
- Achieves 81.3% accuracy on a 1,000-sample test set, outperforming GPT-5.2 (80.5%).
- Fine-tuned on 30k samples using a multi-model consensus pipeline.
- Community discussion highlights the value of specialized models and their practical applications.
- Some users express interest in broader applications of such models.

**Discussion Highlights:** The community appreciates specialized models like Eva-4B, with discussions focusing on their efficiency and potential broader applications. Some users humorously suggest extending such models to personal contexts.

---

## 11. [Local LLM + Internet Search Capability = WOW](https://reddit.com/r/LocalLLaMA/comments/1qajxrg/local_llm_internet_search_capability_wow/)

**Author:** u/alex_godspeed | **Upvotes:** 235 | **Comments:** 89 | **Date:** 2026-01-11

**Summary:** The post discusses a user's positive experience with integrating internet search capabilities into a local LLM (Qwen 3), highlighting the ease of use and the 'wow-moment' of achieving functionality similar to ChatGPT locally. The discussion focuses on enhancing local LLMs with additional features and ensuring privacy. Key points include successful integration of internet search, suggestions for adding current time context, using Brave Leo for memory and privacy, and routing searches through Tor for enhanced privacy. Tools like Harbor and TTS/STT integration were mentioned as ways to further enhance local LLM capabilities. The discussion consensus emphasizes the importance of privacy and the potential of local LLMs when combined with internet search and additional tools.

---

## 12. [Qwen cutoff date makes our current reality too dystopian to be credible](https://reddit.com/r/LocalLLaMA/comments/1qagaaq/qwen_cutoff_date_makes_our_current_reality_too/)

**Author:** u/Swimming_Cover_9686 | **Upvotes:** 291 | **Comments:** 153 | **Date:** 2026-01-11

**Summary:** The post critiques the Qwen-3-80B model for rejecting recent, extreme news events as implausible due to its cutoff date, highlighting its inability to process dystopian realities. Users emphasize the need for real-time data and better geopolitical reasoning.

**Key Points:**
- Qwen-3-80B rejects extreme events (e.g., Elon Musk's Nazi salute, U.S. kidnapping Maduro) as impossible based on logical reasoning.
- The model's cutoff date limits its ability to accept recent, dystopian news.
- Users suggest using internet access for grounding and critique the model's geopolitical understanding.
- A simple system prompt updating the current year (2026) is proposed to address skepticism.

**Discussion Highlights:** The discussion highlights a consensus on the model's limitations in processing recent events and the importance of real-time data. Users also critique the model's geopolitical reasoning and suggest practical solutions like system prompts.

---

## 13. [LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)](https://reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 1008 | **Comments:** 110 | **Date:** 2026-01-11

**Summary:** The post introduces TimeCapsuleLLM, a 1.2B parameter language model trained exclusively on 1800-1875 London texts to minimize modern bias. The model demonstrates period-specific knowledge and behaviors, such as unfamiliarity with post-1875 concepts like telephones. The project is open-source and available on GitHub and Hugging Face.

**Key Points:**
- TimeCapsuleLLM is trained on 90GB of 1800-1875 London texts with no modern data or fine-tuning.
- The model exhibits period-specific behaviors, like generating arguments relevant to the Catholic Emancipation Act of 1829.
- The model is unfamiliar with concepts post-1875, such as telephones, treating them as unknown terms.
- Future steps include creating synthetic Q&A pairs from the dataset.
- The project is open-source and has gained significant community interest and support.

**Discussion Highlights:** The community shows strong support for the project, with comments highlighting its uniqueness and potential. Some users share similar interests in training models on historical datasets, and there is enthusiasm for the project's open-source nature and future developments.

---

## 14. [Dual Strix Halo: No Frankenstein setup, no huge power bill, big LLMs](https://reddit.com/r/LocalLLaMA/comments/1qa9dha/dual_strix_halo_no_frankenstein_setup_no_huge/)

**Author:** u/Zyj | **Upvotes:** 100 | **Comments:** 45 | **Date:** 2026-01-11

**Summary:** The post discusses a cost-effective dual Strix Halo setup for running large language models (LLMs) efficiently, highlighting its performance and affordability. The author shares their experience with the setup, including token speeds and future experiments, while noting a bottleneck in prompt preprocessing.

**Key Points:**
- Dual Strix Halo setup offers high performance for LLMs at a reasonable cost (~3440€).
- Token speeds are impressive, e.g., GPT-OSS-120B at >50 tokens/s on a single PC.
- Prompt preprocessing is slow and identified as a bottleneck.
- The setup leverages Thunderbolt networking and quad-channel DDR5 memory.
- Future experiments include vLLM and DeepSeek-V3.2-REAP-345B-A37B.

**Discussion Highlights:** The discussion highlights the setup's strengths for large MoE models but notes limitations for agentic coding due to slow prompt processing. Users express interest in leveraging the NPU for prompt processing and discuss potential improvements like aggregating USB4 connections.

---

## 15. [I bought a €9k GH200 “desktop” to save $1.27 on Claude Code (vLLM tuning notes)](https://reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/)

**Author:** u/Reddactor | **Upvotes:** 678 | **Comments:** 175 | **Date:** 2026-01-11

**Summary:** The author built a high-end GH200 desktop system for €9k to run Claude Code locally, achieving better speeds and results than the cloud version. They shared optimized vLLM settings for dual 96GB systems and highlighted the cost savings and performance benefits of local execution.

**Key Points:**
- Author spent €9k on a GH200 desktop to run Claude Code locally, achieving better performance than cloud-based Sonnet.
- Optimized vLLM settings for dual 96GB systems were shared, including tensor parallel size, context length, and sequence settings.
- The setup uses MiniMax M2.1 FP8+INT4 AWQ for full offline coding, blocking telemetry and unnecessary traffic.
- The post humorously notes the high upfront cost but emphasizes the long-term savings and performance benefits.
- Community reactions include humor about cost vs. savings, appreciation for the setup, and discussions about specific model details.

**Discussion Highlights:** The community reacted with humor about the cost justification, appreciation for the technical achievement, and discussions about specific model configurations and performance metrics.

---

## 16. [It works! Abliteration can reduce slop without training](https://reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/)

**Author:** u/-p-e-w- | **Upvotes:** 386 | **Comments:** 122 | **Date:** 2026-01-11

**Summary:** The post discusses using abliteration to reduce 'slop' (flowery, cliched language) in LLM outputs without training. The author successfully applied this technique to Mistral Nemo, creating a slop-reduced model using Heretic, a tool originally for censorship removal.

**Key Points:**
- Abliteration can reduce slop in LLM outputs without finetuning.
- Heretic tool was adapted to inject prompt prefixes/suffixes for slop reduction.
- Mistral Nemo model showed clear semantic separation between layers 7 and 10.
- The process took 2.5 hours on an A6000 but can be faster with quantization.
- Community feedback is mixed, with some preferring reduced slop but noting potential loss of creativity.

**Discussion Highlights:** The community is divided on the effectiveness of slop reduction. Some appreciate the cleaner output, while others feel it makes the prose too dry or lacks imagination. There is also interest in whether this technique could be applied to other overused patterns in writing.

---

## 17. [Leader of Qwen team says Chinese companies severely constrained on compute for large scale research experiments](https://reddit.com/r/LocalLLaMA/comments/1qa0ph9/leader_of_qwen_team_says_chinese_companies/)

**Author:** u/Old-School8916 | **Upvotes:** 306 | **Comments:** 104 | **Date:** 2026-01-11

**Summary:** The Reddit post discusses constraints on compute resources faced by Chinese AI research teams, highlighting potential innovative solutions and future competition. The discussion includes skepticism about the claims and mentions specific hardware examples.

**Key Points:**
- Chinese AI teams face compute constraints
- Necessity may drive innovation
- Skepticism about claims of resource shortages
- Specific hardware examples mentioned

**Discussion Highlights:** The discussion highlights a consensus that constraints may lead to innovation, with some skepticism about the severity of the compute shortages and mentions of specific hardware available.

---

## 18. [Gigabyte Announces Support for 256GB of DDR5-7200 CQDIMMs at CES 2026](https://reddit.com/r/LocalLLaMA/comments/1q9xn78/gigabyte_announces_support_for_256gb_of_ddr57200/)

**Author:** u/GoodSamaritan333 | **Upvotes:** 168 | **Comments:** 40 | **Date:** 2026-01-11

**Summary:** Gigabyte announced support for 256GB of DDR5-7200 CQDIMMs at CES 2026, sparking discussions about its usefulness and performance compared to older systems.

**Key Points:**
- Gigabyte's announcement of 256GB DDR5-7200 CQDIMMs support
- Discussion on the timing of the announcement during a DDR5 shortage
- Debate on the usefulness of dual-channel configuration for high memory capacity
- Comparison with older Threadripper systems using quad-channel DDR4-3200
- Mixed opinions on the suitability for AI purposes due to memory and channel limitations

**Discussion Highlights:** The community had mixed reactions, with some questioning the usefulness of dual-channel configuration for high memory capacity, while others highlighted its performance benefits over older systems. There was also debate about its suitability for AI applications.

---

## 19. [Announcing Kreuzberg v4 (Open Source)](https://reddit.com/r/LocalLLaMA/comments/1q9t9op/announcing_kreuzberg_v4_open_source/)

**Author:** u/Eastern-Surround7763 | **Upvotes:** 120 | **Comments:** 27 | **Date:** 2026-01-11

**Summary:** Kreuzberg v4 is a ground-up rewrite in Rust of a document intelligence library that extracts structured data from 56+ formats, offering improved performance, multi-language support, and production-ready features like OCR, semantic chunking, and embeddings. It is MIT-licensed and open-source.

**Key Points:**
- Rust rewrite for faster extraction and lower memory usage
- Native Rust parsers replacing Pandoc, reducing dependencies
- Support for 10 languages with identical APIs
- Plugin system for custom extractors, OCR backends, and post-processors
- Production-ready features like REST API, Docker images, and ONNX embeddings

**Discussion Highlights:** The community expressed enthusiasm for the rewrite and new features, with questions about integrations (e.g., Docling), chunking support, and handling of graph/diagram-rich documents. Some users appreciated the local connection to Kreuzberg, Berlin.

---

## 20. [Model: cerebras/GLM-4.7-REAP-268B-A32B incoming!](https://reddit.com/r/LocalLLaMA/comments/1q9io50/model_cerebrasglm47reap268ba32b_incoming/)

**Author:** u/LegacyRemaster | **Upvotes:** 196 | **Comments:** 48 | **Date:** 2026-01-10

**Summary:** The Reddit post announces the upcoming release of the cerebras/GLM-4.7-REAP-268B-A32B model, generating excitement and discussion among users. Key points include concerns about benchmark improvements, performance comparisons, and issues with multilingual capabilities.

**Key Points:**
- Excited anticipation for the new GLM-4.7-REAP-268B-A32B model
- Concerns about benchmark improvements being a potential red flag
- Performance comparisons with other model variants
- Reports of broken multilingual capabilities, particularly in Chinese
- Community engagement and recognition for the post's popularity

**Discussion Highlights:** The discussion highlights a mix of enthusiasm and technical scrutiny. Users are excited about the new model but raise concerns about its benchmark performance and multilingual capabilities. The community also acknowledges the post's popularity and engagement.

---

## 21. [I made a website to turn any confusing UI into a step-by-step guide via screen sharing (open source)](https://reddit.com/r/LocalLLaMA/comments/1q9bj5j/i_made_a_website_to_turn_any_confusing_ui_into_a/)

**Author:** u/bullmeza | **Upvotes:** 116 | **Comments:** 25 | **Date:** 2026-01-10

**Summary:** The post introduces Screen Vision, an open-source tool that guides users through tasks via screen sharing with AI, emphasizing privacy and local LLM support. It uses advanced models like GPT-5.2 and Qwen 3VL for step-by-step guidance and visual verification.

**Key Points:**
- Screen Vision is an open-source tool for task guidance via screen sharing.
- It prioritizes privacy by not storing screen data or using it for model training.
- Supports local LLM mode for users who prefer not to use cloud APIs.
- Uses GPT-5.2 for instruction and Qwen 3VL for visual verification.
- Users express concerns about potential AI hallucinations and destructive actions.

**Discussion Highlights:** Users generally appreciate the idea but raise concerns about AI accuracy and potential hallucinations. Some suggest providing a full list of actions to users for better control.

---

## 22. [Visualizing RAG, PART 2- visualizing retrieval](https://reddit.com/r/LocalLLaMA/comments/1q998is/visualizing_rag_part_2_visualizing_retrieval/)

**Author:** u/Fear_ltself | **Upvotes:** 226 | **Comments:** 42 | **Date:** 2026-01-10

**Summary:** The post discusses a project that visualizes RAG using UMAP to reduce a 768D vector space to 3D, showing how context chunks are retrieved. The code is available on GitHub, and the visualization is praised for its aesthetic and conceptual similarity to brain function.

**Key Points:**
- Project visualizes RAG using UMAP for dimensionality reduction
- Code is live on GitHub with instructions for setup
- Visualization shows how RAG retrieves relevant context chunks
- Users appreciate the aesthetic and conceptual aspects of the visualization
- Interest in connecting the project with Qdrant

**Discussion Highlights:** The post is popular and has been featured on Discord. Users are interested in integrating the visualization with other tools like Qdrant and appreciate its brain-like appearance and functionality.

---

## 23. [Jensen Huang at CES on how open models have really revolutionized AI last year. “When AI is open, it proliferates everywhere.”](https://reddit.com/r/LocalLLaMA/comments/1q90ye2/jensen_huang_at_ces_on_how_open_models_have/)

**Author:** u/Nunki08 | **Upvotes:** 180 | **Comments:** 87 | **Date:** 2026-01-10

**Summary:** Jensen Huang of NVIDIA discussed the impact of open AI models at CES, emphasizing their widespread proliferation. The discussion in the comments highlights mixed reactions, with some criticizing NVIDIA's pricing and others acknowledging the importance of open models.

**Key Points:**
- Jensen Huang's statement on the proliferation of open AI models
- Criticism of NVIDIA's high GPU prices
- Mixed reactions to the impact of open models on AI development
- Discussion on the role of NVIDIA in restricting access to open weights
- Acknowledgment of the obvious nature of Huang's statement

**Discussion Highlights:** The discussion highlights a divide between those who appreciate the importance of open AI models and those who criticize NVIDIA's business practices, particularly the high cost of GPUs and perceived restrictions on open weights.

---

## 24. [GLM 5 Is Being Trained!](https://reddit.com/r/LocalLLaMA/comments/1q8wv24/glm_5_is_being_trained/)

**Author:** u/Few_Painter_5588 | **Upvotes:** 221 | **Comments:** 69 | **Date:** 2026-01-10

**Summary:** The Reddit post announces that GLM 5 is currently being trained, following the company's IPO. The community expresses excitement and hopes for various model sizes and continued open-source availability.

**Key Points:**
- GLM 5 is being trained after the company's IPO
- Community hopes for a ~100B 'Air' model
- Expectations for GLM 5 to be a model family with sizes like 9B and 32B
- Concerns about potential negative impact from shareholders
- Speculation about GLM series becoming less open-source

**Discussion Highlights:** The discussion highlights a mix of excitement and concern. Users are hopeful for diverse model sizes and continued quality, but there are worries about the impact of shareholders and potential reduction in open-source availability.

---

## 25. [I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work](https://reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/)

**Author:** u/Ok-Pomegranate1314 | **Upvotes:** 873 | **Comments:** 144 | **Date:** 2026-01-09

**Summary:** The Reddit post describes a user's successful attempt to cluster three DGX Sparks, which NVIDIA officially supports only for two. The user developed a custom NCCL network plugin to overcome networking challenges, achieving distributed inference at 8+ GB/s over RDMA.

**Key Points:**
- NVIDIA officially supports clustering only two DGX Sparks, but the user aimed for three.
- The user developed a custom NCCL network plugin to handle subnet-aware NIC selection and raw RDMA verbs implementation.
- The solution achieved distributed inference across all three nodes at 8+ GB/s over RDMA.
- The implementation involved extensive low-level debugging and is shared on GitHub.
- The community praised the achievement, noting its significance for distributed computing.

**Discussion Highlights:** The community highlighted the technical difficulty of working with NCCL and praised the user's achievement. Questions were raised about the scalability and performance improvements of the solution.

---

## 26. [RTX Blackwell Pro 6000 wholesale pricing has dropped by $150-200](https://reddit.com/r/LocalLLaMA/comments/1q8fagh/rtx_blackwell_pro_6000_wholesale_pricing_has/)

**Author:** u/TastesLikeOwlbear | **Upvotes:** 217 | **Comments:** 90 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses a significant drop in the wholesale pricing of RTX Blackwell Pro 6000 cards by $150-200 from December to January. The author, who has insider access to wholesale pricing, also advises against buying the 72GiB 5000 Pro due to its higher price relative to the 6000 Pro.

**Key Points:**
- Wholesale price of RTX Blackwell Pro 6000 dropped by ~$150-200 from December to January.
- The 6000 Pro is only about $600 more expensive than the 72GiB 5000 Pro at wholesale.
- The author advises against buying the 72GiB 5000 Pro due to its higher price.
- The post is not marketing; the author cannot sell the cards and is sharing information for transparency.
- Community reactions include appreciation for the insider info and discussions about potential upgrades.

**Discussion Highlights:** The community appreciates the insider information and discusses potential upgrades or purchases based on the price drop. Some users mention their recent purchases or considerations, while others speculate on future market trends.

---

## 27. [The reason why RAM has become so expensive](https://reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/)

**Author:** u/InvadersMustLive | **Upvotes:** 4362 | **Comments:** 368 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses the significant increase in RAM prices, with users highlighting potential monopolistic practices and economic implications for AI data centers.

**Key Points:**
- RAM prices have increased dramatically, with some users reporting up to 10 times the cost compared to the previous year.
- There are concerns about monopolization of RAM resources, potentially making AI data centers economically unviable, especially in China.
- The price surge is seen as a strategic move to control future demand and limit competition.
- Users express skepticism about the sustainability of the current pricing trend.

**Discussion Highlights:** The discussion centers around the economic impact of rising RAM prices, with a consensus that the increase is driven by strategic monopolization rather than natural market forces. Users are concerned about the long-term viability of AI infrastructure due to these costs.

---

## 28. [DeepSeek V4 Coming](https://reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/)

**Author:** u/External_Mood4719 | **Upvotes:** 496 | **Comments:** 104 | **Date:** 2026-01-09

**Summary:** DeepSeek is expected to release V4, a next-generation AI model with strong code-generation capabilities, outperforming mainstream models like Claude and GPT. The model shows improvements in handling long code prompts and overall reasoning ability.

**Key Points:**
- DeepSeek V4 focuses on strong code-generation capabilities
- Outperforms existing models like Claude and GPT in code generation
- Improved handling of long code prompts and data patterns
- Enhanced logical rigor and clarity in outputs
- Users appreciate DeepSeek's cost-effectiveness and performance

**Discussion Highlights:** Users express excitement and anticipation for V4, with some noting DeepSeek's cost-effectiveness and performance. There is consensus on the potential impact of V4, though some speculate on the timeline and features based on recent research papers.

---

## 29. [(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability](https://reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/)

**Author:** u/Nunki08 | **Upvotes:** 486 | **Comments:** 102 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release a new flagship AI model with strong coding capabilities, generating significant interest and discussion in the community.

**Key Points:**
- DeepSeek's upcoming model focuses on strong coding ability
- The announcement has sparked excitement and anticipation
- Community members express enthusiasm for more AI models
- Some comments reflect skepticism about performance claims
- Discussion includes hopes for improved role-playing capabilities

**Discussion Highlights:** The community shows a mix of excitement and skepticism, with many welcoming the competition and innovation in AI models. Some users express concerns about overhyped claims and hope for balanced capabilities beyond just coding.

---

## 30. [Big tech companies, now "DRAM beggars," are staying in Pangyo and Pyeongtaek, demanding "give us some supplies."](https://reddit.com/r/LocalLLaMA/comments/1q84u82/big_tech_companies_now_dram_beggars_are_staying/)

**Author:** u/FullstackSensei | **Upvotes:** 296 | **Comments:** 93 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses a significant surge in DRAM prices, with DDR4 prices rising from $1.40 to $9.30 per GB, and further increases expected. Major tech companies are scrambling to secure DRAM supplies, leading to intense competition and price hikes.

**Key Points:**
- DRAM prices have surged dramatically, with DDR4 prices increasing from $1.40 to $9.30 per GB.
- Major suppliers like Samsung and SK Hynix are demanding a 50-60% increase in server DRAM supply prices.
- Tech companies are fiercely competing to secure remaining DRAM inventory.
- The DRAM shortage is expected to continue until the end of the year, with prices continuing to rise.
- The demand for DRAM is spreading beyond HBM to server DRAM due to the AI craze.

**Discussion Highlights:** The discussion highlights the significant impact of rising DRAM prices on the tech industry, with users expressing concern and humor about the situation. There is a consensus that the DRAM shortage and price increases are severe and will worsen.

---

## 31. [Minimax also live on Hong Kong Stock Exchange](https://reddit.com/r/LocalLLaMA/comments/1q82zdm/minimax_also_live_on_hong_kong_stock_exchange/)

**Author:** u/No_Conversation9561 | **Upvotes:** 124 | **Comments:** 20 | **Date:** 2026-01-09

**Summary:** The post discusses Minimax's presence on the Hong Kong Stock Exchange and includes a link to an image showing an addition to their M2.1 Collection. The discussion highlights mixed reactions and skepticism about the company's motives and trustworthiness.

**Key Points:**
- Minimax is listed on the Hong Kong Stock Exchange
- An invisible item was added to their M2.1 Collection
- Mixed reactions and skepticism from the community
- Discussion about trust in Qwen and Alibaba's involvement

**Discussion Highlights:** The discussion includes skepticism about Minimax's guiding principles and a suggestion to trust Qwen unless Alibaba spins it off to its own company. The community seems divided on the trustworthiness of Minimax's actions.

---

## 32. [OK I get it, now I love llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1q7uuxo/ok_i_get_it_now_i_love_llamacpp/)

**Author:** u/vulcan4d | **Upvotes:** 236 | **Comments:** 55 | **Date:** 2026-01-08

**Summary:** The author switched from Ollama to llama.cpp for running LLMs, highlighting the performance gains and tuning required for optimal results. They shared specific commands that significantly improved speed and discussed the learning curve involved in optimizing settings. Key points include the switch to llama.cpp for better performance, hardware setup and tuning, specific commands for performance improvement, discussion on the learning curve, and community feedback on further optimizations and critiques. The discussion includes suggestions for further optimizations like increasing batch sizes and enabling flash attention, with critiques about the commands shared and alternative recommendations.

---

## 33. [The NO FAKES Act has a "Fingerprinting" Trap that kills Open Source. We need to lobby for a Safe Harbor.](https://reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/)

**Author:** u/PostEasy7183 | **Upvotes:** 606 | **Comments:** 87 | **Date:** 2026-01-08

**Summary:** The Reddit post discusses the NO FAKES Act, highlighting its potential negative impact on open-source AI development by imposing liability on developers for tools used to create digital replicas. The author urges the community to lobby for a Safe Harbor provision to protect open-source projects. Key points include the act's creation of a 'digital replica right,' potential liability for developers hosting TTS or voice-conversion models, lack of Section 230 protection, and the need for advocacy. The discussion reflects strong opposition to the act, with concerns about its impact on innovation and the influence of big tech corporations.

---

## 34. [Z.ai  (the AI lab behind GLM) has officially IPO'd on the Hong Kong Stock Exchange](https://reddit.com/r/LocalLLaMA/comments/1q7mvuf/zai_the_ai_lab_behind_glm_has_officially_ipod_on/)

**Author:** u/Old-School8916 | **Upvotes:** 264 | **Comments:** 29 | **Date:** 2026-01-08

**Summary:** Z.ai, the AI lab behind GLM, has officially IPO'd on the Hong Kong Stock Exchange, with its stock price rising by 13.17% on the first day. The community is optimistic about future developments, including the training of GLM 5 and potential open-weight releases.

**Key Points:**
- Z.ai IPO'd on the Hong Kong Stock Exchange with a 13.17% stock price increase on the first day
- GLM 5 is currently in training, with hopes for an open-weight release
- Community reactions are positive, with expectations for continued innovation
- Minimax is also set to IPO shortly after Z.ai
- Stock opened at HK$120 and reached HK$131.50 on the first day

**Discussion Highlights:** The community is excited about Z.ai's IPO and future projects like GLM 5. There is optimism about potential open-weight releases and continued innovation, with some humor about the costs involved in providing free resources.

---

## 35. [LFM2.5 1.2B Instruct is amazing](https://reddit.com/r/LocalLLaMA/comments/1q7jd1a/lfm25_12b_instruct_is_amazing/)

**Author:** u/Paramecium_caudatum_ | **Upvotes:** 160 | **Comments:** 38 | **Date:** 2026-01-08

**Summary:** The LFM2.5 1.2B Instruct model is praised for its performance and efficiency, outperforming other models in its size range and running smoothly on various hardware. It is recommended for agentic tasks, data extraction, and RAG, but not for knowledge-intensive tasks or programming.

**Key Points:**
- The model punches above its weight and outperforms other models in its size range.
- It runs smoothly on basically any hardware.
- Recommended for agentic tasks, data extraction, and RAG.
- Not recommended for knowledge-intensive tasks and programming.
- Users appreciate its speed and effectiveness for tasks like creating tags and chat headlines.

**Discussion Highlights:** Users highlight the model's effectiveness as a small 'helper' model for tasks like creating tags and chat headlines. There is consensus on its speed and performance, though some note that edge cases may appear when using tools or RAG. The model's recent addition of tool use is also praised.

---

