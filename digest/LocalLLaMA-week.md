# r/LocalLLaMA Reading Digest

**Period:** 2026-01-20 to 2026-01-20
**Posts Summarized:** 45
**Total Posts Analyzed:** 46

---

## 1. [Liquid AI released the best thinking Language Model Under 1GB](https://reddit.com/r/LocalLLaMA/comments/1qi512t/liquid_ai_released_the_best_thinking_language/)

**Author:** u/PauLabartaBajo | **Upvotes:** 160 | **Comments:** 41 | **Date:** 2026-01-20

**Summary:** Liquid AI released LFM2.5-1.2B-Thinking, a compact reasoning model that runs on-device with 900 MB of memory, excelling in math, tool use, and instruction following. It outperforms larger models in speed and efficiency, with broad ecosystem support.

**Key Points:**
- LFM2.5-1.2B-Thinking is a 1.2B parameter model optimized for on-device reasoning with 900 MB memory usage.
- It generates internal thinking traces for systematic problem-solving and excels in math, tool use, and instruction following.
- The model matches or exceeds Qwen3-1.7B in performance despite having 40% fewer parameters.
- Users raised concerns about memory requirements, licensing, and the need for larger models for real-world applications.
- The model is available on Hugging Face, LEAP, and Liquid AI Playground.

**Discussion Highlights:** Users discussed memory requirements for edge deployment, licensing concerns, and the trade-offs between model size and performance. Some praised the model's architecture but expressed a desire for larger models for broader applications.

---

## 2. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 535 | **Comments:** 160 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, fully enclosed 10x GPU mobile AI system designed for running large MoE models and supporting graphic design tasks. The build features a Threadripper Pro 3995WX, 512GB DDR4, and a mix of 3090 and 5090 GPUs, all within a Thermaltake Core W200 case for mobility and protection from pets. The total cost was around $17k, balancing performance and budget constraints.

**Key Points:**
- Custom-built system for large MoE models and graphic design tasks
- Features Threadripper Pro 3995WX, 512GB DDR4, and 10 GPUs (8x 3090 + 2x 5090)
- Fully enclosed in a Thermaltake Core W200 case for mobility and protection
- Total cost around $17k, balancing performance and budget
- Community reactions include humor and appreciation for the build's uniqueness

**Discussion Highlights:** The top comments highlight humor and appreciation for the build's uniqueness, with one comment joking about plugging the system into a McDonald's socket and another acknowledging the build's impressive nature despite potential airflow concerns.

---

## 3. [glm-4.7-flash has the best thinking process with clear steps, I love it](https://reddit.com/r/LocalLLaMA/comments/1qhxlgy/glm47flash_has_the_best_thinking_process_with/)

**Author:** u/uptonking | **Upvotes:** 118 | **Comments:** 30 | **Date:** 2026-01-20

**Summary:** The Reddit post discusses the user's experience with the glm-4.7-flash model, highlighting its structured thinking process and comparing it favorably to other models like nemotron-nano and qwen3-30b. The user appreciates the model's clear reasoning steps but notes its slower performance and occasional looping issues.

**Key Points:**
- glm-4.7-flash has a detailed and structured thinking process with clear steps.
- The model's thinking duration is longer compared to other models, but the quality of reasoning is preferred.
- The user faces performance issues with the model, including slow token generation and occasional looping.
- Adjusting parameters like temperature and repeat penalty helps improve the model's performance.
- The community generally appreciates the model's reasoning process but acknowledges its performance limitations.

**Discussion Highlights:** The discussion highlights a consensus on the model's superior reasoning process but also acknowledges its performance issues. Users share tips on adjusting parameters to improve performance and express a general preference for the model's structured thinking approach.

---

## 4. [It's been one year since the release of Deepseek-R1](https://reddit.com/r/LocalLLaMA/comments/1qhs2sd/its_been_one_year_since_the_release_of_deepseekr1/)

**Author:** u/Recoil42 | **Upvotes:** 266 | **Comments:** 48 | **Date:** 2026-01-19

**Summary:** The Reddit post commemorates the one-year anniversary of the Deepseek-R1 release, highlighting its significant impact on the AI community. The discussion reflects on the model's influence and the rapid advancements in the field.

**Key Points:**
- Deepseek-R1 had a major impact, leading to significant changes in the AI landscape.
- The release was so impactful that it reportedly caused major disruptions, including the disbandment of a flagship AI training team.
- The rapid pace of advancements in AI is noted, with the past year feeling like several years due to the volume of progress.
- There is interest in comparing current smaller models to R1 to measure progress.
- The release is considered one of the most important in AI history, second only to the original Llama release.

**Discussion Highlights:** The discussion highlights the transformative impact of Deepseek-R1, with users emphasizing its role in accelerating AI development and disrupting existing projects. The consensus is that the release was a pivotal moment in AI history.

---

## 5. [Mosquito - 7.3M parameter tiny knowledge model](https://reddit.com/r/LocalLLaMA/comments/1qhqzsi/mosquito_73m_parameter_tiny_knowledge_model/)

**Author:** u/Lopsided-Repair-3638 | **Upvotes:** 113 | **Comments:** 47 | **Date:** 2026-01-19

**Summary:** The post introduces 'Mosquito', a tiny 7.3M parameter language model that can answer general knowledge questions, though with some humorous and inaccurate responses. Users shared mixed feedback, highlighting both its surprising capabilities and obvious limitations.

**Key Points:**
- Mosquito is a 7.3M parameter model designed for general knowledge questions.
- The model produces some accurate answers but also humorous inaccuracies (e.g., defining a dog incorrectly).
- Users requested improvements like quantization and noted odd responses to simple questions.
- The model's knowledge gaps were highlighted, such as knowing 'LLM' but not 'dog'.
- The demo and model are available on Hugging Face.

**Discussion Highlights:** The discussion was lighthearted, with users sharing funny examples of the model's inaccuracies while acknowledging its potential. There was a consensus on the need for improvements, such as quantization, and amusement at the model's quirks.

---

## 6. [Bartowski comes through again. GLM 4.7 flash GGUF](https://reddit.com/r/LocalLLaMA/comments/1qhpima/bartowski_comes_through_again_glm_47_flash_gguf/)

**Author:** u/RenewAi | **Upvotes:** 173 | **Comments:** 48 | **Date:** 2026-01-19

**Summary:** The post announces the release of GLM 4.7 Flash GGUF by Bartowski, with mixed user feedback on its performance. Some users report issues with the model's responsiveness and accuracy.

**Key Points:**
- Bartowski released GLM 4.7 Flash GGUF on Hugging Face.
- Users report mixed results, with some finding the model unresponsive or inaccurate.
- Comparisons are made with other versions like Unsloth's release.
- Performance issues include slow response times and incorrect answers.

**Discussion Highlights:** Users express concerns about the model's performance, with reports of slow response times and inaccuracies. Some are trying different versions to find a functional one.

---

## 7. [Unsloth GLM 4.7-Flash GGUF](https://reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/)

**Author:** u/Wooden-Deer-1276 | **Upvotes:** 227 | **Comments:** 44 | **Date:** 2026-01-19

**Summary:** The Reddit post discusses the release of the GLM-4.7-Flash GGUF model, with community feedback on its performance and recommendations for optimal usage. Key points include the model being released with caution to ensure proper functionality, specific quantization recommendations and parameters for optimal performance, issues with looping in quantized versions with BF16 recommended for best results, and active community engagement in testing and providing feedback. The discussion highlights a collaborative effort to improve the model, with users sharing technical details, troubleshooting issues, and celebrating milestones like the BF16 release.

---

## 8. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 436 | **Comments:** 149 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights GLM 4.7 Flash as a reliable local agent, praised for its performance in an agentic framework and anticipation for local use. The discussion includes comparisons with other models and positive user experiences.

**Key Points:**
- GLM 4.7 Flash is reliable in an agentic framework
- Anticipation for local use with GGUFs
- Comparisons with Nemotron 30b and Qwen3
- Performance benchmarks and user experiences shared

**Discussion Highlights:** Users are excited about GLM 4.7 Flash's performance and reliability, with some comparing it favorably to other models like Nemotron 30b and Qwen3. The discussion also includes benchmarks and positive feedback on its capabilities.

---

## 9. [New in llama.cpp: Anthropic Messages API](https://reddit.com/r/LocalLLaMA/comments/1qhaq21/new_in_llamacpp_anthropic_messages_api/)

**Author:** u/paf1138 | **Upvotes:** 160 | **Comments:** 48 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the introduction of the Anthropic Messages API in llama.cpp, generating enthusiasm among users who are eager to try it out. The discussion includes practical examples and technical details for implementation.

**Key Points:**
- Introduction of Anthropic Messages API in llama.cpp
- Users expressing enthusiasm and eagerness to try the new feature
- Practical examples and technical details provided for implementation
- Mention of compatibility with specific hardware like M3 Ultra

**Discussion Highlights:** The discussion highlights a positive reception to the new API, with users sharing practical tips and technical details for implementation. There is a consensus on the usefulness of the feature, although some users note its age and context limitations.

---

## 10. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 719 | **Comments:** 225 | **Date:** 2026-01-19

**Summary:** The Reddit post discusses the release of the zai-org/GLM-4.7-Flash model on Hugging Face, highlighting its technical features and community excitement.

**Key Points:**
- The model is a 30B parameter model with a 3B thinking component.
- It uses MLA, which reduces KV cache memory usage, enabling longer context lengths.
- The community expresses enthusiasm and anticipation for the release.
- Some users note the absence of larger models like 70B.

**Discussion Highlights:** The discussion reflects strong community interest in the model's capabilities, particularly its memory efficiency and potential for running at full 200k context. There is also nostalgia for larger models and excitement about the technical advancements.

---

## 11. [I made a Top-K implementation that's up to 20x faster than PyTorch CPU (open source)](https://reddit.com/r/LocalLLaMA/comments/1qh0yq8/i_made_a_topk_implementation_thats_up_to_20x/)

**Author:** u/andreabarbato | **Upvotes:** 142 | **Comments:** 103 | **Date:** 2026-01-19

**Summary:** The author developed an AVX2-optimized Top-K implementation that significantly outperforms PyTorch CPU, achieving up to 20x speed improvements depending on vocabulary size. Integrated into llama.cpp, it resulted in 63% faster prompt processing for a large model. The implementation uses advanced techniques like SIMD and cache optimization. Key points include the performance gains, integration into llama.cpp, and community feedback requesting clearer benchmarks. The discussion highlights strong interest and some criticism about reproducibility.

---

## 12. [how do you pronounce “gguf”?](https://reddit.com/r/LocalLLaMA/comments/1qglyqz/how_do_you_pronounce_gguf/)

**Author:** u/Hamfistbumhole | **Upvotes:** 104 | **Comments:** 154 | **Date:** 2026-01-18

**Summary:** The Reddit post discusses the pronunciation of 'gguf', with users debating whether it should be pronounced as 'jee-guff', 'giguff', or 'jee jee you eff'. The top comments suggest various pronunciations, with some users advocating for pronouncing each letter individually.

**Key Points:**
- The pronunciation of 'gguf' is debated among users.
- Suggestions include 'jee-guff', 'giguff', and 'jee jee you eff'.
- Some users prefer pronouncing each letter individually.
- A humorous comment suggests 'you don't pronounce gguf, you download it silently.'

**Discussion Highlights:** The discussion highlights a lack of consensus on the pronunciation of 'gguf', with various interpretations and humorous takes on the topic.

---

## 13. [Are most major agents really just markdown todo list processors?](https://reddit.com/r/LocalLLaMA/comments/1qgj2n9/are_most_major_agents_really_just_markdown_todo/)

**Author:** u/TheDigitalRhino | **Upvotes:** 100 | **Comments:** 38 | **Date:** 2026-01-18

**Summary:** The Reddit post discusses how major AI agents typically decompose tasks into todo lists and process them sequentially, with users sharing observations and examples of this approach.

**Key Points:**
- Major AI agents often break down tasks into smaller, manageable todo lists.
- This approach involves processing tasks one by one, similar to how humans handle complex tasks.
- Users confirm this method has been effective since earlier models like GPT-3.5.
- The discussion includes examples of task decomposition and tool usage.
- Some users humorously compare this method to simplifying complex systems into basic components.

**Discussion Highlights:** The consensus among users is that decomposing tasks into smaller, sequential steps is a common and effective strategy used by major AI agents. This method mirrors human problem-solving techniques and has been a consistent approach since earlier AI models.

---

## 14. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 340 | **Comments:** 91 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to stay within a ~10,000€ budget. The system is designed for running large language models (120B+ parameters) locally, with benchmark results showing strong performance across various models. Key points include the motivation behind the build, the hardware specifications, benchmark results, and discussion highlights such as admiration for the build and curiosity about the sourcing of components.

---

## 15. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 445 | **Comments:** 71 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses a potential slowdown in the development of Qwen 4, with the lead developer emphasizing a focus on quality over quantity. The community generally supports this approach, appreciating the commitment to improvement.

**Key Points:**
- Qwen 4 development may be slowing down to focus on quality
- Community appreciates the focus on quality over quantity
- Uncertainty about whether the statement specifically refers to Qwen 4
- Support for taking necessary time to advance the technology meaningfully

**Discussion Highlights:** The discussion highlights a positive consensus around the focus on quality, with some users cautioning against jumping to conclusions based on limited information. The community values meaningful advancements over frequent, incremental updates.

---

## 16. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 531 | **Comments:** 111 | **Date:** 2026-01-17

**Summary:** The author upgraded their server from a dual MI100 setup to a quad R9700 configuration, achieving 128GB VRAM and 128GB RAM for under $7,035. The new setup offers better prompt processing performance with minimal token generation loss, making it a cost-effective alternative to high-end GPUs like the RTX 6000 Blackwell. Key points include the server featuring 4 AMD Radeon AI PRO R9700 GPUs with 32GB VRAM each, totaling 128GB VRAM, the total cost of the build being $7,035, performance benchmarks showing strong prompt processing capabilities, and the community praising the build for its cost-effectiveness and performance. The discussion highlights include positive community responses, with comments highlighting the build's cost-effectiveness and performance, and some users joking about the financial irresponsibility of such upgrades.

---

## 17. [The Search for Uncensored AI (That Isn’t Adult-Oriented)](https://reddit.com/r/LocalLLaMA/comments/1qfq9ez/the_search_for_uncensored_ai_that_isnt/)

**Author:** u/Fun-Situation-4358 | **Upvotes:** 273 | **Comments:** 214 | **Date:** 2026-01-17

**Summary:** The post discusses the challenge of finding uncensored AI models that prioritize reasoning and creativity over adult-oriented content. The author highlights a gap in the market between heavily restricted corporate AI and shallow adult-focused models, seeking recommendations for genuinely unfiltered AI tools.

**Key Points:**
- The author seeks an AI that is uncensored and technically advanced, focusing on reasoning and creativity.
- Most models marketed as 'uncensored' are optimized for adult use rather than intelligence or depth.
- There is a perceived gap between heavily restricted corporate AI and shallow adult-focused models.
- Techniques used to decensor open-source models often reduce their intelligence.
- The Uncensored General Intelligence Leaderboard is referenced as a potential resource.

**Discussion Highlights:** The discussion highlights a shared frustration with the lack of genuinely uncensored AI models that focus on reasoning and creativity. Many users agree that most 'uncensored' models are either adult-oriented or lose intelligence when decensored. The Uncensored General Intelligence Leaderboard is suggested as a useful resource for finding such models.

---

## 18. [China's AGI-NEXT Conference (Qwen, Kimi, Zhipu, Tencent)](https://reddit.com/r/LocalLLaMA/comments/1qfmc05/chinas_aginext_conference_qwen_kimi_zhipu_tencent/)

**Author:** u/nuclearbananana | **Upvotes:** 117 | **Comments:** 22 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses China's AGI-NEXT Conference, highlighting insights on China vs US AGI development, compute resources, and marketing strategies. Key points include Qwen's internal advancements, the belief that the next AI paradigm may come from OpenAI, and cultural differences in risk-taking for innovation.

**Key Points:**
- Qwen has internally developed Qwen3.5 and context windows in the millions.
- The next AI paradigm is believed to likely come from OpenAI rather than Google.
- Chinese work culture is described as less willing to take risks for innovation.
- Deepseek is noted for its talent concentration but was absent from the conference.

**Discussion Highlights:** The discussion highlights Qwen's advancements and the belief in OpenAI's potential leadership in the next AI paradigm. There is also a note on the risk-averse culture in Chinese AI development and the absence of Deepseek despite its strong talent pool.

---

## 19. [Best "End of world" model that will run on 24gb VRAM](https://reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/)

**Author:** u/gggghhhhiiiijklmnop | **Upvotes:** 333 | **Comments:** 176 | **Date:** 2026-01-17

**Summary:** The user is seeking recommendations for the best LLM model that can run on a PC with 24GB VRAM and 64GB RAM, in preparation for a hypothetical 'end of world' scenario. The discussion includes suggestions for specific models and practical advice on data storage.

**Key Points:**
- User wants to hoard data like Wikipedia, Wiktionary, etc.
- Looking for models that fit within 24GB VRAM and 64GB RAM
- Suggestions include Gemma3:27b and practical advice on data storage
- Discussion highlights the importance of saving the best LLM possible, even if it means running it off SSD
- Mention of downloading actual Wikipedia backups for offline use

**Discussion Highlights:** The discussion emphasizes practicality, with a consensus on saving the best possible LLM and running it off SSD if necessary. Specific model recommendations include Gemma3:27b, and there is a focus on downloading comprehensive data backups like Wikipedia.

---

## 20. [KoboldCpp v1.106 finally adds MCP server support, drop-in replacement for Claude Desktop](https://reddit.com/r/LocalLLaMA/comments/1qfb0gk/koboldcpp_v1106_finally_adds_mcp_server_support/)

**Author:** u/HadesThrowaway | **Upvotes:** 100 | **Comments:** 22 | **Date:** 2026-01-17

**Summary:** KoboldCpp v1.106 introduces native MCP server support, offering a drop-in replacement for Claude Desktop with enhanced compatibility and tool integration capabilities.

**Key Points:**
- KoboldCpp v1.106 adds native MCP support, compatible with Claude Desktop configurations.
- The MCP bridge supports both HTTP and STDIO transports, enabling seamless tool integration.
- Users can manage tools from multiple servers and enable tool call approvals.
- The update is praised for its compatibility and ease of use, with positive feedback from the community.
- A guide for using MCP in KoboldCpp is available in the project's wiki.

**Discussion Highlights:** The community response is overwhelmingly positive, highlighting the significance of MCP integration and the convenience of drop-in compatibility with existing setups. Users appreciate the ease of transition and the availability of a guide for setup.

---

## 21. [DeepSeek Engram : A static memory unit for LLMs](https://reddit.com/r/LocalLLaMA/comments/1qf5oj0/deepseek_engram_a_static_memory_unit_for_llms/)

**Author:** u/Technical-Love-8479 | **Upvotes:** 321 | **Comments:** 47 | **Date:** 2026-01-17

**Summary:** DeepSeek AI introduced Engram, a memory unit for LLMs that separates remembering from reasoning, enabling O(1) knowledge lookup and improving performance without GPU limits.

**Key Points:**
- Engram introduces conditional memory, separating static knowledge from reasoning.
- Knowledge lookup is O(1), improving efficiency and performance.
- Enables massive memory scaling without GPU constraints.
- Improves reasoning, math, and code performance.
- Frees attention for global reasoning rather than static knowledge.

**Discussion Highlights:** The community is excited about the separation of memory and reasoning, highlighting the efficiency gains and scalability benefits. There is consensus on the potential for significant performance improvements and cost savings.

---

## 22. [Prompt Repetition Improves Non-Reasoning LLMs - a paper](https://reddit.com/r/LocalLLaMA/comments/1qeuh0z/prompt_repetition_improves_nonreasoning_llms_a/)

**Author:** u/Foreign-Beginning-49 | **Upvotes:** 109 | **Comments:** 51 | **Date:** 2026-01-16

**Summary:** The Reddit post discusses a paper showing that repeating prompts can significantly improve the performance of non-reasoning LLMs without affecting latency or output format. The technique is simple yet effective across various models and benchmarks.

**Key Points:**
- Prompt repetition improves non-reasoning LLM performance
- No impact on latency or output format
- Simple technique with notable gains
- Deepseek is highlighted as an open weights model tested
- Discussion highlights potential undiscovered simple techniques

**Discussion Highlights:** The discussion emphasizes the simplicity and effectiveness of the technique, with users expressing surprise at its impact and speculating about other potential undiscovered tricks. There is consensus on the value of systematic testing for such techniques.

---

## 23. [performance benchmarks (72GB VRAM) - llama.cpp server - January 2026](https://reddit.com/r/LocalLLaMA/comments/1qennp2/performance_benchmarks_72gb_vram_llamacpp_server/)

**Author:** u/jacek2023 | **Upvotes:** 111 | **Comments:** 34 | **Date:** 2026-01-16

**Summary:** The Reddit post discusses performance benchmarks for various AI models on a setup with three RTX 3090 GPUs and a Ryzen Threadripper 1920X, measuring tokens per second for each model. The author notes that the benchmarks are not scientific and focus solely on speed, not accuracy.

**Key Points:**
- Performance benchmarks for multiple AI models on a 72GB VRAM setup
- Hardware setup includes three RTX 3090 GPUs and a Ryzen Threadripper 1920X
- Tokens per second measurements for models like ERNIE-4.5-21B-A3B-Thinking-Q8_0 (147.85 tokens/s) and Qwen_Qwen3-VL-30B-A3B-Instruct-Q8_0 (131.20 tokens/s)
- Suggestions for further testing and performance improvements in the discussion
- Technical recommendations such as using the -DGGML_CUDA_PEER_COPY=ON flag for better GPU performance

**Discussion Highlights:** The discussion includes suggestions for filling the context to ~10k tokens for further testing, comparisons with other setups, and technical recommendations for improving performance, such as using specific compilation flags for direct GPU data copying.

---

## 24. [I reproduced DeepSeek's mHC at 1.7B params (8xH100). The instability is 3x worse than reported (10k vs 3k), but the model didn't explode.](https://reddit.com/r/LocalLLaMA/comments/1qek917/i_reproduced_deepseeks_mhc_at_17b_params_8xh100/)

**Author:** u/poisson_labs | **Upvotes:** 176 | **Comments:** 29 | **Date:** 2026-01-16

**Summary:** The author reproduced DeepSeek's mHC at 1.7B parameters and found that the instability was 3x worse than reported, with signal amplification of 10,924x. Despite this, the model continued learning, and the author verified that Manifold Hyper-Connections (mHC) with Sinkhorn projection solves the issue with zero compute overhead.

**Key Points:**
- Instability at 1.7B parameters was 3x worse than reported (10,924x signal amplification).
- The model continued learning despite high signal amplification, possibly due to modern optimizers and gradient clipping.
- Manifold Hyper-Connections (mHC) with Sinkhorn projection solves the instability issue with zero compute overhead.
- The author provided detailed breakdowns and loss curves in external links.
- Discussion highlights include skepticism about zero compute overhead and interest in alternative optimizers like muon.

**Discussion Highlights:** The discussion includes skepticism about the claim of zero compute overhead, interest in alternative optimizers like muon, and appreciation for the resourcefulness of DeepSeek's work.

---

## 25. [Maxsun joins Sparkle in making Intel Arc B60 Pro GPUs available to regular consumers, with up to 48GB VRAM](https://reddit.com/r/LocalLLaMA/comments/1qehf0p/maxsun_joins_sparkle_in_making_intel_arc_b60_pro/)

**Author:** u/reps_up | **Upvotes:** 138 | **Comments:** 50 | **Date:** 2026-01-16

**Summary:** Maxsun and Sparkle are making Intel Arc B60 Pro GPUs available to regular consumers, featuring up to 48GB VRAM. The Reddit discussion highlights user interest in higher VRAM capacities and support for frameworks like torch/JAX/ONNX.

**Key Points:**
- Intel Arc B60 Pro GPUs now available to consumers
- Up to 48GB VRAM capacity
- User interest in support for torch/JAX/ONNX frameworks
- Requests for higher VRAM capacities (e.g., 128GB)
- Inquiries about availability in Europe

**Discussion Highlights:** Users expressed strong interest in higher VRAM capacities, with one comment suggesting 128GB as a desirable option. There were also questions about the support for popular machine learning frameworks like torch, JAX, and ONNX, as well as inquiries about purchasing options in Europe.

---

## 26. [GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)](https://reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/)

**Author:** u/CuriousPlatypus1881 | **Upvotes:** 376 | **Comments:** 89 | **Date:** 2026-01-16

**Summary:** The post discusses the updated SWE-bench leaderboard results from December 2025, highlighting the performance of various models on 48 fresh GitHub PR tasks. Claude Opus 4.5 leads with a 63.3% resolved rate, followed closely by GPT-5.2 at 61.5%. The discussion emphasizes the strong showing of open-source models like GLM-4.7 and the surprising performance of Gemini 3 Flash Preview. Key points include the leadership of Claude Opus 4.5, the close performance of GPT-5.2, the outperformance of Gemini 3 Flash Preview over Gemini 3 Pro Preview, the strength of GLM-4.7 as an open-source model, and the community's excitement for future releases like DeepSeek v4. The discussion highlights the credibility of the benchmark and the enthusiasm around open-source models.

---

## 27. [I fucking love this community](https://reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/)

**Author:** u/alhinai_03 | **Upvotes:** 490 | **Comments:** 54 | **Date:** 2026-01-16

**Summary:** The author expresses gratitude towards the open-source community for enabling them to run large language models on a 10-year-old PC with limited hardware. They highlight the impressive performance of models like nemotron-3-nano-30B-a3b-iq4_nl, achieving 14-13.5 tokens per second with a 65k context on a system with only 4GB of VRAM.

**Key Points:**
- Gratitude towards the open-source community for their contributions
- Running large models on older hardware with impressive performance
- Importance of system memory and MoE architecture for decent performance
- Specific performance metrics: 14-13.5 t/s with 65k context on a 10-year-old PC
- Community appreciation and recognition for the author's contribution

**Discussion Highlights:** The community appreciates the author's post, with comments highlighting the impressive optimization achievements and the practicality of using system RAM with MoE models. There is also interest in learning more about running large models on limited hardware.

---

## 28. [New FLUX.2 [Klein] 9B is INSANELY Fast](https://reddit.com/r/LocalLLaMA/comments/1qe9xfi/new_flux2_klein_9b_is_insanely_fast/)

**Author:** u/Lopsided_Dot_4557 | **Upvotes:** 105 | **Comments:** 26 | **Date:** 2026-01-16

**Summary:** The Reddit post highlights the new FLUX.2 [Klein] 9B model by Black Forest Labs, praising its speed and efficiency in text-to-image tasks. It achieves sub-second inference on RTX 4090 hardware and matches the performance of larger models with 9B parameters. The model is step-distilled from 50 to 4 steps without quality loss and supports unified text-to-image and multi-reference editing. Key points include sub-second inference, 9B parameters matching larger models, step-distillation without quality loss, unified text-to-image capabilities, and efficient GPU usage. The discussion highlights the model's speed and efficiency, with users appreciating its performance and lower GPU usage, though some minor issues like occasional image artifacts are noted.

---

## 29. [Dang, M2 drives are the new DDR5 apparently.](https://reddit.com/r/LocalLLaMA/comments/1qe4so5/dang_m2_drives_are_the_new_ddr5_apparently/)

**Author:** u/Porespellar | **Upvotes:** 214 | **Comments:** 97 | **Date:** 2026-01-15

**Summary:** The Reddit post discusses the significant increase in prices of M2 drives, with users expressing frustration over the rising costs and sharing their experiences with price hikes.

**Key Points:**
- M2 drive prices have increased significantly.
- Users are frustrated with the rising costs.
- Some users have experienced price hikes on drives they purchased recently.
- Users are considering keeping old PCs as backups due to the high costs.
- The price increases are seen as a trend similar to the rise in DDR5 prices.

**Discussion Highlights:** The discussion highlights a consensus among users about the frustration and concern over the rising prices of M2 drives, with many sharing personal experiences of price hikes and considering alternatives like keeping old PCs as backups.

---

## 30. [My story of underestimating /r/LocalLLaMA's thirst for VRAM](https://reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/)

**Author:** u/EmPips | **Upvotes:** 1301 | **Comments:** 88 | **Date:** 2026-01-15

**Summary:** The post highlights the author's underestimation of the subreddit's demand for VRAM, with discussions focusing on hardware recommendations and market dynamics.

**Key Points:**
- Author underestimated VRAM demand
- Hardware recommendations (3090s, R9700)
- Market behavior (selling cards)
- Community engagement (Discord feature)

**Discussion Highlights:** The discussion includes hardware advice, market strategies, and community engagement, with a focus on optimizing VRAM usage and hardware choices.

---

## 31. [Latest upgrade…A100 40 GB](https://reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/)

**Author:** u/inserterikhere | **Upvotes:** 404 | **Comments:** 54 | **Date:** 2026-01-15

**Summary:** The user upgraded their gaming rig to an AI rig by repurposing old parts and purchasing a faulty A100 GPU, which surprisingly worked perfectly. The community reacted positively, with some expressing concerns about cooling the A100.

**Key Points:**
- User transitioned from a gaming rig to an AI rig using repurposed parts.
- Purchased a faulty A100 GPU for $1000, which worked without issues.
- Community provided advice on cooling the A100 and celebrated the user's success.
- Post gained significant traction with 404 upvotes and 54 comments.

**Discussion Highlights:** The community was largely supportive, with some users offering technical advice on cooling the A100 GPU. The post was well-received, as indicated by the high number of upvotes and comments.

---

## 32. [Not as impressive as most here, but really happy I made it in time!](https://reddit.com/r/LocalLLaMA/comments/1qdtvgs/not_as_impressive_as_most_here_but_really_happy_i/)

**Author:** u/Kahvana | **Upvotes:** 144 | **Comments:** 44 | **Date:** 2026-01-15

**Summary:** The Reddit post discusses the author's experience building a PC in the Netherlands, highlighting challenges with GPU availability and their successful setup with two RTX 5060 Ti GPUs. The discussion includes questions about CPU upgrades, comments on the build's tidiness, and discussions about GPU performance and motherboard recommendations.

**Key Points:**
- GPU availability in the Netherlands is challenging, with the author recommending calling stores for stock updates.
- The build features two RTX 5060 Ti GPUs, a Ryzen 5 9600X CPU, and 96GB of DDR5 RAM.
- Discussion includes questions about CPU upgrades for inference speed and recommendations for motherboards that support dual GPUs.
- Comments highlight the build's performance and cost-effectiveness compared to high-end alternatives.

**Discussion Highlights:** The discussion focuses on potential CPU upgrades for better inference performance, the tidiness of the build, and recommendations for motherboards that can effectively utilize dual GPUs. There is also praise for the build's cost-effectiveness and performance.

---

## 33. [Nemotron-3-nano:30b is a spectacular general purpose local LLM](https://reddit.com/r/LocalLLaMA/comments/1qdrf3o/nemotron3nano30b_is_a_spectacular_general_purpose/)

**Author:** u/DrewGrgich | **Upvotes:** 217 | **Comments:** 125 | **Date:** 2026-01-15

**Summary:** The Reddit post praises Nemotron-3-nano:30b for its exceptional performance as a general-purpose local LLM, noting its intelligence and superior reasoning quality compared to larger models like Llama 3.3:70b. Users highlight its effectiveness for research and analysis, despite its robotic tone.

**Key Points:**
- Nemotron-3-nano:30b is highly intelligent and performs well for general-purpose tasks.
- It outperforms larger models like Llama 3.3:70b in reasoning quality.
- The robotic tone is seen as a feature for research and analysis purposes.
- Users are looking forward to the upcoming Nemotron 3 super (100b) model.
- Some users prefer Qwen3-vl-30b-a3b-instruct for its vision-language capabilities.

**Discussion Highlights:** The discussion highlights the model's impressive reasoning capabilities and its suitability for research and analysis tasks. Users also express anticipation for future models and compare it with other LLMs like Qwen3-vl-30b-a3b-instruct.

---

## 34. [Thanks to you guys, Soprano TTS now supports OpenAI-compatible endpoint, ONNX, ComfyUI, WebUI, and CLI on CUDA, MPS, ROCm, and CPU!](https://reddit.com/r/LocalLLaMA/comments/1qdpb2v/thanks_to_you_guys_soprano_tts_now_supports/)

**Author:** u/eugenekwek | **Upvotes:** 106 | **Comments:** 26 | **Date:** 2026-01-15

**Summary:** The Reddit post announces major updates to Soprano TTS, including support for OpenAI-compatible endpoints, ONNX, ComfyUI, WebUI, and CLI across various hardware platforms like CUDA, MPS, ROCm, and CPU. The author thanks the community for their contributions and highlights several new features and integrations.

**Key Points:**
- Soprano TTS now supports multiple inference methods and hardware platforms.
- Community contributions have added WebUI, CLI, OpenAI-compatible endpoints, ONNX, and ComfyUI support.
- Additional features include an automatic hallucination detector and transformers streaming support.
- The post highlights the importance of community collaboration in improving the project.
- Discussion includes questions about performance comparisons, finetuning plans, and hardware compatibility.

**Discussion Highlights:** The discussion includes inquiries about Soprano's performance compared to other TTS models, plans for finetuning support, and the significance of local TTS for accessibility and privacy. There is also a humorous comment about the hallucination detector's variable naming.

---

## 35. [google/translategemma](https://reddit.com/r/LocalLLaMA/comments/1qdok2i/googletranslategemma/)

**Author:** u/BreakfastFriendly728 | **Upvotes:** 173 | **Comments:** 56 | **Date:** 2026-01-15

**Summary:** The Reddit post discusses Google's TranslateGemma model, highlighting its technical report and Hugging Face collection. The discussion focuses on the model's training data, context limitations, and availability of GGUF format. Key points include the model's use of 4.3 billion tokens during SFT and 10.2 million tokens during reinforcement learning, its 2K token context limit, user requests for comparisons to other models, demand for GGUF format, and questions about setting language codes for chat completions. The discussion highlights concerns about the model's context limitations and the lack of comparisons to other models, with users expressing interest in the GGUF format and seeking guidance on using the model with specific tools like Koboldcpp and llama.cpp server.

---

## 36. [7x Longer Context Reinforcement Learning in Unsloth](https://reddit.com/r/LocalLLaMA/comments/1qdna3t/7x_longer_context_reinforcement_learning_in/)

**Author:** u/danielhanchen | **Upvotes:** 249 | **Comments:** 28 | **Date:** 2026-01-15

**Summary:** Unsloth introduces techniques enabling 7x longer context lengths for Reinforcement Learning, allowing training of models like gpt-oss 20b QLoRA with up to 20K context on a 24GB card without accuracy loss. The post highlights compatibility with various models and GPUs, and mentions additional features like weight-sharing and Flex Attention.

**Key Points:**
- Unsloth enables 7x longer context lengths for RL, supporting up to 20K context on a 24GB card.
- Supports larger GPUs with up to 380K context on a 192GB NVIDIA B200 GPU.
- Features like weight-sharing, Flex Attention, and Float8 training are combinable for enhanced performance.
- Free Colab notebooks and educational resources are provided for implementation.
- Community engagement includes Discord features and special flairs for contributions.

**Discussion Highlights:** The community shows enthusiasm for the advancements, with comments highlighting the rapid progress and practical applications. Some users inquire about data sources for long-context training and compatibility with specific models like Qwen3 30B-3A.

---

## 37. [RTX 5070 Ti and RTX 5060 Ti 16 GB no longer manufactured](https://reddit.com/r/LocalLLaMA/comments/1qdh28f/rtx_5070_ti_and_rtx_5060_ti_16_gb_no_longer/)

**Author:** u/Paramecium_caudatum_ | **Upvotes:** 235 | **Comments:** 95 | **Date:** 2026-01-15

**Summary:** Nvidia has reduced supply of RTX 5070 Ti and RTX 5060 Ti 16 GB due to memory shortages, leading to price increases and limited availability. The 8 GB configuration of RTX 5060 Ti remains unaffected.

**Key Points:**
- RTX 5070 Ti and RTX 5060 Ti 16 GB supply significantly reduced due to memory shortages
- Prices for RTX 5070 Ti have increased by ~$100 over MSRP
- Most AIBs will no longer manufacture these GPUs
- 8 GB configuration of RTX 5060 Ti remains available
- Users express frustration and share their experiences with the affected GPUs

**Discussion Highlights:** Users express disappointment over the supply issues and price hikes, with some sharing their recent purchases and experiences. There is a consensus that the situation has disrupted upgrade plans and increased costs for consumers.

---

## 38. [LFM 2.5 is insanely good](https://reddit.com/r/LocalLLaMA/comments/1qdax6z/lfm_25_is_insanely_good/)

**Author:** u/guiopen | **Upvotes:** 107 | **Comments:** 33 | **Date:** 2026-01-14

**Summary:** The Reddit post highlights the impressive performance of LFM 2.5, a ~1B parameter model that performs comparably to larger models like Llama 2 7B and Llama 3 8B. The author praises its effectiveness in Portuguese and its utility for basic QA and summarization tasks.

**Key Points:**
- LFM 2.5 is highly effective for its size, performing comparably to larger models.
- It excels in Portuguese despite not being officially supported.
- The model is useful for basic QA and summarization tasks.
- Performance varies with quantization levels (e.g., Q6 vs. Q8).
- Some users report issues with data extraction and summarization tasks.

**Discussion Highlights:** The discussion highlights a mix of positive and negative experiences, with some users praising the model's accuracy and strength for its size, while others note limitations in summarization and data extraction tasks. There is a consensus that the model performs well for basic QA when provided with sufficient context.

---

## 39. [I trained a model to 'unslop' AI prose](https://reddit.com/r/LocalLLaMA/comments/1qd88v2/i_trained_a_model_to_unslop_ai_prose/)

**Author:** u/N8Karma | **Upvotes:** 202 | **Comments:** 71 | **Date:** 2026-01-14

**Summary:** The author trained a model to reverse the 'enslopping' effect of AI-generated prose, creating a tool that can restore human-like quality to AI-generated text. The model, called Unslopper, is open-source and has shown promising results in making AI-generated text more readable and human-like.

**Key Points:**
- The model was trained to reverse the 'enslopping' effect of AI-generated prose.
- Unslopper can fool AI detectors like Pangram, indicating improved human-like quality.
- The model is open-source and available on Hugging Face.
- The goal is to improve readability of AI-generated text, not to deceive.
- The community response has been largely positive, with some constructive criticism.

**Discussion Highlights:** The discussion highlights the innovative approach of the model, with users appreciating the improved readability of AI-generated text. Some users drew parallels to diffusion models, while others expressed skepticism about the training data size.

---

## 40. [Zhipu AI breaks US chip reliance with first major model trained on Huawei stack (GLM-Image)](https://reddit.com/r/LocalLLaMA/comments/1qd6nho/zhipu_ai_breaks_us_chip_reliance_with_first_major/)

**Author:** u/fallingdowndizzyvr | **Upvotes:** 415 | **Comments:** 45 | **Date:** 2026-01-14

**Summary:** Zhipu AI has developed GLM-Image 9B, a major AI model trained on Huawei hardware, marking a significant step in reducing reliance on US chips. The development is seen as a response to the Chinese ban on Nvidia, with discussions highlighting both its technological significance and current limitations.

**Key Points:**
- Zhipu AI's GLM-Image 9B is trained on Huawei hardware, reducing US chip dependence.
- The Chinese ban on Nvidia is driving local innovation in AI hardware.
- Rapid advancements in AI models are noted, with GLM-Image 9B being a notable milestone.
- Mixed reception on the model's output quality, with some viewing it as a tech demo.

**Discussion Highlights:** The discussion highlights the significance of Zhipu AI's achievement in breaking US chip reliance, with many seeing it as a response to the Nvidia ban. While the model's outputs are currently not highly rated, the technological milestone is acknowledged as a step towards larger, more advanced models in the future.

---

## 41. [Popularity of DDR3 motherboards is growing rapidly - VideoCardz.com](https://reddit.com/r/LocalLLaMA/comments/1qcvk9n/popularity_of_ddr3_motherboards_is_growing/)

**Author:** u/FullstackSensei | **Upvotes:** 147 | **Comments:** 66 | **Date:** 2026-01-14

**Summary:** The author expresses frustration over rising DDR4 RAM prices and fears that DDR3 prices may also skyrocket, making homelabbing difficult. The discussion highlights a shift towards reusing and recycling older hardware due to stagnant consumer hardware evolution.

**Key Points:**
- Author's frustration with rising DDR4 prices and potential DDR3 price hikes
- Impact on homelabbing and local LLM setups
- Shift towards reusing and recycling older hardware
- Historical context of RAM price cycles and market expectations
- Personal experiences with DDR3 systems and upgrades

**Discussion Highlights:** The discussion reflects a consensus on the growing trend of reusing older hardware due to stagnant consumer hardware evolution and high prices. Many users share personal experiences and market observations, indicating a shift towards recycling and repurposing older components.

---

## 42. [NeuTTS Nano: 120M Parameter On-Device TTS based on Llama3](https://reddit.com/r/LocalLLaMA/comments/1qcv304/neutts_nano_120m_parameter_ondevice_tts_based_on/)

**Author:** u/TeamNeuphonic | **Upvotes:** 213 | **Comments:** 44 | **Date:** 2026-01-14

**Summary:** Neuphonic has released NeuTTS Nano, a 120M parameter TTS model based on Llama3, designed for on-device deployment with ultra-lightweight specifications and instant voice cloning capabilities.

**Key Points:**
- NeuTTS Nano is a 120M parameter TTS model, 3x smaller than NeuTTS Air.
- Built on Llama3 with a simple LM + codec architecture, optimized for mobile and embedded devices.
- Supports instant voice cloning with a 3-second sample and realistic prosody.
- Community interest in multilingual support, particularly for European languages.
- Mixed feedback on voice quality, with some users finding it unnatural.

**Discussion Highlights:** The community shows strong interest in multilingual capabilities, especially for European languages, and requests for finetuning options. Some users express concerns about voice naturalness, while others are excited about the lightweight design for embedded applications.

---

## 43. [Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M](https://reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/)

**Author:** u/eugenekwek | **Upvotes:** 324 | **Comments:** 54 | **Date:** 2026-01-14

**Summary:** The post announces Soprano 1.1, an improved version of the Soprano TTS model with significant reductions in hallucinations and audio artifacts, along with enhanced stability and support for longer sentences. The community response is overwhelmingly positive, with users praising the model's performance and expressing interest in future developments.

**Key Points:**
- Soprano 1.1 reduces hallucinations by 95% and lowers WER by 50% compared to the previous version.
- The model now supports sentences up to 30 seconds long, doubling the previous limit.
- Audio artifacts and high-frequency noise have been significantly reduced through further training.
- A blind study showed a 63% preference rate for Soprano 1.1 over the original model.
- The community appreciates the improvements and inquires about future support like ONNX compatibility.

**Discussion Highlights:** The discussion highlights strong community approval, with users expressing surprise at the model's quality for its size (80M parameters). There is interest in additional features like ONNX support, and the developer's efforts are widely praised.

---

## 44. [NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency](https://reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/)

**Author:** u/Fear_ltself | **Upvotes:** 714 | **Comments:** 129 | **Date:** 2026-01-14

**Summary:** NVIDIA's Orchestrator-8B is an 8-billion-parameter AI model designed to manage and route complex tasks to various tools for greater efficiency, sparking discussions about the future of AI systems and their integration.

**Key Points:**
- Orchestrator-8B is a specialized 8B model for task management and routing.
- The model aims to enhance efficiency by connecting with other tools and models.
- Discussions highlight the potential of such systems in achieving functional AI integration.
- Comparisons to middle management and existing frameworks were noted.
- The post gained significant attention with 714 upvotes and 129 comments.

**Discussion Highlights:** The discussion emphasized the importance of integrating various AI tools and models for enhanced functionality. Some users humorously compared the model to a 'middle manager,' while others noted its potential in creating hierarchical AI systems. The overall consensus was positive, with many seeing this as a step towards more efficient and functional AI systems.

---

## 45. [Which are the top LLMs under 8B right now?](https://reddit.com/r/LocalLLaMA/comments/1qcl543/which_are_the_top_llms_under_8b_right_now/)

**Author:** u/Additional_Secret_75 | **Upvotes:** 177 | **Comments:** 108 | **Date:** 2026-01-14

**Summary:** The post discusses the best LLMs under 8B parameters for local use, focusing on models suitable for chat, research, and coding with low VRAM requirements. Users share their experiences and recommendations for various models.

**Key Points:**
- Qwen3 4B and Qwen3 8B are highlighted for their performance in the under 8B range.
- Gemma-3n-E4B is praised for its reasoning and multimodal capabilities.
- Models like Nanbeige3B are also mentioned as viable options.
- The discussion emphasizes the importance of low VRAM usage and versatility.

**Discussion Highlights:** The consensus leans towards Qwen3 and Gemma-3n-E4B as top performers in the under 8B category, with a focus on their capabilities in chat, research, and coding tasks. Users also appreciate models that are not overly censored and run efficiently on limited hardware.

---

