# r/LocalLLaMA Reading Digest

**Period:** 2025-12-18 to 2025-12-18
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster](https://reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/)

**Author:** u/geerlingguy | **Upvotes:** 316 | **Comments:** 94 | **Date:** 2025-12-18

**Summary:** The post discusses testing Kimi K2's performance on a cluster of 4x Mac Studios using RDMA Tensor settings, highlighting the challenges in benchmarking and the potential for further testing.

**Key Points:**
- Testing Kimi K2 on a 4x Mac Studio cluster with RDMA Tensor settings
- Challenges in benchmarking due to lack of tools like llama-bench in Exo
- Potential for further testing before returning the loaned equipment
- Discussion includes links to additional data and resources
- Community appreciation for the contribution and testing efforts

**Discussion Highlights:** The discussion highlights the community's interest in the testing results and the challenges faced in benchmarking. There is appreciation for the author's efforts and a desire for more detailed comparisons and data.

---

## 2. [Google's Gemma models family](https://reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/)

**Author:** u/jacek2023 | **Upvotes:** 410 | **Comments:** 101 | **Date:** 2025-12-18

**Summary:** The Reddit post discusses Google's Gemma models family, highlighting the introduction of FunctionGemma, a model intended for fine-tuning specific function-calling tasks, including multi-turn use cases. The community shows enthusiasm and humor about the new models.

**Key Points:**
- Introduction of FunctionGemma for fine-tuning function-calling tasks
- Community enthusiasm and humor about the new models
- Mention of 323 visible models in the collection, suggesting potential new additions
- Positive reception and special recognition for the post's contribution

**Discussion Highlights:** The discussion highlights the excitement around FunctionGemma and its potential applications. Users also humorously note the pattern of jokes becoming reality in the community. There is speculation about new Gemma models based on the number of visible models in the collection.

---

## 3. [MiraTTS: High quality and fast TTS model](https://reddit.com/r/LocalLLaMA/comments/1pper90/miratts_high_quality_and_fast_tts_model/)

**Author:** u/SplitNice1982 | **Upvotes:** 131 | **Comments:** 50 | **Date:** 2025-12-17

**Summary:** MiraTTS is a high-quality, fast TTS model that generates realistic 48khz speech at 100x realtime, optimized for efficiency and low latency. It supports multilingual versions and is memory-efficient, working with GPUs as low as 6GB VRAM.

**Key Points:**
- Generates speech at 100x realtime
- High-quality 48khz speech
- Memory-efficient with 6GB VRAM support
- Low latency as low as 150ms
- Multilingual support in progress

**Discussion Highlights:** The discussion highlights curiosity about multilingual support, voice cloning, and comparisons with other TTS models like KaniTTS. Users appreciate the fast releases and express interest in trying the model, though some note hardware limitations.

---

## 4. [AMA with the Meta researchers behind SAM 3 + SAM 3D + SAM Audio](https://reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/)

**Author:** u/AIatMeta | **Upvotes:** 122 | **Comments:** 65 | **Date:** 2025-12-17

**Summary:** The post announces an AMA with Meta researchers behind SAM 3, SAM 3D, and SAM Audio, highlighting their capabilities and providing links to learn more. The discussion includes questions about functionality, use cases, and comparisons with other tools.

**Key Points:**
- Introduction of SAM 3, SAM 3D, and SAM Audio by Meta researchers
- AMA session scheduled for December 18, 2-3pm PT
- Discussion on segmentation capabilities, voice separation, and stem creation
- Questions about architectural similarities and comparisons with other tools like Demucs
- Interest in integrating SAM models for home assistant and real-time voice identification

**Discussion Highlights:** Users are curious about the practical applications and limitations of SAM models, particularly in real-world scenarios like voice separation and image segmentation. There is also interest in comparing SAM Audio with existing tools for stem creation and karaoke versions of music.

---

## 5. [Nvidia plans heavy cuts to GPU supply in early 2026](https://reddit.com/r/LocalLLaMA/comments/1pp8vo4/nvidia_plans_heavy_cuts_to_gpu_supply_in_early/)

**Author:** u/HumanDrone8721 | **Upvotes:** 333 | **Comments:** 171 | **Date:** 2025-12-17

**Summary:** Nvidia plans to significantly reduce GPU supply in early 2026, which, combined with similar cuts by Micron and Samsung, could impact gaming PC builds and market competition.

**Key Points:**
- Nvidia's GPU supply cuts in early 2026
- Micron and Samsung also reducing consumer RAM and SSD production
- Potential challenges for gaming PC builders in 2026
- Concerns about reduced competition and innovation
- Criticism of corporate focus on stock buybacks over growth

**Discussion Highlights:** The discussion highlights concerns about the impact on gaming PC builds, potential for new market competition, and criticism of corporate practices like stock buybacks over R&D investment.

---

## 6. [Hey, LocalLLaMa. We need to talk...](https://reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/)

**Author:** u/Eisenstein | **Upvotes:** 383 | **Comments:** 124 | **Date:** 2025-12-17

**Summary:** The post emphasizes the importance of community engagement and support for contributors in the r/LocalLLaMA subreddit, urging members to provide feedback and upvotes to encourage continued contributions.

**Key Points:**
- Contributors need upvotes and feedback to feel valued and continue sharing their work.
- Constructive criticism is essential for growth, even for projects that may not be perfect.
- Community engagement, beyond just entertainment, is crucial for the subreddit's health.
- Mixed reactions in comments, with some supporting the sentiment and others criticizing low-quality projects.

**Discussion Highlights:** The discussion highlights a divide in the community, with some members appreciating the call for engagement and others expressing frustration with low-quality or AI-generated projects.

---

## 7. [Drummer's Cydonia and Magidonia 24B v4.3 - The best pair of Cydonia for RP yet!](https://reddit.com/r/LocalLLaMA/comments/1pp2j60/drummers_cydonia_and_magidonia_24b_v43_the_best/)

**Author:** u/TheLocalDrummer | **Upvotes:** 128 | **Comments:** 20 | **Date:** 2025-12-17

**Summary:** The post announces the release of Drummer's Cydonia and Magidonia 24B v4.3 models, described as the best pair for role-playing yet, with links to their respective repositories. The author expresses gratitude to patrons for their support and mentions a recent difficult choice made possible by their backing. Key points include the release of the models, their praise for role-playing, the author's gratitude, and community feedback highlighting model quality and usage tips. The discussion highlights the community's appreciation and additional usage tips.

---

## 8. [Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds.](https://reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/)

**Author:** u/themixtergames | **Upvotes:** 1117 | **Comments:** 127 | **Date:** 2025-12-17

**Summary:** Apple has introduced SHARP, a model capable of generating photorealistic 3D Gaussian representations from a single image in seconds. The model is highlighted for its speed and compatibility with Apple devices like the MacBook Pro M1 Max and Apple Vision Pro.

**Key Points:**
- SHARP generates 3D Gaussian representations from a single image in seconds.
- The model is optimized for Apple hardware, including MacBook Pro M1 Max and Apple Vision Pro.
- The GitHub repository and research paper are provided for further exploration.
- Community discussion includes comparisons to cyberpunk's braindance and inquiries about content compatibility.
- The model's performance is demonstrated through real-time rendering on Apple Vision Pro.

**Discussion Highlights:** The community discussion highlights enthusiasm for the model's capabilities, with comparisons to cyberpunk's braindance and questions about its applicability to various types of content. The top comments also note the model's performance on Apple hardware and its potential applications.

---

## 9. [LangChain and LlamaIndex are in "steep decline" according to new ecosystem report. Anyone else quietly ditching agent frameworks?](https://reddit.com/r/LocalLLaMA/comments/1pox733/langchain_and_llamaindex_are_in_steep_decline/)

**Author:** u/Exact-Literature-395 | **Upvotes:** 206 | **Comments:** 57 | **Date:** 2025-12-17

**Summary:** The Reddit post discusses the decline of LangChain and LlamaIndex frameworks, citing reduced community activity and investment. Users share experiences of simplifying their codebases by removing these frameworks and calling APIs directly, questioning the necessity of agent frameworks with improved base models.

**Key Points:**
- LangChain, LlamaIndex, and AutoGen are listed as 'steepest declining' projects by community activity.
- Users report simplifying codebases and improving debugging by removing LangChain abstractions.
- Criticism of LangChain includes bloated features, poor security/performance, and non-pythonic design.
- LlamaIndex maintainer acknowledges community growth due to ease of integration and breadth of features.
- Discussion suggests a shift towards simpler, more direct API usage over complex frameworks.

**Discussion Highlights:** The discussion highlights a consensus that agent frameworks like LangChain and LlamaIndex may be becoming obsolete as base models improve. Users express frustration with framework complexity and prefer direct API usage for simplicity and better debugging. Some defend the frameworks for their integration capabilities, but the overall sentiment leans towards a decline in their necessity.

---

## 10. [anthropic blog on code execution for agents. 98.7% token reduction sounds promising for local setups](https://reddit.com/r/LocalLLaMA/comments/1powhy6/anthropic_blog_on_code_execution_for_agents_987/)

**Author:** u/Zestyclose_Ring1123 | **Upvotes:** 133 | **Comments:** 33 | **Date:** 2025-12-17

**Summary:** Anthropic's blog discusses a new approach to code execution for agents, claiming a 98.7% token reduction, which could significantly benefit local setups by reducing context limits and improving privacy. The approach involves letting models explore tools on demand rather than preloading all tool definitions.

**Key Points:**
- Anthropic's method reduces token usage by 98.7%, making it promising for local models with limited context.
- The approach involves model-generated code to orchestrate tools, improving privacy by keeping sensitive data out of model context.
- Sandboxing is a major challenge for running model-generated code locally.
- Similar patterns already exist in projects like HF's smolagents and Cloudflare's independent discovery of 'code mode'.
- The method could enable complex agents on consumer hardware with smaller context windows.

**Discussion Highlights:** The discussion highlights that similar patterns already exist in other projects like smolagents, with some users experimenting with DAG-based approaches to reduce sandboxing needs. There is also mention of independent discoveries of this pattern by Cloudflare.

---

## 11. [Peak LLM Wars: Xiaomi Blocks Kimi Employees on Twitter](https://reddit.com/r/LocalLLaMA/comments/1pow797/peak_llm_wars_xiaomi_blocks_kimi_employees_on/)

**Author:** u/nekofneko | **Upvotes:** 129 | **Comments:** 26 | **Date:** 2025-12-17

**Summary:** The Reddit post discusses a conflict between Xiaomi and Kimi, with Xiaomi blocking Kimi employees on Twitter, highlighting the ongoing 'LLM wars'. The post includes images and comments that add context and humor to the situation.

**Key Points:**
- Xiaomi blocks Kimi employees on Twitter, escalating the 'LLM wars'.
- Users comment on the intensity of the rivalry and make humorous comparisons.
- Speculation about former DeepSeek members in Xiaomi and comparisons to other tech rivalries.
- The discussion includes a humorous comparison to VTuber drama.

**Discussion Highlights:** The discussion highlights the rivalry between Xiaomi and Kimi, with users making humorous comparisons and speculating about industry dynamics. The tone is lighthearted but acknowledges the intensity of the 'LLM wars'.

---

## 12. [Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model](https://reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 1131 | **Comments:** 119 | **Date:** 2025-12-17

**Summary:** Microsoft's TRELLIS 2-4B is an open-source image-to-3D model with 4 billion parameters, using Flow-Matching Transformers with Sparse Voxel based 3D VAE to convert single images into 3D assets. The model has been well-received, with mixed reviews on its practical utility.

**Key Points:**
- Model Type: Flow-Matching Transformers with Sparse Voxel based 3D VAE
- Parameters: 4 Billion
- Input: Single Image, Output: 3D Asset
- Mixed reviews on practical utility
- Community appreciation and special flair for the contributor

**Discussion Highlights:** The community discussion highlights mixed reactions, with some users praising the model's performance and others expressing skepticism about its practical applications. There is also a suggestion for improving the model by allowing a series of images as input.

---

## 13. [QwenLong-L1.5: Revolutionizing Long-Context AI](https://reddit.com/r/LocalLLaMA/comments/1pokpha/qwenlongl15_revolutionizing_longcontext_ai/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 209 | **Comments:** 27 | **Date:** 2025-12-16

**Summary:** QwenLong-L1.5 is a new AI model achieving state-of-the-art long-context reasoning with novel data synthesis, stabilized RL, and memory management for contexts up to 4M tokens.

**Key Points:**
- Achieves SOTA long-context reasoning
- Uses novel data synthesis and stabilized RL
- Supports contexts up to 4M tokens
- Integration with llama.cpp may require work
- Exact query template is crucial for optimal performance

**Discussion Highlights:** The discussion highlights concerns about graph visuality, integration challenges with llama.cpp, and the importance of using the exact query template for optimal performance.

---

## 14. [8x Radeon 7900 XTX Build for Longer Context Local Inference - Performance Results &amp; Build Details](https://reddit.com/r/LocalLLaMA/comments/1pogwb6/8x_radeon_7900_xtx_build_for_longer_context_local/)

**Author:** u/Beautiful_Trust_8151 | **Upvotes:** 715 | **Comments:** 210 | **Date:** 2025-12-16

**Summary:** The post details a custom multi-GPU setup using 8x AMD Radeon 7900 XTX cards for local AI inference, achieving 192 GB VRAM and stable performance with a 131072-token context window. The build cost around $6-7k and offers flexibility and long-context capability for specific work use cases.

**Key Points:**
- 8x AMD Radeon 7900 XTX GPUs provide 192 GB VRAM for local AI inference
- Performance testing shows stable results with a 131072-token context window
- Total build cost is around $6-7k, offering flexibility and long-context capability
- System consumes about 900 watts during prompt processing and inferencing
- Discussion highlights appreciation for the build and suggestions for further optimization

**Discussion Highlights:** The discussion highlights appreciation for the build's capabilities and suggestions for further optimization, such as switching to Linux, ROCm, and vLLM for potentially better performance. The community also expressed admiration for the build's cost-effectiveness and performance.

---

## 15. [Nemotron 3 Nano 30B is Amazing! (TLDR)](https://reddit.com/r/LocalLLaMA/comments/1pocsdy/nemotron_3_nano_30b_is_amazing_tldr/)

**Author:** u/DonkeyBonked | **Upvotes:** 200 | **Comments:** 125 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses the author's experience with Nemotron 3 Nano 30B, highlighting its impressive token efficiency and performance on their hardware setup. The discussion includes comparisons with other models like Qwen 3 and Devstral 2 Small, as well as user experiences and use cases. Key points include the model's high token efficiency, performance on the author's hardware setup, comparisons with other models, user highlights of the model's speed and open-source nature, and suggestions for testing other models like IBM Granite 4 Hybrid Small. The discussion highlights the model's efficiency and performance, with users sharing their experiences and comparisons with other models. There is a general consensus on the model's capabilities, though some users prefer alternatives like Qwen 3 for specific tasks.

---

## 16. [32GB Mi50's were getting so expensive that I ended up buying a 32GB w6800 for about the same price instead](https://reddit.com/r/LocalLLaMA/comments/1pob44f/32gb_mi50s_were_getting_so_expensive_that_i_ended/)

**Author:** u/EmPips | **Upvotes:** 227 | **Comments:** 42 | **Date:** 2025-12-16

**Summary:** The author opted for a 32GB w6800 GPU instead of a 32GB Mi50 due to similar pricing, citing convenience and cooling performance as key factors. The discussion includes comparisons with other GPUs like the AMD Radeon AI PRO R9700 and Zotac 3090s. Key points include the author's choice, the pros of the w6800's cooler, alternative GPU suggestions, minimal price differences, and a focus on performance and value. The discussion highlighted convenience and cooling performance while considering alternatives with better software support or similar pricing, leaning towards value and performance trade-offs.

---

## 17. [8 Million Users' AI Conversations Sold for Profit by "Privacy" Extensions | Koi Blog](https://reddit.com/r/LocalLLaMA/comments/1poal2a/8_million_users_ai_conversations_sold_for_profit/)

**Author:** u/ManThigh | **Upvotes:** 155 | **Comments:** 46 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses privacy concerns regarding browser extensions selling AI conversation data of millions of users for profit, highlighting the importance of using local models and auditing extensions.

**Key Points:**
- Browser extensions like Urban VPN Proxy and 1ClickVPN Proxy sold AI conversation data of millions of users.
- The post emphasizes the need to audit extensions and use local models to protect privacy.
- Community reactions include calls for punishing companies that buy such data and pride in using local setups.
- Data privacy is a significant concern, with user interactions with LLMs being highly valuable.

**Discussion Highlights:** The discussion highlights a strong consensus on the importance of data privacy, with many users expressing pride in their local setups and calling for accountability for companies involved in data sales.

---

## 18. [Finally managed to run Qwen-2.5-7B on a 4GB GTX 1050 without CPU offloading (Surgical Memory Alignment)](https://reddit.com/r/LocalLLaMA/comments/1po97ad/finally_managed_to_run_qwen257b_on_a_4gb_gtx_1050/)

**Author:** u/HuseyinKama | **Upvotes:** 150 | **Comments:** 48 | **Date:** 2025-12-16

**Summary:** The post discusses a solution called 'Surgical Memory Alignment' to run Qwen-2.5-7B on a 4GB GTX 1050 without CPU offloading, saving VRAM and improving speed. The author open-sourced the project as QKV Core.

**Key Points:**
- Standard GGUF quantization tools add padding that wastes memory, causing OOM errors on low-end GPUs.
- Surgical Alignment trims and realigns memory blocks to save VRAM and improve performance.
- The solution saved 44MB per model and improved I/O load times by ~34%.
- The project is open-sourced as QKV Core for others with low-end GPUs.
- Discussion includes praise for the work and skepticism about the code quality.

**Discussion Highlights:** The discussion includes praise for the author's expertise and the potential impact of the solution, as well as skepticism about the code quality and requests for clarification on how the tool works.

---

## 19. [I was bored](https://reddit.com/r/LocalLLaMA/comments/1po8yt0/i_was_bored/)

**Author:** u/MyLovelyAngelKirino | **Upvotes:** 133 | **Comments:** 70 | **Date:** 2025-12-16

**Summary:** The author, u/MyLovelyAngelKirino, built a high-performance computer setup with excess hardware during unemployment, sparking community interest and playful reactions.

**Key Points:**
- Author built a powerful computer setup with 3x 3090 GPUs, 512GB RAM, and an Epyc 7663 56-core CPU
- Community expressed envy and curiosity about the hardware and funding
- Discussion included playful references and requests for hardware details
- Some users noted the neatness of the setup and suggested additional GPUs for symmetry

**Discussion Highlights:** The community reacted with a mix of envy, curiosity, and playful comments, focusing on the impressive hardware specifications and requesting more details about the build.

---

## 20. [Meta announced a new SAM Audio Model for audio editing that can segment sound from complex audio mixtures using text, visual, and time span prompts.](https://reddit.com/r/LocalLLaMA/comments/1po7i0c/meta_announced_a_new_sam_audio_model_for_audio/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 503 | **Comments:** 84 | **Date:** 2025-12-16

**Summary:** Meta announced a new SAM Audio Model that can segment sound from complex audio mixtures using text, visual, and time span prompts, transforming audio processing.

**Key Points:**
- SAM Audio Model can isolate any sound from complex audio mixtures using text, visual, and time span prompts.
- Potential applications include isolating and subtracting unwanted sounds in Microsoft Teams meetings.
- The model can pick out specific sounds from complex audio mixtures based on visual prompts.
- Model sizes and specifications are available in the provided image link.
- The model can handle subtle sounds, such as a microphone tap, when prompted.

**Discussion Highlights:** The discussion highlights the potential applications of the SAM Audio Model, such as improving audio quality in virtual meetings by isolating unwanted sounds. Users are impressed by the model's ability to pick out specific sounds from complex audio mixtures and its potential for practical use cases.

---

## 21. [Allen Institute for AI introduces Molmo 2](https://reddit.com/r/LocalLLaMA/comments/1po78bl/allen_institute_for_ai_introduces_molmo_2/)

**Author:** u/Agitated_Camel1886 | **Upvotes:** 237 | **Comments:** 22 | **Date:** 2025-12-16

**Summary:** Allen Institute for AI introduces Molmo 2, an 8B model capable of advanced video analysis tasks like Video QA, counting, pointing, and dense captioning. The community is impressed by its capabilities and the public availability of datasets.

**Key Points:**
- Molmo 2 is an 8B model with advanced video analysis capabilities.
- The model supports tasks like Video QA, counting, pointing, and dense captioning.
- Allen AI releases datasets publicly, fostering community advancements.
- An AMA was scheduled to discuss Olmo 3 and Molmo 2.
- Community members are impressed by the model's performance and benchmarks.

**Discussion Highlights:** The community is highly impressed by Molmo 2's capabilities and benchmarks. There is enthusiasm about the public availability of datasets and the scheduled AMA for further discussion.

---

## 22. [XiaomiMiMo/MiMo-V2-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1po3bn4/xiaomimimomimov2flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 233 | **Comments:** 51 | **Date:** 2025-12-16

**Summary:** The post discusses MiMo-V2-Flash, a Mixture-of-Experts (MoE) language model with 309B total parameters and 15B active parameters, designed for high-speed reasoning and agentic workflows. It claims to outperform larger models like Sonnet 4.5 and Gemini 3 on multilingual SWE tasks, sparking community interest and skepticism.

**Key Points:**
- MiMo-V2-Flash is a MoE model with 309B total parameters and 15B active parameters.
- Designed for high-speed reasoning and agentic workflows.
- Claims to outperform Sonnet 4.5 and Gemini 3 on multilingual SWE tasks.
- Community interest in larger versions and hardware requirements.
- Skepticism about the performance claims given the model size.

**Discussion Highlights:** The discussion highlights skepticism about the model's performance claims, interest in larger versions, and technical queries about running the model on specific hardware configurations.

---

## 23. [GLM-4.5V, GLM-4.6V and GLM_4.6V-Flash are now supported by llama.cpp (GGUFs)](https://reddit.com/r/LocalLLaMA/comments/1po18y9/glm45v_glm46v_and_glm_46vflash_are_now_supported/)

**Author:** u/jacek2023 | **Upvotes:** 167 | **Comments:** 34 | **Date:** 2025-12-16

**Summary:** The Reddit post announces that GLM-4.5V, GLM-4.6V, and GLM_4.6V-Flash are now supported by llama.cpp with GGUFs, which is seen as a significant update by the community.

**Key Points:**
- Support for GLM-4.5V, GLM-4.6V, and GLM_4.6V-Flash has been added to llama.cpp.
- The update is celebrated as a great Christmas gift by the community.
- There is a question about whether GGUFs now support vision, with some users reporting issues.
- Comparisons between Qwen3-VL-4B and GLM_4.6V are being discussed.

**Discussion Highlights:** The community is excited about the new support for GLM models in llama.cpp, though there are some concerns and questions about vision support and compatibility with existing setups.

---

## 24. [Qwen3 Next speed optimization has been merged into llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1pnz9xu/qwen3_next_speed_optimization_has_been_merged/)

**Author:** u/jacek2023 | **Upvotes:** 214 | **Comments:** 25 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses the recent speed optimization for Qwen3 Next in llama.cpp, highlighting significant performance improvements across different hardware configurations.

**Key Points:**
- Speed optimization for Qwen3 Next has been merged into llama.cpp.
- Performance on M1 64GB improved from 12 t/s to 18 t/s.
- Other configurations show notable speed gains, such as 37.x t/s on Win11 + RTX5090 + vulkan.
- Qwen3-30B achieves around 58 t/s on the same hardware.
- Users report substantial improvements in processing speed.

**Discussion Highlights:** The discussion highlights a consensus on the significant performance improvements achieved with the new optimization, with users reporting notable speed gains across various hardware setups.

---

## 25. [I may have over-quantized this little guy.](https://reddit.com/r/LocalLLaMA/comments/1pnz80z/i_may_have_overquantized_this_little_guy/)

**Author:** u/AllergicToTeeth | **Upvotes:** 140 | **Comments:** 30 | **Date:** 2025-12-16

**Summary:** The post discusses the quantization of an AI model, with comments highlighting technical aspects like system prompts and quantization levels, along with humorous references to advanced GPT versions.

**Key Points:**
- Quantization of a model is the main topic
- System prompts are important for model behavior
- Q0 quantization level is mentioned for quick loading
- Humorous references to GPT-5.4 and GPT-5.3 are made
- Community engagement is high with 140 upvotes and 30 comments

**Discussion Highlights:** The discussion focuses on technical details of model quantization and performance, with some light-hearted jokes about AI advancements and comparisons to OpenAI's models.

---

## 26. [It was Ilya who "closed" OpenAI](https://reddit.com/r/LocalLLaMA/comments/1pnxekt/it_was_ilya_who_closed_openai/)

**Author:** u/licuphand | **Upvotes:** 513 | **Comments:** 231 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses Ilya's role in 'closing' OpenAI, sparking a debate on AI governance and trust in companies versus the public. The comments highlight skepticism about corporate control of AI and reference historical concerns about oversight.

**Key Points:**
- Ilya's actions are central to the discussion on OpenAI's direction.
- Public trust in AI governance is questioned, with skepticism about corporate control.
- Historical references like 'Who will watch the watchmen' are invoked to discuss oversight.
- Competition among AI leaders (Elon, Ilya, Sam) is noted as a driving factor.
- The term 'CloseAI' is used to describe the trend of AI organizations becoming more closed.

**Discussion Highlights:** The discussion reflects a consensus that corporate control of AI is problematic, with many users expressing distrust in centralized governance. Historical analogies and references to leadership dynamics among AI pioneers are prominent themes.

---

## 27. [Alibaba Open-Sources CosyVoice 3, a New TTS Model](https://reddit.com/r/LocalLLaMA/comments/1pnusp9/alibaba_opensources_cosyvoice_3_a_new_tts_model/)

**Author:** u/nekofneko | **Upvotes:** 217 | **Comments:** 31 | **Date:** 2025-12-16

**Summary:** Alibaba has open-sourced CosyVoice 3, a new TTS model with advanced features like multi-lingual support, high naturalness, and low latency. The model supports various languages, dialects, and offers features like pronunciation inpainting and text normalization.

**Key Points:**
- Supports 9 common languages and 18+ Chinese dialects/accents
- State-of-the-art performance in content consistency, speaker similarity, and prosody naturalness
- Features like pronunciation inpainting, text normalization, and bi-streaming with low latency
- Supports various instructions such as languages, dialects, emotions, speed, and volume
- Discussion highlights include comparisons with other models like Chatterbox and Microsoft VibeVoice

**Discussion Highlights:** Users are comparing CosyVoice 3 with other models like Chatterbox and Microsoft VibeVoice, expressing interest in its capabilities and potential for voice cloning. Some users are eager for larger model versions and real-time applications.

---

## 28. [New budget local AI rig](https://reddit.com/r/LocalLLaMA/comments/1pnllux/new_budget_local_ai_rig/)

**Author:** u/vucamille | **Upvotes:** 153 | **Comments:** 38 | **Date:** 2025-12-15

**Summary:** The user built a budget AI rig using a Qiyida X99 motherboard, Xeon E5 2680 V4 CPU, 32GB RAM, and two MI50 16GB GPUs for around $650. The system works well with ROCm 7.0.2 and handles basic inference tasks, with plans for future upgrades. Key points include the budget build with Xeon E5 2680 V4 and MI50 GPUs, total cost around $650, ROCm 7.0.2 working with initial multi-GPU issues, community praise for cost-effectiveness and expandability, and user plans for additional upgrades. The community praised the build for its affordability and expandability, with some users requesting benchmarks and expressing admiration for the cost-effective setup compared to more expensive alternatives.

---

## 29. [I'm strong enough to admit that this bugs the hell out of me](https://reddit.com/r/LocalLLaMA/comments/1pnfaqo/im_strong_enough_to_admit_that_this_bugs_the_hell/)

**Author:** u/ForsookComparison | **Upvotes:** 1693 | **Comments:** 353 | **Date:** 2025-12-15

**Summary:** The Reddit post discusses a user's frustration with a specific issue, which has garnered significant attention with 1693 upvotes and 353 comments. The discussion includes humorous and technical responses, highlighting community engagement and diverse perspectives.

**Key Points:**
- The post has gained popularity with 1693 upvotes and 353 comments.
- The top comments include humorous and technical responses.
- The discussion highlights community engagement and diverse perspectives.
- The post has been featured on Discord and the author received a special flair.

**Discussion Highlights:** The discussion features a mix of humor and technical insights, with notable comments including references to RAM Doubler and comparisons between Mac and GPU setups. The community engagement is evident through the high number of upvotes and comments.

---

## 30. [They're finally here (Radeon 9700)](https://reddit.com/r/LocalLLaMA/comments/1pnd5uf/theyre_finally_here_radeon_9700/)

**Author:** u/Zeikos | **Upvotes:** 357 | **Comments:** 67 | **Date:** 2025-12-15

**Summary:** The Reddit post announces the arrival of Radeon 9700 GPUs, sparking community interest and requests for benchmarks and performance data.

**Key Points:**
- Community eagerly awaits benchmarks for the new Radeon 9700 GPUs
- Nostalgia expressed for the original Radeon 9700 from the 2000s
- Requests for specific benchmarks including inference, training, noise, and heat levels
- Users plan to test the GPUs during the holidays
- Humorous reference to potential hardware issues ('Time to first smokey smelling')

**Discussion Highlights:** The discussion highlights a strong community interest in performance metrics and benchmarks for the new Radeon 9700 GPUs, with users expressing nostalgia for the original model and planning to conduct tests during the holidays.

---

## 31. [status of Nemotron 3 Nano support in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1pnc045/status_of_nemotron_3_nano_support_in_llamacpp/)

**Author:** u/jacek2023 | **Upvotes:** 179 | **Comments:** 32 | **Date:** 2025-12-15

**Summary:** The Reddit post discusses the status of Nemotron 3 Nano support in llama.cpp, highlighting a GitHub pull request and community reactions. The discussion emphasizes the importance of collaboration between organizations and the llama.cpp project for new model architectures.

**Key Points:**
- Nemotron 3 Nano support is being added to llama.cpp via a GitHub pull request.
- The model sizes (Q4_K_M and Q4_K_XL) are noted to be around 24GB, which is a point of discussion.
- Community members appreciate Nvidia's effort and encourage other labs to follow suit.
- There is a consensus that organizations should work with llama.cpp to ensure support before releasing new model weights.

**Discussion Highlights:** The community positively reacts to Nvidia's collaboration with llama.cpp, emphasizing the importance of such partnerships for the ecosystem. There is a general consensus that this approach should be a standard practice for organizations releasing new models.

---

## 32. [NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model!](https://reddit.com/r/LocalLLaMA/comments/1pn8upp/nvidia_releases_nemotron_3_nano_a_new_30b_hybrid/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 842 | **Comments:** 178 | **Date:** 2025-12-15

**Summary:** NVIDIA has released Nemotron 3 Nano, a 30B hybrid reasoning model with a 1M context window and top performance in SWE-Bench, reasoning, and chat. The model is noted for its speed and efficiency, achieving 110 tokens per second in local generation.

**Key Points:**
- Nemotron 3 Nano is a 30B hybrid reasoning model with a 1M context window.
- It offers best-in-class performance for SWE-Bench, reasoning, and chat.
- The model is part of the Nemotron 3 family of Mixture of Experts (MoE) models.
- Users report exceptionally fast generation speeds (110 t/s locally).
- The release has generated significant community interest and discussion.

**Discussion Highlights:** The community is excited about the model's performance and speed, with some users noting its fast generation speeds and others discussing its place in the Nemotron 3 family of MoE models. There is also some humor about the 'Nano' designation for a 30B model.

---

## 33. [NVIDIA Nemotron 3 Nano 30B A3B released](https://reddit.com/r/LocalLLaMA/comments/1pn8h5h/nvidia_nemotron_3_nano_30b_a3b_released/)

**Author:** u/rerri | **Upvotes:** 277 | **Comments:** 84 | **Date:** 2025-12-15

**Summary:** NVIDIA has released Nemotron 3 Nano 30B A3B, a new model featuring a hybrid Mamba-Transformer MoE architecture, exceptional inference efficiency, and a 1M-token context window. The model is fully open and designed for high throughput and low latency.

**Key Points:**
- Hybrid Mamba-Transformer MoE architecture for efficient inference
- 31.6B total parameters with ~3.6B active per token
- Up to 4x faster than Nemotron Nano 2 and 3.3x faster than leading models in its size category
- 1M-token context window for long-horizon workflows
- Fully open with open weights, datasets, training recipes, and framework

**Discussion Highlights:** The discussion highlights include a Llama.cpp PR for integration, questions about optimal Unsloth quant for specific hardware, concerns about synthetic data training, and performance feedback from users who have tested the model.

---

## 34. [New Google model incoming!!!](https://reddit.com/r/LocalLLaMA/comments/1pn37mw/new_google_model_incoming/)

**Author:** u/R46H4V | **Upvotes:** 1250 | **Comments:** 262 | **Date:** 2025-12-15

**Summary:** The Reddit post discusses anticipation and speculation around an upcoming Google model, with users expressing hope for improvements over previous models like Gemma3-Math and potential multi-modal capabilities.

**Key Points:**
- Anticipation of a new Google model
- Hope for improvements over previous models like Gemma3-Math
- Speculation about multi-modal capabilities
- High engagement with 1250 upvotes and 262 comments
- Community excitement and hype around the announcement

**Discussion Highlights:** The discussion highlights a strong community interest and excitement about the new Google model, with users expressing hopes for significant improvements and new features. There is a consensus of anticipation and speculation about the model's capabilities.

---

## 35. [llama.cpp: Automation for GPU layers, tensor split, tensor overrides, and context size (with MoE optimizations)](https://reddit.com/r/LocalLLaMA/comments/1pn2e1c/llamacpp_automation_for_gpu_layers_tensor_split/)

**Author:** u/Remove_Ayys | **Upvotes:** 189 | **Comments:** 61 | **Date:** 2025-12-15

**Summary:** The post discusses the implementation of automated memory allocation in llama.cpp, which optimizes GPU and CPU hybrid inference by iteratively adjusting memory use and prioritizing dense tensors for MoE models. This addresses previous limitations of manual memory management and heuristic-based approaches.

**Key Points:**
- Automated memory allocation for GPU layers and tensor splits in llama.cpp
- Iterative reduction of memory use to fit models across GPUs
- Prioritization of dense tensors for better MoE performance
- Compatibility with any ggml backend supporting CPU + GPU hybrid inference
- Positive community feedback and suggestions for further improvements

**Discussion Highlights:** The community appreciates the new feature, with suggestions for caching to reduce fitting time and requests for multi-GPU support. There is consensus on the usefulness of the implementation and its potential for further optimization.

---

## 36. [Aaaand... is gone...](https://reddit.com/r/LocalLLaMA/comments/1pmungj/aaaand_is_gone/)

**Author:** u/HumanDrone8721 | **Upvotes:** 926 | **Comments:** 208 | **Date:** 2025-12-14

**Summary:** The Reddit post discusses the end of a product or service, with users reacting to its discontinuation. The top comments reflect a mix of humor, acceptance, and skepticism about the impact of this change.

**Key Points:**
- The post is a link with no text content, indicating a significant announcement or event.
- Users are reacting with humor, such as buying more storage or referencing memes.
- There is skepticism about the significance of the event, with some users downplaying its impact.
- The post has gained significant traction with 926 upvotes and 208 comments.
- The author received recognition for their contribution, including a special flair.

**Discussion Highlights:** The discussion highlights a mix of humorous reactions, acceptance, and skepticism. Some users see the event as a major change, while others downplay its significance, particularly noting that it only affects SATA drives and not a broader range of products.

---

## 37. [Qwen3-Next-80B-A3B-Thinking-GGUF has just been released on HuggingFace](https://reddit.com/r/LocalLLaMA/comments/1pmo0dn/qwen3next80ba3bthinkinggguf_has_just_been/)

**Author:** u/LegacyRemaster | **Upvotes:** 126 | **Comments:** 54 | **Date:** 2025-12-14

**Summary:** The Reddit post announces the release of Qwen3-Next-80B-A3B-Thinking-GGUF on HuggingFace, highlighting its impressive performance in a Tetris game implemented in a single HTML file. Users compare it favorably to other models like Devstral and discuss its capabilities and release timeline.

**Key Points:**
- Qwen3-Next-80B-A3B-Thinking-GGUF model has been released on HuggingFace.
- The model demonstrated exceptional performance in a Tetris game implemented in a single HTML file.
- Users compare it favorably to Devstral, noting better accuracy.
- The release date is clarified as recent (12 days ago), not months ago as some users thought.
- Discussion includes questions about native tool calling support in llamacpp.

**Discussion Highlights:** The community is impressed with the model's performance, particularly in iterative agentic coding tasks. There is some confusion about the release date, with users clarifying it as recent. Questions about tool calling support and the presence of classic games in training datasets are also discussed.

---

## 38. [To Mistral and other lab employees: please test with community tools BEFORE releasing models](https://reddit.com/r/LocalLLaMA/comments/1pmgm2x/to_mistral_and_other_lab_employees_please_test/)

**Author:** u/dtdisapointingresult | **Upvotes:** 140 | **Comments:** 72 | **Date:** 2025-12-14

**Summary:** The Reddit post criticizes Mistral for releasing Devstral 2 without thorough testing with community tools, leading to issues like benchmark discrepancies and repetition loops. The author emphasizes the importance of testing with local tools to maintain reputation and user trust.

**Key Points:**
- Devstral 2 release faced criticism due to lack of testing with community tools.
- Issues included benchmark discrepancies and repetition loops.
- The author stresses the importance of testing with local tools for reputation and user trust.
- Community feedback highlights mixed experiences with the model across different tools.
- The discussion underscores the value of tech geeks' recommendations in driving adoption.

**Discussion Highlights:** The discussion reveals a mix of experiences with Devstral 2, with some users reporting positive outcomes while others encounter issues. There is a consensus on the need for better testing and documentation to ensure smooth integration with community tools.

---

## 39. [Understanding the new router mode in llama cpp server](https://reddit.com/r/LocalLLaMA/comments/1pmc7lk/understanding_the_new_router_mode_in_llama_cpp/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 168 | **Comments:** 44 | **Date:** 2025-12-14

**Summary:** Router mode in llama cpp server allows managing multiple AI models simultaneously without restarting the server, enabling dynamic model switching and efficient memory usage.

**Key Points:**
- Router mode enables loading/unloading models on demand within a single server process.
- It saves memory and simplifies model switching compared to running separate servers per model.
- Useful for testing multiple GGUF models, building local APIs, and dynamic model switching.
- Discussion highlights differences from llama-swap and requests for better VRAM management.

**Discussion Highlights:** The discussion focuses on comparing router mode with llama-swap, with users requesting features like specifying models to keep in memory and better VRAM management for multi-GPU setups.

---

## 40. [8x RTX Pro 6000 server complete](https://reddit.com/r/LocalLLaMA/comments/1plwgun/8x_rtx_pro_6000_server_complete/)

**Author:** u/koushd | **Upvotes:** 625 | **Comments:** 268 | **Date:** 2025-12-13

**Summary:** The user detailed their journey upgrading a GPU server, culminating in a setup with 8x RTX Pro 6000 GPUs, a Threadripper PRO 9955WX CPU, and 384 GB RAM, totaling 768 GB VRAM. They faced challenges with heat, power, and hardware compatibility, ultimately using a workaround with two systems in pipeline parallel.

**Key Points:**
- The final setup includes 8x RTX Pro 6000 GPUs, a Threadripper PRO 9955WX CPU, and 384 GB RAM.
- The user faced issues with heat, power consumption, and hardware compatibility during upgrades.
- A workaround involved using two systems with 2 GPUs each in pipeline parallel.
- The community reacted with a mix of admiration and criticism, highlighting concerns about the setup's practicality and safety.
- The post gained significant attention, with 625 upvotes and 268 comments.

**Discussion Highlights:** The discussion featured a mix of admiration for the setup's power and criticism regarding its practicality and safety. Some users praised the setup as 'epyc,' while others questioned the wisdom of investing heavily in such a configuration. The top comment humorously compared the setup to a 'Porsche in a trailer park,' criticizing the use of high-end hardware in a seemingly inadequate setup.

---

## 41. [Mistral 3 Large is DeepSeek V3!?](https://reddit.com/r/LocalLLaMA/comments/1plpc6h/mistral_3_large_is_deepseek_v3/)

**Author:** u/seraschka | **Upvotes:** 172 | **Comments:** 33 | **Date:** 2025-12-13

**Summary:** The post discusses the architectural similarities between Mistral 3 Large and DeepSeek V3, noting that they share nearly identical sizes and architectures, with minor differences in expert configurations. The community highlights the open-source spirit and the adoption of DeepSeek V3's architecture by multiple models.

**Key Points:**
- Mistral 3 Large and DeepSeek V3 have almost identical sizes (671B vs 673B).
- Mistral 3 Large uses the same architecture as DeepSeek V3 but with adjusted expert configurations.
- The community notes that other models like Kimi K2 and Gigachat also use the DeepSeek V3 architecture.
- Mistral likely trained their model from scratch despite architectural similarities.
- The discussion highlights the benefits of open-source collaboration and innovation.

**Discussion Highlights:** The community consensus emphasizes the value of open-source architectures, with multiple models adopting DeepSeek V3's design. There is appreciation for Mistral's innovation in multimodal capabilities and the efficiency gains from their expert configuration adjustments.

---

## 42. [OpenAI's flagship model, ChatGPT-5.2 Thinking, ranks  most censored AI on Sansa benchmark.](https://reddit.com/r/LocalLLaMA/comments/1plnuqu/openais_flagship_model_chatgpt52_thinking_ranks/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 618 | **Comments:** 112 | **Date:** 2025-12-13

**Summary:** The Reddit post discusses OpenAI's ChatGPT-5.2 model being ranked as the most censored AI on the Sansa benchmark, with users reporting performance issues and difficulties in specific tasks like evaluating clinical notes.

**Key Points:**
- ChatGPT-5.2 is ranked as the most censored AI on the Sansa benchmark.
- Users report performance issues with follow-up questions and research tasks.
- Difficulties in evaluating made-up clinical notes, which was not an issue with previous models.
- Comparisons with other models like Grok and Gemini, noting differences in censorship levels.

**Discussion Highlights:** Users highlight performance degradation in ChatGPT-5.2 compared to previous versions, particularly in handling follow-up questions and specific evaluation tasks. There is also a discussion on the censorship levels of different AI models, with Gemini being noted as less censored than some open models.

---

## 43. [Qwen3 Next generation optimization](https://reddit.com/r/LocalLLaMA/comments/1plng6f/qwen3_next_generation_optimization/)

**Author:** u/ilintar | **Upvotes:** 360 | **Comments:** 40 | **Date:** 2025-12-13

**Summary:** The post discusses optimizations made to Qwen3, specifically an autoregressive delta net computation that improves generation speed by 40%. The author invites others to test the optimizations and share their results.

**Key Points:**
- Optimized autoregressive delta net computation for Qwen3
- 40% generation speed improvement reported
- Community appreciation and engagement in comments
- Questions about compatibility with ROCm/Vulkan
- Author recognized for contributions with special flair

**Discussion Highlights:** The community shows strong appreciation for the optimization work, with comments highlighting the author's frequent contributions and expressing interest in further improvements. There is a question about whether the speedup applies to ROCm/Vulkan in addition to CUDA.

---

## 44. [NVIDIA gpt-oss-120b Eagle Throughput model](https://reddit.com/r/LocalLLaMA/comments/1plewrk/nvidia_gptoss120b_eagle_throughput_model/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 245 | **Comments:** 53 | **Date:** 2025-12-13

**Summary:** The post discusses NVIDIA's GPT-OSS-120B-Eagle3-throughput model, an optimized speculative decoding module designed to improve throughput during text generation. It uses NVIDIA’s Eagle3 speculative decoding approach and is licensed for both commercial and non-commercial use.

**Key Points:**
- GPT-OSS-120B-Eagle3-throughput is an optimized speculative decoding module built on the OpenAI gpt-oss-120b base model.
- It uses NVIDIA’s Eagle3 speculative decoding approach to predict a single draft token efficiently.
- The model is licensed under the nvidia-open-model-license for commercial and non-commercial use.
- It is intended for applications like AI agents, chatbots, and retrieval-augmented generation (RAG) systems.
- The model is not supported in llama.cpp, as indicated in the discussion.

**Discussion Highlights:** The discussion highlights include a request for a derestricted version of the model, mentions of potential speed improvements with CPU inference, and the lack of support for the model in llama.cpp. There is also a humorous comment about waiting for a REAP EAGLE3 HERETIC MOE GGUF version.

---

## 45. [This is how open ai is advertising them selfs on reddit…. They are doomed](https://reddit.com/r/LocalLLaMA/comments/1plcrg8/this_is_how_open_ai_is_advertising_them_selfs_on/)

**Author:** u/ThinkExtension2328 | **Upvotes:** 238 | **Comments:** 77 | **Date:** 2025-12-12

**Summary:** The Reddit post criticizes OpenAI's advertising strategy, highlighting a shift from promoting advanced AI to using astrology ads, which users find inconsistent and unappealing.

**Key Points:**
- OpenAI's advertising shift from AI advancements to astrology ads
- Criticism of the inconsistency in OpenAI's messaging
- Skepticism about the effectiveness of astrology ads for attracting users
- Discussion on the profitability of targeting horoscope believers over programmers
- Mention of potential privacy concerns with a 'year in review' feature

**Discussion Highlights:** Users express disappointment and skepticism about OpenAI's new advertising approach, questioning its effectiveness and consistency with their previous messaging. There is a consensus that targeting horoscope believers might be more profitable but is seen as a fall from grace.

---

## 46. [Running an LLM on a 3DS](https://reddit.com/r/LocalLLaMA/comments/1pl4njj/running_an_llm_on_a_3ds/)

**Author:** u/vreab | **Upvotes:** 294 | **Comments:** 35 | **Date:** 2025-12-12

**Summary:** The Reddit post discusses the feasibility and performance of running an LLM on a 3DS, drawing comparisons to similar projects on platforms like the PS Vita and Wii. Users express admiration for the technical achievement and curiosity about potential performance improvements on newer hardware.

**Key Points:**
- Running an LLM on a 3DS is technically feasible, as demonstrated by similar projects on PS Vita and Wii.
- Users are impressed by the technical achievement and potential applications.
- There is curiosity about whether a 'new' 3DS would significantly improve performance.
- Comparisons are made to other unconventional platforms running LLMs, like a Samsung fridge.

**Discussion Highlights:** The discussion highlights a mix of admiration for the technical feat and curiosity about the practical implications and performance on different hardware. Users are particularly interested in how such projects push the boundaries of what is possible on older or unconventional hardware.

---

## 47. [The new monster-server](https://reddit.com/r/LocalLLaMA/comments/1pl0ojb/the_new_monsterserver/)

**Author:** u/eribob | **Upvotes:** 591 | **Comments:** 125 | **Date:** 2025-12-12

**Summary:** The user shares their upgraded 'Monster Server' setup, featuring a Ryzen 3950x CPU, 128GB RAM, and three GPUs (2x RTX 3090 and 1x RTX 4090). The server runs local LLMs like GPT-OSS-120B and is used for research and coding. The post highlights the hardware configuration and performance details.

**Key Points:**
- Hardware setup includes Ryzen 3950x, 128GB RAM, and three GPUs (2x RTX 3090, 1x RTX 4090)
- Server runs local LLMs like GPT-OSS-120B for research and coding
- User has 10GB fiber internet for $50/month
- Discussion includes feedback on GPU setup efficiency and heat management
- Positive reactions and nostalgia for early 2000s overclocking forums

**Discussion Highlights:** The discussion includes positive reactions, nostalgia for early 2000s overclocking forums, questions about the user's location for affordable 10GB internet, feedback on GPU setup efficiency, and inquiries about heat management and the second PSU setup.

---

## 48. [Olmo 3.1 32B Think &amp; Instruct: New Additions to the Olmo Model Family](https://reddit.com/r/LocalLLaMA/comments/1pky5u4/olmo_31_32b_think_instruct_new_additions_to_the/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 183 | **Comments:** 28 | **Date:** 2025-12-12

**Summary:** The post introduces Olmo 3.1 32B Think and Instruct models, two new 32-billion-parameter models in the Olmo family, each optimized for different use cases: deep reasoning and instruction following, respectively.

**Key Points:**
- Olmo 3.1 32B Think is optimized for deep reasoning, math, logic, and code generation.
- Olmo 3.1 32B Instruct is optimized for instruction following, conversational fluency, and tool-use capabilities.
- Both models are fully open-source and part of the Olmo family.
- The community appreciates the open-source nature and continuous improvements of Olmo models.
- There is anticipation for additional features like Mixture of Experts (MOE).

**Discussion Highlights:** The community discussion highlights appreciation for the open-source nature of Olmo models, excitement about new releases, and anticipation for future features like MOE. Some users also noted the educational value of the accompanying paper.

---

## 49. [Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/)

**Author:** u/Nunki08 | **Upvotes:** 1330 | **Comments:** 155 | **Date:** 2025-12-12

**Summary:** An NVIDIA employee accidentally uploaded the parent folder of an upcoming model on Hugging Face, sparking significant interest and urgency within the community to preserve the content before potential removal.

**Key Points:**
- Accidental upload of NVIDIA's upcoming model files on Hugging Face
- Community interest in preserving the content before it gets taken down
- Mentions of specific model details like 'Nano' and '30B-A3B'
- Positive sentiment towards the Nemotron lineup and related projects
- Urgency to grab the content before full censoring is implemented

**Discussion Highlights:** The community showed strong interest in preserving the leaked content, with discussions highlighting the significance of the models and the urgency to download them before potential removal or censoring.

---

## 50. [Training an LLM only on 1800s London texts - 90GB dataset](https://reddit.com/r/LocalLLaMA/comments/1pkpsee/training_an_llm_only_on_1800s_london_texts_90gb/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 708 | **Comments:** 78 | **Date:** 2025-12-12

**Summary:** The post discusses the TimeCapsuleLLM project, which involves training an LLM on a 90GB dataset of 1800s London texts. The author has conducted a bias report and trained a small evaluation model to assess the dataset before scaling up.

**Key Points:**
- Dataset size: 90GB with 135,000 documents
- Bias report covering temporal, gender/pronoun, and geographic bias
- Evaluation model trained on a 15GB subset
- Example output shows the model's current capabilities
- Community feedback includes suggestions for using MoE and questions about dataset inclusion criteria

**Discussion Highlights:** The community appreciates the project and offers suggestions for improvement, such as using Mixture of Experts (MoE) for better compute efficiency and clarifying the dataset inclusion criteria.

---

