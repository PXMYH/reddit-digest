# r/LocalLLaMA Reading Digest

**Period:** 2026-01-24 to 2026-01-24
**Posts Summarized:** 44
**Total Posts Analyzed:** 44

---

## 1. [Your post is getting popular and we just featured it on our Discord!](https://reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/)

**Author:** u/roculus | **Upvotes:** 386 | **Comments:** 38 | **Date:** 2026-01-23

**Summary:** The Reddit post announces that a user's post has been featured on Discord and they have received a special flair. The user expresses annoyance at the bot's public posts and suggests sending private messages instead. The community discusses the bot's behavior and potential monetization of the Discord server.

**Key Points:**
- The bot announces that a user's post has been featured on Discord and they have received a special flair.
- The user finds the bot's public posts annoying and suggests sending private messages instead.
- The community discusses the bot's behavior and potential monetization of the Discord server.
- There is a pinned thread about the Discord server that has been there for 5 months.
- Some users find the bot's posts annoying and question the motives behind them.

**Discussion Highlights:** The community generally agrees that the bot's public posts are annoying and suggests that private messages would be a better approach. There is also speculation about the monetization of the Discord server and the motives behind the bot's behavior.

---

## 2. [The 'Infinite Context' Trap: Why 1M tokens won't solve Agentic Amnesia (and why we need a Memory OS)](https://reddit.com/r/LocalLLaMA/comments/1qkrhec/the_infinite_context_trap_why_1m_tokens_wont/)

**Author:** u/Sweet121 | **Upvotes:** 116 | **Comments:** 34 | **Date:** 2026-01-23

**Summary:** The post argues that large context windows are not the solution to AI memory issues, advocating instead for a structured Memory OS to manage memory lifecycle efficiently. The discussion highlights skepticism and alternative views on the proposed solution. Key points include the inefficiency of large context windows, the proposal of a Memory OS for memory lifecycle management, skepticism about the need for a Memory OS, and the importance of attention and salience in memory retrieval. The discussion reveals a mix of skepticism and agreement, with some commenters questioning the necessity of a Memory OS and others highlighting the importance of attention and salience in memory retrieval.

---

## 3. [Llama.cpp merges in OpenAI Responses API Support](https://reddit.com/r/LocalLLaMA/comments/1qkm9zb/llamacpp_merges_in_openai_responses_api_support/)

**Author:** u/SemaMod | **Upvotes:** 148 | **Comments:** 38 | **Date:** 2026-01-23

**Summary:** The Reddit post discusses the integration of OpenAI Responses API support into Llama.cpp, highlighting its successful implementation with GLM-4.7-Flash and Codex CLI. Users share their experiences and concerns about the new API.

**Key Points:**
- Llama.cpp now supports OpenAI Responses API.
- Successful integration with GLM-4.7-Flash and Codex CLI.
- Concerns about potential deprecation of the old API.
- Stateful interaction capabilities of the new API.
- Security concerns regarding data leakage with stateful interactions.

**Discussion Highlights:** The discussion highlights both enthusiasm for the new API's capabilities and concerns about its implications, including potential security risks and the future of the old API. Users share their experiences and opinions on the integration and its impact.

---

## 4. [OpenAI CFO hinting at "Outcome-Based Pricing" (aka royalties on your work)? Makes the case for local even stronger.](https://reddit.com/r/LocalLLaMA/comments/1qkiylw/openai_cfo_hinting_at_outcomebased_pricing_aka/)

**Author:** u/distalx | **Upvotes:** 226 | **Comments:** 101 | **Date:** 2026-01-23

**Summary:** The post discusses OpenAI's potential shift to outcome-based pricing for enterprise deals, clarifying that it does not apply to regular users or indie developers. The author uses a solar power analogy to argue for local AI solutions to avoid dependency on cloud APIs.

**Key Points:**
- OpenAI's CFO mentioned outcome-based pricing for high-value enterprise deals, not regular users.
- The author initially misinterpreted the scope but corrected it after finding the primary source.
- The post argues for local AI solutions to avoid potential future costs or restrictions from cloud APIs.
- Top comments highlight concerns about data usage and the benefits of self-hosting AI models.
- The discussion emphasizes the importance of controlling AI infrastructure to avoid unexpected costs.

**Discussion Highlights:** The discussion highlights a consensus on the benefits of local AI solutions for avoiding dependency on cloud APIs and potential future costs. Users express concerns about data usage and the importance of self-hosting AI models.

---

## 5. [Nvidia Introduces PersonaPlex: An Open-Source, Real-Time Conversational AI Voice](https://reddit.com/r/LocalLLaMA/comments/1qkimzg/nvidia_introduces_personaplex_an_opensource/)

**Author:** u/44th--Hokage | **Upvotes:** 215 | **Comments:** 24 | **Date:** 2026-01-22

**Summary:** Nvidia has introduced PersonaPlex, an open-source, real-time conversational AI voice model that enables persona control through text-based role prompts and audio-based voice conditioning. It is trained on synthetic and real conversations to produce natural, low-latency spoken interactions.

**Key Points:**
- PersonaPlex is a real-time, full-duplex speech-to-speech conversational model.
- It enables persona control through text-based role prompts and audio-based voice conditioning.
- The model is trained on a combination of synthetic and real conversations.
- It requires significant VRAM (96GB) for optimal performance.
- Some users have noted concerns about model quality and audio fidelity.

**Discussion Highlights:** Users have mixed reactions, with some praising the technology while others criticize its high VRAM requirements, model quality, and audio fidelity. There are also discussions about potential future improvements and comparisons to other models like Unmute.

---

## 6. [Quiet Threadripper AI Workstation - 768GB DDR5 and 160GB VRAM (RTX 5090 + 4x R9700)](https://reddit.com/r/LocalLLaMA/comments/1qkghpk/quiet_threadripper_ai_workstation_768gb_ddr5_and/)

**Author:** u/sloptimizer | **Upvotes:** 151 | **Comments:** 81 | **Date:** 2026-01-22

**Summary:** The Reddit post details a high-performance AI workstation build featuring an RTX 5090 and four R9700 GPUs, with 768GB DDR5 RAM and 160GB VRAM. The user shares insights on cooling, power management, and performance optimizations for running local LLMs like DeepSeek-V3.1-Terminus.

**Key Points:**
- The workstation includes an RTX 5090 and four R9700 GPUs, with a total of 768GB DDR5 RAM and 160GB VRAM.
- Performance metrics for DeepSeek-V3.1-Terminus show 151.76 tps for PP and 10.85 tps for TG.
- Key optimizations include adding RAM fans for cooling, disabling remote management for faster boot, and adjusting power limits for quieter operation.
- The build uses two power supplies: a 1600W for the main system and an 850W for three of the Radeons.
- Top comments highlight the impressive performance and humorously discuss the cost and value of the build.

**Discussion Highlights:** The discussion highlights the impressive performance of the workstation, with users expressing admiration for the near-SOTA capabilities and humorously commenting on the cost and value of the build. Some users joke about the high cost, while others express interest in using such a powerful system for gaming.

---

## 7. [Am I the only one who feels that, with all the AI boom, everyone is basically doing the same thing?](https://reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/)

**Author:** u/[deleted] | **Upvotes:** 383 | **Comments:** 183 | **Date:** 2026-01-22

**Summary:** The post discusses the redundancy of AI tools and applications during the AI boom, noting that many new tools are less polished versions of existing ones. The discussion highlights the enthusiasm and low barrier to entry in the AI field, leading to shallow implementations and repetitive projects. Key points include the redundancy of AI tools, the low barrier to entry resulting in shallow implementations, and the focus on niche applications that fill specific gaps. The discussion highlights a consensus that the AI field is currently in a hype stage, with many redundant projects and shallow implementations, but also recognizes the potential for meaningful innovation.

---

## 8. [vLLM raising $150M confirms it: We have moved from the "Throughput Era" to the "Latency(Cold Starts)."](https://reddit.com/r/LocalLLaMA/comments/1qk68n8/vllm_raising_150m_confirms_it_we_have_moved_from/)

**Author:** u/pmv143 | **Upvotes:** 169 | **Comments:** 88 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses the significant investment in vLLM, signaling a shift from the 'Throughput Era' to the 'Latency Era' in AI. The focus is moving from training foundation models to optimizing serving efficiency, latency, and throughput.

**Key Points:**
- The $150M investment in vLLM highlights the shift from training to serving in AI.
- Software optimization is becoming more critical than hardware investments.
- vLLM aims to standardize inference engines across different hardware platforms.
- The community debates whether vLLM will focus on horizontal compatibility or vertical optimization.
- Latency, particularly cold starts and time-to-first-token, is identified as the next major challenge.

**Discussion Highlights:** The discussion includes debates on vLLM's role in the AI ecosystem, with some comparing it to Linux or FreeBSD of inference. There is also a consensus on the importance of addressing latency issues, particularly cold starts, in cloud environments.

---

## 9. [Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp; 1.8B), Support for 10 languages](https://reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/)

**Author:** u/Nunki08 | **Upvotes:** 697 | **Comments:** 102 | **Date:** 2026-01-22

**Summary:** Qwen has open-sourced the Qwen3-TTS model family, including 5 models (0.6B & 1.8B) with support for 10 languages. The release includes resources on GitHub, Hugging Face, a blog post, a paper, and a demo.

**Key Points:**
- Qwen3-TTS model family open-sourced
- 5 models available (0.6B & 1.8B)
- Support for 10 languages
- Multiple resources provided (GitHub, Hugging Face, blog, paper, demo)
- Community feedback highlights both praise and concerns about model performance and compatibility

**Discussion Highlights:** The community appreciates Qwen's open-sourcing efforts but notes concerns about the English voice quality sounding like anime dubs and requests for better compatibility with tools like llama.cpp and mistral.rs.

---

## 10. [Qwen dev on Twitter!!](https://reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 713 | **Comments:** 60 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses Qwen's TTS model announcement, with the community clarifying its origin and sharing relevant links.

**Key Points:**
- Qwen's TTS model announcement
- Clarification that it's the TTS model from the vLLM leak
- Link to Hugging Face collection for Qwen3-TTS

**Discussion Highlights:** The community is engaged in discussing the release of Qwen's TTS model, with some users providing clarifications and relevant links.

---

## 11. [GLM 4.7 flash FA fix for CUDA has been merged into llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qjrsur/glm_47_flash_fa_fix_for_cuda_has_been_merged_into/)

**Author:** u/jacek2023 | **Upvotes:** 157 | **Comments:** 51 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses the recent merge of GLM 4.7 flash FA fix for CUDA into llama.cpp, highlighting both progress and ongoing issues.

**Key Points:**
- GLM 4.7 flash FA fix for CUDA has been merged into llama.cpp
- Quantized cache is not working well, causing high CPU usage
- Performance on Pascal GPUs is reported to be half the speed of non-flash-attention kernels
- Users report successful builds and usage of GLM-4.7-Flash-MXFP4_MOE.gguf
- General feedback includes observations about model behavior and performance

**Discussion Highlights:** The discussion highlights mixed experiences with the new fix, including performance issues on certain hardware and ongoing problems with quantized cache. However, some users report successful builds and usage, indicating progress despite the challenges.

---

## 12. [Fei Fei Li dropped a non-JEPA world model, and the spatial intelligence is insane](https://reddit.com/r/LocalLLaMA/comments/1qjjrmq/fei_fei_li_dropped_a_nonjepa_world_model_and_the/)

**Author:** u/coloradical5280 | **Upvotes:** 185 | **Comments:** 86 | **Date:** 2026-01-21

**Summary:** Fei-Fei Li's World Labs launched Marble, a generative world model using Neural Radiance Fields (NeRF) and Gaussian splatting, enabling fast creation of explorable 3D worlds. The technology allows for persistent, editable environments and supports VR and exports to various platforms. Despite its innovative approach, the post highlights mixed reactions from the community regarding its capabilities and value.

**Key Points:**
- Marble uses NeRF and Gaussian splatting for fast 3D world generation.
- The model supports VR and exports to platforms like Unreal, Unity, and Blender.
- The technology enables persistent, editable, and stateful 3D environments.
- Community reactions are mixed, with some criticizing its lack of open-source availability and perceived limitations.
- The author emphasizes the spatial intelligence and potential future impact of the technology.

**Discussion Highlights:** The discussion reflects a divide in opinions, with some users criticizing the lack of open-source availability and questioning the technology's classification as a 'world model.' Others express skepticism about the scale and quality of the generated environments. However, there is acknowledgment of the innovative approach and potential future developments.

---

## 13. [Wrote a guide for running Claude Code with GLM-4.7 Flash locally with llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qjf6ys/wrote_a_guide_for_running_claude_code_with_glm47/)

**Author:** u/tammamtech | **Upvotes:** 113 | **Comments:** 45 | **Date:** 2026-01-21

**Summary:** The post provides a guide for running Claude Code with GLM-4.7 Flash locally using llama.cpp, including installation steps and command examples for both direct and Docker setups. It highlights features like model swapping and GPU memory management. The discussion includes comparisons with Ollama, questions about performance and resource usage, and suggestions for open-source alternatives like OpenCode and Harbor.

---

## 14. [8x AMD MI50 32GB at 26 t/s (tg) with MiniMax-M2.1 and 15 t/s (tg) with GLM 4.7 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1qjaxfy/8x_amd_mi50_32gb_at_26_ts_tg_with_minimaxm21_and/)

**Author:** u/ai-infos | **Upvotes:** 315 | **Comments:** 124 | **Date:** 2026-01-21

**Summary:** The post details a cost-effective local inference setup using 8x AMD MI50 GPUs, achieving high token generation speeds with MiniMax-M2.1 and GLM 4.7 models. The setup is praised for its performance and affordability, with a total VRAM of 256GB for under $1k.

**Key Points:**
- MiniMax-M2.1 achieves 26.8 tokens/s output and 3000 tokens/s input with a context length of 196,608.
- GLM 4.7 achieves 15.6 tokens/s output and 3000 tokens/s input with a context length of 95,000.
- Total cost for 8 GPUs is $880, providing 256GB VRAM.
- Power draw is 280W idle and 1200W during inference.
- The setup is stable for long context use cases like coding agents.

**Discussion Highlights:** The community highly praises the setup for its cost-effectiveness and performance. Comments highlight the impressive VRAM capacity for the price and the stability of the models for long context tasks. Some users express interest in replicating the setup but note challenges in sourcing the GPUs at the stated price.

---

## 15. [VibeVoice-ASR released!](https://reddit.com/r/LocalLLaMA/comments/1qj40er/vibevoiceasr_released/)

**Author:** u/k_means_clusterfuck | **Upvotes:** 149 | **Comments:** 45 | **Date:** 2026-01-21

**Summary:** Microsoft released VibeVoice-ASR, a multilingual automatic speech recognition model with 9B parameters. Users report high accuracy, though some note challenges with polyphonic characters in names.

**Key Points:**
- VibeVoice-ASR is a new ASR model by Microsoft
- Model size is 9B parameters
- Multilingual capabilities with reported high accuracy
- Challenges noted with polyphonic characters in names
- Comparisons made to other models like Whisper and Parakeet

**Discussion Highlights:** Users generally praise the model's accuracy and multilingual capabilities, though some express concerns about its large size and lack of benchmarks. A test with Chinese audio reported 91% accuracy, with polyphonic characters being a bottleneck.

---

## 16. [One-shot single page web development: pacman clone - GLM 4.7 vs GLM 4.7 Flash vs GLM 4.5 Air vs Gemini 3 Pro vs Gemini 3 Flash - Results available for online testing - Prompt and instructions provided for testing with other models](https://reddit.com/r/LocalLLaMA/comments/1qj13uh/oneshot_single_page_web_development_pacman_clone/)

**Author:** u/ex-arman68 | **Upvotes:** 109 | **Comments:** 48 | **Date:** 2026-01-21

**Summary:** The Reddit post discusses a test comparing various AI models' ability to create a Pacman clone as a single webpage. GLM 4.7 emerged as the clear winner, followed by Minimax M2.1, with Gemini models performing less effectively than expected. The post provides links to the generated webpages and encourages further testing with other models. Key points include GLM 4.7's top performance, Minimax M2.1's sound implementation, Gemini models' underperformance, the use of a temperature setting of 0 for reproducibility, and community feedback highlighting GLM 4.7's effectiveness. The discussion highlights the community's surprise at GLM 4.7's performance and the importance of token capacity and memory for AI coding tasks.

---

## 17. [GLM-4.7-Flash-GGUF bug fix - redownload for better outputs](https://reddit.com/r/LocalLLaMA/comments/1qiy0ha/glm47flashgguf_bug_fix_redownload_for_better/)

**Author:** u/etherd0t | **Upvotes:** 112 | **Comments:** 58 | **Date:** 2026-01-21

**Summary:** A bug fix for the GLM-4.7-Flash-GGUF model has been released, improving outputs and fixing looping issues. Users are advised to re-download the model and use recommended parameters for optimal performance.

**Key Points:**
- Bug fix released for GLM-4.7-Flash-GGUF model
- Users should re-download the model for better outputs
- Recommended parameters provided for general use and tool-calling
- Previous version had issues with looping during 'thinking'
- Model performance praised despite being small in size

**Discussion Highlights:** Users reported significant improvements with the fixed version, praising its performance and noting issues with the previous version. Some users mentioned performance differences compared to other models like GPT-OSS-20b.

---

## 18. [Fix for GLM 4.7 Flash has been merged into llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qiwm3c/fix_for_glm_47_flash_has_been_merged_into_llamacpp/)

**Author:** u/jacek2023 | **Upvotes:** 309 | **Comments:** 86 | **Date:** 2026-01-21

**Summary:** A fix for GLM 4.7 Flash has been merged into llama.cpp, with ongoing work on CUDA support. The community is actively discussing performance metrics and user experiences.

**Key Points:**
- Fix for GLM 4.7 Flash merged into llama.cpp
- CUDA support in progress
- Performance metrics shared for different quantizations and GPUs
- Community feedback on model improvements and issues

**Discussion Highlights:** Users are sharing performance data and experiences with the new fix, noting improvements in model behavior and discussing potential issues like slow prompt processing in certain environments.

---

## 19. [Knowledge distillation with Claude as the interface: trained a 0.6B model to match GPT-class performance on Text2SQL in a singe conversation](https://reddit.com/r/LocalLLaMA/comments/1qiu6jo/knowledge_distillation_with_claude_as_the/)

**Author:** u/party-horse | **Upvotes:** 166 | **Comments:** 37 | **Date:** 2026-01-21

**Summary:** The post describes a workflow for training small, task-specific models using knowledge distillation via Claude, achieving significant performance improvements in Text2SQL tasks with minimal setup overhead.

**Key Points:**
- Off-the-shelf small models often perform poorly on specialized tasks like Text2SQL.
- Knowledge distillation via Claude simplifies the fine-tuning process by generating synthetic training data and handling the training pipeline.
- The test run showed a significant improvement in model performance, from 36% to 74% accuracy.
- The approach can be applied to various tasks, including understanding service/OS logs for on-device agents.
- Commenters suggested potential improvements, such as using SQL AST for checking matches and avoiding dependency on specific tools like Claude.

**Discussion Highlights:** The discussion highlights positive feedback on the approach, potential applications for on-device agents, and suggestions for improvements like using SQL AST for validation and avoiding proprietary tools.

---

## 20. [vLLM v0.14.0 released](https://reddit.com/r/LocalLLaMA/comments/1qim0e9/vllm_v0140_released/)

**Author:** u/jinnyjuice | **Upvotes:** 167 | **Comments:** 34 | **Date:** 2026-01-20

**Summary:** The Reddit post announces the release of vLLM v0.14.0, highlighting new features and updates. Users discuss improvements like automatic context length fitting and the deprecation of certain quantization methods.

**Key Points:**
- Automatic context length fitting to GPU memory to prevent OOM failures
- Deprecation of some quantization methods, including HQQ
- Introduction of Marlin for Turing (sm75) as a major upgrade
- User interest in future sm120 optimizations

**Discussion Highlights:** Users expressed excitement about the automatic context length feature and discussed the implications of deprecated quantization methods. The Marlin upgrade for Turing was noted as a significant improvement, and there was anticipation for future optimizations.

---

## 21. [Current GLM-4.7-Flash implementation confirmed to be broken in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qih9r8/current_glm47flash_implementation_confirmed_to_be/)

**Author:** u/Sweet_Albatross9772 | **Upvotes:** 243 | **Comments:** 60 | **Date:** 2026-01-20

**Summary:** The Reddit post discusses issues with the current GLM-4.7-Flash implementation in llama.cpp, highlighting significant differences in logprobs compared to vLLM, which may explain reported problems like looping and poor performance. A potential fix is already proposed in a pull request.

**Key Points:**
- Current GLM-4.7-Flash implementation in llama.cpp is confirmed broken.
- Significant differences in logprobs compared to vLLM may cause performance issues.
- A potential fix is available in a pull request.
- Community acknowledges the issue and expects a resolution soon.
- Some users suggest waiting before using new models to avoid bugs.

**Discussion Highlights:** The community is aware of the issue and appreciates the open-source nature of the project. There is a consensus that the problem will be resolved shortly, with some users recommending patience before adopting new models.

---

## 22. [You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?](https://reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/)

**Author:** u/Adventurous-Gold6413 | **Upvotes:** 536 | **Comments:** 303 | **Date:** 2026-01-20

**Summary:** The Reddit post discusses the best local models to use with 64GB RAM and 16GB VRAM when internet access is unavailable. Users share their preferences and experiences with various models.

**Key Points:**
- The post asks for recommendations on local models to use with specific hardware constraints.
- Top comments highlight models like Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B.
- GPT-OSS-120B is praised for its performance and versatility on the given hardware.
- The community appreciates the contribution and engages actively in the discussion.

**Discussion Highlights:** The discussion highlights a consensus around models like GPT-OSS-120B, which is noted for its good performance and fit for the specified hardware. Other models like Gemma 3 27B and GLM 4.5 Air are also mentioned as strong contenders.

---

## 23. [Liquid AI released the best thinking Language Model Under 1GB](https://reddit.com/r/LocalLLaMA/comments/1qi512t/liquid_ai_released_the_best_thinking_language/)

**Author:** u/PauLabartaBajo | **Upvotes:** 226 | **Comments:** 52 | **Date:** 2026-01-20

**Summary:** Liquid AI released LFM2.5-1.2B-Thinking, a compact reasoning model optimized for on-device use, excelling in math, tool use, and instruction following. It outperforms larger models in efficiency and is available across multiple platforms.

**Key Points:**
- LFM2.5-1.2B-Thinking runs on devices with 900 MB memory, offering edge-scale latency.
- Specialized for concise reasoning with internal thinking traces before answering.
- Outperforms Qwen3-1.7B in benchmarks despite having 40% fewer parameters.
- Users question memory requirements and licensing (not Apache/MIT).
- Some users desire larger models for broader real-world applications.

**Discussion Highlights:** The discussion highlights concerns about memory usage, licensing, and model size limitations, with some users praising its math capabilities but noting trade-offs in other benchmarks.

---

## 24. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 870 | **Comments:** 263 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, fully enclosed 10-GPU mobile AI system designed for running large MoE models and supporting graphic design tasks. The build, costing around $17k, balances performance and budget constraints while addressing mobility and enclosure challenges.

**Key Points:**
- The system features a Threadripper Pro 3995WX, 512GB DDR4, and 10 GPUs (8x 3090 + 2x 5090).
- It is designed for large MoE models, video generation, and high-detail image generation.
- The enclosure was a major challenge, solved using a Thermaltake Core W200 case.
- Budget constraints led to a mix of GPUs to optimize cost and performance.
- The post received significant engagement, with comments highlighting its uniqueness and practicality.

**Discussion Highlights:** The discussion highlights the system's impressive capabilities and the creative solution to the enclosure problem. Comments also joke about the system's portability and power requirements, emphasizing its uniqueness in the community.

---

## 25. [Over 6K novels with reasoning traces to train full book writing LLMs](https://reddit.com/r/LocalLLaMA/comments/1qi3nmd/over_6k_novels_with_reasoning_traces_to_train/)

**Author:** u/XMasterDE | **Upvotes:** 110 | **Comments:** 46 | **Date:** 2026-01-20

**Summary:** The post announces an update to the LongPage dataset, expanding it to over 6,000 novels with hierarchical planning traces to train full-book writing LLMs. The team is also training a model on this dataset and plans to release it soon. The community shows strong interest and requests more details about the dataset and model.

**Key Points:**
- LongPage dataset updated to 6K+ novels with hierarchical planning traces
- Dataset aims to support training full-book writing LLMs
- Team is training a model on LongPage and plans to release it soon
- Community shows interest and requests more details
- Inquiries about dataset content and code availability

**Discussion Highlights:** The community is eager to see the results, with many requesting more details about how the dataset and model work. There are also inquiries about specific content in the dataset and the availability of data processing code for other languages.

---

## 26. [glm-4.7-flash has the best thinking process with clear steps, I love it](https://reddit.com/r/LocalLLaMA/comments/1qhxlgy/glm47flash_has_the_best_thinking_process_with/)

**Author:** u/uptonking | **Upvotes:** 141 | **Comments:** 34 | **Date:** 2026-01-20

**Summary:** The Reddit post discusses the user's experience with the glm-4.7-flash model, highlighting its detailed thinking process and clear steps, despite being slower than other models. The user appreciates the model's reasoning capabilities and plans to replace other models with it, while also seeking ways to improve its speed. Key points include the model's structured thinking process, its slower speed compared to others, and user suggestions for improving performance. The discussion highlights general appreciation for the model's reasoning process and suggestions for tweaks to improve speed.

---

## 27. [It's been one year since the release of Deepseek-R1](https://reddit.com/r/LocalLLaMA/comments/1qhs2sd/its_been_one_year_since_the_release_of_deepseekr1/)

**Author:** u/Recoil42 | **Upvotes:** 301 | **Comments:** 51 | **Date:** 2026-01-19

**Summary:** The Reddit post commemorates the one-year anniversary of the Deepseek-R1 release, highlighting its significant impact on the AI community, including its influence on major tech companies and the rapid pace of advancements in the field.

**Key Points:**
- Deepseek-R1 had a major impact, reportedly causing significant disruptions at Meta (referred to as 'Zuck's back').
- The release is considered one of the most important in AI history, second only to the original Llama release.
- The model's release led to price reductions and increased transparency in AI reasoning outputs.
- The rapid pace of AI advancements is noted, with the past year feeling like two or three years of progress.
- There is interest in comparing current smaller models to R1 to measure progress.

**Discussion Highlights:** The discussion highlights the transformative impact of Deepseek-R1 on the AI landscape, with users emphasizing its disruptive effects on major tech companies and the broader industry. The consensus is that the release was a pivotal moment, accelerating advancements and increasing transparency in AI development.

---

## 28. [Mosquito - 7.3M parameter tiny knowledge model](https://reddit.com/r/LocalLLaMA/comments/1qhqzsi/mosquito_73m_parameter_tiny_knowledge_model/)

**Author:** u/Lopsided-Repair-3638 | **Upvotes:** 122 | **Comments:** 54 | **Date:** 2026-01-19

**Summary:** The post introduces 'Mosquito', a tiny 7.3M parameter language model that can answer general knowledge questions, though with notable limitations and humorous inaccuracies. The demo and model are available on Hugging Face.

**Key Points:**
- Mosquito is a small language model with 7.3M parameters.
- It can answer general knowledge questions but with significant inaccuracies.
- The model's responses include humorous errors, such as defining a dog incorrectly.
- Users have requested a quantized version of the model.
- The model's knowledge gaps are highlighted by its inability to answer basic questions correctly.

**Discussion Highlights:** The discussion highlights the model's limitations and humorous inaccuracies, with users pointing out incorrect responses to basic questions. There is a request for a quantized version of the model, and the overall consensus is that while the model is impressive for its size, it has notable flaws.

---

## 29. [Bartowski comes through again. GLM 4.7 flash GGUF](https://reddit.com/r/LocalLLaMA/comments/1qhpima/bartowski_comes_through_again_glm_47_flash_gguf/)

**Author:** u/RenewAi | **Upvotes:** 184 | **Comments:** 50 | **Date:** 2026-01-19

**Summary:** The post announces the release of GLM 4.7 Flash GGUF by Bartowski, with mixed user experiences reported in the comments.

**Key Points:**
- Bartowski released GLM 4.7 Flash GGUF on Hugging Face.
- Users report mixed results with the model, with some finding it ineffective.
- An Unsloth version of the model was recently uploaded.
- The post has garnered significant engagement with 184 upvotes and 50 comments.

**Discussion Highlights:** Users are discussing their experiences with different versions of GLM 4.7 Flash, with some reporting issues and others exploring new releases like the Unsloth version.

---

## 30. [Unsloth GLM 4.7-Flash GGUF](https://reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/)

**Author:** u/Wooden-Deer-1276 | **Upvotes:** 231 | **Comments:** 44 | **Date:** 2026-01-19

**Summary:** The post announces the release of the GLM-4.7-Flash GGUF model on Hugging Face, with discussions focusing on its performance, quantization issues, and recommendations for optimal use.

**Key Points:**
- The model is available on Hugging Face with various quantizations.
- Users are advised to use UD-Q4_K_XL and above for better performance.
- There are ongoing issues with looping in quantized versions, with BF16 recommended for best results.
- Specific settings for LM Studio are suggested to avoid issues with repeat_penalty.
- The community is actively engaged in testing and providing feedback on the model.

**Discussion Highlights:** The community emphasizes patience and thorough testing before full release. Key discussions include quantization recommendations, troubleshooting for looping issues, and sharing of optimal settings for different use cases. There is a consensus on using BF16 for the best performance while the team works on resolving remaining issues.

---

## 31. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 362 | **Comments:** 60 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the official support for GLM 4.7 Flash in llama.cpp, highlighting its community-driven development and performance improvements. Users discuss its speed and share additional resources.

**Key Points:**
- GLM 4.7 Flash now officially supported in llama.cpp
- Support is community-driven, not by Z.ai developers
- Performance improvements noted, with some users reporting faster speeds without flash-attention
- Additional resources and model versions shared by users
- Post recognized and featured in the community Discord

**Discussion Highlights:** The discussion highlights the community effort behind the integration, with users sharing performance insights and additional resources. Some users note that disabling flash-attention can improve speed, and the post has been recognized by the community.

---

## 32. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 460 | **Comments:** 162 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights the effectiveness of GLM 4.7 Flash as a reliable local agent for tasks like coding and command execution, with users eagerly awaiting its local availability via GGUFs. The discussion includes comparisons with other models and notes on its performance and output quality.

**Key Points:**
- GLM 4.7 Flash is praised for its reliability in agentic tasks, such as coding and command execution.
- Users are excited about the upcoming GGUFs for local use.
- Comparisons with other models like Nemotron 30B and Qwen3 are mentioned.
- The model is noted for its deep thinking and performance efficiency.
- Initial benchmarks suggest it is as smart as SEED OSS 36B but with better performance.

**Discussion Highlights:** The discussion highlights enthusiasm for GLM 4.7 Flash's capabilities and performance, with users sharing comparisons and benchmarks. There is a consensus on its potential as a strong local agent, though some await further comparisons and local testing.

---

## 33. [New in llama.cpp: Anthropic Messages API](https://reddit.com/r/LocalLLaMA/comments/1qhaq21/new_in_llamacpp_anthropic_messages_api/)

**Author:** u/paf1138 | **Upvotes:** 164 | **Comments:** 51 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the introduction of the Anthropic Messages API in llama.cpp, generating excitement among users who share practical tips and experiences.

**Key Points:**
- Introduction of Anthropic Messages API in llama.cpp
- Users express enthusiasm and eagerness to try it out
- Practical tips for quick setup and usage
- Discussion on hardware requirements and context usage

**Discussion Highlights:** The discussion highlights a positive reception with users sharing practical advice for implementation and discussing hardware requirements and context usage.

---

## 34. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 743 | **Comments:** 231 | **Date:** 2026-01-19

**Summary:** The Reddit post discusses the release of the zai-org/GLM-4.7-Flash model on Hugging Face, highlighting its technical features and community excitement.

**Key Points:**
- The model is a 30b variant with efficient memory usage due to MLA.
- Community members express anticipation and enthusiasm for the release.
- The model supports a 200k context length, making it accessible for many users.
- There is nostalgia for larger models like 70b variants.

**Discussion Highlights:** The community is highly engaged, with many users praising the model's technical capabilities and expressing excitement about its potential applications. The discussion also reflects on past models and their features.

---

## 35. [I made a Top-K implementation that's up to 20x faster than PyTorch CPU (open source)](https://reddit.com/r/LocalLLaMA/comments/1qh0yq8/i_made_a_topk_implementation_thats_up_to_20x/)

**Author:** u/andreabarbato | **Upvotes:** 151 | **Comments:** 103 | **Date:** 2026-01-19

**Summary:** The author developed an AVX2-optimized Top-K implementation that significantly outperforms PyTorch CPU, achieving up to 20x speed improvements depending on vocabulary size. It has been integrated into llama.cpp, resulting in 63% faster prompt processing for large models. The implementation uses SIMD and cache optimizations and is available as open-source software.

**Key Points:**
- AVX2-optimized Top-K implementation achieves 4-20x speedup over PyTorch CPU
- Integrated into llama.cpp with 63% faster prompt processing on 120B MoE model
- Uses SIMD, cache optimization, and adaptive sampling techniques
- Open-source with pre-built DLLs and llama.cpp implementation available
- Community feedback includes requests for PR submission and explanations of performance gains

**Discussion Highlights:** The community showed strong interest in the implementation, with top comments requesting a pull request for llama.cpp and explanations of the performance improvements. Some users expressed concerns about the lack of reproducible benchmarks and the authenticity of the post.

---

## 36. [how do you pronounce “gguf”?](https://reddit.com/r/LocalLLaMA/comments/1qglyqz/how_do_you_pronounce_gguf/)

**Author:** u/Hamfistbumhole | **Upvotes:** 109 | **Comments:** 155 | **Date:** 2026-01-18

**Summary:** The Reddit post discusses the pronunciation of 'gguf', with users debating various interpretations such as 'jee-guff', 'giguff', or 'jee jee you eff'. The top comments suggest pronouncing each letter individually or humorously avoiding pronunciation altogether. The discussion is lighthearted and humorous, with no clear consensus on the correct pronunciation. Users enjoy the playful debate and share creative interpretations.

---

## 37. [Are most major agents really just markdown todo list processors?](https://reddit.com/r/LocalLLaMA/comments/1qgj2n9/are_most_major_agents_really_just_markdown_todo/)

**Author:** u/TheDigitalRhino | **Upvotes:** 103 | **Comments:** 37 | **Date:** 2026-01-18

**Summary:** The Reddit post discusses how major LLM agents typically decompose tasks into todo lists and process them sequentially. The discussion highlights that this approach is common and effective, with some users noting its similarity to human problem-solving methods.

**Key Points:**
- Major LLM agents decompose tasks into todo lists and process them one by one.
- This approach includes tool calls and the ability to run terminal commands.
- Breaking down complex tasks into smaller ones is a method used by both humans and agents.
- This task decomposition method has been effective since earlier versions like GPT-3.5.

**Discussion Highlights:** The discussion highlights a consensus that decomposing tasks into smaller, manageable parts is a common and effective strategy used by major LLM agents. Users compare this method to human problem-solving techniques and note its effectiveness in handling complex tasks.

---

## 38. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 349 | **Comments:** 94 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to stay within a ~10,000€ budget. The system is designed for running large AI models locally, with benchmark results showing strong performance across various models. Key points include the system's purpose for local AI model inference, the budget details, strong benchmark performance, community praise for the hardware, and the trend of similar builds. The discussion highlights the community's positive reaction and interest in the hardware setup.

---

## 39. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 450 | **Comments:** 71 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses the potential delay in Qwen 4 development, with the lead developer indicating a focus on quality over speed. The community generally supports this approach, appreciating the emphasis on quality improvements.

**Key Points:**
- Qwen 4 development may be delayed as the team focuses on quality
- Community largely supports the decision to prioritize quality over speed
- Some users caution against jumping to conclusions based on limited information
- There is appreciation for the potential long-term benefits of a more polished release

**Discussion Highlights:** The discussion highlights a consensus around the value of quality-focused development, with many users expressing support for the decision to slow down. Some comments also urge caution against overinterpreting the developer's statement.

---

## 40. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 539 | **Comments:** 117 | **Date:** 2026-01-17

**Summary:** The author upgraded their server from a dual MI100 setup to a quad R9700 setup, achieving 128GB VRAM and 128GB RAM for a cost-effective price. The post includes detailed specs, benchmarks, and a comparison with other high-end GPUs. Key points include the upgrade decision based on benchmarks and price changes, detailed specs totaling $7,035.00, high performance benchmarks with llama 7B Q4_0 model, and positive community feedback. The discussion highlights appreciation for the build and jokes about financial irresponsibility, with the post being well-received and featured on the subreddit's Discord.

---

## 41. [The Search for Uncensored AI (That Isn’t Adult-Oriented)](https://reddit.com/r/LocalLLaMA/comments/1qfq9ez/the_search_for_uncensored_ai_that_isnt/)

**Author:** u/Fun-Situation-4358 | **Upvotes:** 280 | **Comments:** 217 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses the challenge of finding uncensored AI models that prioritize reasoning and creativity over adult-oriented content. The author highlights a gap in the market between heavily restricted corporate AI and shallow adult-focused models, seeking recommendations for genuinely unfiltered AI tools. Key points include the desire for technically advanced AI, the prevalence of adult-oriented models, and the trade-offs of decensoring techniques. The discussion highlights a shared frustration and suggests exploring open-source projects or specific leaderboards for uncensored AI.

---

## 42. [China's AGI-NEXT Conference (Qwen, Kimi, Zhipu, Tencent)](https://reddit.com/r/LocalLLaMA/comments/1qfmc05/chinas_aginext_conference_qwen_kimi_zhipu_tencent/)

**Author:** u/nuclearbananana | **Upvotes:** 117 | **Comments:** 22 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses China's AGI-NEXT Conference, highlighting insights on China vs US AGI development, paths to AGI, compute, and marketing. Key points include Qwen's internal advancements, the belief that the next paradigm may come from OpenAI, and cultural differences in risk-taking for innovation.

**Key Points:**
- Qwen already has Qwen3.5 internally and context windows in the millions.
- The next paradigm in AI is believed to likely come from OpenAI rather than Google.
- Chinese work culture is described as less willing to take risks for innovation.
- Deepseek is noted for its talent concentration but was absent from the conference.

**Discussion Highlights:** The discussion highlights Qwen's advancements and the belief in OpenAI's potential for the next AI paradigm. There is also a note on the cultural differences in innovation and the absence of Deepseek from the conference.

---

## 43. [Best "End of world" model that will run on 24gb VRAM](https://reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/)

**Author:** u/gggghhhhiiiijklmnop | **Upvotes:** 342 | **Comments:** 180 | **Date:** 2026-01-17

**Summary:** The user is seeking recommendations for the best LLM model that can run on a PC with 24GB VRAM and 64GB RAM, in preparation for a hypothetical 'end of world' scenario. The discussion highlights various suggestions, including prioritizing the best available model and considering specific models like Gemma3:27b.

**Key Points:**
- User wants to hoard data and run an LLM on limited hardware (24GB VRAM, 64GB RAM).
- Suggestions include saving the best possible LLM and running it off SSD if necessary.
- Gemma3:27b is recommended for its capabilities, including vision.
- Alternative suggestions include downloading actual Wikipedia backups for offline use.
- The discussion emphasizes practicality in an 'end of world' scenario.

**Discussion Highlights:** The consensus leans towards prioritizing the best available LLM model, even if it requires running off SSD. Specific recommendations include Gemma3:27b for its advanced features. There is also a focus on practical data preservation, such as downloading Wikipedia backups.

---

## 44. [KoboldCpp v1.106 finally adds MCP server support, drop-in replacement for Claude Desktop](https://reddit.com/r/LocalLLaMA/comments/1qfb0gk/koboldcpp_v1106_finally_adds_mcp_server_support/)

**Author:** u/HadesThrowaway | **Upvotes:** 102 | **Comments:** 22 | **Date:** 2026-01-17

**Summary:** KoboldCpp v1.106 introduces native MCP server support, offering a drop-in replacement for Claude Desktop with high compatibility and support for both HTTP and STDIO transports. The update includes a UI overhaul and allows seamless integration with existing tool setups.

**Key Points:**
- KoboldCpp v1.106 adds native MCP server support
- Designed as a drop-in replacement for Claude Desktop with high compatibility
- Supports both HTTP and STDIO transports
- Includes a major UI overhaul
- Allows seamless integration with existing tool configurations

**Discussion Highlights:** The community response is overwhelmingly positive, with users appreciating the compatibility with existing setups and the ease of transition. There is also anticipation for similar MCP support in other tools like llama.cpp, and a guide has been shared to help users get started with MCP in KoboldCpp.

---

