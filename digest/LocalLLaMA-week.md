# r/LocalLLaMA Reading Digest

**Period:** 2026-01-23 to 2026-01-23
**Posts Summarized:** 45
**Total Posts Analyzed:** 45

---

## 1. [Your post is getting popular and we just featured it on our Discord!](https://reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/)

**Author:** u/roculus | **Upvotes:** 216 | **Comments:** 28 | **Date:** 2026-01-23

**Summary:** The Reddit post announces that a user's post has been featured on the subreddit's Discord server and they have been given a special flair. The user expresses annoyance at the bot's public posts and suggests sending private messages instead, questioning if the Discord is being monetized. Key points include the bot's announcement, user annoyance, monetization concerns, community agreement on annoyance, and the existence of a pinned Discord thread. The discussion highlights a general consensus that the bot's public posts are annoying, with some humor about the post gaining traction and speculation about Discord monetization.

---

## 2. [Llama.cpp merges in OpenAI Responses API Support](https://reddit.com/r/LocalLLaMA/comments/1qkm9zb/llamacpp_merges_in_openai_responses_api_support/)

**Author:** u/SemaMod | **Upvotes:** 132 | **Comments:** 35 | **Date:** 2026-01-23

**Summary:** The post discusses the successful integration of OpenAI Responses API support in llama.cpp, highlighting its effectiveness with GLM-4.7-Flash in the Codex CLI harness. Users are generally positive but express concerns about API deprecation and data security.

**Key Points:**
- OpenAI Responses API support has been merged into llama.cpp
- Works well with GLM-4.7-Flash in Codex CLI harness
- Users appreciate the functionality but are cautious about API changes
- Concerns raised about data security due to stateful nature of the API
- Some users are unsure about the implications of the new API

**Discussion Highlights:** The discussion highlights a mix of enthusiasm for the new API's capabilities and concerns about potential API deprecation and data security risks. Users appreciate the functionality but remain cautious about future changes and security implications.

---

## 3. [OpenAI CFO hinting at "Outcome-Based Pricing" (aka royalties on your work)? Makes the case for local even stronger.](https://reddit.com/r/LocalLLaMA/comments/1qkiylw/openai_cfo_hinting_at_outcomebased_pricing_aka/)

**Author:** u/distalx | **Upvotes:** 214 | **Comments:** 100 | **Date:** 2026-01-23

**Summary:** The post discusses OpenAI's potential shift to outcome-based pricing for high-value enterprise deals, clarifying that it does not apply to regular users or indie developers. The author initially misunderstood the scope but corrected it after finding the primary source. Key points include OpenAI's CFO mentioning outcome-based pricing for enterprise deals, the pricing model being aimed at high-value industries like pharmaceuticals, the author's initial misinterpretation and subsequent correction, the importance of self-hosting and local solutions to avoid dependency on cloud APIs, and users expressing concerns about potential royalties and the need for control over their projects. The discussion revolves around the implications of OpenAI's pricing model, with users emphasizing the benefits of local solutions and self-hosting to maintain control and avoid potential royalties.

---

## 4. [Nvidia Introduces PersonaPlex: An Open-Source, Real-Time Conversational AI Voice](https://reddit.com/r/LocalLLaMA/comments/1qkimzg/nvidia_introduces_personaplex_an_opensource/)

**Author:** u/44th--Hokage | **Upvotes:** 199 | **Comments:** 23 | **Date:** 2026-01-22

**Summary:** Nvidia has introduced PersonaPlex, an open-source, real-time conversational AI voice model that enables persona control through text-based role prompts and audio-based voice conditioning. It is trained on synthetic and real conversations to produce natural, low-latency spoken interactions.

**Key Points:**
- PersonaPlex is a real-time, full-duplex speech-to-speech conversational model.
- It enables persona control through text-based role prompts and audio-based voice conditioning.
- The model is trained on a combination of synthetic and real conversations.
- It requires significant VRAM (96GB) to run effectively.
- Some users have noted concerns about the model's performance and audio quality.

**Discussion Highlights:** Users have expressed concerns about the high VRAM requirements (96GB) and the model's performance, comparing it to other models like Moshi and Unmute. There are also comments about the audio quality, with some noting it sounds like narrowband audio.

---

## 5. [Quiet Threadripper AI Workstation - 768GB DDR5 and 160GB VRAM (RTX 5090 + 4x R9700)](https://reddit.com/r/LocalLLaMA/comments/1qkghpk/quiet_threadripper_ai_workstation_768gb_ddr5_and/)

**Author:** u/sloptimizer | **Upvotes:** 130 | **Comments:** 79 | **Date:** 2026-01-22

**Summary:** The post describes a high-performance AI workstation with a Threadripper PRO 7975WX, 768GB DDR5 RAM, and a combination of RTX 5090 and four R9700 GPUs, achieving impressive performance with DeepSeek-V3.1-Terminus. The build includes dual power supplies and custom cooling solutions to manage heat and power demands.

**Key Points:**
- The workstation features a Threadripper PRO 7975WX, 768GB DDR5 RAM, RTX 5090, and four R9700 GPUs.
- Performance metrics for DeepSeek-V3.1-Terminus show 151.76 tps for PP and 10.85 tps for TG.
- Key optimizations include adding RAM fans for cooling, disabling remote management for faster boot, and adjusting power limits for GPUs.
- The build uses dual power supplies (1600W and 850W) to handle the high power demands.
- Top comments highlight the impressive performance and humorously discuss the cost and value of the setup.

**Discussion Highlights:** The discussion highlights the impressive performance of the workstation, with users expressing admiration for the setup and humorously commenting on its cost and value. Key points include the near-SOTA performance, the humor around the cost, and the desire to use such a powerful machine for various applications.

---

## 6. [Am I the only one who feels that, with all the AI boom, everyone is basically doing the same thing?](https://reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/)

**Author:** u/[deleted] | **Upvotes:** 364 | **Comments:** 180 | **Date:** 2026-01-22

**Summary:** The post discusses the redundancy of AI tools and applications during the AI boom, highlighting that many new tools are less polished versions of existing ones. The discussion reflects on the early days of AI technology and the enthusiasm driving shallow implementations.

**Key Points:**
- Many AI tools are redundant or less polished versions of existing ones.
- The barrier to entry for AI development is low, leading to numerous similar applications.
- The current phase is described as the 'hype stage' of AI technology.
- Some developers are focusing on niche tools and specific needs rather than general AI applications.
- There is a consensus that while AI is exciting, the market is saturated with similar ideas.

**Discussion Highlights:** The discussion highlights the enthusiasm and low barrier to entry in AI development, leading to many similar tools. There is a consensus that while AI is promising, the current phase is marked by redundancy and shallow implementations. Some developers are focusing on niche tools to address specific needs.

---

## 7. [vLLM raising $150M confirms it: We have moved from the "Throughput Era" to the "Latency(Cold Starts)."](https://reddit.com/r/LocalLLaMA/comments/1qk68n8/vllm_raising_150m_confirms_it_we_have_moved_from/)

**Author:** u/pmv143 | **Upvotes:** 162 | **Comments:** 88 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses the significant investment in vLLM, signaling a shift from the 'Throughput Era' to the 'Latency Era' in AI. It highlights the growing importance of software over hardware and the need for efficient serving solutions.

**Key Points:**
- vLLM's $150M funding signals a shift from training to serving in AI.
- Software efficiency is becoming more critical than hardware investments.
- The debate around standardization vs. optimization in AI inference.
- Latency, particularly cold starts, is the next major challenge.
- Community discussion includes skepticism and comparisons to other inference solutions.

**Discussion Highlights:** The discussion includes skepticism about the investment's implications, comparisons to other inference solutions like llama.cpp, and debates on the importance of latency versus throughput. Some users argue that cold starts are less of an issue in cloud environments, while others emphasize the potential of local solutions.

---

## 8. [Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp; 1.8B), Support for 10 languages](https://reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/)

**Author:** u/Nunki08 | **Upvotes:** 690 | **Comments:** 96 | **Date:** 2026-01-22

**Summary:** Qwen has open-sourced the Qwen3-TTS model family, including VoiceDesign, CustomVoice, and Base models in 0.6B and 1.8B sizes, supporting 10 languages. The release includes resources on GitHub, Hugging Face, a blog post, a paper, and a demo.

**Key Points:**
- Qwen3-TTS models released in 0.6B and 1.8B sizes
- Supports 10 languages
- Resources available on GitHub, Hugging Face, blog, paper, and demo
- Community feedback highlights model performance and requests for additional support
- Positive reception for Qwen's open-source contributions

**Discussion Highlights:** The community appreciates Qwen's open-source contributions and the model's performance, though some note the English voices sound like anime dubs. There are requests for support in running the models in llama.cpp or similar compiled languages.

---

## 9. [Qwen dev on Twitter!!](https://reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 715 | **Comments:** 60 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses Qwen's TTS model announcement, with community reactions and links to the model on Hugging Face.

**Key Points:**
- Qwen's TTS model announcement
- Community reaction and discussion
- Link to the model on Hugging Face
- Thread locked due to announcements being out
- Mention of vLLM leak related to the TTS model

**Discussion Highlights:** The community is excited about the TTS model, with some users sharing links to the model on Hugging Face. There is also mention of a vLLM leak related to the model, and the thread was locked as announcements were already out.

---

## 10. [GLM 4.7 flash FA fix for CUDA has been merged into llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qjrsur/glm_47_flash_fa_fix_for_cuda_has_been_merged_into/)

**Author:** u/jacek2023 | **Upvotes:** 158 | **Comments:** 51 | **Date:** 2026-01-22

**Summary:** The post announces the merge of GLM 4.7 flash FA fix for CUDA into llama.cpp, with users reporting mixed experiences including issues with quantized cache and performance on Pascal GPUs.

**Key Points:**
- GLM 4.7 flash FA fix for CUDA has been merged into llama.cpp
- Quantized cache is not working well for some users
- Performance issues reported on Pascal GPUs
- General feedback includes successful builds and ongoing bug reports
- CPU usage is high for some configurations

**Discussion Highlights:** Users report mixed experiences with the new implementation, highlighting issues with quantized cache and performance on Pascal GPUs, while others successfully built and ran the model. Ongoing bug reports and discussions indicate active community engagement.

---

## 11. [Fei Fei Li dropped a non-JEPA world model, and the spatial intelligence is insane](https://reddit.com/r/LocalLLaMA/comments/1qjjrmq/fei_fei_li_dropped_a_nonjepa_world_model_and_the/)

**Author:** u/coloradical5280 | **Upvotes:** 179 | **Comments:** 86 | **Date:** 2026-01-21

**Summary:** Fei-Fei Li's World Labs launched Marble, a generative world model using Neural Radiance Fields (NeRF) and Gaussian splatting, enabling fast 3D world creation with stateful, editable environments and VR support. Despite its innovative approach, the technology has faced criticism for not being open-source and for its limited scope.

**Key Points:**
- Marble uses NeRF and Gaussian splatting for fast 3D world generation.
- The model supports stateful, editable environments and VR integration.
- Criticism includes lack of open-source availability and limited environment scope.
- The technology is seen as promising but not yet fully realized.

**Discussion Highlights:** The discussion highlights mixed reactions, with some users criticizing the lack of open-source availability and the limited scope of the generated environments, while others acknowledge the potential of the technology.

---

## 12. [Wrote a guide for running Claude Code with GLM-4.7 Flash locally with llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qjf6ys/wrote_a_guide_for_running_claude_code_with_glm47/)

**Author:** u/tammamtech | **Upvotes:** 110 | **Comments:** 45 | **Date:** 2026-01-21

**Summary:** The post details a guide for running Claude Code with GLM-4.7 Flash locally using llama.cpp, highlighting features like model swapping and GPU memory management. It includes installation and configuration instructions for both direct and Docker setups.

**Key Points:**
- Guide for running Claude Code with GLM-4.7 Flash locally using llama.cpp
- Features like model swapping and GPU memory management
- Installation and configuration instructions provided
- Support for both direct and Docker setups

**Discussion Highlights:** The discussion includes comments about the Anthropic API endpoint implementation, performance queries regarding VRAM and tokens per second, and suggestions for open-source alternatives like OpenCode and Harbor.

---

## 13. [8x AMD MI50 32GB at 26 t/s (tg) with MiniMax-M2.1 and 15 t/s (tg) with GLM 4.7 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1qjaxfy/8x_amd_mi50_32gb_at_26_ts_tg_with_minimaxm21_and/)

**Author:** u/ai-infos | **Upvotes:** 309 | **Comments:** 122 | **Date:** 2026-01-21

**Summary:** The post details a cost-effective local inference setup using 8x AMD MI50 GPUs, achieving 26.8 tok/s with MiniMax-M2.1 and 15.6 tok/s with GLM 4.7, both at 4-bit quantization. The setup costs $880 for 256GB VRAM and aims to provide a highly efficient solution for fast intelligent local inference.

**Key Points:**
- Performance: MiniMax-M2.1 at 26.8 tok/s and GLM 4.7 at 15.6 tok/s
- Cost: $880 for 8 GPUs with 256GB VRAM
- Power draw: 280W idle / 1200W during inference
- Stable long context performance for coding agents
- Community appreciation for cost-effectiveness and performance

**Discussion Highlights:** The community praised the setup for its cost-effectiveness and performance, with comments highlighting the impressive VRAM capacity for under $1k and the stability of the models for long context tasks.

---

## 14. [VibeVoice-ASR released!](https://reddit.com/r/LocalLLaMA/comments/1qj40er/vibevoiceasr_released/)

**Author:** u/k_means_clusterfuck | **Upvotes:** 153 | **Comments:** 44 | **Date:** 2026-01-21

**Summary:** Microsoft has released VibeVoice-ASR, a multilingual automatic speech recognition model with 9B parameters. Users report high accuracy, especially in multilingual contexts, though some note challenges with polyphonic characters.

**Key Points:**
- VibeVoice-ASR is a 9B parameter model released by Microsoft.
- The model supports multiple languages and shows high accuracy in tests.
- Users report around 91% accuracy in Chinese audio transcription.
- Some challenges noted with polyphonic characters in names.
- Comparisons made to other models like Whisper and Parakeet.

**Discussion Highlights:** Users generally praise the model's performance and multilingual capabilities, though some express concerns about its large size and lack of benchmarks. A few users share their test results, highlighting both strengths and limitations.

---

## 15. [One-shot single page web development: pacman clone - GLM 4.7 vs GLM 4.7 Flash vs GLM 4.5 Air vs Gemini 3 Pro vs Gemini 3 Flash - Results available for online testing - Prompt and instructions provided for testing with other models](https://reddit.com/r/LocalLLaMA/comments/1qj13uh/oneshot_single_page_web_development_pacman_clone/)

**Author:** u/ex-arman68 | **Upvotes:** 106 | **Comments:** 48 | **Date:** 2026-01-21

**Summary:** The post compares AI models in generating a Pacman clone webpage, with GLM 4.7 ranking as the top performer, followed by Minimax M2.1 and Gemini 3 Flash. The discussion highlights the effectiveness of the testing methodology and the surprising performance of GLM 4.7.

---

## 16. [GLM-4.7-Flash-GGUF bug fix - redownload for better outputs](https://reddit.com/r/LocalLLaMA/comments/1qiy0ha/glm47flashgguf_bug_fix_redownload_for_better/)

**Author:** u/etherd0t | **Upvotes:** 113 | **Comments:** 58 | **Date:** 2026-01-21

**Summary:** The post announces a bug fix for the GLM-4.7-Flash-GGUF model, which previously caused looping and poor outputs. Users are advised to re-download the model and use specific parameters for optimal performance.

**Key Points:**
- Bug fix for GLM-4.7-Flash-GGUF model addressing looping and poor outputs
- Recommended parameters for general use and tool-calling provided
- Users report significant improvement in model performance post-update
- Some users note the model is slower compared to alternatives like GPT-OSS-20b
- Positive feedback on the fix and appreciation for the update

**Discussion Highlights:** The discussion highlights a consensus on the effectiveness of the bug fix, with users reporting better outputs and fewer issues. However, some users mention performance concerns, particularly in comparison to other models. Overall, the update is well-received and appreciated by the community.

---

## 17. [Fix for GLM 4.7 Flash has been merged into llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qiwm3c/fix_for_glm_47_flash_has_been_merged_into_llamacpp/)

**Author:** u/jacek2023 | **Upvotes:** 308 | **Comments:** 86 | **Date:** 2026-01-21

**Summary:** The post announces the integration of GLM 4.7 Flash into llama.cpp, highlighting its significance and ongoing developments like CUDA support. The discussion includes performance metrics and user experiences.

**Key Points:**
- GLM 4.7 Flash has been merged into llama.cpp
- CUDA support is in progress
- Performance metrics for GLM 4.7 on different GPUs are discussed
- Users report improved model performance with no gibberish or repetition
- Discussion includes queries about CPU-only performance and model downloads

**Discussion Highlights:** Users share performance metrics for GLM 4.7 on various GPUs, report improved model behavior, and discuss practical aspects like model downloads and CPU performance.

---

## 18. [Knowledge distillation with Claude as the interface: trained a 0.6B model to match GPT-class performance on Text2SQL in a singe conversation](https://reddit.com/r/LocalLLaMA/comments/1qiu6jo/knowledge_distillation_with_claude_as_the/)

**Author:** u/party-horse | **Upvotes:** 164 | **Comments:** 37 | **Date:** 2026-01-21

**Summary:** The post describes a workflow for training small, task-specific models using knowledge distillation via Claude, achieving significant performance improvements in Text2SQL tasks with minimal setup overhead.

**Key Points:**
- Off-the-shelf small models often perform poorly on specialized tasks like Text2SQL.
- Knowledge distillation via Claude simplifies the fine-tuning process by automating data preparation, training, and packaging.
- The approach improved a 0.6B model's Text2SQL performance from 36% to 74% using synthetic data from a larger teacher model.
- The method is praised for its potential in training small models for on-device inference and log analysis.
- Some discussion points include the use of SQL AST for validation and the feasibility of using open-source alternatives to Claude.

**Discussion Highlights:** The discussion highlights the innovation and practicality of the approach, with users praising its simplicity and potential applications. Some suggestions include using SQL AST for validation and exploring open-source alternatives for broader accessibility.

---

## 19. [vLLM v0.14.0 released](https://reddit.com/r/LocalLLaMA/comments/1qim0e9/vllm_v0140_released/)

**Author:** u/jinnyjuice | **Upvotes:** 165 | **Comments:** 34 | **Date:** 2026-01-20

**Summary:** The Reddit post announces the release of vLLM v0.14.0, highlighting new features and updates. Users discuss improvements like automatic context length fitting and the deprecation of certain quantization methods. Key points include automatic context length fitting to GPU memory, deprecation of some quantization methods including HQQ, Marlin for Turing (sm75) as a major upgrade, user excitement about eliminating OOM startup failures, and discussion about future optimizations for sm120. Users expressed enthusiasm about the automatic context length feature and discussed the implications of deprecated quantization methods. The Marlin upgrade for Turing was noted as a significant improvement.

---

## 20. [Current GLM-4.7-Flash implementation confirmed to be broken in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qih9r8/current_glm47flash_implementation_confirmed_to_be/)

**Author:** u/Sweet_Albatross9772 | **Upvotes:** 243 | **Comments:** 60 | **Date:** 2026-01-20

**Summary:** The Reddit post discusses issues with the current GLM-4.7-Flash implementation in llama.cpp, highlighting significant differences in logprobs compared to vLLM, which may explain reported problems like looping and poor performance. A potential fix is already proposed in a pull request.

**Key Points:**
- Current GLM-4.7-Flash implementation in llama.cpp is confirmed broken
- Significant differences in logprobs compared to vLLM are noted
- A potential fix is available in a pull request
- Community acknowledges the issue and expects a resolution soon
- Some users suggest waiting before adopting new models to avoid bugs

**Discussion Highlights:** The discussion highlights a confirmed issue with the GLM-4.7-Flash implementation in llama.cpp, with users acknowledging the problem and pointing to a potential fix. The community is generally optimistic about a quick resolution, with some users recommending patience before adopting new models.

---

## 21. [You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?](https://reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/)

**Author:** u/Adventurous-Gold6413 | **Upvotes:** 539 | **Comments:** 299 | **Date:** 2026-01-20

**Summary:** The post discusses the selection of local models for use with 64GB RAM and 16GB VRAM when internet access is unavailable. Users share their preferred models and experiences.

**Key Points:**
- The post asks for recommendations on local models to use with specific hardware constraints.
- Top comments highlight models like Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B.
- GPT-OSS 120B is praised for its performance and versatility on the given hardware.
- Community consensus leans towards models that fit well within the hardware specifications and offer good performance.

**Discussion Highlights:** The discussion highlights a preference for models like GPT-OSS 120B, which is noted for its good performance and fit within the hardware constraints. Other models like Gemma 3 27B and GLM 4.5 Air are also mentioned as viable options.

---

## 22. [Liquid AI released the best thinking Language Model Under 1GB](https://reddit.com/r/LocalLLaMA/comments/1qi512t/liquid_ai_released_the_best_thinking_language/)

**Author:** u/PauLabartaBajo | **Upvotes:** 229 | **Comments:** 52 | **Date:** 2026-01-20

**Summary:** Liquid AI released LFM2.5-1.2B-Thinking, a reasoning model that runs on-device with 900 MB of memory, excelling in math, tool use, and instruction following. It outperforms larger models in speed and efficiency, with broad ecosystem support.

**Key Points:**
- LFM2.5-1.2B-Thinking is optimized for concise reasoning and runs on-device with 900 MB memory.
- It outperforms larger models in speed and memory efficiency, especially in math and tool use.
- The model is available on Hugging Face, LEAP, and Liquid AI Playground.
- Discussion highlights concerns about memory requirements, performance trade-offs, and licensing.
- Some users wish for larger models for real-world applications.

**Discussion Highlights:** The discussion includes concerns about memory requirements for edge deployment, comparisons with other models, and criticism of the non-Apache/MIT licensing. Some users appreciate the model's capabilities but desire larger versions for broader applications.

---

## 23. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 869 | **Comments:** 261 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, fully enclosed 10-GPU mobile AI system designed for running large MoE models and supporting graphic design tasks. The system balances performance and cost, with a focus on mobility and protection from pets.

**Key Points:**
- Custom-built system with Threadripper Pro 3995WX, 512GB DDR4, and 10 GPUs (8x 3090 + 2x 5090)
- Designed for large MoE models, video generation, and high-detail image generation
- Fully enclosed for mobility and protection from pets, with a total cost of ~$17k
- Top comment highlights the system's portability and power requirements
- Discussion includes concerns about airflow and the physical setup of the GPUs

**Discussion Highlights:** The discussion highlights the system's portability and power needs, with humorous comments about its size and power consumption. Some users express concerns about airflow and the physical arrangement of the GPUs.

---

## 24. [Over 6K novels with reasoning traces to train full book writing LLMs](https://reddit.com/r/LocalLLaMA/comments/1qi3nmd/over_6k_novels_with_reasoning_traces_to_train/)

**Author:** u/XMasterDE | **Upvotes:** 110 | **Comments:** 46 | **Date:** 2026-01-20

**Summary:** The post announces an update to the LongPage dataset, expanding it to over 6,000 novels with hierarchical planning traces for training full-book writing LLMs. The team is also training a model on this dataset and plans to release it soon.

**Key Points:**
- LongPage dataset expanded to 6K+ novels with hierarchical planning traces
- Dataset supports training full-book writing LLMs
- Early model checkpoints are being tested internally
- Community shows strong interest in the project
- Requests for more details on dataset usage and processing

**Discussion Highlights:** The community is eager to see the results, with many requesting more details on how the dataset works and whether it includes specific books. There is also interest in the code for data processing to create datasets in other languages.

---

## 25. [glm-4.7-flash has the best thinking process with clear steps, I love it](https://reddit.com/r/LocalLLaMA/comments/1qhxlgy/glm47flash_has_the_best_thinking_process_with/)

**Author:** u/uptonking | **Upvotes:** 139 | **Comments:** 34 | **Date:** 2026-01-20

**Summary:** The post discusses the user's experience with glm-4.7-flash, highlighting its structured thinking process and comparing it favorably to other models like nemotron-nano and qwen3-30b. The user appreciates the model's clear reasoning steps but notes its slow performance and occasional looping issues.

**Key Points:**
- glm-4.7-flash has a detailed and structured thinking process with clear steps.
- The model's thinking duration is longer compared to other models, but the quality of reasoning is preferred.
- The user faces performance issues with slow token generation and occasional looping.
- Adjusting parameters like temperature and repeat penalty helps improve performance.
- The community generally appreciates the model's reasoning process but acknowledges its performance limitations.

**Discussion Highlights:** The discussion highlights a consensus on the model's strong reasoning capabilities but also notes its performance issues. Users suggest adjusting parameters to improve speed and stability, and there is general appreciation for the model's structured thinking process.

---

## 26. [It's been one year since the release of Deepseek-R1](https://reddit.com/r/LocalLLaMA/comments/1qhs2sd/its_been_one_year_since_the_release_of_deepseekr1/)

**Author:** u/Recoil42 | **Upvotes:** 302 | **Comments:** 51 | **Date:** 2026-01-19

**Summary:** The Reddit post commemorates the one-year anniversary of the Deepseek-R1 model release, highlighting its significant impact on the AI community. The discussion reflects on the model's disruptive influence, including its role in reshaping AI development and competition.

**Key Points:**
- Deepseek-R1's release had a major impact, leading to significant changes in the AI landscape.
- The model's influence was so profound that it reportedly caused major shifts in competing AI teams and strategies.
- The release is considered one of the most important in AI history, second only to the original Llama model.
- The rapid pace of AI advancements is noted, with the past year feeling like multiple years due to the volume of changes.
- There is curiosity about how current smaller models compare to R1 in terms of performance and size.

**Discussion Highlights:** The discussion highlights the model's disruptive impact, with users emphasizing its role in forcing changes in AI development strategies, reducing costs, and increasing transparency. The consensus is that Deepseek-R1 was a pivotal release that accelerated progress in the field.

---

## 27. [Mosquito - 7.3M parameter tiny knowledge model](https://reddit.com/r/LocalLLaMA/comments/1qhqzsi/mosquito_73m_parameter_tiny_knowledge_model/)

**Author:** u/Lopsided-Repair-3638 | **Upvotes:** 117 | **Comments:** 53 | **Date:** 2026-01-19

**Summary:** The post introduces 'Mosquito,' a tiny 7.3M parameter language model that can answer general knowledge questions, though with some humorous inaccuracies. Users shared mixed reactions, highlighting both its surprising capabilities and obvious limitations.

**Key Points:**
- Mosquito is a small language model with 7.3M parameters.
- It can answer general knowledge questions but with notable inaccuracies.
- Users found some responses humorous, such as defining a dog incorrectly.
- There is a request for a quantized version of the model.
- The model's knowledge gaps are highlighted, like knowing 'LLM' but not 'dog.'

**Discussion Highlights:** The discussion is lighthearted, with users sharing funny or incorrect responses from the model. There is a consensus that while the model is impressive for its size, it has significant limitations and inaccuracies.

---

## 28. [Bartowski comes through again. GLM 4.7 flash GGUF](https://reddit.com/r/LocalLLaMA/comments/1qhpima/bartowski_comes_through_again_glm_47_flash_gguf/)

**Author:** u/RenewAi | **Upvotes:** 185 | **Comments:** 50 | **Date:** 2026-01-19

**Summary:** The post announces the release of GLM 4.7 Flash GGUF by Bartowski, with mixed user experiences reported in the comments.

**Key Points:**
- Bartowski released GLM 4.7 Flash GGUF on Hugging Face
- Users report mixed results with different versions of GLM 4.7 Flash
- Some users find the model non-functional or 'brain dead'
- Unsloth also released a version of GLM 4.7 Flash GGUF
- Community is actively testing and discussing the model's performance

**Discussion Highlights:** The discussion reveals mixed experiences with GLM 4.7 Flash, with some users reporting issues while others are exploring different versions. There is no clear consensus on the model's effectiveness yet.

---

## 29. [Unsloth GLM 4.7-Flash GGUF](https://reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/)

**Author:** u/Wooden-Deer-1276 | **Upvotes:** 230 | **Comments:** 44 | **Date:** 2026-01-19

**Summary:** The Reddit post discusses the release of the Unsloth GLM 4.7-Flash GGUF model, with updates on available quantizations and ongoing fixes for issues like looping in quantized versions. The community emphasizes patience and proper testing before release. Key points include recommendations to use UD-Q4_K_XL and above, known issues with looping in quantized versions, the release of the BF16 version for best results, and specific settings for tools like LM Studio. The discussion highlights community enthusiasm and a focus on addressing technical issues.

---

## 30. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 366 | **Comments:** 60 | **Date:** 2026-01-19

**Summary:** The post announces official support for GLM 4.7 Flash in llama.cpp, highlighting community efforts and providing additional resources.

**Key Points:**
- GLM 4.7 Flash now officially supported in llama.cpp
- Support is a community effort, not from Z.ai developers
- Performance comparisons with VLLM and CUDA noted
- Additional resources and model versions shared
- Mixed feedback on flash-attention performance

**Discussion Highlights:** The discussion highlights the community-driven nature of the support, performance comparisons, and additional resources shared by users. Some users noted performance issues with flash-attention, suggesting alternatives.

---

## 31. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 459 | **Comments:** 162 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights the effectiveness of GLM 4.7 Flash as a reliable local agent for various tasks, with the author praising its performance and stability. The discussion includes comparisons with other models and notes on its performance and output quality.

**Key Points:**
- GLM 4.7 Flash is praised for its reliability and performance in agentic frameworks.
- The model has been tested extensively without errors in tasks like cloning repos and running commands.
- Comparisons with other models like Nemotron 30B and Qwen3 are discussed.
- GGUFs for local testing are anticipated.
- Performance benchmarks suggest it is comparable to SEED OSS 36B but with better performance due to MoE.

**Discussion Highlights:** The discussion highlights a positive consensus on GLM 4.7 Flash's performance and reliability, with users expressing enthusiasm for its potential as a local agent. Comparisons with other models and notes on its performance and output quality are also discussed.

---

## 32. [New in llama.cpp: Anthropic Messages API](https://reddit.com/r/LocalLLaMA/comments/1qhaq21/new_in_llamacpp_anthropic_messages_api/)

**Author:** u/paf1138 | **Upvotes:** 165 | **Comments:** 51 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the introduction of the Anthropic Messages API in llama.cpp, generating excitement among users. Practical tips for implementation and discussions about hardware compatibility are highlighted.

**Key Points:**
- Introduction of Anthropic Messages API in llama.cpp
- Enthusiasm and immediate interest in trying out the new feature
- Practical tips for setting up and using the API
- Discussion about hardware compatibility and context usage
- Mixed reactions regarding the timeliness of the announcement

**Discussion Highlights:** The discussion highlights a positive reception to the new API, with users sharing practical setup tips and discussing hardware compatibility. Some users noted the announcement was not recent, but overall, the sentiment was enthusiastic and collaborative.

---

## 33. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 733 | **Comments:** 231 | **Date:** 2026-01-19

**Summary:** The Reddit post discusses the release of the zai-org/GLM-4.7-Flash model on Hugging Face, highlighting its technical features and community excitement.

**Key Points:**
- The model is a 30B parameter model with a 3B thinking component.
- It uses MLA, which reduces KV cache memory usage, enabling longer context lengths.
- The community expresses enthusiasm and anticipation for the release.
- The model is noted for its potential to run efficiently with a 200k context length.

**Discussion Highlights:** The community is highly positive about the release, emphasizing the model's technical advancements and potential usability improvements.

---

## 34. [I made a Top-K implementation that's up to 20x faster than PyTorch CPU (open source)](https://reddit.com/r/LocalLLaMA/comments/1qh0yq8/i_made_a_topk_implementation_thats_up_to_20x/)

**Author:** u/andreabarbato | **Upvotes:** 150 | **Comments:** 103 | **Date:** 2026-01-19

**Summary:** The author developed an AVX2-optimized Top-K implementation that significantly outperforms PyTorch CPU, achieving up to 20x speed improvements depending on vocabulary size. It has been integrated into llama.cpp, resulting in 63% faster prompt processing for large models.

**Key Points:**
- AVX2-optimized batched Top-K implementation beats PyTorch CPU by 4-20x
- Integrated into llama.cpp with 63% faster prompt processing on a 120B MoE model
- Uses adaptive sampling, AVX2 SIMD, and cache-optimized scanning
- GitHub repository provided for open-source access
- Community feedback includes requests for PRs and explanations of performance gains

**Discussion Highlights:** The community showed strong interest, with top comments requesting a pull request for llama.cpp, explanations for the performance improvements, and some criticism regarding the lack of reproducible benchmarks. There was also a comment about the authenticity of the post.

---

## 35. [how do you pronounce “gguf”?](https://reddit.com/r/LocalLLaMA/comments/1qglyqz/how_do_you_pronounce_gguf/)

**Author:** u/Hamfistbumhole | **Upvotes:** 111 | **Comments:** 155 | **Date:** 2026-01-18

**Summary:** The Reddit post discusses the pronunciation of 'gguf,' with users offering various suggestions such as 'jee-guff,' 'giguff,' and 'jee jee you eff.' The comments reflect a mix of humorous and serious responses, with no clear consensus on the correct pronunciation. Key points include the post asking how to pronounce 'gguf' and providing several options, top comments suggesting pronouncing each letter individually, similar to '.PNG,' other suggestions including 'jee-guff,' 'giguff,' and 'jee jee you eff,' and some users humorously suggesting not pronouncing it at all, but rather downloading it silently. The discussion highlights a lack of consensus on the pronunciation of 'gguf,' with users offering a variety of interpretations. The top comment suggests pronouncing each letter individually, while others propose different phonetic interpretations. The overall tone is lighthearted, with some humorous responses.

---

## 36. [Are most major agents really just markdown todo list processors?](https://reddit.com/r/LocalLLaMA/comments/1qgj2n9/are_most_major_agents_really_just_markdown_todo/)

**Author:** u/TheDigitalRhino | **Upvotes:** 102 | **Comments:** 37 | **Date:** 2026-01-18

**Summary:** The Reddit post discusses the observation that most major AI agents decompose tasks into todo lists and process them sequentially. The discussion highlights that this approach is common and effective, with some users noting its similarity to human problem-solving methods.

**Key Points:**
- Most major AI agents decompose tasks into todo lists and process them one by one.
- This approach includes tool calls and the ability to run terminal commands.
- Breaking down complex tasks into smaller ones is a method used by humans as well.
- This task decomposition method has been effective since earlier versions of language models like GPT-3.5.

**Discussion Highlights:** The discussion generally agrees that decomposing tasks into smaller, manageable parts is a common and effective strategy used by AI agents. Some users draw parallels to human problem-solving methods, emphasizing the practicality of this approach.

---

## 37. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 345 | **Comments:** 94 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to stay within a ~10,000€ budget. The system is designed for running large language models (120B+ parameters) locally, with benchmark results provided for various models. Key points include maximizing VRAM for local model running, a total cost of ~9,800€ with a 50% subsidy, and performance metrics for models ranging from 8B to 230B parameters. The discussion highlights admiration for the build and questions about component sourcing and job context.

---

## 38. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 455 | **Comments:** 71 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses a potential slowdown in the development of Qwen 4, with the lead developer emphasizing a focus on quality over quantity. The community generally appreciates this approach, though some caution against overinterpreting the announcement.

**Key Points:**
- Qwen 4 development may be slowing down to prioritize quality.
- The community largely supports the focus on quality improvements.
- Some users urge caution against jumping to conclusions based on limited information.
- Incremental updates are seen as less impactful compared to meaningful advancements.

**Discussion Highlights:** The discussion highlights a positive reception to the focus on quality, with many users expressing appreciation for a more deliberate development approach. However, there is also a note of caution about misinterpreting the announcement, emphasizing the need for more concrete information before drawing conclusions.

---

## 39. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 540 | **Comments:** 117 | **Date:** 2026-01-17

**Summary:** The post details a server build featuring four AMD Radeon AI PRO R9700 GPUs, providing 128GB VRAM for under $7,035. The author switched from MI100 GPUs due to better performance and cost efficiency, sharing benchmarks and build specifications.

**Key Points:**
- Server build with 4x AMD Radeon AI PRO R9700 GPUs (128GB VRAM total)
- Cost-effective alternative to MI100 GPUs with better performance
- Total build cost: $7,035, including high-end components like a 1600W PSU and MSI MEG X570 GODLIKE motherboard
- Benchmarks show strong performance in prompt processing
- Community reaction highlights appreciation for the build and humor about financial irresponsibility

**Discussion Highlights:** The community praised the build, with top comments highlighting its appeal and joking about the financial cost. One comment noted the post's popularity, while others expressed admiration for the setup.

---

## 40. [The Search for Uncensored AI (That Isn’t Adult-Oriented)](https://reddit.com/r/LocalLLaMA/comments/1qfq9ez/the_search_for_uncensored_ai_that_isnt/)

**Author:** u/Fun-Situation-4358 | **Upvotes:** 281 | **Comments:** 216 | **Date:** 2026-01-17

**Summary:** The post discusses the challenge of finding uncensored AI models that prioritize reasoning and creativity over adult-oriented content, highlighting a gap between heavily restricted corporate AI and shallow adult-focused models.

**Key Points:**
- The author seeks an AI that is genuinely unfiltered and technically advanced, focusing on reasoning and creativity.
- Most models marketed as 'uncensored' are optimized for adult use rather than intelligence or depth.
- There is a perceived gap between heavily restricted corporate AI and shallow adult-focused models.
- The Uncensored General Intelligence Leaderboard is suggested as a resource for finding suitable models.
- Decensoring techniques often result in reduced intelligence or performance.

**Discussion Highlights:** The discussion highlights a shared frustration with the lack of AI models that balance uncensored capabilities with serious problem-solving and creativity. Users express a desire for AI that acts like an AI rather than a hall monitor, without resorting to adult-oriented content. The consensus suggests that while there are resources like the Uncensored General Intelligence Leaderboard, the current landscape lacks models that meet these criteria effectively.

---

## 41. [China's AGI-NEXT Conference (Qwen, Kimi, Zhipu, Tencent)](https://reddit.com/r/LocalLLaMA/comments/1qfmc05/chinas_aginext_conference_qwen_kimi_zhipu_tencent/)

**Author:** u/nuclearbananana | **Upvotes:** 121 | **Comments:** 22 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses China's AGI-NEXT Conference, highlighting insights on China vs US AGI development, paths to AGI, compute, and marketing. Key points include Qwen's internal advancements, the belief that the next paradigm may come from OpenAI, and cultural differences in innovation.

**Key Points:**
- Qwen has internally developed Qwen3.5 and context windows in the millions.
- The next paradigm in AI is believed to likely come from OpenAI rather than Google.
- Chinese work culture is described as less willing to take risks for innovation.
- Deepseek is noted as a top lab in talent concentration but was absent from the conference.

**Discussion Highlights:** The discussion highlights Qwen's advancements and the belief in OpenAI's potential leadership in the next AI paradigm. There is also a note on the risk-averse culture in Chinese AI development and the absence of Deepseek from the conference.

---

## 42. [Best "End of world" model that will run on 24gb VRAM](https://reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/)

**Author:** u/gggghhhhiiiijklmnop | **Upvotes:** 336 | **Comments:** 180 | **Date:** 2026-01-17

**Summary:** The post discusses finding the best LLM model to download and store for an 'end of world' scenario, with a focus on models that fit within 24GB VRAM and 64GB RAM. The discussion includes suggestions for specific models and practical advice on data storage.

**Key Points:**
- User seeks LLM models that fit within 24GB VRAM and 64GB RAM for an 'end of world' scenario.
- Suggestions include saving the best LLM possible and running it off SSD if necessary.
- Specific model recommendations include gemma3:27b and Midnight Miku.
- Advice to download actual Wikipedia backups for offline access.
- Discussion highlights practical considerations for data storage and accessibility.

**Discussion Highlights:** The discussion emphasizes practicality, with a consensus on prioritizing the best available LLM models and ensuring data accessibility through methods like running models off SSD. Specific model recommendations and advice on downloading Wikipedia backups are notable highlights.

---

## 43. [KoboldCpp v1.106 finally adds MCP server support, drop-in replacement for Claude Desktop](https://reddit.com/r/LocalLLaMA/comments/1qfb0gk/koboldcpp_v1106_finally_adds_mcp_server_support/)

**Author:** u/HadesThrowaway | **Upvotes:** 101 | **Comments:** 22 | **Date:** 2026-01-17

**Summary:** KoboldCpp v1.106 introduces native MCP server support, offering a drop-in replacement for Claude Desktop with enhanced tool management and compatibility features.

**Key Points:**
- KoboldCpp v1.106 adds native MCP server support
- Designed as a drop-in replacement for Claude Desktop with full compatibility
- Supports both HTTP and STDIO transports for MCP servers
- Allows tool selection and optional tool call approvals
- Community feedback highlights ease of integration and compatibility

**Discussion Highlights:** The community appreciates the seamless integration with existing setups and the comprehensive guide provided in the wiki. There is also interest in similar MCP support for other platforms like llama.cpp.

---

## 44. [DeepSeek Engram : A static memory unit for LLMs](https://reddit.com/r/LocalLLaMA/comments/1qf5oj0/deepseek_engram_a_static_memory_unit_for_llms/)

**Author:** u/Technical-Love-8479 | **Upvotes:** 328 | **Comments:** 48 | **Date:** 2026-01-17

**Summary:** DeepSeek AI introduced Engram, a memory unit for LLMs that separates remembering from reasoning, enabling O(1) knowledge lookup and improving reasoning, math, and code performance.

**Key Points:**
- Engram introduces conditional memory, separating remembering from reasoning.
- Knowledge is looked up in O(1) instead of recomputed, improving efficiency.
- Uses explicit parametric memory, enabling massive memory scaling without GPU limits.
- Improves reasoning, math, and code performance.
- Frees attention for global reasoning rather than static knowledge.

**Discussion Highlights:** The community highlights the significance of separating memory from reasoning and appreciates the scalability of memory independent of model size, which addresses GPU memory limits.

---

## 45. ["Welcome to the Local Llama. How janky's your rig?](https://reddit.com/r/LocalLLaMA/comments/1qf514i/welcome_to_the_local_llama_how_jankys_your_rig/)

**Author:** u/ForsookComparison | **Upvotes:** 104 | **Comments:** 22 | **Date:** 2026-01-16

**Summary:** The Reddit post from r/LocalLLaMA discusses various unconventional and humorous setups for running GPUs, highlighting the creative and sometimes janky solutions users employ.

**Key Points:**
- Users share unconventional GPU setups and cooling solutions.
- Discussion includes humorous and creative approaches to hardware limitations.
- Specific mentions of hardware like 3090, MI50, and cooling solutions like baby oil and blower fans.
- Comments highlight the challenges and improvisations in building and maintaining GPU rigs.

**Discussion Highlights:** The discussion is light-hearted and humorous, with users sharing their unique and sometimes unconventional solutions to hardware challenges. There is no clear consensus but a shared appreciation for creativity and improvisation in GPU setups.

---

