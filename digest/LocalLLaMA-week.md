# r/LocalLLaMA Reading Digest

**Period:** 2026-01-23 to 2026-01-23
**Posts Summarized:** 45
**Total Posts Analyzed:** 45

---

## 1. [Your post is getting popular and we just featured it on our Discord!](https://reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/)

**Author:** u/roculus | **Upvotes:** 309 | **Comments:** 32 | **Date:** 2026-01-23

**Summary:** The post announces that a user's contribution was featured on Discord and they received a special flair. The community expresses annoyance at the bot's public posts, suggesting private messages instead.

**Key Points:**
- The bot announces a user's post being featured on Discord.
- The user receives a special flair for their contribution.
- The community finds the bot's public posts annoying.
- Suggestions are made to send private messages instead.
- Concerns about monetization and spam are raised.

**Discussion Highlights:** The community consensus is that the bot's public posts are annoying and should be replaced with private messages. There are also concerns about potential monetization and the increase in bot spam.

---

## 2. [Llama.cpp merges in OpenAI Responses API Support](https://reddit.com/r/LocalLLaMA/comments/1qkm9zb/llamacpp_merges_in_openai_responses_api_support/)

**Author:** u/SemaMod | **Upvotes:** 144 | **Comments:** 35 | **Date:** 2026-01-23

**Summary:** The post discusses the successful integration of OpenAI Responses API support in llama.cpp, highlighting the author's positive experience with GLM-4.7-Flash capability in the Codex CLI harness for exploring large codebases.

**Key Points:**
- OpenAI Responses API support has been merged into llama.cpp
- Author successfully used GLM-4.7-Flash with Codex CLI
- The new API enables stateful interaction with OpenAI models
- Concerns about potential data leaks due to stateful nature of the API
- Discussion about the future of the old API and its potential deprecation

**Discussion Highlights:** The discussion highlights a mix of enthusiasm for the new API's capabilities and concerns about data security and the future of the old API. Some users are unsure about the implications of the new API, while others appreciate its functionality.

---

## 3. [OpenAI CFO hinting at "Outcome-Based Pricing" (aka royalties on your work)? Makes the case for local even stronger.](https://reddit.com/r/LocalLLaMA/comments/1qkiylw/openai_cfo_hinting_at_outcomebased_pricing_aka/)

**Author:** u/distalx | **Upvotes:** 225 | **Comments:** 101 | **Date:** 2026-01-23

**Summary:** The post discusses OpenAI's potential shift to outcome-based pricing for high-value enterprise deals, clarifying that it does not apply to regular users or indie developers. The author initially misinterpreted the scope but corrected it after finding the primary source. Key points include: OpenAI's CFO mentioned outcome-based pricing for enterprise deals, not regular users; the pricing model is aimed at high-value industries like pharmaceuticals; the author corrected their initial misinterpretation of the pricing model's scope; the discussion highlights the importance of self-hosting and local AI stacks to avoid potential future costs; and users expressed concerns about OpenAI's potential to charge royalties on discoveries made using their AI. The discussion revolves around the implications of OpenAI's pricing model, with users emphasizing the benefits of local AI stacks and self-hosting to maintain control and avoid potential future costs. There is a consensus on the importance of understanding the terms and conditions of using cloud-based AI services.

---

## 4. [Nvidia Introduces PersonaPlex: An Open-Source, Real-Time Conversational AI Voice](https://reddit.com/r/LocalLLaMA/comments/1qkimzg/nvidia_introduces_personaplex_an_opensource/)

**Author:** u/44th--Hokage | **Upvotes:** 206 | **Comments:** 23 | **Date:** 2026-01-22

**Summary:** Nvidia's PersonaPlex is an open-source, real-time conversational AI voice model that enables persona control through text-based role prompts and audio-based voice conditioning. It is trained on synthetic and real conversations to produce natural, low-latency spoken interactions.

**Key Points:**
- PersonaPlex is a real-time, full-duplex speech-to-speech conversational model.
- It supports persona control via text-based role prompts and audio-based voice conditioning.
- The model is trained on a mix of synthetic and real conversations.
- It requires significant VRAM (96GB) for optimal performance.
- Some users report mixed experiences with model performance and audio quality.

**Discussion Highlights:** Users discussed the high VRAM requirements (96GB), compared its performance to other models like Moshi and Unmute, and noted issues with audio quality, describing it as narrowband. Some users also questioned its ability to handle multitasking, such as tool calls during conversations.

---

## 5. [Quiet Threadripper AI Workstation - 768GB DDR5 and 160GB VRAM (RTX 5090 + 4x R9700)](https://reddit.com/r/LocalLLaMA/comments/1qkghpk/quiet_threadripper_ai_workstation_768gb_ddr5_and/)

**Author:** u/sloptimizer | **Upvotes:** 145 | **Comments:** 80 | **Date:** 2026-01-22

**Summary:** The Reddit post details a high-performance AI workstation build featuring a Threadripper PRO 7975WX CPU, 768GB DDR5 RAM, an RTX 5090, and four R9700 GPUs. The setup achieves impressive performance metrics with DeepSeek-V3.1-Terminus, including 151.76 tokens per second in prefill and 10.85 tokens per second in generation. The build also includes insights on optimizing performance and cooling for such a powerful system.

**Key Points:**
- The workstation combines Nvidia and AMD GPUs, achieving high performance with DeepSeek-V3.1-Terminus.
- Optimizations include adjusting power limits for the RTX 5090 and setting performance levels for the R9700 GPUs.
- Cooling challenges with water-cooled CPU systems were addressed by adding RAM fans, resulting in a 30% performance boost.
- The build uses dual power supplies to handle the high power demands of the system.
- The post highlights the capability to run near-state-of-the-art AI models locally at usable speeds.

**Discussion Highlights:** The top comments express admiration for the build's performance and capabilities, with one user noting the impressive speeds achieved with DeepSeek-V3.1-Terminus. Other comments humorously reference the high cost of the build, suggesting it might be out of reach for many users.

---

## 6. [Am I the only one who feels that, with all the AI boom, everyone is basically doing the same thing?](https://reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/)

**Author:** u/[deleted] | **Upvotes:** 374 | **Comments:** 182 | **Date:** 2026-01-22

**Summary:** The post discusses the redundancy of AI tools and applications during the AI boom, highlighting that many new tools are less polished versions of existing ones. The discussion reflects on the early days of AI technology and the enthusiasm driving shallow implementations. Key points include the low barrier to entry for AI development, the 'hype stage' of AI technology, and the focus on niche tools and improvements. The discussion highlights the early and hype-driven stage of AI technology, with many participants noting the redundancy and shallow nature of new AI tools.

---

## 7. [vLLM raising $150M confirms it: We have moved from the "Throughput Era" to the "Latency(Cold Starts)."](https://reddit.com/r/LocalLLaMA/comments/1qk68n8/vllm_raising_150m_confirms_it_we_have_moved_from/)

**Author:** u/pmv143 | **Upvotes:** 166 | **Comments:** 88 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses vLLM's $150M funding round, signaling a shift from training to serving in AI, emphasizing software over hardware and the importance of latency optimization. The comments debate the implications of this funding and the future of AI inference.

**Key Points:**
- vLLM's $150M funding signals a shift from training to serving in AI.
- Software optimization is crucial for efficient AI inference.
- Latency, particularly cold starts, is the next major challenge.
- Debate on whether vLLM will focus on horizontal compatibility or vertical optimization.
- Discussion on the role of open-source projects like llama.cpp in AI inference.

**Discussion Highlights:** The discussion highlights a mix of opinions on the significance of vLLM's funding, with some questioning the broader implications for the AI sector. There is also a debate on the future direction of AI inference, with mentions of open-source alternatives like llama.cpp.

---

## 8. [Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp; 1.8B), Support for 10 languages](https://reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/)

**Author:** u/Nunki08 | **Upvotes:** 687 | **Comments:** 97 | **Date:** 2026-01-22

**Summary:** Qwen has open-sourced the Qwen3-TTS model family, including VoiceDesign, CustomVoice, and Base models in 0.6B and 1.8B sizes, supporting 10 languages. The release includes resources on GitHub, Hugging Face, a blog post, a paper, and a demo.

**Key Points:**
- Qwen3-TTS models released in 0.6B and 1.8B sizes
- Supports 10 languages
- Resources available on GitHub, Hugging Face, blog, paper, and demo
- Community feedback highlights the quality of samples and requests for support in compiled languages like llama.cpp
- Positive reception for Qwen's open-sourcing efforts

**Discussion Highlights:** The community appreciates Qwen's open-sourcing efforts and the quality of the TTS samples. There are requests for support in compiled languages and some feedback on the English voice samples sounding like anime dubs.

---

## 9. [Qwen dev on Twitter!!](https://reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 713 | **Comments:** 60 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses the announcement of Qwen's TTS model, with the community confirming its availability and sharing a link to the Hugging Face collection.

**Key Points:**
- Thread locked due to announcements being out
- TTS model from vLLM leak
- Link to Qwen3-TTS on Hugging Face provided

**Discussion Highlights:** The community is focused on the release of Qwen's TTS model, with confirmation of its availability and a shared link to the Hugging Face collection.

---

## 10. [GLM 4.7 flash FA fix for CUDA has been merged into llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qjrsur/glm_47_flash_fa_fix_for_cuda_has_been_merged_into/)

**Author:** u/jacek2023 | **Upvotes:** 157 | **Comments:** 51 | **Date:** 2026-01-22

**Summary:** The post announces the merge of GLM 4.7 flash FA fix for CUDA into llama.cpp, with discussions highlighting performance issues, successful builds, and ongoing bug reports.

**Key Points:**
- GLM 4.7 flash FA fix for CUDA has been merged into llama.cpp
- Quantized cache performance issues reported
- Successful builds reported by users
- Performance issues on Pascal GPUs noted
- Ongoing bug reports and discussions

**Discussion Highlights:** Users report mixed experiences with the new merge, including performance issues on certain hardware and successful builds. There is ongoing discussion about bug reports and performance optimizations.

---

## 11. [Fei Fei Li dropped a non-JEPA world model, and the spatial intelligence is insane](https://reddit.com/r/LocalLLaMA/comments/1qjjrmq/fei_fei_li_dropped_a_nonjepa_world_model_and_the/)

**Author:** u/coloradical5280 | **Upvotes:** 183 | **Comments:** 86 | **Date:** 2026-01-21

**Summary:** Fei-Fei Li's World Labs has launched Marble, a generative world model using Neural Radiance Fields (NeRF) and Gaussian splatting, enabling fast creation of explorable 3D worlds. The technology supports persistent, editable environments and is praised for its spatial intelligence, though it has received mixed reactions from the community.

**Key Points:**
- Marble uses NeRF and Gaussian splatting for fast 3D world generation.
- The model supports persistent, editable environments and VR integration.
- The technology is noted for its spatial intelligence and exportability.
- Criticisms include lack of open-source availability and limited scene sizes.
- Mixed reactions highlight both excitement and skepticism about the project.

**Discussion Highlights:** The discussion reveals a divide in opinions, with some users criticizing the lack of open-source availability and the limited scope of the generated environments, while others acknowledge the innovative aspects of the technology.

---

## 12. [Wrote a guide for running Claude Code with GLM-4.7 Flash locally with llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qjf6ys/wrote_a_guide_for_running_claude_code_with_glm47/)

**Author:** u/tammamtech | **Upvotes:** 108 | **Comments:** 45 | **Date:** 2026-01-21

**Summary:** The post provides a guide for running Claude Code with GLM-4.7 Flash locally using llama.cpp, highlighting features like model swapping and GPU memory management. It includes installation instructions and commands for running the model directly or via Docker.

**Key Points:**
- Guide for running Claude Code with GLM-4.7 Flash using llama.cpp
- Features like model swapping and GPU memory management are supported
- Instructions for installation and running the model via Docker
- Discussion highlights include clarification on Anthropic API implementation and performance queries

**Discussion Highlights:** The discussion includes clarification on the timeline of Anthropic API implementation, queries about performance metrics like VRAM usage and tokens per second, and suggestions for open-source alternatives like OpenCode and Harbor.

---

## 13. [8x AMD MI50 32GB at 26 t/s (tg) with MiniMax-M2.1 and 15 t/s (tg) with GLM 4.7 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1qjaxfy/8x_amd_mi50_32gb_at_26_ts_tg_with_minimaxm21_and/)

**Author:** u/ai-infos | **Upvotes:** 315 | **Comments:** 124 | **Date:** 2026-01-21

**Summary:** The post discusses a cost-effective local inference setup using 8x AMD MI50 GPUs, achieving high token generation speeds with MiniMax-M2.1 and GLM 4.7 models. The setup is praised for its performance and affordability.

**Key Points:**
- MiniMax-M2.1 achieves 26.8 tokens per second (output) and 3000 tokens per second (input) with a context length of 196,608.
- GLM 4.7 achieves 15.6 tokens per second (output) and 3000 tokens per second (input) with a context length of 95,000.
- The total cost for 8 GPUs is $880, providing 256GB VRAM, making it highly cost-effective.
- Power draw is 280W idle and 1200W during inference.
- The setup is stable for long context use cases, suitable for coding agents.

**Discussion Highlights:** The community response is overwhelmingly positive, with users praising the cost-effectiveness and performance of the setup. Many express interest in replicating the setup, though some note challenges in sourcing the GPUs at the stated price.

---

## 14. [VibeVoice-ASR released!](https://reddit.com/r/LocalLLaMA/comments/1qj40er/vibevoiceasr_released/)

**Author:** u/k_means_clusterfuck | **Upvotes:** 151 | **Comments:** 45 | **Date:** 2026-01-21

**Summary:** Microsoft released VibeVoice-ASR, a multilingual ASR model with 9B parameters. Users report good quality despite its size, though some concerns about benchmarks and comparisons with other models like Whisper were raised.

**Key Points:**
- VibeVoice-ASR is a new ASR model by Microsoft
- It is multilingual and has 9B parameters
- Users report good quality despite size
- Lack of benchmarks and comparison concerns
- 91% accuracy reported in Chinese audio tests

**Discussion Highlights:** General positive feedback on quality, but concerns about size and lack of benchmarks compared to alternatives like Whisper.

---

## 15. [One-shot single page web development: pacman clone - GLM 4.7 vs GLM 4.7 Flash vs GLM 4.5 Air vs Gemini 3 Pro vs Gemini 3 Flash - Results available for online testing - Prompt and instructions provided for testing with other models](https://reddit.com/r/LocalLLaMA/comments/1qj13uh/oneshot_single_page_web_development_pacman_clone/)

**Author:** u/ex-arman68 | **Upvotes:** 110 | **Comments:** 48 | **Date:** 2026-01-21

**Summary:** The Reddit post discusses a one-shot Pacman clone development test comparing various models, with GLM 4.7 emerging as the top performer, surpassing expectations and outperforming Gemini models. The author provides detailed results and links to the generated webpages for each model. Key points include GLM 4.7 being the top performer, followed by Minimax M2.1 and Gemini 3 Flash, the author's surprise at GLM 4.7's performance, the testing methodology involving temperature set to 0, and community reactions highlighting the usefulness of the testing approach. The discussion emphasized the effectiveness of the testing methodology and the surprising performance of GLM 4.7 over Gemini models, with community members sharing additional results and discussing limitations of LLMs in coding tasks, such as token cap and memory constraints.

---

## 16. [GLM-4.7-Flash-GGUF bug fix - redownload for better outputs](https://reddit.com/r/LocalLLaMA/comments/1qiy0ha/glm47flashgguf_bug_fix_redownload_for_better/)

**Author:** u/etherd0t | **Upvotes:** 111 | **Comments:** 58 | **Date:** 2026-01-21

**Summary:** A bug fix for the GLM-4.7-Flash-GGUF model has been released, improving output quality and fixing looping issues. Users are advised to re-download the model and use recommended parameters for optimal performance.

**Key Points:**
- Bug fix released for GLM-4.7-Flash-GGUF model, addressing looping and poor outputs
- Recommended parameters provided for general use and tool-calling
- Users report significant improvements in model performance post-update
- Some users note the model is slower compared to alternatives like GPT-OSS-20b
- Positive feedback on the fix from users who experienced issues with the previous version

**Discussion Highlights:** Users generally appreciate the bug fix and report improved performance. Some note the model's speed compared to alternatives, but overall consensus is positive about the update.

---

## 17. [Fix for GLM 4.7 Flash has been merged into llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qiwm3c/fix_for_glm_47_flash_has_been_merged_into_llamacpp/)

**Author:** u/jacek2023 | **Upvotes:** 308 | **Comments:** 86 | **Date:** 2026-01-21

**Summary:** A fix for GLM 4.7 Flash has been merged into llama.cpp, with ongoing work on CUDA support. The update has received significant engagement (308 upvotes, 86 comments) and positive feedback on performance improvements.

**Key Points:**
- Fix for GLM 4.7 Flash merged into llama.cpp
- CUDA support in progress via GitHub pull request
- Performance data shared for GLM 4.7 unsloth on a 4090 GPU
- Users report improved model intelligence with no gibberish or repetition
- Discussion includes queries about CPU-only performance and LMStudio issues

**Discussion Highlights:** Users highlight performance improvements and model intelligence gains, with some discussing GPU-specific speeds and CPU-only performance. A consensus on the positive impact of the fix is evident, though some users report slow prompt processing in LMStudio.

---

## 18. [Knowledge distillation with Claude as the interface: trained a 0.6B model to match GPT-class performance on Text2SQL in a singe conversation](https://reddit.com/r/LocalLLaMA/comments/1qiu6jo/knowledge_distillation_with_claude_as_the/)

**Author:** u/party-horse | **Upvotes:** 165 | **Comments:** 37 | **Date:** 2026-01-21

**Summary:** The post describes a workflow for training small, task-specific models using knowledge distillation via Claude, achieving significant performance improvements in Text2SQL tasks with minimal setup overhead.

**Key Points:**
- Knowledge distillation via Claude simplifies the fine-tuning process for small models.
- The approach uses a large teacher model (DeepSeek-V3) to generate synthetic training data.
- The fine-tuned 0.6B model achieved a 74% score, significantly higher than the base model's 36%.
- The workflow includes task selection, data conversion, teacher evaluation, training, and packaging.
- The method is praised for its potential in training small models for local inference tasks.

**Discussion Highlights:** The discussion highlights the effectiveness of the approach, its potential for on-device agents, and some technical considerations like using SQL AST for validation. There is also a debate about the necessity of using Claude-specific code versus open-source alternatives.

---

## 19. [vLLM v0.14.0 released](https://reddit.com/r/LocalLLaMA/comments/1qim0e9/vllm_v0140_released/)

**Author:** u/jinnyjuice | **Upvotes:** 171 | **Comments:** 34 | **Date:** 2026-01-20

**Summary:** The Reddit post announces the release of vLLM v0.14.0, highlighting new features and updates. The discussion focuses on improvements like automatic context length fitting and the deprecation of certain quantization methods.

**Key Points:**
- Automatic context length fitting to GPU memory to prevent OOM failures
- Deprecation of some quantization methods, including HQQ
- Marlin for Turing (sm75) upgrade as a major improvement
- Community interest in future sm120 optimizations

**Discussion Highlights:** The community is excited about the automatic context length feature and the Marlin upgrade. There is some discussion about the deprecation of quantization methods and anticipation for future optimizations.

---

## 20. [Current GLM-4.7-Flash implementation confirmed to be broken in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qih9r8/current_glm47flash_implementation_confirmed_to_be/)

**Author:** u/Sweet_Albatross9772 | **Upvotes:** 243 | **Comments:** 60 | **Date:** 2026-01-20

**Summary:** The Reddit post confirms that the current GLM-4.7-Flash implementation in llama.cpp is broken, with significant differences in logprobs compared to vLLM, leading to issues like looping and poor performance. A potential fix is already available in a pull request.

**Key Points:**
- GLM-4.7-Flash implementation in llama.cpp is confirmed broken
- Significant differences in logprobs compared to vLLM
- Potential fix available in pull request #18980
- Community consensus is to wait for fixes
- Common issues include looping, overthinking, and poor performance

**Discussion Highlights:** The community is generally understanding of the issues, noting that open-source projects often require minor tweaks. There is a consensus to wait for fixes rather than attempting to troubleshoot immediately. The discussion also highlights the usual process of bug fixes when new models are merged.

---

## 21. [You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?](https://reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/)

**Author:** u/Adventurous-Gold6413 | **Upvotes:** 535 | **Comments:** 303 | **Date:** 2026-01-20

**Summary:** The Reddit post discusses the selection of local models for use with 64GB RAM and 16GB VRAM when internet access is unavailable. Users share their preferred models and experiences. Key points include recommendations for models like Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B, with GPT-OSS-120B being particularly praised for its performance and fit within the given hardware specifications. The discussion highlights a consensus leaning towards these models, emphasizing their balance of performance and compatibility.

---

## 22. [Liquid AI released the best thinking Language Model Under 1GB](https://reddit.com/r/LocalLLaMA/comments/1qi512t/liquid_ai_released_the_best_thinking_language/)

**Author:** u/PauLabartaBajo | **Upvotes:** 230 | **Comments:** 52 | **Date:** 2026-01-20

**Summary:** Liquid AI released LFM2.5-1.2B-Thinking, a compact reasoning model that runs on-device with 900 MB of memory, excelling in math, tool use, and instruction following. It outperforms larger models in speed and efficiency, with broad ecosystem support.

**Key Points:**
- LFM2.5-1.2B-Thinking is a 1.2B parameter model optimized for on-device reasoning with 900 MB memory usage.
- It generates internal thinking traces for systematic problem-solving and excels in math, tool use, and instruction following.
- The model matches or exceeds Qwen3-1.7B in performance despite having fewer parameters.
- Concerns raised about memory requirements for quantized versions and real-world usability of smaller models.
- Licensing (non-Apache/MIT) and model size limitations were points of criticism in the discussion.

**Discussion Highlights:** The discussion highlighted concerns about memory requirements for edge deployment, performance trade-offs compared to the Instruct variant, and a desire for larger models. Licensing was also a point of contention, with some users preferring Apache or MIT licenses.

---

## 23. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 875 | **Comments:** 262 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, fully enclosed 10-GPU mobile AI system designed for running large MoE models and supporting graphic design tasks. The build, costing around $17k, balances performance and budget constraints while addressing mobility and enclosure challenges.

**Key Points:**
- The system features a Threadripper Pro 3995WX, 512GB DDR4, and 10 GPUs (8x 3090 + 2x 5090).
- It is designed for large MoE models, video generation, and high-detail image generation.
- The enclosure was a major challenge, solved with a Thermaltake Core W200 case.
- Budget constraints led to a mix of GPUs to optimize cost and performance.
- The post received significant engagement, with comments highlighting its uniqueness and practicality.

**Discussion Highlights:** The discussion highlights the system's impressive capabilities and the creative solution to the enclosure problem. Comments also joke about the system's portability and power requirements.

---

## 24. [Over 6K novels with reasoning traces to train full book writing LLMs](https://reddit.com/r/LocalLLaMA/comments/1qi3nmd/over_6k_novels_with_reasoning_traces_to_train/)

**Author:** u/XMasterDE | **Upvotes:** 113 | **Comments:** 46 | **Date:** 2026-01-20

**Summary:** The post announces an update to the LongPage dataset, expanding it to over 6,000 novels with hierarchical planning traces to support training full-book writing LLMs. The team is also training a model on this dataset and plans to release it soon.

**Key Points:**
- LongPage dataset expanded to 6K+ novels with hierarchical planning traces
- Dataset aims to support training full-book writing LLMs
- Early model checkpoints are being tested internally
- Community shows interest in the project and requests more details
- Questions about dataset content and code availability for other languages

**Discussion Highlights:** The community is eager to see the results, with some users requesting more details about the dataset and model functionality. There is also interest in expanding the dataset to other languages.

---

## 25. [glm-4.7-flash has the best thinking process with clear steps, I love it](https://reddit.com/r/LocalLLaMA/comments/1qhxlgy/glm47flash_has_the_best_thinking_process_with/)

**Author:** u/uptonking | **Upvotes:** 141 | **Comments:** 34 | **Date:** 2026-01-20

**Summary:** The Reddit post discusses the user's experience with the glm-4.7-flash model, highlighting its detailed thinking process and comparing it favorably to other models like nemotron-nano and qwen3-30b. The user appreciates the model's structured reasoning steps but notes its slower performance and occasional looping issues. Key points include the model's detailed and structured thinking process, longer thinking duration but preferred reasoning quality, performance issues like slow token generation and looping, and community appreciation of the reasoning process despite performance limitations. The discussion highlights a consensus on the model's superior reasoning process, with users praising its structured and logical approach but acknowledging concerns about its speed and tendency to loop.

---

## 26. [It's been one year since the release of Deepseek-R1](https://reddit.com/r/LocalLLaMA/comments/1qhs2sd/its_been_one_year_since_the_release_of_deepseekr1/)

**Author:** u/Recoil42 | **Upvotes:** 300 | **Comments:** 51 | **Date:** 2026-01-19

**Summary:** The Reddit post commemorates the one-year anniversary of the release of Deepseek-R1, highlighting its significant impact on the AI community. The discussion reflects on the model's influence and the rapid pace of advancements in the field.

**Key Points:**
- Deepseek-R1 had a major impact, reportedly causing significant disruption in the AI community.
- The release is considered one of the most important in AI history, second only to the original Llama model.
- The rapid pace of advancements in AI is noted, with the past year feeling like several years due to the volume of progress.
- The model's release led to price reductions and increased transparency in AI reasoning outputs.

**Discussion Highlights:** The discussion highlights the transformative impact of Deepseek-R1, with users emphasizing its role in disrupting the AI landscape and accelerating progress. There is a consensus on the model's significance and the rapid pace of advancements in the field.

---

## 27. [Mosquito - 7.3M parameter tiny knowledge model](https://reddit.com/r/LocalLLaMA/comments/1qhqzsi/mosquito_73m_parameter_tiny_knowledge_model/)

**Author:** u/Lopsided-Repair-3638 | **Upvotes:** 116 | **Comments:** 53 | **Date:** 2026-01-19

**Summary:** The post introduces 'Mosquito,' a tiny knowledge model with 7.3M parameters that can answer general knowledge questions, though with some humorous inaccuracies. The demo and model are available on Hugging Face.

**Key Points:**
- Mosquito is a small model (7.3M parameters) designed for general knowledge questions.
- The model provides a demo and is available on Hugging Face.
- Users noted inaccuracies, such as defining a dog incorrectly and providing odd answers to simple questions.
- There is a request for a quantized version of the model.
- The model's knowledge gaps are highlighted humorously in the comments.

**Discussion Highlights:** The discussion highlights the model's limitations and inaccuracies, with users pointing out humorous and incorrect responses. There is also a request for a quantized version of the model.

---

## 28. [Bartowski comes through again. GLM 4.7 flash GGUF](https://reddit.com/r/LocalLLaMA/comments/1qhpima/bartowski_comes_through_again_glm_47_flash_gguf/)

**Author:** u/RenewAi | **Upvotes:** 186 | **Comments:** 50 | **Date:** 2026-01-19

**Summary:** The post announces the release of GLM 4.7 Flash GGUF by Bartowski, with mixed user experiences reported in the comments.

**Key Points:**
- Bartowski released GLM 4.7 Flash GGUF on Hugging Face.
- Users report mixed results with different versions of the model.
- An Unsloth version was recently uploaded.
- Some users find the model non-functional or 'brain dead'.

**Discussion Highlights:** Users are experimenting with different versions of GLM 4.7 Flash, with some reporting issues and others trying new releases like the Unsloth version.

---

## 29. [Unsloth GLM 4.7-Flash GGUF](https://reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/)

**Author:** u/Wooden-Deer-1276 | **Upvotes:** 232 | **Comments:** 44 | **Date:** 2026-01-19

**Summary:** The Reddit post discusses the release of the Unsloth GLM 4.7-Flash GGUF model, with updates on available quantizations and ongoing fixes for issues like looping in quantized versions. The community emphasizes patience and proper testing before release. Key points include recommendations to use UD-Q4_K_XL and above, looping issues in quantized versions with BF16 recommended for best results, specific settings for LM Studio, and the release of a BF16 version. The discussion highlights a collaborative effort to address technical issues, with a consensus on the importance of thorough testing and patience in the development process.

---

## 30. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 363 | **Comments:** 60 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the official support for GLM 4.7 Flash in llama.cpp, highlighting its integration and community contributions. The discussion clarifies the term 'official' and shares performance insights. Key points include: GLM 4.7 Flash now officially supported in llama.cpp, 'Official' refers to proper integration, not developer involvement, Performance observations shared, including flash-attention speed issues, and Community contributions and resources highlighted. The discussion clarifies that 'official' means proper integration with llama.cpp, not involvement from the original developers. Users share performance insights, noting that flash-attention can be slow and that disabling it may improve speed. Community contributions and additional resources are also highlighted.

---

## 31. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 459 | **Comments:** 162 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights the effectiveness of GLM 4.7 Flash as a reliable local agent for various tasks, with the author praising its performance and stability. The discussion includes comparisons with other models and notes on its performance and quality. Key points include its reliability in an agentic framework, extensive testing without errors, eagerness for local testing, comparisons with models like Nemotron 30B and Qwen3, and its speed and deep thinking capabilities. The discussion highlights comparisons with other models, notes on performance and speed, and enthusiasm for local testing.

---

## 32. [New in llama.cpp: Anthropic Messages API](https://reddit.com/r/LocalLLaMA/comments/1qhaq21/new_in_llamacpp_anthropic_messages_api/)

**Author:** u/paf1138 | **Upvotes:** 167 | **Comments:** 51 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the introduction of the Anthropic Messages API in llama.cpp, generating excitement among users. Practical tips and user experiences are shared in the comments. Key points include the introduction of the API, user enthusiasm, practical implementation tips, discussion about hardware compatibility and context usage, and mixed reactions regarding the timeliness of the announcement. The discussion highlights a positive reception to the new API, with users sharing practical advice on implementation and expressing their experiences with different hardware setups. Some users noted the announcement was not recent, but overall, the sentiment was enthusiastic.

---

## 33. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 740 | **Comments:** 231 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the release of the GLM-4.7-Flash model on Hugging Face, generating significant community interest and discussion about its technical features and capabilities.

**Key Points:**
- The GLM-4.7-Flash model has been released on Hugging Face.
- The model uses MLA, reducing KV cache memory usage and enabling longer context lengths.
- Community members express excitement and anticipation for the release.
- Discussion includes technical details like model size and efficiency.

**Discussion Highlights:** The community is highly enthusiastic about the release, with discussions focusing on the model's technical advantages such as memory efficiency and extended context length. Some users express nostalgia for larger models while appreciating the new features.

---

## 34. [I made a Top-K implementation that's up to 20x faster than PyTorch CPU (open source)](https://reddit.com/r/LocalLLaMA/comments/1qh0yq8/i_made_a_topk_implementation_thats_up_to_20x/)

**Author:** u/andreabarbato | **Upvotes:** 151 | **Comments:** 103 | **Date:** 2026-01-19

**Summary:** The author developed an AVX2-optimized batched Top-K implementation that outperforms PyTorch CPU by 4-20x, with benchmarks showing significant speed improvements across different vocabulary sizes. It has been integrated into llama.cpp, resulting in 63% faster prompt processing for a 120B MoE model.

**Key Points:**
- AVX2-optimized batched Top-K implementation achieves 4-20x speedup over PyTorch CPU.
- Benchmarks show performance improvements: 0.043ms vs 0.173ms for vocab=32K, 0.057ms vs 0.777ms for vocab=128K, and 0.079ms vs 1.56ms for vocab=256K.
- Integration into llama.cpp resulted in 63% faster prompt processing (81→142 tokens/sec) for a 120B MoE model.
- Uses adaptive sampling, AVX2 SIMD, and cache-optimized scanning with fast paths for sorted/constant inputs.
- Community feedback includes requests for PR submission, explanations for the speedup, and concerns about reproducible benchmarks.

**Discussion Highlights:** The community expressed enthusiasm for the performance improvements and requested further explanations and reproducible benchmarks. Some users criticized the lack of detailed benchmarks and raised concerns about the implementation being 'vibe coded.'

---

## 35. [how do you pronounce “gguf”?](https://reddit.com/r/LocalLLaMA/comments/1qglyqz/how_do_you_pronounce_gguf/)

**Author:** u/Hamfistbumhole | **Upvotes:** 107 | **Comments:** 155 | **Date:** 2026-01-18

**Summary:** The Reddit post discusses various ways to pronounce 'gguf', with users sharing their interpretations and preferences. The comments highlight a mix of humorous and literal pronunciations, with no clear consensus. Key points include suggestions like 'jee-guff', 'giguff', and 'jee jee you eff', as well as humorous takes like not pronouncing it at all. The discussion is lighthearted and playful, with no definitive answer.

---

## 36. [Are most major agents really just markdown todo list processors?](https://reddit.com/r/LocalLLaMA/comments/1qgj2n9/are_most_major_agents_really_just_markdown_todo/)

**Author:** u/TheDigitalRhino | **Upvotes:** 101 | **Comments:** 37 | **Date:** 2026-01-18

**Summary:** The Reddit post discusses how major LLM agents decompose tasks into todo lists and process them sequentially, with users sharing insights and examples of this approach.

**Key Points:**
- Major LLM agents decompose tasks into todo lists and process them one by one.
- This approach includes tool calls and terminal command execution.
- Breaking down complex tasks into smaller ones is a common strategy, similar to human problem-solving.
- This method has been effective since GPT-3.5 for generating detailed documents.
- The discussion highlights the simplicity and effectiveness of this approach.

**Discussion Highlights:** The discussion highlights a consensus that decomposing tasks into smaller, manageable parts is a common and effective strategy among major LLM agents, with users providing examples and insights into this approach.

---

## 37. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 352 | **Comments:** 94 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to maximize VRAM for running large models locally. Benchmark results show impressive performance across various models, with the system costing around 9,800€ (effectively 4,900€ after refund).

**Key Points:**
- System built for running large models (120B+) locally with a focus on data privacy.
- Hardware includes 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU.
- Total cost was ~9,800€, with a 50% subsidy reducing the effective cost to ~4,900€.
- Benchmark results show high performance across various models, with notable throughput and latency metrics.
- Discussion highlights include admiration for the build and questions about component sourcing and job context.

**Discussion Highlights:** The discussion highlights admiration for the build, with comments like 'HE HAS RAM GET HIM...' and 'G O D D A A A A A Y U U U U M...' expressing awe. There are also questions about component sourcing and the author's job, indicating interest in the practical aspects of the build.

---

## 38. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 452 | **Comments:** 71 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses the potential delay in Qwen 4's release, as the lead developer mentioned slowing down to focus on quality. The community generally appreciates this focus on quality over quantity.

**Key Points:**
- Qwen 4 development may be delayed to focus on quality
- Community appreciates the focus on quality over quantity
- Some users caution against jumping to conclusions based on limited information
- General consensus that rushed releases don't significantly advance the field
- Post gained popularity and was featured on Discord

**Discussion Highlights:** The discussion highlights a positive reception to the focus on quality, with many users expressing appreciation for taking the necessary time to improve the product. Some users also advise caution against spreading rumors based on limited information.

---

## 39. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 537 | **Comments:** 117 | **Date:** 2026-01-17

**Summary:** The author transitioned from MI100 GPUs to R9700 GPUs for better performance and cost efficiency, detailing the hardware specifications and benchmarks of their new server setup.

**Key Points:**
- Author switched from MI100 to R9700 GPUs due to better performance and cost
- Detailed hardware specifications and cost breakdown provided
- Performance benchmarks for the new setup included
- Community reaction highlights appreciation and humor about financial irresponsibility

**Discussion Highlights:** The community appreciated the detailed build and benchmarks, with humorous comments about the financial implications of such high-end setups.

---

## 40. [The Search for Uncensored AI (That Isn’t Adult-Oriented)](https://reddit.com/r/LocalLLaMA/comments/1qfq9ez/the_search_for_uncensored_ai_that_isnt/)

**Author:** u/Fun-Situation-4358 | **Upvotes:** 281 | **Comments:** 216 | **Date:** 2026-01-17

**Summary:** The post discusses the challenge of finding uncensored AI models that prioritize reasoning and creativity over adult-oriented content, highlighting a gap between heavily restricted corporate AI and shallow adult-focused models.

**Key Points:**
- The author seeks an AI that is genuinely unfiltered and technically advanced, focusing on reasoning and creativity.
- Most 'uncensored' models are optimized for adult use rather than intelligence or depth.
- There is a perceived gap between heavily restricted corporate AI and shallow adult-focused models.
- Suggestions include self-hosted models, open-source projects, or lesser-known platforms.
- Decensoring techniques often reduce model intelligence as a side effect.

**Discussion Highlights:** The discussion highlights a shared frustration with the lack of uncensored AI models that focus on serious problem-solving and creativity. Users agree that most 'uncensored' models are either adult-oriented or lack depth. Some suggest exploring open-source projects or self-hosted models, while others point out that decensoring techniques can compromise model intelligence.

---

## 41. [China's AGI-NEXT Conference (Qwen, Kimi, Zhipu, Tencent)](https://reddit.com/r/LocalLLaMA/comments/1qfmc05/chinas_aginext_conference_qwen_kimi_zhipu_tencent/)

**Author:** u/nuclearbananana | **Upvotes:** 120 | **Comments:** 22 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses China's AGI-NEXT Conference, highlighting insights on China vs US AGI development, paths to AGI, compute resources, and marketing strategies. Key points include Qwen's internal advancements, the belief that the next paradigm may come from OpenAI, and cultural differences in risk-taking for innovation.

**Key Points:**
- Qwen has internally developed Qwen3.5 and context windows in the millions.
- The next paradigm in AGI is believed to likely come from OpenAI rather than Google.
- Chinese work culture is described as less willing to take risks for innovation.
- Deepseek is noted for its talent concentration but was absent from the conference.

**Discussion Highlights:** The discussion highlights Qwen's advancements and the belief in OpenAI's potential leadership in the next AGI paradigm. There is also a note on the risk-averse culture in Chinese labs and the absence of Deepseek despite its strong talent pool.

---

## 42. [Best "End of world" model that will run on 24gb VRAM](https://reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/)

**Author:** u/gggghhhhiiiijklmnop | **Upvotes:** 336 | **Comments:** 180 | **Date:** 2026-01-17

**Summary:** The user is seeking recommendations for the best LLM model that can run on a PC with 24GB VRAM and 64GB RAM, aiming to hoard data like Wikipedia and other educational resources. The discussion highlights various suggestions, including prioritizing the best available model regardless of size and specific model recommendations like gemma3:27b.

**Key Points:**
- User wants to download and store large datasets like Wikipedia, Wiktionary, etc.
- Seeking LLM models that fit within 24GB VRAM and 64GB RAM constraints
- Suggestions include saving the best LLM available and running it off SSD if necessary
- Specific model recommendations: gemma3:27b with vision capabilities
- Additional advice to download actual Wikipedia backups for offline use

**Discussion Highlights:** The discussion emphasizes practicality, with a consensus leaning towards prioritizing the best available model even if it requires running off SSD. Specific model recommendations like gemma3:27b are highlighted, along with advice on downloading comprehensive data backups.

---

## 43. [KoboldCpp v1.106 finally adds MCP server support, drop-in replacement for Claude Desktop](https://reddit.com/r/LocalLLaMA/comments/1qfb0gk/koboldcpp_v1106_finally_adds_mcp_server_support/)

**Author:** u/HadesThrowaway | **Upvotes:** 103 | **Comments:** 22 | **Date:** 2026-01-17

**Summary:** KoboldCpp v1.106 introduces native MCP server support, offering a drop-in replacement for Claude Desktop with enhanced tool integration and UI improvements. The update allows seamless connection to MCP servers and provides flexibility in tool selection and approvals.

**Key Points:**
- KoboldCpp v1.106 adds native MCP server support
- Designed as a drop-in replacement for Claude Desktop with compatible configurations
- Supports both HTTP and STDIO transports for MCP servers
- Allows tool selection and optional approvals for AI usage
- Positive community feedback on compatibility and ease of use

**Discussion Highlights:** The community praised the MCP integration for its compatibility with existing setups and ease of use. Users highlighted the convenience of not having to rebuild their tool configurations. There were also requests for similar MCP support in other tools like llama.cpp.

---

## 44. [DeepSeek Engram : A static memory unit for LLMs](https://reddit.com/r/LocalLLaMA/comments/1qf5oj0/deepseek_engram_a_static_memory_unit_for_llms/)

**Author:** u/Technical-Love-8479 | **Upvotes:** 320 | **Comments:** 48 | **Date:** 2026-01-17

**Summary:** DeepSeek AI introduced Engram, a memory unit for LLMs that separates remembering from reasoning, enabling O(1) knowledge lookup and improving performance without GPU limits.

**Key Points:**
- Engram introduces conditional memory, separating it from reasoning.
- Knowledge lookup is O(1), improving efficiency.
- Enables massive memory scaling without GPU constraints.
- Improves reasoning, math, and code performance.
- Frees attention for global reasoning rather than static knowledge.

**Discussion Highlights:** The community is excited about the separation of memory and reasoning, highlighting the efficiency gains and scalability benefits. There is consensus on the potential for significant performance improvements and cost savings.

---

## 45. ["Welcome to the Local Llama. How janky's your rig?](https://reddit.com/r/LocalLLaMA/comments/1qf514i/welcome_to_the_local_llama_how_jankys_your_rig/)

**Author:** u/ForsookComparison | **Upvotes:** 103 | **Comments:** 22 | **Date:** 2026-01-16

**Summary:** The Reddit post in r/LocalLLaMA discusses various unconventional and humorous setups for running local AI models, highlighting the creative and sometimes 'janky' rigs users have assembled.

**Key Points:**
- Users share unconventional hardware setups for running AI models.
- Discussion includes humorous and creative solutions to hardware limitations.
- Specific mentions of hardware like GPUs, RAM, and cooling solutions.
- Community engagement with upvotes and comments indicating interest in the topic.

**Discussion Highlights:** The discussion highlights the creative and sometimes humorous approaches users take to overcome hardware limitations, with a focus on unconventional cooling solutions and hardware setups.

---

