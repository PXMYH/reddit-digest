# r/LocalLLaMA Reading Digest

**Period:** 2026-01-23 to 2026-01-23
**Posts Summarized:** 43
**Total Posts Analyzed:** 43

---

## 1. [OpenAI CFO hinting at "Outcome-Based Pricing" (aka royalties on your work)? Makes the case for local even stronger.](https://reddit.com/r/LocalLLaMA/comments/1qkiylw/openai_cfo_hinting_at_outcomebased_pricing_aka/)

**Author:** u/distalx | **Upvotes:** 180 | **Comments:** 85 | **Date:** 2026-01-23

**Summary:** The post discusses OpenAI's potential shift to outcome-based pricing for high-value enterprise deals, clarifying that it does not apply to regular users or indie developers. The author initially misunderstood the scope but corrected it after finding the primary source. Key points include OpenAI's CFO mentioning outcome-based pricing for enterprise deals, the pricing model being aimed at high-value industries like pharmaceuticals, the author correcting their initial misunderstanding, the importance of self-hosting and local stacks to avoid potential future costs, and users expressing concerns about OpenAI's potential to charge royalties on discoveries made using their AI. The discussion highlights a consensus on the importance of self-hosting and local stacks to maintain control over costs and terms, with users expressing skepticism and concern about OpenAI's potential pricing changes.

---

## 2. [Quiet Threadripper AI Workstation - 768GB DDR5 and 160GB VRAM (RTX 5090 + 4x R9700)](https://reddit.com/r/LocalLLaMA/comments/1qkghpk/quiet_threadripper_ai_workstation_768gb_ddr5_and/)

**Author:** u/sloptimizer | **Upvotes:** 104 | **Comments:** 48 | **Date:** 2026-01-22

**Summary:** The Reddit post describes a high-performance AI workstation featuring a Threadripper PRO 7975WX CPU, 768GB DDR5 RAM, an RTX 5090, and four R9700 GPUs, achieving impressive performance with DeepSeek-V3.1-Terminus. The build includes dual power supplies and custom cooling solutions to manage heat and power demands.

**Key Points:**
- The workstation combines Nvidia and AMD GPUs using llama.cpp with specific compilation flags.
- RAM cooling is critical for performance, with RAM fans providing a 30% boost.
- Power and noise optimizations include limiting the RTX 5090 to 400W and adjusting R9700 performance levels.
- The build achieves near-SOTA performance with DeepSeek-V3.1-Terminus at usable speeds.
- The cost and complexity of the build sparked humorous discussions about its expense.

**Discussion Highlights:** The discussion highlights the impressive performance of the workstation, with users expressing admiration for its capabilities and humorously commenting on its likely high cost. Key takeaways include the effectiveness of combining Nvidia and AMD GPUs, the importance of RAM cooling, and optimizations for power and noise management.

---

## 3. [Am I the only one who feels that, with all the AI boom, everyone is basically doing the same thing?](https://reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/)

**Author:** u/Empty_Enthusiasm_167 | **Upvotes:** 322 | **Comments:** 156 | **Date:** 2026-01-22

**Summary:** The post discusses the redundancy of AI projects during the AI boom, noting that many new tools and applications are essentially reinventing existing solutions. The author acknowledges the potential of AI but criticizes the lack of innovation and the financial waste involved in these redundant projects.

**Key Points:**
- Many AI projects are redundant, reinventing existing tools or applications.
- The barrier to entry for AI development is low, leading to shallow implementations.
- There is a lot of enthusiasm and hype around AI, with many people claiming expertise.
- Some developers are focusing on niche tools and specific needs rather than reinventing the wheel.
- The current phase is seen as a hype stage, similar to past technological trends.

**Discussion Highlights:** The discussion highlights a consensus that the AI field is currently in a hype stage, with many redundant projects. However, there is also a focus on niche tools and specific applications that address unique needs. The community acknowledges the potential of AI but criticizes the lack of innovation and the financial waste involved in redundant projects.

---

## 4. [vLLM raising $150M confirms it: We have moved from the "Throughput Era" to the "Latency(Cold Starts)."](https://reddit.com/r/LocalLLaMA/comments/1qk68n8/vllm_raising_150m_confirms_it_we_have_moved_from/)

**Author:** u/pmv143 | **Upvotes:** 151 | **Comments:** 86 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses the significant investment in vLLM, signaling a shift in focus from training to serving in the AI space. It highlights the importance of software over hardware and the race for standardization in inference engines.

**Key Points:**
- vLLM's $150M funding signals a shift from training to serving in AI.
- Software optimization is crucial for efficient inference.
- Standardization in inference engines is a key focus.
- Latency, particularly cold starts, is the next major challenge.
- Discussion includes debates on horizontal compatibility vs. vertical optimization.

**Discussion Highlights:** The discussion highlights debates on the role of vLLM in the inference space, with comparisons to other tools like llama.cpp. There is also a focus on the importance of latency and cold starts in cloud environments, and a consensus that local solutions may ultimately prevail.

---

## 5. [Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp; 1.8B), Support for 10 languages](https://reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/)

**Author:** u/Nunki08 | **Upvotes:** 654 | **Comments:** 91 | **Date:** 2026-01-22

**Summary:** Qwen has open-sourced the Qwen3-TTS model family, including VoiceDesign, CustomVoice, and Base models in 0.6B and 1.8B sizes, supporting 10 languages. The release includes resources on GitHub, Hugging Face, and a demo.

**Key Points:**
- Open-sourced Qwen3-TTS models (0.6B & 1.8B)
- Supports 10 languages
- Resources available on GitHub, Hugging Face, blog, paper, and demo
- Positive community reception with some concerns about voice quality
- Requests for compatibility with other tools like llama.cpp

**Discussion Highlights:** The community appreciates Qwen's open-sourcing efforts but has mixed feedback on voice quality and requests for broader tool compatibility.

---

## 6. [Qwen dev on Twitter!!](https://reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 701 | **Comments:** 60 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses Qwen's TTS model announcement, with community reactions and links to the model on Hugging Face.

**Key Points:**
- Qwen's TTS model announcement
- Community reaction and discussion
- Link to the model on Hugging Face
- Thread locked due to announcements being out
- Mention of vLLM leak related to the TTS model

**Discussion Highlights:** The community is excited about the Qwen TTS model, with some users sharing links to the model on Hugging Face. There is also mention of a vLLM leak related to the model, and the thread was locked as announcements were already out.

---

## 7. [GLM 4.7 flash FA fix for CUDA has been merged into llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qjrsur/glm_47_flash_fa_fix_for_cuda_has_been_merged_into/)

**Author:** u/jacek2023 | **Upvotes:** 155 | **Comments:** 48 | **Date:** 2026-01-22

**Summary:** The Reddit post announces the merge of GLM 4.7 flash FA fix for CUDA into llama.cpp, with users reporting mixed results including issues with quantized cache and performance on Pascal GPUs.

**Key Points:**
- GLM 4.7 flash FA fix for CUDA has been merged into llama.cpp
- Quantized cache is not working well for some users
- Performance on Pascal GPUs is reported to be half the speed of non-flash-attention kernels
- Some users report successful builds and usage of the model
- General feedback includes reports of the model 'thinking a lot'

**Discussion Highlights:** The discussion highlights issues with quantized cache and performance on Pascal GPUs, while some users report successful usage of the model. There is a consensus on the need for further bug fixes and optimizations.

---

## 8. [Fei Fei Li dropped a non-JEPA world model, and the spatial intelligence is insane](https://reddit.com/r/LocalLLaMA/comments/1qjjrmq/fei_fei_li_dropped_a_nonjepa_world_model_and_the/)

**Author:** u/coloradical5280 | **Upvotes:** 182 | **Comments:** 86 | **Date:** 2026-01-21

**Summary:** Fei-Fei Li's World Labs launched Marble, a generative world model using Neural Radiance Fields (NeRF) and Gaussian splatting, enabling fast creation of explorable 3D worlds. The technology supports persistent, editable environments and is praised for its spatial intelligence, though it has faced criticism for not being open source and its limited scope.

**Key Points:**
- Marble uses NeRF and Gaussian splatting for fast 3D world generation.
- The model supports persistent, editable environments and VR integration.
- Criticism includes lack of open-source availability and limited scene sizes.
- The technology is noted for its spatial intelligence and potential future impact.
- Mixed reactions in comments, with some dismissing it as overhyped.

**Discussion Highlights:** The discussion highlights a divide between appreciation for the technological innovation and skepticism about its practicality and accessibility. Many commenters criticized the lack of open-source availability and the limited size of generated environments, while others acknowledged the potential of the spatial intelligence demonstrated.

---

## 9. [Wrote a guide for running Claude Code with GLM-4.7 Flash locally with llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qjf6ys/wrote_a_guide_for_running_claude_code_with_glm47/)

**Author:** u/tammamtech | **Upvotes:** 112 | **Comments:** 45 | **Date:** 2026-01-21

**Summary:** The Reddit post by u/tammamtech provides a guide for running Claude Code with GLM-4.7 Flash locally using llama.cpp. It includes installation instructions, commands for running the model, and a Docker setup. The post highlights the ability to replicate Ollama features in llama.cpp, such as model swapping and freeing GPU memory on idle. The discussion includes comments on the implementation details, such as the timing of the Anthropic API endpoint implementation and inquiries about performance metrics like VRAM usage and tokens per second. There are also suggestions for using open-source alternatives like OpenCode and Harbor.

---

## 10. [8x AMD MI50 32GB at 26 t/s (tg) with MiniMax-M2.1 and 15 t/s (tg) with GLM 4.7 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1qjaxfy/8x_amd_mi50_32gb_at_26_ts_tg_with_minimaxm21_and/)

**Author:** u/ai-infos | **Upvotes:** 311 | **Comments:** 117 | **Date:** 2026-01-21

**Summary:** The post discusses a cost-effective local inference setup using 8x AMD MI50 GPUs, achieving high token generation speeds with MiniMax-M2.1 and GLM 4.7 models. The setup is praised for its performance and affordability.

**Key Points:**
- MiniMax-M2.1 achieves 26.8 tok/s output and 3000 tok/s input on vllm-gfx906 with a context length of 196608.
- GLM 4.7 achieves 15.6 tok/s output and 3000 tok/s input on vllm-gfx906 with a context length of 95000.
- The setup costs $880 for 256GB VRAM and has a power draw of 280W idle / 1200W during inference.
- The goal is to create one of the most cost-effective solutions for fast intelligent local inference.
- The community appreciates the setup for its performance and affordability.

**Discussion Highlights:** The community response is overwhelmingly positive, with users praising the setup's performance and cost-effectiveness. Some users express interest in replicating the setup but note that current prices for the GPUs are higher than those mentioned in the post.

---

## 11. [VibeVoice-ASR released!](https://reddit.com/r/LocalLLaMA/comments/1qj40er/vibevoiceasr_released/)

**Author:** u/k_means_clusterfuck | **Upvotes:** 152 | **Comments:** 44 | **Date:** 2026-01-21

**Summary:** Microsoft released VibeVoice-ASR, a multilingual ASR model with 9B parameters. Users report good quality and performance, though some concerns about benchmarks and comparisons with other models like Whisper exist.

**Key Points:**
- VibeVoice-ASR is a new multilingual ASR model by Microsoft
- Model size is 9B parameters
- Users report good quality despite size
- Lack of benchmarks and comparisons with other models noted
- Performance metrics shared (e.g., 91% accuracy in Chinese audio tests)

**Discussion Highlights:** Positive feedback on quality and multilingual capabilities, concerns about model size and lack of benchmarks, comparisons with existing models like Whisper

---

## 12. [One-shot single page web development: pacman clone - GLM 4.7 vs GLM 4.7 Flash vs GLM 4.5 Air vs Gemini 3 Pro vs Gemini 3 Flash - Results available for online testing - Prompt and instructions provided for testing with other models](https://reddit.com/r/LocalLLaMA/comments/1qj13uh/oneshot_single_page_web_development_pacman_clone/)

**Author:** u/ex-arman68 | **Upvotes:** 103 | **Comments:** 47 | **Date:** 2026-01-21

**Summary:** The post compares AI models' performance in generating a Pacman clone webpage, with GLM 4.7 ranking as the top performer, followed by Minimax M2.1 and Gemini 3 Flash. The discussion highlights the effectiveness of the testing methodology and the surprising performance of GLM 4.7. Key points include GLM 4.7's top performance, Minimax M2.1's sound implementation, Gemini 3 Pro's underperformance, the praised testing methodology, and GLM 4.7's superiority over Google's Gemini models. The discussion emphasized the effectiveness of the testing approach and the unexpected superiority of GLM 4.7 over Google's Gemini models, with users sharing additional results and insights on model performance.

---

## 13. [GLM-4.7-Flash-GGUF bug fix - redownload for better outputs](https://reddit.com/r/LocalLLaMA/comments/1qiy0ha/glm47flashgguf_bug_fix_redownload_for_better/)

**Author:** u/etherd0t | **Upvotes:** 112 | **Comments:** 57 | **Date:** 2026-01-21

**Summary:** The post announces a bug fix for the GLM-4.7-Flash-GGUF model, with updated GGUF files available for download. Users are advised to re-download the model for improved outputs and provided with recommended parameters for different use cases.

**Key Points:**
- Bug fix for GLM-4.7-Flash-GGUF model addressing looping and poor outputs
- Updated GGUF files available for download
- Recommended parameters for general use and tool-calling provided
- Users report significant improvements in the fixed version
- Performance comparison with other models like GPT-OSS-20b mentioned

**Discussion Highlights:** Users express satisfaction with the bug fix, noting improved performance and usability. Some discuss performance comparisons with other models and anticipate further optimizations.

---

## 14. [Fix for GLM 4.7 Flash has been merged into llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qiwm3c/fix_for_glm_47_flash_has_been_merged_into_llamacpp/)

**Author:** u/jacek2023 | **Upvotes:** 311 | **Comments:** 86 | **Date:** 2026-01-21

**Summary:** A fix for GLM 4.7 Flash has been merged into llama.cpp, with ongoing work on CUDA support. The update has received significant engagement, with users discussing performance metrics and sharing their experiences.

**Key Points:**
- Fix for GLM 4.7 Flash merged into llama.cpp
- CUDA support in progress via a GitHub pull request
- Performance metrics shared for different quantizations and GPUs
- Users report improved model behavior with no gibberish or repetition
- Discussion includes CPU-only performance and potential issues with LMStudio

**Discussion Highlights:** Users are actively sharing performance data and experiences, with a consensus that the model is now more reliable. Some users report slow prompt processing in LMStudio, and there is interest in CPU-only performance for those without GPUs.

---

## 15. [Knowledge distillation with Claude as the interface: trained a 0.6B model to match GPT-class performance on Text2SQL in a singe conversation](https://reddit.com/r/LocalLLaMA/comments/1qiu6jo/knowledge_distillation_with_claude_as_the/)

**Author:** u/party-horse | **Upvotes:** 164 | **Comments:** 37 | **Date:** 2026-01-21

**Summary:** The post describes a workflow for training small, task-specific models using knowledge distillation via Claude, achieving significant performance improvements in Text2SQL tasks with minimal setup overhead.

**Key Points:**
- Off-the-shelf small models perform poorly on specialized tasks like Text2SQL.
- Knowledge distillation using a large teacher model (DeepSeek-V3) and a small student model improves performance from 36% to 74%.
- Claude handles task selection, data conversion, teacher evaluation, training, and packaging.
- The approach reduces the need for extensive data preparation and training infrastructure.
- Potential applications include training models for understanding service/OS logs and running local inference on devices.

**Discussion Highlights:** The discussion highlights the positive reception of the approach, with comments praising its potential for training small models for specific tasks and running local inference. Some users suggested improvements like using SQL AST for checking matches and noted that the process could be replicated with open-source tools.

---

## 16. [vLLM v0.14.0 released](https://reddit.com/r/LocalLLaMA/comments/1qim0e9/vllm_v0140_released/)

**Author:** u/jinnyjuice | **Upvotes:** 169 | **Comments:** 32 | **Date:** 2026-01-20

**Summary:** The Reddit post announces the release of vLLM v0.14.0, highlighting new features and updates. Users discuss improvements like automatic context length fitting and the deprecation of certain quantization methods.

**Key Points:**
- Automatic context length fitting to GPU memory to prevent OOM failures
- Deprecation of some quantization methods, including HQQ
- Introduction of Marlin for Turing (sm75) as a major upgrade
- User interest in future sm120 optimizations

**Discussion Highlights:** Users expressed excitement about the automatic context length feature and discussed the implications of deprecated quantization methods. The Marlin upgrade for Turing was noted as a significant improvement, and there was anticipation for future optimizations.

---

## 17. [Current GLM-4.7-Flash implementation confirmed to be broken in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qih9r8/current_glm47flash_implementation_confirmed_to_be/)

**Author:** u/Sweet_Albatross9772 | **Upvotes:** 241 | **Comments:** 57 | **Date:** 2026-01-20

**Summary:** The Reddit post confirms that the current GLM-4.7-Flash implementation in llama.cpp is broken, causing issues like looping and poor performance due to differences in logprobs compared to vLLM. A potential fix is already proposed in a PR.

**Key Points:**
- GLM-4.7-Flash implementation in llama.cpp is broken
- Significant differences in logprobs compared to vLLM
- Issues include looping, overthinking, and poor performance
- Potential fix proposed in PR #18980
- Community expects a quick resolution

**Discussion Highlights:** The community acknowledges the issue and is optimistic about a quick fix. Some users share their strategy of waiting before adopting new models to avoid initial bugs.

---

## 18. [You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?](https://reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/)

**Author:** u/Adventurous-Gold6413 | **Upvotes:** 531 | **Comments:** 298 | **Date:** 2026-01-20

**Summary:** The post discusses selecting local models for use with 64GB RAM and 16GB VRAM when internet access is unavailable. Users share their preferred models and experiences.

**Key Points:**
- Users recommend models like Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B
- GPT-OSS-120B is highlighted for its performance and versatility
- The discussion emphasizes the importance of model compatibility with the given hardware
- Some users appreciate the contribution of models like GPT-OSS-120B to the community

**Discussion Highlights:** The consensus leans towards models like GPT-OSS-120B for its balance of performance and compatibility with the specified hardware. Users also appreciate the variety of models available for local use.

---

## 19. [Liquid AI released the best thinking Language Model Under 1GB](https://reddit.com/r/LocalLLaMA/comments/1qi512t/liquid_ai_released_the_best_thinking_language/)

**Author:** u/PauLabartaBajo | **Upvotes:** 228 | **Comments:** 52 | **Date:** 2026-01-20

**Summary:** Liquid AI released LFM2.5-1.2B-Thinking, a compact reasoning model that runs on-device with 900 MB of memory, excelling in math, tool use, and instruction following. It outperforms larger models in speed and efficiency, with broad ecosystem support.

**Key Points:**
- LFM2.5-1.2B-Thinking is optimized for on-device reasoning with low memory usage.
- It generates internal thinking traces for systematic problem-solving.
- The model outperforms larger models in speed and memory efficiency.
- Concerns raised about memory requirements and quantization trade-offs.
- Performance varies across benchmarks, with notable strength in math tasks.

**Discussion Highlights:** The discussion highlights concerns about memory requirements and the trade-offs of quantization. Users noted that while the model excels in math tasks, its performance is comparable to other models in other areas. There is also a desire for larger models and criticism of the licensing terms.

---

## 20. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 866 | **Comments:** 260 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, fully enclosed mobile AI system with 10 GPUs, designed for running large MoE models and supporting graphic design tasks. The build balances performance and cost, with a focus on mobility and protection from pets.

**Key Points:**
- The system features a Threadripper Pro 3995WX, 512GB DDR4, and 10 GPUs (8x 3090 + 2x 5090).
- It is designed for large MoE models, video generation, and high-detail image generation.
- The enclosure was a major challenge, solved with a Thermaltake Core W200 case.
- The total cost was around $17k, with a focus on avoiding unnecessary expenses.
- The post received significant engagement, with comments highlighting its uniqueness and practicality.

**Discussion Highlights:** The discussion highlights the system's impressive capabilities and the creative solution to the enclosure problem. Comments also joke about its portability and power requirements.

---

## 21. [Over 6K novels with reasoning traces to train full book writing LLMs](https://reddit.com/r/LocalLLaMA/comments/1qi3nmd/over_6k_novels_with_reasoning_traces_to_train/)

**Author:** u/XMasterDE | **Upvotes:** 111 | **Comments:** 46 | **Date:** 2026-01-20

**Summary:** The post announces an update to the LongPage dataset, expanding it to over 6,000 novels with hierarchical planning traces for training full-book writing LLMs. The team is also training a model on this dataset and plans to release it soon. Key points include the dataset's expansion, its support for training full-book writing LLMs, the team's ongoing model training, the dataset's availability on Hugging Face, and community interest. The discussion highlights enthusiasm for the dataset's potential in fiction writing, requests for more details on how the dataset works, and inquiries about the inclusion of specific works and the availability of data processing code.

---

## 22. [glm-4.7-flash has the best thinking process with clear steps, I love it](https://reddit.com/r/LocalLLaMA/comments/1qhxlgy/glm47flash_has_the_best_thinking_process_with/)

**Author:** u/uptonking | **Upvotes:** 141 | **Comments:** 34 | **Date:** 2026-01-20

**Summary:** The author praises glm-4.7-flash for its structured thinking process and clear responses, despite its slower performance compared to other models. They share their configuration settings and seek advice on improving speed without disabling the thinking feature. Key points include the model's detailed 7-step thinking process, its slower speed (19 tokens/s), and the user's preference for it over other models. The community agrees on the model's excellent reasoning process, with suggestions to adjust temperature settings for better performance.

---

## 23. [It's been one year since the release of Deepseek-R1](https://reddit.com/r/LocalLLaMA/comments/1qhs2sd/its_been_one_year_since_the_release_of_deepseekr1/)

**Author:** u/Recoil42 | **Upvotes:** 300 | **Comments:** 51 | **Date:** 2026-01-19

**Summary:** The Reddit post commemorates the one-year anniversary of the Deepseek-R1 release, highlighting its significant impact on the AI community. The discussion reflects on the rapid advancements in AI over the past year and the model's disruptive influence.

**Key Points:**
- Deepseek-R1 had a major impact, reportedly causing significant disruptions in the AI community.
- The release is considered one of the most important in AI history, second only to the original Llama model.
- The rapid pace of AI advancements is noted, with the past year feeling much longer due to the volume of changes.
- The model's release led to price reductions and increased transparency in AI reasoning outputs.

**Discussion Highlights:** The community consensus highlights Deepseek-R1's disruptive impact, with comments emphasizing its role in forcing industry changes and accelerating AI progress. The discussion also reflects on the rapid pace of advancements in the field over the past year.

---

## 24. [Mosquito - 7.3M parameter tiny knowledge model](https://reddit.com/r/LocalLLaMA/comments/1qhqzsi/mosquito_73m_parameter_tiny_knowledge_model/)

**Author:** u/Lopsided-Repair-3638 | **Upvotes:** 119 | **Comments:** 53 | **Date:** 2026-01-19

**Summary:** The post introduces 'Mosquito,' a tiny knowledge model with 7.3M parameters that can answer general knowledge questions, though with some humorous inaccuracies. The discussion highlights both amusement and criticism of the model's responses.

**Key Points:**
- Mosquito is a 7.3M parameter model capable of answering general knowledge questions.
- The model has a demo and is available on Hugging Face.
- Users found some answers humorous or inaccurate, such as defining a dog incorrectly.
- There is a request for a quantized version of the model.
- The model's knowledge gaps are noted, like knowing 'LLM' but not 'dog'.

**Discussion Highlights:** The discussion is marked by a mix of amusement and criticism, with users pointing out inaccuracies in the model's responses, such as defining a dog incorrectly or providing odd answers to simple questions. There is also a request for a quantized version of the model.

---

## 25. [Bartowski comes through again. GLM 4.7 flash GGUF](https://reddit.com/r/LocalLLaMA/comments/1qhpima/bartowski_comes_through_again_glm_47_flash_gguf/)

**Author:** u/RenewAi | **Upvotes:** 184 | **Comments:** 50 | **Date:** 2026-01-19

**Summary:** The post announces the release of GLM 4.7 Flash GGUF by Bartowski, with mixed user experiences reported in the comments. Some users find the model ineffective, while others note recent updates.

**Key Points:**
- Bartowski released GLM 4.7 Flash GGUF on Hugging Face
- Some users report the model as ineffective or 'brain dead'
- An 8-bit MLX and 16-bit Unsloth version were tried by users
- Unsloth uploaded a version shortly after the post
- Mixed experiences with different versions of the model

**Discussion Highlights:** Users are experimenting with different versions of GLM 4.7 Flash, with varying results. Some report issues with model performance, while others note recent updates from Unsloth. The consensus is still unclear, with ongoing testing and feedback.

---

## 26. [Unsloth GLM 4.7-Flash GGUF](https://reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/)

**Author:** u/Wooden-Deer-1276 | **Upvotes:** 226 | **Comments:** 44 | **Date:** 2026-01-19

**Summary:** The Reddit post discusses the release of the GLM-4.7-Flash GGUF model, with community feedback emphasizing patience for proper testing and recommendations for specific quantization settings and parameters. Key points include advising patience and thorough testing, recommending specific quantization settings and parameters, noting looping issues in quantized versions with BF16 suggested for best results, and providing environment details. The discussion highlights community enthusiasm and technical challenges, focusing on optimizing model performance and addressing issues like looping in quantized versions.

---

## 27. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 360 | **Comments:** 60 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the official support for GLM 4.7 Flash in llama.cpp, highlighting its community-driven development and performance improvements.

**Key Points:**
- GLM 4.7 Flash now officially supported in llama.cpp
- The implementation was a community effort, not by Z.ai developers
- Performance improvements noted, with some users reporting faster execution with specific settings
- Additional versions of the model have been uploaded to Hugging Face
- Mixed feedback on performance, with some users experiencing slower speeds with flash-attention

**Discussion Highlights:** The discussion highlights the community's enthusiasm for the new support and the collaborative effort behind its development. Users share their experiences with performance, noting both improvements and potential issues with certain configurations.

---

## 28. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 458 | **Comments:** 161 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights the effectiveness of GLM 4.7 Flash as a reliable local agent for various tasks, with users praising its performance and looking forward to its local availability. The discussion includes comparisons with other models and notes on its performance and output quality.

**Key Points:**
- GLM 4.7 Flash is praised for its reliability and performance in agentic frameworks.
- Users are eager for GGUFs to try the model locally.
- Comparisons with other models like Nemotron 30B and Qwen3 are discussed.
- The model's performance is noted to be fast on a 4090 GPU.
- Initial benchmarks suggest it is as smart as SEED OSS 36B but with better performance.

**Discussion Highlights:** The discussion highlights a positive consensus on GLM 4.7 Flash's performance and reliability. Users are interested in comparing it with other models and are eager to test it locally. Some users have already started testing it and report decent performance on high-end GPUs.

---

## 29. [New in llama.cpp: Anthropic Messages API](https://reddit.com/r/LocalLLaMA/comments/1qhaq21/new_in_llamacpp_anthropic_messages_api/)

**Author:** u/paf1138 | **Upvotes:** 167 | **Comments:** 50 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the introduction of the Anthropic Messages API in llama.cpp, which has been well-received by the community. Users are enthusiastic about trying it out and have shared practical tips for implementation.

**Key Points:**
- Introduction of Anthropic Messages API in llama.cpp
- Enthusiastic community response with users trying it out immediately
- Practical implementation tips shared, including a bash script for quick setup
- Mentions of using the API with specific hardware like M3 Ultra
- Discussion about context usage, noting that Claude codes use 12k of context from the start

**Discussion Highlights:** The discussion highlights a positive consensus around the new API, with users sharing practical advice and expressing excitement about its capabilities. Some users noted the age of the news and discussed specific hardware and context usage details.

---

## 30. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 731 | **Comments:** 230 | **Date:** 2026-01-19

**Summary:** The Reddit post discusses the release of zai-org/GLM-4.7-Flash on Hugging Face, highlighting its popularity and key features such as memory efficiency and large context support.

**Key Points:**
- The post has gained significant attention with 731 upvotes and 230 comments.
- The model uses MLA, which reduces KV cache memory usage, allowing more users to run it at full 200k context.
- There is excitement about the 30B model and mentions of a 3B thinking model.
- Users express nostalgia for larger models like 70B.
- The release is considered promising by the community.

**Discussion Highlights:** The community is enthusiastic about the new model, particularly its memory efficiency and large context capabilities. There is a consensus that this release is promising and will be accessible to a broader audience due to its reduced memory requirements.

---

## 31. [I made a Top-K implementation that's up to 20x faster than PyTorch CPU (open source)](https://reddit.com/r/LocalLLaMA/comments/1qh0yq8/i_made_a_topk_implementation_thats_up_to_20x/)

**Author:** u/andreabarbato | **Upvotes:** 148 | **Comments:** 104 | **Date:** 2026-01-19

**Summary:** The author developed an AVX2-optimized Top-K implementation that significantly outperforms PyTorch CPU, achieving up to 20x speed improvements depending on vocabulary size. Integrated into llama.cpp, it resulted in 63% faster prompt processing for a large model. The implementation uses SIMD and cache optimizations and is open-source.

**Key Points:**
- AVX2-optimized Top-K implementation beats PyTorch CPU by 4-20x
- Integrated into llama.cpp for 63% faster prompt processing
- Uses SIMD, cache optimizations, and adaptive sampling
- Open-source with pre-built DLLs and GitHub repository
- Community feedback includes requests for PRs and explanations

**Discussion Highlights:** The community showed strong interest, with top comments requesting a pull request for llama.cpp, asking for simplified explanations of the speed improvements, and discussing the need for reproducible benchmarks. Some criticism was directed at the lack of detailed benchmarks and the authenticity of the post.

---

## 32. [how do you pronounce “gguf”?](https://reddit.com/r/LocalLLaMA/comments/1qglyqz/how_do_you_pronounce_gguf/)

**Author:** u/Hamfistbumhole | **Upvotes:** 109 | **Comments:** 154 | **Date:** 2026-01-18

**Summary:** The Reddit post discusses various ways to pronounce 'gguf,' with users suggesting options like 'jee-guff,' 'giguff,' or spelling out each letter. The discussion includes humorous and practical perspectives on the pronunciation.

**Key Points:**
- The post asks for opinions on how to pronounce 'gguf'.
- Top comments suggest pronouncing each letter individually, similar to '.PNG'.
- Other suggestions include 'jee-guff,' 'giguff,' and 'guh-GUFF'.
- Some users humorously suggest not pronouncing it at all, just downloading it.

**Discussion Highlights:** The discussion highlights a lack of consensus on the pronunciation of 'gguf,' with users offering a variety of interpretations. The most upvoted comment suggests pronouncing each letter individually, while others provide creative or humorous alternatives.

---

## 33. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 349 | **Comments:** 93 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to stay within a ~10,000€ budget. The system is designed for running large AI models locally, with benchmark results provided for various models. Key points include the system specifications, cost details, and discussion highlights such as admiration for the build and questions about sourcing components.

---

## 34. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 456 | **Comments:** 71 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses the potential delay in Qwen 4 development, with the lead developer indicating a focus on quality over speed. The community generally supports this approach, appreciating the emphasis on quality improvements.

**Key Points:**
- Qwen 4 development may be delayed as the team focuses on quality
- Community largely supports the decision to prioritize quality over speed
- Some users caution against jumping to conclusions based on limited information
- There is appreciation for meaningful advancements rather than incremental updates
- The post gained significant attention with 456 upvotes and 71 comments

**Discussion Highlights:** The discussion highlights a consensus that focusing on quality is beneficial for the Qwen series. Users appreciate the developer's approach and caution against spreading unconfirmed rumors. There is a shared sentiment that meaningful improvements are more valuable than frequent, minor updates.

---

## 35. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 539 | **Comments:** 117 | **Date:** 2026-01-17

**Summary:** The author transitioned from MI100 GPUs to R9700 GPUs for a new server build, detailing the specifications and performance benchmarks. The build includes 128GB VRAM and 128GB RAM, costing less than an RTX 6000 Blackwell. The post highlights the cost-effectiveness and performance gains of the R9700 setup. Key points include the transition to R9700 GPUs, detailed specifications of the build, performance benchmarks, and community appreciation. The discussion highlights strong community appreciation for the build, with comments on cost-effectiveness and performance gains, and some concerns about financial implications.

---

## 36. [The Search for Uncensored AI (That Isn’t Adult-Oriented)](https://reddit.com/r/LocalLLaMA/comments/1qfq9ez/the_search_for_uncensored_ai_that_isnt/)

**Author:** u/Fun-Situation-4358 | **Upvotes:** 280 | **Comments:** 216 | **Date:** 2026-01-17

**Summary:** The post discusses the challenge of finding uncensored AI models that prioritize reasoning and creativity over adult-oriented content, highlighting a gap in the market between heavily restricted corporate AI and shallow adult-focused models.

**Key Points:**
- The author seeks an AI that is genuinely unfiltered and technically advanced, focusing on reasoning and creativity.
- Most models marketed as 'uncensored' are optimized for adult use rather than intelligence or depth.
- There is a perceived gap between heavily restricted corporate AI and shallow adult-focused models.
- Suggestions include self-hosted models, open-source projects, or lesser-known platforms.
- Top comments echo the sentiment and mention challenges with decensoring techniques.

**Discussion Highlights:** The discussion highlights a shared frustration with the lack of uncensored AI models that focus on serious problem-solving and creativity. Users agree that most decensoring techniques compromise the model's intelligence. Some suggest exploring open-source projects and lesser-known platforms for potential solutions.

---

## 37. [China's AGI-NEXT Conference (Qwen, Kimi, Zhipu, Tencent)](https://reddit.com/r/LocalLLaMA/comments/1qfmc05/chinas_aginext_conference_qwen_kimi_zhipu_tencent/)

**Author:** u/nuclearbananana | **Upvotes:** 119 | **Comments:** 22 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses China's AGI-NEXT Conference, highlighting insights on China vs US AGI development, paths to AGI, compute resources, and marketing strategies. Key points include Qwen's internal advancements, the belief that the next AI paradigm may come from OpenAI rather than Google, and observations about work culture and risk-taking in Chinese AI labs.

**Key Points:**
- Qwen has internally developed Qwen3.5 and context windows in the millions.
- The next AI paradigm is believed to likely come from OpenAI rather than Google.
- Chinese AI labs are described as less willing to take risks for innovation.
- Deepseek is noted for its talent concentration but was absent from the conference.

**Discussion Highlights:** The discussion highlights Qwen's advancements and the belief in OpenAI's potential for the next AI paradigm. There is also a consensus on the risk-averse culture in Chinese AI labs and the notable absence of Deepseek from the conference.

---

## 38. [Best "End of world" model that will run on 24gb VRAM](https://reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/)

**Author:** u/gggghhhhiiiijklmnop | **Upvotes:** 343 | **Comments:** 178 | **Date:** 2026-01-17

**Summary:** The user is seeking recommendations for the best LLM model that can run on a PC with 24GB VRAM and 64GB RAM, motivated by a desire to hoard data in anticipation of a potential 'end of the world' scenario. The discussion includes suggestions for specific models and practical advice on data storage.

**Key Points:**
- User wants to download and store large datasets like Wikipedia, Wiktionary, etc.
- Looking for LLM models that fit within 24GB VRAM and 64GB RAM constraints
- Suggestions include using the best available LLM and running it off SSD if necessary
- Specific model recommendations: gemma3:27b (with vision capabilities)
- Advice to download actual Wikipedia backups for offline access

**Discussion Highlights:** The discussion highlights practical considerations for data hoarding and model selection. There is a consensus on prioritizing the best available LLM, even if it requires running off SSD. Specific model recommendations and advice on downloading Wikipedia backups are also prominent.

---

## 39. [KoboldCpp v1.106 finally adds MCP server support, drop-in replacement for Claude Desktop](https://reddit.com/r/LocalLLaMA/comments/1qfb0gk/koboldcpp_v1106_finally_adds_mcp_server_support/)

**Author:** u/HadesThrowaway | **Upvotes:** 100 | **Comments:** 22 | **Date:** 2026-01-17

**Summary:** KoboldCpp v1.106 introduces native MCP server support, offering a drop-in replacement for Claude Desktop with high compatibility and support for both HTTP and STDIO transports. The update includes a user-friendly interface for managing tools and has been well-received by the community.

**Key Points:**
- KoboldCpp v1.106 adds native MCP server support
- Designed as a drop-in replacement for Claude Desktop with high compatibility
- Supports both HTTP and STDIO transports
- User-friendly interface for managing tools and enabling tool call approvals
- Positive community reception and additional resources like a guide are available

**Discussion Highlights:** The community has positively received the update, praising its compatibility with existing setups and the ease of integration. A guide for using MCP in KoboldCpp has been shared, and there is interest in seeing similar support in other tools like llama.cpp.

---

## 40. [DeepSeek Engram : A static memory unit for LLMs](https://reddit.com/r/LocalLLaMA/comments/1qf5oj0/deepseek_engram_a_static_memory_unit_for_llms/)

**Author:** u/Technical-Love-8479 | **Upvotes:** 325 | **Comments:** 48 | **Date:** 2026-01-17

**Summary:** DeepSeek AI introduced Engram, a memory unit for LLMs that separates remembering from reasoning, enabling efficient memory lookup and improved performance.

**Key Points:**
- Engram adds native memory lookup to LLMs
- Separates remembering from reasoning
- Improves reasoning, math, and code performance
- Enables massive memory scaling without GPU limits
- Frees attention for global reasoning rather than static knowledge

**Discussion Highlights:** The discussion highlights the importance of separating memory and reasoning, with consensus on the scalability benefits of Engram and its potential to offload compute from GPUs.

---

## 41. ["Welcome to the Local Llama. How janky's your rig?](https://reddit.com/r/LocalLLaMA/comments/1qf514i/welcome_to_the_local_llama_how_jankys_your_rig/)

**Author:** u/ForsookComparison | **Upvotes:** 104 | **Comments:** 22 | **Date:** 2026-01-16

**Summary:** The Reddit post in r/LocalLLaMA discusses various unconventional and humorous setups for running local AI models, highlighting the creative and sometimes 'janky' solutions users have implemented.

**Key Points:**
- Users share unconventional hardware setups for running AI models.
- Humorous and creative solutions are highlighted, such as using pallet wood to hold GPUs.
- Discussion includes technical details about hardware modifications and limitations.
- Mentions of specific hardware like 3090 GPUs and MI50s.
- Comments reflect a mix of technical advice and lighthearted banter.

**Discussion Highlights:** The discussion is characterized by a mix of technical insights and humorous anecdotes about the challenges and creative solutions involved in setting up hardware for local AI models. Users share their experiences with hardware limitations and unconventional fixes, creating a lively and engaging conversation.

---

## 42. [Prompt Repetition Improves Non-Reasoning LLMs - a paper](https://reddit.com/r/LocalLLaMA/comments/1qeuh0z/prompt_repetition_improves_nonreasoning_llms_a/)

**Author:** u/Foreign-Beginning-49 | **Upvotes:** 110 | **Comments:** 51 | **Date:** 2026-01-16

**Summary:** The Reddit post discusses a paper showing that repeating prompts can significantly improve the performance of non-reasoning LLMs without affecting latency or output format. The technique is simple yet effective across various models and benchmarks. Key points include: Prompt repetition improves non-reasoning LLM performance, the technique does not impact latency or output format, Deepseek is highlighted as an open weights model tested in the study, the simplicity of the technique raises questions about other untapped improvements, and some users suggest that repetition of context might contribute to agentic coder performance. The discussion highlights the surprising effectiveness of such a simple technique and speculates on other potential improvements that might be overlooked. Users also reflect on the broader implications for understanding and optimizing LLM performance.

---

## 43. [performance benchmarks (72GB VRAM) - llama.cpp server - January 2026](https://reddit.com/r/LocalLLaMA/comments/1qennp2/performance_benchmarks_72gb_vram_llamacpp_server/)

**Author:** u/jacek2023 | **Upvotes:** 112 | **Comments:** 39 | **Date:** 2026-01-16

**Summary:** The Reddit post discusses performance benchmarks for various models run on a setup with three RTX 3090 GPUs and 72GB VRAM. The author measures the speed of different models, noting that smaller models often run faster with fewer GPUs. The post includes a list of models and their respective tokens per second (tokens/s) performance.

**Key Points:**
- Performance benchmarks for various models on a 72GB VRAM setup with three RTX 3090 GPUs
- ERNIE-4.5-21B-A3B-Thinking-Q8_0 achieved the highest speed at 147.85 tokens/s
- The author uses the default 'llama-fit' mechanism and suggests manual tuning for better performance
- Discussion includes suggestions for further testing and optimization techniques
- The benchmarks are not scientific and focus solely on speed, not accuracy

**Discussion Highlights:** The discussion includes suggestions for further testing, such as filling the context to ~10k tokens and measuring performance. There are also recommendations for using specific flags during compilation to improve performance, like -DGGML_CUDA_PEER_COPY=ON for direct GPU data copying.

---

