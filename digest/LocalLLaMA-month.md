# r/LocalLLaMA Reading Digest

**Period:** 2026-01-24 to 2026-01-24
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [Your post is getting popular and we just featured it on our Discord!](https://reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/)

**Author:** u/roculus | **Upvotes:** 531 | **Comments:** 58 | **Date:** 2026-01-23

**Summary:** The Reddit post announces that a user's contribution has been featured on Discord and they have received a special flair. The community expresses annoyance at the bot's public posts, suggesting private messages would be more appropriate. Key points include the bot's announcement, the user's special flair, community annoyance, the existence of a pinned Discord thread, and suspicions of monetization. The discussion highlights a consensus that the bot's public announcements are spammy and disruptive, with a preference for private messages and skepticism about moderator intentions.

---

## 2. [Am I the only one who feels that, with all the AI boom, everyone is basically doing the same thing?](https://reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/)

**Author:** u/[deleted] | **Upvotes:** 395 | **Comments:** 186 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses the redundancy of AI tools and applications during the AI boom, highlighting that many new tools are less polished versions of existing ones. The discussion includes insights on the early days of AI technology and the enthusiasm driving shallow implementations. Key points include the low barrier to entry for AI development, the 'hype stage' with many self-proclaimed AI experts, and a consensus that while AI is exciting, the market is saturated with repetitive ideas. The discussion highlights the enthusiasm and low barrier to entry in AI development, leading to many similar and often redundant tools.

---

## 3. [Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp; 1.8B), Support for 10 languages](https://reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/)

**Author:** u/Nunki08 | **Upvotes:** 699 | **Comments:** 110 | **Date:** 2026-01-22

**Summary:** Qwen has open-sourced the Qwen3-TTS model family, including VoiceDesign, CustomVoice, and Base models in 0.6B and 1.8B sizes, supporting 10 languages. Resources are available on GitHub, Hugging Face, and other platforms.

**Key Points:**
- Open-sourcing of Qwen3-TTS model family
- Model sizes: 0.6B and 1.8B
- Support for 10 languages
- Resources available on GitHub, Hugging Face, blog, paper, and demo
- Community feedback highlights model performance and requests for additional support

**Discussion Highlights:** The community appreciates Qwen's open-sourcing efforts and provides feedback on model performance, including comments on voice quality and requests for support in running models on platforms like llama.cpp and mistral.rs.

---

## 4. [Qwen dev on Twitter!!](https://reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 722 | **Comments:** 60 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses Qwen's new TTS model announcement, with references to a vLLM leak and a link to the Hugging Face collection for Qwen3-TTS.

**Key Points:**
- Qwen's TTS model announcement
- Reference to a vLLM leak
- Link to Hugging Face collection for Qwen3-TTS
- Community discussion on the new model

**Discussion Highlights:** The community is engaged in discussing the new TTS model from Qwen, with some references to leaks and official announcements. The thread includes a link to the Hugging Face collection for further details.

---

## 5. [You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?](https://reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/)

**Author:** u/Adventurous-Gold6413 | **Upvotes:** 537 | **Comments:** 306 | **Date:** 2026-01-20

**Summary:** The post discusses the best local models to use with 64GB RAM and 16GB VRAM when internet access is unavailable. Users recommend models like Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B for their performance and capabilities.

**Key Points:**
- Users are discussing the best local models for offline use with specific hardware constraints.
- Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B are highly recommended models.
- GPT-OSS 120B is praised for its versatility and performance on the given hardware.
- The post gained significant attention, as indicated by upvotes and comments.

**Discussion Highlights:** The discussion highlights a consensus around GPT-OSS 120B as a top choice due to its fit for the hardware and strong performance across various domains. Other notable mentions include Gemma 3 27B and GLM 4.5 Air, with users appreciating their capabilities in an offline environment.

---

## 6. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 889 | **Comments:** 268 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, fully enclosed 10-GPU mobile AI system designed for running large MoE models and supporting graphic design tasks. The build balances performance and cost, using a mix of GPUs and a sturdy enclosure to protect hardware from pets.

**Key Points:**
- The system features a Threadripper Pro 3995WX, 512GB DDR4, and 10 GPUs (8x 3090 + 2x 5090).
- The build prioritizes mobility, enclosure, and cost-effectiveness (~$17k).
- The enclosure was critical to protect hardware from cats and ensure sturdiness.
- The system is optimized for large MoE models, video generation, and high-detail image generation.
- Top comments highlight the system's portability and airflow concerns.

**Discussion Highlights:** The discussion focuses on the system's portability, airflow challenges, and the creative solution to enclose the hardware. Comments also joke about the system's power requirements and its suitability for the intended tasks.

---

## 7. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 362 | **Comments:** 60 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the official support for GLM 4.7 Flash in llama.cpp, highlighting its community-driven development and performance improvements. Users discuss its efficiency compared to other implementations and share additional resources. Key points include: GLM 4.7 Flash now officially supported in llama.cpp, support is community-driven, performance improvements noted, additional resources and model versions shared by community members, and the post recognized with special flair for its contribution. The discussion highlights the community effort behind the implementation and shares performance insights, with some users noting that disabling flash-attention can lead to faster execution. Additional model versions and resources are also shared.

---

## 8. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 465 | **Comments:** 162 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights the effectiveness of GLM 4.7 Flash as a reliable local agent for various tasks, with users praising its performance and capabilities. The discussion includes comparisons with other models and notes on its efficiency and output quality.

**Key Points:**
- GLM 4.7 Flash is praised for its reliability and performance in agentic frameworks.
- Users report successful execution of tasks like cloning repos, running commands, and editing files without errors.
- Comparisons with other models like Nemotron 30B and Qwen3 are discussed.
- The model's efficiency and speed on hardware like the 4090 are noted.
- Initial GGUF versions of the model are already available for local testing.

**Discussion Highlights:** The discussion highlights a positive consensus around GLM 4.7 Flash's capabilities, with users eager to test it locally. Comparisons with other models and notes on performance and output quality are key topics.

---

## 9. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 741 | **Comments:** 230 | **Date:** 2026-01-19

**Summary:** The post announces the release of GLM-4.7-Flash model, generating significant interest and discussion in the r/LocalLLaMA community.

**Key Points:**
- The model uses MLA, reducing KV cache memory usage
- Supports full 200k context, making it accessible to more users
- Community excitement about 30B models and anticipation for larger models
- Special recognition given to the poster for their contribution
- Technical details about the model's architecture are being discussed

**Discussion Highlights:** The community shows strong enthusiasm for the new model release, particularly noting its memory efficiency and context length capabilities. There's anticipation for larger models and appreciation for the technical advancements demonstrated in this release.

---

## 10. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 347 | **Comments:** 101 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to stay within a ~10,000€ budget. The system is designed for running large AI models (120B+ parameters) locally, with benchmark results provided for various models.

**Key Points:**
- The build was motivated by a 50% subsidy for digitalization investments, allowing a high-end setup within budget.
- The system features 4x AMD R9700 GPUs (128GB VRAM total) and a Threadripper 9955WX CPU, optimized for large AI models.
- Benchmark results show performance metrics for models ranging from 8B to 230B parameters.
- The community praised the build, with comments highlighting its power and cost.
- The author emphasized data privacy as a key reason for running models locally.

**Discussion Highlights:** The community reacted positively, with comments praising the build's power and cost-effectiveness. Some users asked about the sourcing of components and the author's job, while others noted similarities to their own setups.

---

## 11. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 453 | **Comments:** 71 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses a potential slowdown in the development of Qwen 4, with the lead developer emphasizing a focus on quality over quantity. The community generally supports this approach, appreciating the potential for meaningful improvements.

**Key Points:**
- Qwen 4 development may be slowing down to focus on quality
- Community appreciates the focus on quality over quantity
- Uncertainty about whether the statement specifically refers to Qwen 4
- Support for taking necessary time to make significant advancements
- Mixed reactions with some cautioning against speculative rumors

**Discussion Highlights:** The discussion highlights a general consensus in support of focusing on quality improvements rather than rapid, incremental updates. Some users express caution about interpreting the developer's statement as definitive news about Qwen 4, while others see it as a positive step towards more meaningful advancements in the field.

---

## 12. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 539 | **Comments:** 117 | **Date:** 2026-01-17

**Summary:** The author upgraded their server from a dual MI100 setup to a quad R9700 setup, achieving 128GB VRAM and 128GB RAM for a cost-effective price. They detailed the hardware components and provided benchmarks for performance. Key points include the upgrade to quad R9700 GPUs, achieving 128GB VRAM and RAM, cost-effectiveness compared to RTX 6000 Blackwell, detailed hardware specifications and benchmarks, and positive community feedback. The community appreciated the detailed build and benchmarks, with some users joking about the financial irresponsibility of such upgrades. Overall, the post was well-received and featured on the subreddit's Discord.

---

## 13. [Best "End of world" model that will run on 24gb VRAM](https://reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/)

**Author:** u/gggghhhhiiiijklmnop | **Upvotes:** 337 | **Comments:** 180 | **Date:** 2026-01-17

**Summary:** The user is seeking recommendations for the best LLM model that can run on a PC with 24GB VRAM and 64GB RAM, in preparation for an 'end of the world' scenario. The discussion highlights various suggestions, including prioritizing the best available model and considering specific models like Gemma3:27b. Key points include the user's goal of hoarding data, suggestions to save the best possible LLM and run it off SSD if necessary, and the recommendation of Gemma3:27b for its capabilities. The consensus leans towards prioritizing the best available LLM model, even if it requires running off SSD, with a focus on practical data preservation.

---

## 14. [GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)](https://reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/)

**Author:** u/CuriousPlatypus1881 | **Upvotes:** 386 | **Comments:** 89 | **Date:** 2026-01-16

**Summary:** The post discusses the updated SWE-bench leaderboard results from December 2025, highlighting the performance of various AI models on GitHub PR tasks. Claude Opus 4.5 leads with a 63.3% resolved rate, followed closely by GPT-5.2 (extra high effort) at 61.5%. The post also notes the strong performance of open-source models like GLM-4.7. Key points include: Claude Opus 4.5 leads the leaderboard with a 63.3% resolved rate; GPT-5.2 (extra high effort) follows closely at 61.5%; GLM-4.7 is the strongest open-source model, ranking alongside closed models; Gemini 3 Flash Preview outperforms Gemini 3 Pro Preview despite being smaller and cheaper; The community is excited about the performance of open-source models and upcoming releases like DeepSeek v4. The discussion highlights the community's excitement about the performance of open-source models like GLM-4.7 and the anticipation for future releases such as DeepSeek v4. There is also a consensus that this benchmark is more believable compared to others.

---

## 15. [I fucking love this community](https://reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/)

**Author:** u/alhinai_03 | **Upvotes:** 526 | **Comments:** 54 | **Date:** 2026-01-16

**Summary:** The author expresses gratitude to the open-source community for enabling them to run large language models on older hardware, achieving impressive performance metrics. They highlight the importance of system memory and MoE architecture for their setup.

**Key Points:**
- Gratitude towards the open-source community for their contributions
- Achieving 14-13.5 tokens per second on a 10-year-old PC with limited VRAM
- Importance of system memory and MoE architecture for running large models
- Community appreciation for optimization efforts
- Discussion on practicality of system RAM and MoE combo

**Discussion Highlights:** The community appreciates the author's achievement and discusses the practicality of using system RAM and MoE architecture for running large models on older hardware. There is a consensus on the effectiveness of these optimizations.

---

## 16. [My story of underestimating /r/LocalLLaMA's thirst for VRAM](https://reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/)

**Author:** u/EmPips | **Upvotes:** 1355 | **Comments:** 91 | **Date:** 2026-01-15

**Summary:** The post highlights the author's underestimation of the r/LocalLLaMA community's demand for VRAM, with discussions focusing on hardware recommendations and market dynamics.

**Key Points:**
- Author underestimated VRAM demand
- Discord feature and special flair mentioned
- Hardware recommendations (3090s, R9700)
- Market behavior (selling cards after posts)

**Discussion Highlights:** The discussion includes hardware recommendations and insights into market behavior, with a focus on VRAM and GPU choices.

---

## 17. [Latest upgrade…A100 40 GB](https://reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/)

**Author:** u/inserterikhere | **Upvotes:** 405 | **Comments:** 54 | **Date:** 2026-01-15

**Summary:** The user upgraded their gaming rig to an AI rig by purchasing a faulty A100 GPU for $1000, which worked perfectly upon installation. They detailed their journey from using a 3080 to a 5070ti, then a 3090, and finally the A100, highlighting the cost-effective nature of their upgrades.

**Key Points:**
- User transitioned from a gaming rig to an AI rig using existing parts.
- Purchased an A100 GPU listed as faulty for $1000, which worked upon installation.
- Community expressed concerns about cooling the A100 GPU.
- Post received positive reception with 405 upvotes and 54 comments.
- User shared their upgrade path, including a 3090 and 7950x CPU.

**Discussion Highlights:** The community was generally impressed with the upgrade and offered technical advice, particularly regarding cooling the A100 GPU. Some users shared memes and jokes, while others provided practical suggestions for maintaining the hardware.

---

## 18. [Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M](https://reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/)

**Author:** u/eugenekwek | **Upvotes:** 328 | **Comments:** 54 | **Date:** 2026-01-14

**Summary:** The post announces the release of Soprano 1.1, highlighting significant improvements in stability, audio quality, and a 95% reduction in hallucinations. The model is praised for its performance and usability despite its small size.

**Key Points:**
- Soprano 1.1 reduces hallucinations by 95% and has a 50% lower WER than Soprano-80M
- The model supports sentences up to 30 seconds long, up from 15 seconds
- Blind study shows 63% preference rate for Soprano 1.1
- Community feedback highlights the model's impressive performance for its size
- Inquiries about future support, such as ONNX compatibility

**Discussion Highlights:** The community response is overwhelmingly positive, with users expressing surprise at the model's quality given its small size. There is interest in future developments and support for additional features.

---

## 19. [NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency](https://reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/)

**Author:** u/Fear_ltself | **Upvotes:** 717 | **Comments:** 130 | **Date:** 2026-01-14

**Summary:** NVIDIA's Orchestrator-8B is an 8-billion-parameter AI designed to manage and route complex tasks to various tools for greater efficiency. The post suggests this approach aligns with the idea of achieving AGI by integrating separate components effectively.

**Key Points:**
- Orchestrator-8B is a specialized AI for task management and routing.
- The model is relatively small but functional in connecting with other tools and models.
- The approach is seen as a step towards very functional AI systems.
- Comparisons to middle management and existing frameworks were made in the comments.
- The discussion highlights the potential of agentic frameworks and hierarchical model management.

**Discussion Highlights:** The discussion includes humorous comparisons to middle management and mentions of existing agentic frameworks. There is a consensus on the potential of hierarchical model management and the effectiveness of integrating separate AI components.

---

## 20. [GLM-Image is released!](https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/)

**Author:** u/foldl-li | **Upvotes:** 600 | **Comments:** 83 | **Date:** 2026-01-13

**Summary:** GLM-Image is a new image generation model with a hybrid autoregressive + diffusion decoder architecture. It excels in text-rendering and knowledge-intensive tasks while supporting various image-to-image tasks.

**Key Points:**
- Hybrid autoregressive + diffusion decoder architecture
- Strong performance in text-rendering and knowledge-intensive generation
- Supports image editing, style transfer, and multi-subject consistency
- MIT license praised for openness
- Community interest in quantization and optimization

**Discussion Highlights:** The community highlights the model's open MIT license, its competitive performance with other models, and the need for optimization (quantization) for broader accessibility. There is also curiosity about the model's capabilities in generating specific types of content.

---

## 21. [My wishes for 2026](https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/)

**Author:** u/jacek2023 | **Upvotes:** 654 | **Comments:** 179 | **Date:** 2026-01-13

**Summary:** The Reddit post discusses predictions for 2026, focusing on the possibility of affordable GPUs with more than 32GB of memory. The community is skeptical about this happening soon, with humorous and realistic responses.

**Key Points:**
- The post asks which predictions for 2026 are likely to happen first.
- A major focus is on the availability of affordable GPUs with >32GB memory.
- The community expresses skepticism about affordable high-memory GPUs becoming available in 2026.
- Comments include humorous remarks and references to other AI models like Qwen 4 and Mistral.

**Discussion Highlights:** The discussion highlights a mix of humor and realism, with many users doubting the feasibility of affordable high-memory GPUs in 2026. Some comments reference other AI models, indicating broader interest in AI advancements.

---

## 22. [kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop—no GPU required](https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/)

**Author:** u/Nunki08 | **Upvotes:** 400 | **Comments:** 93 | **Date:** 2026-01-13

**Summary:** Kyutai introduced Pocket TTS, a 100M-parameter text-to-speech model with high-quality voice cloning that runs on a laptop without requiring a GPU. The model is available on GitHub and Hugging Face, with a blog post and arXiv paper providing more details.

**Key Points:**
- Pocket TTS is a 100M-parameter model for high-quality voice cloning
- Runs on a laptop without needing a GPU
- Available on GitHub, Hugging Face, and detailed in a blog post and arXiv paper
- Memory usage can balloon during generation, reaching up to 32 GB
- Interest in fine-tuning for different languages and comparisons with other small models

**Discussion Highlights:** The discussion highlights concerns about memory usage during generation, interest in multilingual support, and comparisons with other small models. Users are cautious about the practicality of small models and suggest alternatives for specific use cases.

---

## 23. [GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/)

**Author:** u/TKGaming_11 | **Upvotes:** 371 | **Comments:** 93 | **Date:** 2026-01-12

**Summary:** The Reddit post highlights a GitHub repository by DeepSeek-AI called 'Engram,' which introduces a novel approach to memory in large language models using scalable lookup and n-gram embeddings. The discussion praises the originality of the work and explores its technical and biological implications.

**Key Points:**
- DeepSeek-AI's 'Engram' introduces a new memory approach for LLMs via scalable lookup
- The method uses n-gram embeddings as a complementary sparsity axis with O(1) lookup
- Commenters highlight the originality and potential of the approach
- Comparisons are drawn to biological memory systems in animals and humans
- The paper is well-received for its innovative ideas and technical depth

**Discussion Highlights:** The discussion is overwhelmingly positive, with commenters praising DeepSeek's consistent innovation. Key technical points include the use of n-gram embeddings and the potential for this approach to complement existing methods like MoE. Some commenters draw parallels to biological memory systems, suggesting the approach aligns with natural cognitive processes.

---

## 24. [LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)](https://reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 1064 | **Comments:** 114 | **Date:** 2026-01-11

**Summary:** The Reddit post introduces TimeCapsuleLLM, an open-source project training language models from scratch using 1800s London texts. The model, with 1.2B parameters and a 90GB dataset, generates historically accurate outputs, such as treating the telephone as unfamiliar due to its 1876 invention date.

**Key Points:**
- Project name: TimeCapsuleLLM, trained on 1800-1875 London texts.
- Model specifics: 1.2B parameters, 90GB dataset, 182k training steps.
- Unique outputs: Historically accurate responses, e.g., unfamiliarity with the telephone.
- Future plans: Creating synthetic Q&A pairs from the dataset.
- Community reception: Positive feedback, with top comments praising the project and sharing related ideas.

**Discussion Highlights:** The community showed strong support, with top comments praising the project's uniqueness and sharing related ideas. Notable interactions included humorous references to the model's historical cutoff date and playful comments about the telephone example.

---

## 25. [I bought a €9k GH200 “desktop” to save $1.27 on Claude Code (vLLM tuning notes)](https://reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/)

**Author:** u/Reddactor | **Upvotes:** 697 | **Comments:** 177 | **Date:** 2026-01-11

**Summary:** The author built a €9k GH200 'desktop' to run Claude Code locally, achieving better speeds than the cloud version and sharing optimized vLLM settings for dual 96GB systems. The setup uses MiniMax M2.1 for offline coding and blocks telemetry, though the cost is humorously noted as 321X the yearly subscription fee.

**Key Points:**
- Author spent €9k on a GH200 setup to run Claude Code locally, achieving better performance than cloud-based Sonnet.
- Optimized vLLM settings include TP2, 163,840 context, and specific tuning for dual 96GB systems.
- The setup uses MiniMax M2.1 for offline coding and blocks telemetry, with humorous accounting on cost vs. subscription fees.
- Top comments highlight the fun and novelty of the project, with some expressing envy over the hardware deal.
- Discussion includes technical questions about the specific model used (MiniMax-M2.1 FP8+INT4 AWQ).

**Discussion Highlights:** The community reacted with humor and admiration, noting the high cost but appreciating the technical achievement. Some users expressed envy over the hardware deal, while others asked technical questions about the model and settings used.

---

## 26. [It works! Abliteration can reduce slop without training](https://reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/)

**Author:** u/-p-e-w- | **Upvotes:** 404 | **Comments:** 129 | **Date:** 2026-01-11

**Summary:** The Reddit post discusses the use of abliteration to reduce 'slop' (flowery, cliched language) in LLM outputs without training. The author successfully applied this technique to the Mistral Nemo model, creating a slop-reduced version using Heretic, a tool for prompt dataset assembly. Key points include the effectiveness of abliteration, the use of Heretic for configuration, the process duration, and mixed opinions from the discussion on the technique's impact on creativity and prose quality.

---

## 27. [I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work](https://reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/)

**Author:** u/Ok-Pomegranate1314 | **Upvotes:** 897 | **Comments:** 147 | **Date:** 2026-01-09

**Summary:** The Reddit post describes a user's successful effort to cluster three DGX Sparks, which NVIDIA officially supports only for two, by developing a custom NCCL network plugin. The solution involved writing approximately 1500 lines of C code to handle subnet-aware NIC selection, raw RDMA verbs implementation, and a custom TCP handshake protocol, achieving distributed inference at over 8 GB/s across all three nodes.

**Key Points:**
- NVIDIA officially supports clustering only two DGX Sparks, but the user aimed for three.
- The custom NCCL network plugin involved subnet-aware NIC selection and raw RDMA implementation.
- The solution achieved distributed inference at over 8 GB/s across three nodes.
- The project required extensive low-level debugging and is documented on GitHub.
- The community praised the technical achievement and its potential impact.

**Discussion Highlights:** The community highlighted the technical difficulty of working with NCCL and praised the achievement as significant. Questions were raised about scalability and performance improvements with additional nodes.

---

## 28. [The reason why RAM has become so expensive](https://reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/)

**Author:** u/InvadersMustLive | **Upvotes:** 4551 | **Comments:** 380 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses the significant increase in RAM prices, with users noting a 4-10x rise in costs. The discussion highlights concerns about market manipulation and monopolization of key resources by major AI companies, potentially making it economically unviable for competitors, particularly in China.

**Key Points:**
- RAM prices have increased significantly, with reports of 4-10x higher costs.
- Concerns about market manipulation and monopolization of RAM by major AI companies.
- Potential economic inviability for competitors, especially in China, due to high RAM costs.
- Observations that the price increase may not be sustainable, hinting at a possible bubble.

**Discussion Highlights:** The discussion consensus suggests that the sharp increase in RAM prices is driven by strategic market control, with implications for competition in the AI industry. Users express skepticism about the sustainability of these price hikes.

---

## 29. [DeepSeek V4 Coming](https://reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/)

**Author:** u/External_Mood4719 | **Upvotes:** 501 | **Comments:** 109 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release V4, a next-generation AI model with enhanced code-generation capabilities, outperforming mainstream models like Claude and GPT. The model shows improvements in handling long code prompts and overall reasoning ability.

**Key Points:**
- DeepSeek V4 focuses on strong code-generation capabilities.
- V4 outperforms existing models like Claude and GPT in code generation.
- Improved handling of long code prompts and data pattern understanding.
- Users anticipate V4 to be more logically rigorous and reliable.
- Community discussion highlights excitement and expectations for V4's performance.

**Discussion Highlights:** The community is enthusiastic about DeepSeek V4, with users praising its potential improvements in reasoning and cost-effectiveness. Some anticipate significant advancements, while others express excitement about its integration with other technologies like mHC and deepseek-ocr.

---

## 30. [(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability](https://reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/)

**Author:** u/Nunki08 | **Upvotes:** 484 | **Comments:** 102 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release a new flagship AI model with strong coding capabilities, generating excitement and anticipation in the community.

**Key Points:**
- DeepSeek's upcoming AI model focuses on strong coding ability
- The announcement has generated significant interest and excitement
- Community members are looking forward to more models and competition
- Some users express skepticism about performance claims
- There is anticipation for the model's role-playing capabilities

**Discussion Highlights:** The community is largely excited about the new model, with some expressing skepticism about performance claims and anticipation for its role-playing abilities. There is a general consensus that more models are beneficial for the field.

---

## 31. [The NO FAKES Act has a "Fingerprinting" Trap that kills Open Source. We need to lobby for a Safe Harbor.](https://reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/)

**Author:** u/PostEasy7183 | **Upvotes:** 610 | **Comments:** 88 | **Date:** 2026-01-08

**Summary:** The NO FAKES Act proposes a 'digital replica right' that could hold developers liable for hosting open-source AI models used to create deepfakes, potentially stifling innovation. The post urges lobbying for a 'Safe Harbor' provision to protect open-source developers.

**Key Points:**
- The NO FAKES Act targets developers who 'make available' tools primarily used for creating digital replicas, imposing statutory damages.
- Open-source AI model hosting could become legally risky without a Safe Harbor provision.
- The post suggests contacting representatives to advocate for amendments protecting open-source developers.
- Comments highlight concerns about the bill's impact on innovation and the influence of big tech corporations.
- There is a call to action for developers to voice their opposition to the bill.

**Discussion Highlights:** The discussion emphasizes the potential negative impact on innovation and the need for a Safe Harbor provision. Comments suggest that the bill could disproportionately affect open-source developers and benefit large tech corporations.

---

## 32. [Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt](https://reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/)

**Author:** u/Prior-Arm-6705 | **Upvotes:** 942 | **Comments:** 147 | **Date:** 2026-01-08

**Summary:** Summary: A Reddit user created a compilation video of every instance Jensen Huang said 'AI' during the NVIDIA CES 2025 keynote, totaling 121 times, using open-source tools and MCPs for video processing. Key Points: Jensen Huang said 'AI' 121 times during the CES 2025 keynote. The user utilized open-source tools like Dive, yt-dlp-mcp, and ffmpeg-mcp-lite to create the compilation video. The process involved downloading the video, parsing subtitles for timestamps, cutting clips, and concatenating them. The result was described as hypnotic and gained significant attention on Reddit. Top comments included reactions to the video, Jensen's influence on tech prices, and mentions of other tech reviewers. Discussion Highlights: The discussion highlighted the hypnotic nature of the video, Jensen Huang's impact on tech pricing, and references to other tech reviewers like Gamers Nexus.

---

## 33. [16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/)

**Author:** u/ai-infos | **Upvotes:** 461 | **Comments:** 238 | **Date:** 2026-01-07

**Summary:** The post discusses running Deepseek v3.2 on 16 AMD MI50 GPUs, achieving 10 tokens per second (output) and 2000 tokens per second (input) with a 69000 context length. The setup is cost-effective and aims to provide a local AGI solution without excessive spending. Key points include the hardware specifications, performance metrics, power draw, cost-effectiveness, and future plans. The discussion highlights the efficiency of the setup, with comments praising the power usage as a potential heating solution and expressing interest in the cost-effectiveness for professional use.

---

## 34. [DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.](https://reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/)

**Author:** u/Nunki08 | **Upvotes:** 666 | **Comments:** 54 | **Date:** 2026-01-07

**Summary:** The Reddit post discusses the recent update to DeepSeek-R1's paper, which expanded from 22 pages to 86 pages, adding significant detail. The discussion includes comments about potential new architectures, linear attention research, and the value of added implementation specifics.

**Key Points:**
- DeepSeek-R1's paper was updated, expanding from 22 pages to 86 pages.
- The update includes substantial additional detail.
- Discussion highlights potential new architectures and linear attention research.
- Comments mention the value of added implementation specifics.
- The post received significant engagement with 666 upvotes and 54 comments.

**Discussion Highlights:** The discussion includes speculation about new architectures, interest in linear attention research, and appreciation for the added implementation details in the updated paper.

---

## 35. [A 30B Qwen Model Walks Into a Raspberry Pi… and Runs in Real Time](https://reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/)

**Author:** u/ali_byteshape | **Upvotes:** 498 | **Comments:** 79 | **Date:** 2026-01-06

**Summary:** The post discusses the successful optimization of a 30B Qwen model to run efficiently on a Raspberry Pi 5, achieving 8.03 tokens per second while retaining 94.18% of BF16 quality. The optimization process focuses on balancing memory usage and performance, particularly noting the quirks of GPU kernel choices. Key points include the model's performance on a Raspberry Pi 5, the optimization strategy prioritizing memory as a budget, the influence of GPU kernel choices on performance, the request for community feedback on various setups, and a user's experience with adjusting context size to avoid segfaults. The community showed interest in testing the model on different setups, including non-NVIDIA hardware and clusters of Raspberry Pis, with one user successfully running the model on a Raspberry Pi 5 after adjusting the context size and a suggestion to explore combining the model with an exo-like solution for distributed processing.

---

## 36. [Performance improvements in llama.cpp over time](https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/)

**Author:** u/jacek2023 | **Upvotes:** 686 | **Comments:** 85 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses performance improvements in llama.cpp over time, with a focus on NVIDIA GPUs and comparisons with other implementations like ik_llama.cpp. The community highlights significant progress in token generation speed.

**Key Points:**
- Performance gains in llama.cpp are notable, especially for NVIDIA GPUs.
- Comparisons with ik_llama.cpp show llama.cpp is approaching similar token generation speeds.
- Prompt processing in llama.cpp is still slower than token generation.
- The community appreciates the progress and contributions to the project.

**Discussion Highlights:** The discussion highlights the impressive progress in llama.cpp's performance, particularly in token generation speed, and notes that while prompt processing remains slower, the overall improvements are significant. There is a consensus on the value of these updates, especially for NVIDIA GPU users.

---

## 37. [For the first time in 5 years, Nvidia will not announce any new GPUs at CES — company quashes RTX 50 Super rumors as AI expected to take center stage](https://reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/)

**Author:** u/FullstackSensei | **Upvotes:** 629 | **Comments:** 196 | **Date:** 2026-01-05

**Summary:** Nvidia will not announce new GPUs at CES, focusing instead on AI. The company is facing supply constraints for high-end GPUs and may reintroduce older models like the RTX 3060. Rising DDR5 and storage prices are adding to the challenges for consumers.

**Key Points:**
- No new GPU announcements at CES, with AI taking center stage
- Limited supply of RTX 5070Ti, 5080, and 5090 GPUs
- Rumors of RTX 3060 reintroduction to meet demand
- DDR5 and storage prices have significantly increased
- Concerns about corporate greed and the future of local computing

**Discussion Highlights:** The discussion highlights frustration over corporate greed and the potential decline of local computing. Users express concerns about the lack of upgrade paths and the impact of rising hardware prices. Some suggest alternative solutions, such as increased competition from China.

---

## 38. [llama.cpp performance breakthrough for multi-GPU setups](https://reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/)

**Author:** u/Holiday-Injury-9397 | **Upvotes:** 573 | **Comments:** 201 | **Date:** 2026-01-05

**Summary:** The ik_llama.cpp project achieved a significant performance breakthrough for multi-GPU setups, delivering a 3x to 4x speed improvement in local LLM inference. This advancement allows for the utilization of multiple low-cost GPUs instead of expensive high-end cards, making it a game-changer for homelabs, server rooms, or cloud setups.

**Key Points:**
- ik_llama.cpp introduced a new execution mode (split mode graph) for multi-GPU configurations.
- The breakthrough delivers a 3x to 4x speed improvement in local LLM inference.
- This advancement enables the use of multiple low-cost GPUs instead of expensive high-end cards.
- Even on a single GPU or CPU-only, ik_llama.cpp shows consistent 2x prompt processing speeds compared to llama.cpp.
- The performance improvements are significant enough to rival other optimized frameworks like exllama and vllm.

**Discussion Highlights:** The community is excited about the performance gains and the potential cost savings. There is a consensus that ik_llama.cpp offers substantial improvements over the original llama.cpp, even on single GPU or CPU-only setups. Some users have noted bottlenecks in hybrid inference setups, but overall, the feedback is positive.

---

## 39. [GLM-Image model from Z.ai is coming](https://reddit.com/r/LocalLLaMA/comments/1q41bw1/glmimage_model_from_zai_is_coming/)

**Author:** u/Ravencloud007 | **Upvotes:** 316 | **Comments:** 59 | **Date:** 2026-01-04

**Summary:** The Reddit post announces the upcoming GLM-Image model from Z.ai, which has generated significant interest in the community. The model is highly anticipated, with discussions highlighting its potential size and performance.

**Key Points:**
- GLM-Image model from Z.ai is being introduced
- The model is expected to have a large number of parameters (e.g., 103b)
- Z.ai's image model is currently a community favorite
- There is interest in the model's size and ease of fine-tuning
- The post has gained popularity, with recognition from the community

**Discussion Highlights:** The community is excited about the GLM-Image model, with discussions focusing on its potential size and performance. There is a consensus that Z.ai's models are highly regarded, and users are eager to see how this new model will perform compared to existing ones.

---

## 40. [Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched](https://reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/)

**Author:** u/ubrtnk | **Upvotes:** 374 | **Comments:** 195 | **Date:** 2026-01-03

**Summary:** The post discusses the challenges local LLMs face when processing extreme or unlikely breaking news events, such as the US attacking Venezuela. The author shares experiences with different LLMs, highlighting their initial skepticism and eventual acknowledgment of the event's reality.

**Key Points:**
- Local LLMs initially classified extreme breaking news as hoaxes despite credible sources.
- Different LLMs (Qwen Research, Spark, GPT-OSS) showed varying levels of skepticism and eventual acknowledgment.
- The event's extreme nature caused LLMs to question its reality, even with evidence.
- Users shared similar experiences with LLMs doubting unlikely but real events.
- Discussion highlights the bias and limitations of LLMs in processing unfamiliar geopolitical events.

**Discussion Highlights:** The discussion highlights the limitations and biases of LLMs in processing extreme or unfamiliar events, with users sharing similar experiences and noting the models' tendency to doubt unlikely but real events.

---

## 41. [LeCun Says Llama 4 results "were fudged a little bit"](https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/)

**Author:** u/MrPecunius | **Upvotes:** 367 | **Comments:** 88 | **Date:** 2026-01-02

**Summary:** Yann LeCun, departing Meta AI Chief, confirmed that Llama 4 benchmark results were manipulated, leading to organizational changes and departures. The post discusses the impact on Meta's AI efforts and the community's reaction to these revelations.

**Key Points:**
- LeCun confirms Llama 4 benchmark manipulation
- Zuckerberg sidelined the GenAI organization, leading to departures
- Community expresses disappointment over Meta's AI strategy
- Shared article link for further reading
- Discussion on Meta's strategic missteps in AI

**Discussion Highlights:** The discussion highlights disappointment in Meta's handling of its AI initiatives, with users expressing concern over the lack of progress and the impact on open-source AI development. Some users shared additional resources, while others debated the organizational changes and their implications.

---

## 42. [Qwen-Image-2512](https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/)

**Author:** u/Nunki08 | **Upvotes:** 717 | **Comments:** 122 | **Date:** 2025-12-31

**Summary:** The Reddit post announces the release of Qwen-Image-2512, a new model available on multiple platforms including Hugging Face, ModelScope, and GitHub. It provides links to guides, GGUF files, and demos for users to try out the model.

**Key Points:**
- Qwen-Image-2512 is a new model release with guides and GGUF files available.
- The model can be accessed on platforms like Hugging Face, ModelScope, and GitHub.
- Users have shared positive experiences, including running the model on low-end hardware.
- The post includes links to various demos and APIs for users to explore.
- The community has shown appreciation for the new release with positive comments.

**Discussion Highlights:** Users have expressed excitement and appreciation for the new model release. One user successfully ran the model on a low-end desktop without a GPU, showcasing its accessibility. The community has also shared creative uses for the model, such as generating unique images.

---

## 43. [[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It’s running a raw Llama-7B instance with a 2048 token window.](https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/)

**Author:** u/simar-dmg | **Upvotes:** 744 | **Comments:** 110 | **Date:** 2025-12-30

**Summary:** A user reverse-engineered a Snapchat sextortion bot and discovered it was running a raw Llama-7B instance with a 2048 token window. The bot was vulnerable to a persona-adoption jailbreak, revealing its configuration and malicious payload.

**Key Points:**
- The bot used a Llama-7B model with a 2048 token context window and high temperature setting.
- A 'Grandma Protocol' jailbreak forced the bot to reveal its environment variables and configuration.
- The bot was likely running on minimal hardware to reduce costs.
- The bot eventually revealed a malicious link it was programmed to hide.
- Scammers are increasingly using open-source models like Llama-7B to avoid API costs and censorship.

**Discussion Highlights:** The discussion included skepticism about the accuracy of the bot's revealed information, with some users suggesting it could be entirely hallucinated. Others questioned the commonality of system prompts including environment variables.

---

## 44. [Llama-3.3-8B-Instruct](https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/)

**Author:** u/jacek2023 | **Upvotes:** 467 | **Comments:** 77 | **Date:** 2025-12-29

**Summary:** The post discusses the discovery and release of the Llama-3.3-8B-Instruct model, which was previously only available through Meta's API. The author details their process of obtaining the model by exploiting a finetuning feature in the API, despite initial difficulties and bugs.

**Key Points:**
- The Llama-3.3-8B-Instruct model is now available in GGUF format.
- The model was previously only accessible through Meta's Llama API.
- The author obtained the model by using a finetuning feature in the API, despite initial access issues.
- The model's authenticity is being verified through benchmarks and evaluations.
- The community is actively discussing and testing the model.

**Discussion Highlights:** The community is excited about the release, with ongoing evaluations to confirm the model's authenticity and performance. Some users are running benchmarks and comparisons against other Llama models, while others are discussing technical details like position embeddings.

---

## 45. [Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.](https://reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 344 | **Comments:** 120 | **Date:** 2025-12-29

**Summary:** Z AI is set to go public on January 8, aiming to raise $560 million, marking it as the first AI-native LLM company to list globally. The Reddit discussion highlights mixed reactions, with concerns about the future of open-source AI and the inevitability of monetization.

**Key Points:**
- Z AI's IPO is scheduled for January 8, targeting $560 million.
- Community concerns about the impact on open-source AI.
- Debate on whether Z AI will continue releasing open weight models.
- Monetization seen as a necessary step for companies.
- Mixed reactions from the community, with some expressing disappointment.

**Discussion Highlights:** The discussion reflects a consensus that monetization is inevitable, with significant concern about the future of open-source AI. Some users argue that Z AI might still release open weight models, while others see this as a shift away from open-source principles.

---

## 46. [Tencent just released WeDLM 8B Instruct on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 424 | **Comments:** 62 | **Date:** 2025-12-29

**Summary:** Tencent released WeDLM 8B Instruct on Hugging Face, a diffusion language model that performs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks. The release has garnered significant attention and positive feedback from the community.

**Key Points:**
- WeDLM 8B Instruct is a diffusion language model released by Tencent on Hugging Face.
- It runs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks.
- The model is released under the Apache 2.0 license.
- The community finds the benchmark scores impressive and the release promising.
- There is also a 7B version of the model available.

**Discussion Highlights:** The community is highly interested in the performance improvements and the potential of 7-8B models. The Apache 2.0 license and impressive benchmark scores are particularly noted as positive aspects. There is a consensus on the promising future of such models.

---

## 47. [NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux](https://reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/)

**Author:** u/HumanDrone8721 | **Upvotes:** 439 | **Comments:** 185 | **Date:** 2025-12-27

**Summary:** NVIDIA has dropped Pascal support on Linux, causing issues for Arch Linux users. The post highlights concerns and discussions around this change, with users expressing worry and sharing experiences.

**Key Points:**
- NVIDIA's decision to drop Pascal support affects Linux users, particularly those on Arch Linux.
- The 24GB P40, a Pascal card, is mentioned as a favored model before it became expensive.
- Users express concern and anticipation of this change, with some noting it was expected.
- Arch Linux has a history of moving legacy drivers to AUR, as noted in Arch News.

**Discussion Highlights:** The discussion highlights a mix of concern and acceptance among users. Some users reminisce about the P40 card, while others note that this change was anticipated. There is a consensus that Arch Linux's practice of moving legacy drivers to AUR is not surprising.

---

## 48. [Best Local LLMs - 2025](https://reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/)

**Author:** u/rm-rf-rm | **Upvotes:** 363 | **Comments:** 198 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses the best local LLMs of 2025, highlighting models like Minimax M2.1 and GLM4.7, and categorizes them by memory footprint. Users share detailed experiences and recommendations.

**Key Points:**
- Minimax M2.1 and GLM4.7 are noted for frontier model performance.
- Models are categorized by memory footprint: Unlimited (>128GB VRAM), Medium (8-128GB VRAM), and Small (<8GB VRAM).
- Users emphasize detailed descriptions of their setups and usage.
- Specific recommendations include Qwen3-4B-instruct and LFM2-8B-A1B for small models.
- Discussion includes debates on categorization and specialized use cases like RAG for technical documentation.

**Discussion Highlights:** The discussion highlights debates on categorization, with some users suggesting more granular categories. Specific model recommendations like Qwen3-4B-instruct and LFM2-8B-A1B are praised for their performance in small memory footprints. There is also interest in specialized applications such as RAG for technical documentation.

---

## 49. [NVIDIA has 72GB VRAM version now](https://reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/)

**Author:** u/decentralize999 | **Upvotes:** 467 | **Comments:** 148 | **Date:** 2025-12-26

**Summary:** The post discusses NVIDIA's new 72GB VRAM version, questioning if 96GB is too expensive and noting the AI community's lack of interest in 48GB. The discussion highlights varying opinions on the need for larger VRAM capacities and price considerations.

**Key Points:**
- NVIDIA has released a 72GB VRAM version.
- The post questions the cost of 96GB and the AI community's interest in 48GB.
- Top comments suggest a need for even larger VRAM capacities (128GB or more).
- Price comparisons are provided for different VRAM sizes.
- The discussion includes opinions on the value and affordability of higher VRAM options.

**Discussion Highlights:** The community is divided on the necessity of larger VRAM capacities, with some advocating for 128GB or more, while others focus on price per gig and affordability. The consensus leans towards preferring higher VRAM capacities if budget allows.

---

## 50. [Hard lesson learned after a year of running large models locally](https://reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/)

**Author:** u/inboundmage | **Upvotes:** 350 | **Comments:** 146 | **Date:** 2025-12-26

**Summary:** The author shares their experience running large language models locally, highlighting challenges with VRAM limitations, model scaling, and performance trade-offs. They conclude that local inference is viable for smaller models but requires significant hardware investment for larger ones.

**Key Points:**
- Running large models locally is feasible but has hard limits with consumer-grade hardware.
- VRAM fragmentation and memory management are significant challenges.
- Quantization helps but introduces quality trade-offs and bugs.
- Cloud-based solutions offer better performance for fast iteration.
- Community suggests using llama.cpp for CPU offloading and managing VRAM fragmentation.

**Discussion Highlights:** The discussion highlights practical solutions like using llama.cpp for CPU offloading and managing VRAM fragmentation. There is a consensus that while local inference is possible, it requires careful management of resources and may not match the performance of cloud-based solutions.

---

