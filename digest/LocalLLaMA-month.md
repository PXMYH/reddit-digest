# r/LocalLLaMA Reading Digest

**Period:** 2026-01-25 to 2026-01-25
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [Your post is getting popular and we just featured it on our Discord!](https://reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/)

**Author:** u/roculus | **Upvotes:** 537 | **Comments:** 58 | **Date:** 2026-01-23

**Summary:** The post announces that a user's contribution has been featured on Discord and they have received a special flair. The community discusses the annoyance of bot spam and questions the motives behind the bot's actions.

**Key Points:**
- The bot announces a user's post being featured on Discord and receiving a special flair.
- The community finds the bot's public announcements annoying and suggests private messages instead.
- There is speculation about the motives behind the bot's actions, including potential monetization.
- The community highlights other issues with the subreddit, including post removal.
- There is humor about the bot potentially announcing its own post's traction.

**Discussion Highlights:** The community consensus is that the bot's public announcements are annoying and should be sent as private messages. There is speculation about monetization and other subreddit issues, with some humor about the bot's potential self-announcement.

---

## 2. [Am I the only one who feels that, with all the AI boom, everyone is basically doing the same thing?](https://reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/)

**Author:** u/[deleted] | **Upvotes:** 394 | **Comments:** 186 | **Date:** 2026-01-22

**Summary:** The post discusses the redundancy in AI projects and tools, noting that many new AI applications are essentially reinventing existing solutions. The author appreciates AI but criticizes the lack of innovation and the financial investment in less polished versions of existing tools.

**Key Points:**
- Many AI projects are reinventing existing solutions.
- The barrier to entry for AI development is low, leading to shallow implementations.
- There is a trend of people shifting from other tech hypes (like cryptocurrency) to AI.
- Some developers are focusing on niche tools and improvements rather than reinventing the wheel.

**Discussion Highlights:** The discussion highlights a consensus that the AI field is currently in a hype stage, with many redundant projects. However, there is also enthusiasm for niche tools and improvements that address specific needs.

---

## 3. [Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp; 1.8B), Support for 10 languages](https://reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/)

**Author:** u/Nunki08 | **Upvotes:** 703 | **Comments:** 110 | **Date:** 2026-01-22

**Summary:** Qwen has open-sourced the Qwen3-TTS model family, including VoiceDesign, CustomVoice, and Base models in 0.6B and 1.8B sizes, supporting 10 languages. The release includes resources on GitHub, Hugging Face, and a demo.

**Key Points:**
- Qwen3-TTS models (0.6B & 1.8B) released with support for 10 languages
- Resources available on GitHub, Hugging Face, blog, paper, and demo
- Positive community reception with some concerns about voice quality
- Requests for compatibility with tools like llama.cpp and mistral.rs
- Appreciation for Qwen's open-source contributions

**Discussion Highlights:** The community generally appreciates Qwen's open-source release, though some users noted concerns about the voice quality sounding like anime dubs. There are requests for better compatibility with other tools and frameworks.

---

## 4. [Qwen dev on Twitter!!](https://reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 724 | **Comments:** 60 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses Qwen's TTS model announcement, with the community reacting to its source and legitimacy. The thread was locked as announcements were already out.

**Key Points:**
- Qwen's TTS model is from a vLLM leak
- Thread locked due to announcements being out
- Hugging Face collection link provided for the model
- Community discussing the model's legitimacy

**Discussion Highlights:** The community is focused on verifying the source of the TTS model, with some providing links to the model on Hugging Face. The thread was locked as the announcements were already made.

---

## 5. [8x AMD MI50 32GB at 26 t/s (tg) with MiniMax-M2.1 and 15 t/s (tg) with GLM 4.7 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1qjaxfy/8x_amd_mi50_32gb_at_26_ts_tg_with_minimaxm21_and/)

**Author:** u/ai-infos | **Upvotes:** 316 | **Comments:** 127 | **Date:** 2026-01-21

**Summary:** The post details a cost-effective local inference setup using 8x AMD MI50 GPUs, achieving high token generation speeds with MiniMax-M2.1 and GLM 4.7 models. The setup is praised for its performance and affordability, with a total VRAM of 256GB for under $1k.

**Key Points:**
- MiniMax-M2.1 achieves 26.8 tok/s output and 3000 tok/s input with a context length of 196,608.
- GLM 4.7 achieves 15.6 tok/s output and 3000 tok/s input with a context length of 95,000.
- The setup costs $880 for 256GB VRAM and draws 280W idle / 1200W during inference.
- The goal is to create one of the most cost-effective and fast local inference solutions.
- The community highly praises the setup for its performance and affordability.

**Discussion Highlights:** The community is highly enthusiastic about the setup, with comments highlighting its cost-effectiveness and performance. Some users express interest in replicating the setup but note challenges in sourcing the GPUs at the mentioned price.

---

## 6. [You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?](https://reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/)

**Author:** u/Adventurous-Gold6413 | **Upvotes:** 540 | **Comments:** 306 | **Date:** 2026-01-20

**Summary:** The post discusses selecting local models for use with 64GB RAM and 16GB VRAM without internet access. Users recommend models like Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B.

**Key Points:**
- Models should fit within 64GB RAM and 16GB VRAM constraints
- Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B are popular choices
- GPT-OSS 120B is praised for its versatility and performance
- Users appreciate the availability of these models for local use

**Discussion Highlights:** The discussion highlights a consensus around using models like Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B, with particular praise for GPT-OSS 120B's performance and versatility.

---

## 7. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 886 | **Comments:** 268 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, fully enclosed 10-GPU mobile AI system designed for running large MoE models and supporting graphic design tasks. The build, costing around $17k, balances performance and budget constraints while addressing mobility and enclosure challenges.

**Key Points:**
- The system features a Threadripper Pro 3995WX, 512GB DDR4, and a mix of 8x 3090 and 2x 5090 GPUs.
- The enclosure was a critical requirement due to the presence of cats, ruling out mining frames.
- The build aims to maximize performance without unnecessary expenses, avoiding all 5090s or 6000 PROs.
- The system is designed for mobility and supports tasks like video generation and high-detail image generation.
- The post received significant engagement, with comments highlighting its uniqueness and practicality.

**Discussion Highlights:** The discussion highlights the system's impressive specs and practicality, with comments praising its design and humorously noting its portability. The consensus appreciates the balance between performance and cost, as well as the creative solution to enclosure challenges.

---

## 8. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 359 | **Comments:** 60 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the official support for GLM 4.7 Flash in llama.cpp, highlighting its community-driven development and performance improvements.

**Key Points:**
- GLM 4.7 Flash now officially supported in llama.cpp
- The implementation was a community effort, not by Z.ai developers
- Performance improvements noted, with some users reporting faster execution with specific settings
- Additional versions of the model have been uploaded to Hugging Face
- Mixed feedback on flash-attention performance, with some users finding it slower

**Discussion Highlights:** The discussion highlights the community's enthusiasm for the new support, clarifies the nature of the 'official' support, and shares performance insights and additional resources.

---

## 9. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 464 | **Comments:** 162 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights the effectiveness of GLM 4.7 Flash as a reliable local agent for various tasks, including coding and command execution, with users eagerly awaiting its local availability via GGUFs. The discussion compares it favorably to other models like Nemotron 30B and Qwen3, noting its performance and output quality.

**Key Points:**
- GLM 4.7 Flash is praised for its reliability in agentic frameworks and ability to handle complex tasks without errors.
- Users are excited about the upcoming GGUFs for local use.
- Comparisons with other models like Nemotron 30B and Qwen3 are discussed, with GLM 4.7 Flash being favored.
- The model's performance and output quality are noted as impressive.
- The PR for GLM 4.7 Flash has been merged into llama.cpp, indicating active development and community interest.

**Discussion Highlights:** The discussion highlights a positive consensus around GLM 4.7 Flash's capabilities, with users sharing their experiences and comparisons to other models. There is a notable excitement about its potential for local use and its performance in various tasks.

---

## 10. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 742 | **Comments:** 230 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights the release of zai-org/GLM-4.7-Flash on Hugging Face, garnering significant attention with 740 upvotes and 230 comments. The discussion focuses on the model's features, such as its use of MLA for efficient memory usage and its potential for running at full 200k context.

**Key Points:**
- The post is a link to zai-org/GLM-4.7-Flash on Hugging Face.
- The model uses MLA, reducing KV cache memory consumption.
- It can potentially run at full 200k context, making it accessible to more users.
- Community reactions include excitement and nostalgia for larger models.
- The post received a special flair and was featured on Discord.

**Discussion Highlights:** The community is enthusiastic about the release, particularly noting the model's efficiency and potential for widespread use. There is also a sense of nostalgia for larger models, with some users expressing a preference for 70b models.

---

## 11. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 345 | **Comments:** 101 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to stay within a ~10,000€ budget. The system is designed for running large AI models (120B+ parameters) locally, with benchmark results provided for various models. Key points include the system's purpose for local AI inference, the budget details, benchmark results, community interest in hardware sourcing, and recognition of similar builds. The discussion highlights strong community interest in the hardware setup and trends in high-VRAM configurations.

---

## 12. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 452 | **Comments:** 71 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses a potential slowdown in the development of Qwen 4, with the lead developer emphasizing a focus on quality over quantity. The community generally appreciates this approach, though some express skepticism about the rumors.

**Key Points:**
- Qwen 4 development may be slowing down to focus on quality
- Community appreciates the focus on quality over quantity
- Skepticism exists about the validity of the rumors
- Incremental improvements are seen as less meaningful for advancing the field
- The post gained significant attention with 452 upvotes and 71 comments

**Discussion Highlights:** The discussion highlights a general consensus that focusing on quality is beneficial, though there is some debate about the interpretation of the developer's statement and the potential impact on the timeline for Qwen 4.

---

## 13. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 537 | **Comments:** 117 | **Date:** 2026-01-17

**Summary:** The author upgraded from MI100 to R9700 GPUs for better performance and cost efficiency, building a 128GB VRAM server with detailed specifications and benchmarks. Key points include the transition to R9700 GPUs, detailed server specifications, performance benchmarks, and positive community feedback. The community appreciated the detailed build and benchmarks, with some expressing financial irresponsibility humorously.

---

## 14. [Best "End of world" model that will run on 24gb VRAM](https://reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/)

**Author:** u/gggghhhhiiiijklmnop | **Upvotes:** 344 | **Comments:** 180 | **Date:** 2026-01-17

**Summary:** The user is seeking recommendations for the best LLM model that can run on a PC with 24GB VRAM and 64GB RAM, in preparation for a hypothetical 'end of world' scenario. The discussion includes suggestions for specific models and practical advice on data storage.

**Key Points:**
- User wants to hoard data like Wikipedia, Wiktionary, etc.
- Looking for models that fit within 24GB VRAM and 64GB RAM
- Suggestions include Gemma3:27b and practical advice on data storage
- Discussion highlights the importance of saving the best possible LLM and running it off SSD if necessary
- Mention of downloading actual Wikipedia backups for offline use

**Discussion Highlights:** The discussion emphasizes practicality, with a consensus on saving the best possible LLM and running it off SSD if needed. Specific model recommendations include Gemma3:27b, and there is a focus on downloading comprehensive data backups like Wikipedia.

---

## 15. [GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)](https://reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/)

**Author:** u/CuriousPlatypus1881 | **Upvotes:** 379 | **Comments:** 89 | **Date:** 2026-01-16

**Summary:** The post discusses the December 2025 SWE-bench leaderboard results, highlighting the performance of various AI models on GitHub PR tasks. Claude Opus 4.5 leads with a 63.3% resolved rate, followed closely by GPT-5.2 (extra high effort) at 61.5%. The post also notes the strong performance of open-source models like GLM-4.7.

**Key Points:**
- Claude Opus 4.5 leads the leaderboard with a 63.3% resolved rate.
- GPT-5.2 (extra high effort) follows closely at 61.5%.
- Gemini 3 Flash Preview outperforms Gemini 3 Pro Preview despite being smaller and cheaper.
- GLM-4.7 is the strongest open-source model, ranking alongside closed models like GPT-5.1-codex.
- The community is excited about the performance of open-source models and upcoming releases like DeepSeek v4.

**Discussion Highlights:** The community is particularly excited about the performance of open-source models like GLM-4.7 and the potential of upcoming models like DeepSeek v4. There is also a consensus that this benchmark is more believable compared to others.

---

## 16. [I fucking love this community](https://reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/)

**Author:** u/alhinai_03 | **Upvotes:** 520 | **Comments:** 54 | **Date:** 2026-01-16

**Summary:** The author expresses gratitude to the open-source community for enabling them to run large language models on older hardware, highlighting the efficiency of MoE architectures and system memory.

**Key Points:**
- Author runs large models on a 10-year-old PC with 4GB VRAM
- Achieves 14 t/s with nemotron-3-nano-30B-a3b-iq4_nl model
- MoE architecture and system memory are key for performance
- Community optimization efforts are highly praised
- Discussion highlights practicality of system RAM + MoE combo

**Discussion Highlights:** The community appreciates the author's achievement and discusses the practicality of using system RAM and MoE architectures for running large models on limited hardware.

---

## 17. [My story of underestimating /r/LocalLLaMA's thirst for VRAM](https://reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/)

**Author:** u/EmPips | **Upvotes:** 1348 | **Comments:** 91 | **Date:** 2026-01-15

**Summary:** The Reddit post discusses the author's experience with underestimating the VRAM requirements for LocalLLaMA, highlighting the community's enthusiasm and engagement. The post gained significant attention, as evidenced by the high number of upvotes and comments.

**Key Points:**
- The post received 1348 upvotes and 91 comments, indicating high community interest.
- The author was recognized with a special flair for their contribution.
- A top comment references the California gold rush, suggesting a metaphor for the community's enthusiasm.
- Discussion includes recommendations for specific hardware like the R9700 for VRAM-per-slot efficiency.
- Some users mention selling their hardware after gaining attention.

**Discussion Highlights:** The discussion highlights the community's strong interest in VRAM requirements and hardware recommendations. There is a consensus around the importance of efficient VRAM usage, with specific hardware suggestions like the R9700 being mentioned. The metaphor of the gold rush underscores the excitement and competitive nature of the community.

---

## 18. [Latest upgrade…A100 40 GB](https://reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/)

**Author:** u/inserterikhere | **Upvotes:** 405 | **Comments:** 54 | **Date:** 2026-01-15

**Summary:** The user upgraded their gaming rig to an AI rig by purchasing a faulty A100 GPU for $1000, which worked perfectly upon installation, allowing them to run and train larger models. The community responded positively, with discussions focusing on cooling solutions and general admiration for the upgrade.

**Key Points:**
- User transitioned from a gaming rig to an AI rig using existing parts and new components like a 3090 and 7950x.
- Purchased a faulty A100 GPU for $1000, which worked flawlessly upon installation.
- Community expressed interest in cooling solutions for the A100 and praised the upgrade.
- Post gained significant traction with 405 upvotes and 54 comments.

**Discussion Highlights:** The community was engaged and supportive, with discussions centered around practical concerns like cooling the A100 and general admiration for the successful upgrade. Some users shared memes and images to celebrate the achievement.

---

## 19. [Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M](https://reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/)

**Author:** u/eugenekwek | **Upvotes:** 325 | **Comments:** 54 | **Date:** 2026-01-14

**Summary:** The post announces Soprano 1.1, an improved version of the Soprano TTS model with significant reductions in hallucinations and audio artifacts, along with enhanced stability and support for longer sentences. The community response is largely positive, highlighting the model's impressive performance for its size.

**Key Points:**
- Soprano 1.1 reduces hallucinations by 95% and lowers WER by 50% compared to Soprano-80M.
- The model now supports sentences up to 30 seconds long, up from 15 seconds.
- A blind study showed a 63% preference rate for Soprano 1.1's outputs.
- Community feedback highlights the model's usability and performance, with requests for additional features like ONNX support.
- The model is available on Hugging Face and GitHub for further exploration.

**Discussion Highlights:** The community response is overwhelmingly positive, with users expressing surprise at the model's performance given its small size (80M parameters). Notable comments include appreciation for the model's usability, requests for ONNX support, and admiration for the developer's ability to create such a high-quality model independently.

---

## 20. [NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency](https://reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/)

**Author:** u/Fear_ltself | **Upvotes:** 717 | **Comments:** 130 | **Date:** 2026-01-14

**Summary:** NVIDIA's Orchestrator-8B is an 8-billion-parameter AI designed to manage and route complex tasks to various tools, sparking discussions about AGI and functional AI systems.

**Key Points:**
- Orchestrator-8B is a specialized AI for task management and routing.
- It aims to connect with other tools and models for greater efficiency.
- Discussions highlight its potential in creating functional AI systems.
- Comparisons to middle management and existing frameworks are noted.

**Discussion Highlights:** The discussion includes humor about the model being a 'Middle manager LLM' and mentions of existing agentic frameworks like Claude's code style agents. There is a general consensus on the potential of such models in creating more functional AI systems.

---

## 21. [GLM-Image is released!](https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/)

**Author:** u/foldl-li | **Upvotes:** 596 | **Comments:** 83 | **Date:** 2026-01-13

**Summary:** GLM-Image is a new image generation model with a hybrid autoregressive + diffusion decoder architecture. It excels in text-rendering and knowledge-intensive tasks while supporting various image-to-image tasks.

**Key Points:**
- Hybrid autoregressive + diffusion decoder architecture
- Excels in text-rendering and knowledge-intensive generation
- Supports image editing, style transfer, and multi-subject consistency
- MIT license with no restrictions
- Model size: 13GB diffusion model + 20GB text encoder

**Discussion Highlights:** The community appreciates the MIT license and the model's capabilities. There is interest in quantizing the model for easier use and discussions about its performance compared to other models.

---

## 22. [My wishes for 2026](https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/)

**Author:** u/jacek2023 | **Upvotes:** 654 | **Comments:** 179 | **Date:** 2026-01-13

**Summary:** The Reddit post discusses predictions for 2026, focusing on the possibility of affordable GPUs with more than 32GB of memory. The community engages in a mix of hopeful and skeptical comments about this prospect.

**Key Points:**
- The post asks which predictions for 2026 are likely to happen first.
- A major focus is on the availability of affordable GPUs with >32GB memory.
- Comments range from humorous skepticism to hopeful speculation.
- Mentions of AI models like Qwen 4 and Mistral as potential developments.
- Community engagement is high, with the post being featured on Discord.

**Discussion Highlights:** The discussion highlights a mix of humor and skepticism regarding the feasibility of affordable high-memory GPUs in 2026. Some users joke about the idea being a 'dream' or 'manifestation,' while others mention specific AI models as more realistic developments. The overall consensus leans towards skepticism but with a lighthearted tone.

---

## 23. [kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop—no GPU required](https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/)

**Author:** u/Nunki08 | **Upvotes:** 401 | **Comments:** 93 | **Date:** 2026-01-13

**Summary:** Kyutai introduced Pocket TTS, a 100M-parameter text-to-speech model with high-quality voice cloning that runs on a laptop without requiring a GPU. The model is available on GitHub and Hugging Face, with a blog post and arXiv paper providing more details.

**Key Points:**
- Pocket TTS is a 100M-parameter model for high-quality voice cloning.
- It runs on a laptop without needing a GPU.
- Available on GitHub, Hugging Face, and detailed in a blog post and arXiv paper.
- Potential for fine-tuning on different languages.
- Memory usage can balloon during generation.

**Discussion Highlights:** The discussion highlights potential for fine-tuning on different languages, a warning about memory usage ballooning during generation, and comparisons with other small models.

---

## 24. [GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/)

**Author:** u/TKGaming_11 | **Upvotes:** 370 | **Comments:** 93 | **Date:** 2026-01-12

**Summary:** The Reddit post highlights a new GitHub repository by DeepSeek-AI called 'Engram,' which introduces a novel approach to conditional memory in large language models using scalable lookup. The discussion praises the innovation and technical depth of the paper.

**Key Points:**
- DeepSeek-AI's 'Engram' introduces conditional memory via scalable lookup as a new sparsity axis for LLMs.
- The approach uses n-gram embedding and mHC (with M=4) for ablations, suggesting derisked implementation.
- The method adds static memory as a complementary sparsity axis with O(1) lookup, contrasting with MoE's neural computation.
- The discussion highlights the innovation and potential biological plausibility of the approach.

**Discussion Highlights:** The community consensus is highly positive, with users praising DeepSeek's originality and the technical merits of the paper. Key points of interest include the n-gram embedding approach, the use of mHC, and the potential biological parallels of the method.

---

## 25. [LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)](https://reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 1062 | **Comments:** 114 | **Date:** 2026-01-11

**Summary:** The post introduces TimeCapsuleLLM, a 1.2B parameter language model trained exclusively on 1800-1875 London texts to minimize modern bias. The model demonstrates period-specific knowledge and limitations, such as unfamiliarity with post-1875 concepts like telephones. The project aims to create synthetic Q&A pairs next.

**Key Points:**
- TimeCapsuleLLM is trained on 90GB of 1800-1875 London texts with no modern data or fine-tuning.
- The model shows period-specific behaviors, like arguing against the Roman Catholic Church and misunderstanding post-1875 terms.
- Future work includes generating synthetic Q&A pairs from the dataset.
- The project is open-source and available on GitHub and Hugging Face.
- Community feedback highlights enthusiasm and similar projects exploring historical datasets.

**Discussion Highlights:** The community expressed strong support for the project, with comments praising its uniqueness and potential. Some users shared their own experiments with historical datasets, indicating a broader interest in time-period-specific language models. Humorous remarks about the model's limitations (e.g., 'I'm sorry but my cutoff date is 1875') were also noted.

---

## 26. [I bought a €9k GH200 “desktop” to save $1.27 on Claude Code (vLLM tuning notes)](https://reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/)

**Author:** u/Reddactor | **Upvotes:** 692 | **Comments:** 177 | **Date:** 2026-01-11

**Summary:** The author built a €9k GH200 'desktop' to run Claude Code locally, achieving better speeds and results than the cloud version. They shared optimized vLLM settings for dual 96GB systems and highlighted the cost savings and performance benefits of local execution. Key points include the high cost of the setup, the performance improvements, and the community's humorous and appreciative response. Discussion highlights reflect the community's appreciation of the technical achievement and the high cost involved.

---

## 27. [It works! Abliteration can reduce slop without training](https://reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/)

**Author:** u/-p-e-w- | **Upvotes:** 399 | **Comments:** 129 | **Date:** 2026-01-11

**Summary:** The post discusses the use of abliteration to reduce 'slop' (flowery, cliched language) in LLM outputs without training. The author successfully applied this technique to the Mistral Nemo model, creating a slop-reduced version using Heretic, a tool originally designed for censorship removal.

**Key Points:**
- Abliteration can reduce slop in LLM outputs without training.
- The author used Heretic to create a slop-reduced configuration for the Mistral Nemo model.
- The process took 2.5 hours on an A6000 but could be faster with quantization or reduced parameters.
- The technique shows semantic separation between layers 7 and 10 in the model.
- Community feedback is mixed, with some appreciating the reduction in slop while others find the output too dry.

**Discussion Highlights:** The discussion highlights mixed opinions on the effectiveness of the slop reduction technique. Some users appreciate the reduction in cliched language, while others feel the output lacks imagination or becomes too dry. There is also interest in whether this technique could be applied to other patterns or styles of writing.

---

## 28. [I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work](https://reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/)

**Author:** u/Ok-Pomegranate1314 | **Upvotes:** 896 | **Comments:** 147 | **Date:** 2026-01-09

**Summary:** The author successfully clustered three NVIDIA DGX Sparks, overcoming networking limitations by writing a custom NCCL plugin. This achievement allows distributed inference across all three nodes at high speeds, despite NVIDIA's official support only covering two-node clusters.

**Key Points:**
- Author clustered three DGX Sparks using a custom NCCL plugin, overcoming NVIDIA's two-node limit.
- The custom plugin handles subnet-aware NIC selection and raw RDMA verbs implementation.
- Achieved distributed inference at 8+ GB/s over RDMA across three nodes.
- The solution involved extensive low-level debugging and is considered a significant technical feat.
- The GitHub repository for the plugin is available for further exploration.

**Discussion Highlights:** The community praised the technical achievement, with comments highlighting the difficulty of working with NCCL and the potential significance of this solution. Questions were raised about scalability and performance improvements with additional nodes.

---

## 29. [The reason why RAM has become so expensive](https://reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/)

**Author:** u/InvadersMustLive | **Upvotes:** 4563 | **Comments:** 381 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses the significant increase in RAM prices, with users suggesting that OpenAI may be monopolizing RAM to create future demand and make other AI data centers economically unviable. The cost of RAM has reportedly increased by up to 10 times compared to the previous year.

**Key Points:**
- RAM prices have increased significantly, with some users reporting a 10-fold increase.
- OpenAI is accused of monopolizing RAM to create future demand and hinder competitors.
- The high cost of RAM is making other AI data centers, particularly Chinese ones, economically unviable.
- Users express concern about the sustainability of the current pricing trend.

**Discussion Highlights:** The discussion highlights concerns about OpenAI's potential monopolization of RAM, the economic impact on competing AI data centers, and the significant increase in RAM prices over the past year.

---

## 30. [DeepSeek V4 Coming](https://reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/)

**Author:** u/External_Mood4719 | **Upvotes:** 501 | **Comments:** 109 | **Date:** 2026-01-09

**Summary:** DeepSeek is expected to release V4, a next-generation AI model with strong code-generation capabilities, outperforming mainstream models like Claude and GPT. The model shows improvements in handling long code prompts and overall reasoning ability.

**Key Points:**
- DeepSeek V4 focuses on strong code-generation capabilities
- Outperforms existing models like Claude and GPT in code generation
- Improved handling of long code prompts and data patterns
- Enhanced reasoning ability and reliability for complex tasks
- Users appreciate DeepSeek's cost-effectiveness and local API options

**Discussion Highlights:** Users express excitement and anticipation for V4, with many praising DeepSeek's performance and affordability. Some speculate on potential delays due to extensive pre-training and post-training processes.

---

## 31. [(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability](https://reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/)

**Author:** u/Nunki08 | **Upvotes:** 484 | **Comments:** 102 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release a new flagship AI model with strong coding capabilities, generating excitement and discussion in the community.

**Key Points:**
- DeepSeek's upcoming model focuses on strong coding ability
- Community excitement and anticipation for the new model
- Discussion about the potential impact on the AI landscape
- Mixed reactions to typical marketing language used in AI announcements
- Requests for maintaining role-playing capabilities in the new model

**Discussion Highlights:** The community shows enthusiasm for DeepSeek's new model, with some expressing excitement about increased competition in AI models. There is also skepticism about marketing claims and a desire to maintain diverse capabilities like role-playing in the new model.

---

## 32. [The NO FAKES Act has a "Fingerprinting" Trap that kills Open Source. We need to lobby for a Safe Harbor.](https://reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/)

**Author:** u/PostEasy7183 | **Upvotes:** 617 | **Comments:** 88 | **Date:** 2026-01-08

**Summary:** The NO FAKES Act threatens open-source AI development by imposing liability on developers for tools used to create digital replicas, potentially banning open-source AI hosting in the US. The post urges lobbying for a Safe Harbor provision to protect developers. Key points include the creation of a 'digital replica right', potential statutory damages for developers, lack of Section 230 protection, and calls for a 'Safe Harbor' provision. The discussion highlights concerns about the bill's impact on innovation and the potential for big tech monopolies.

---

## 33. [Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt](https://reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/)

**Author:** u/Prior-Arm-6705 | **Upvotes:** 937 | **Comments:** 147 | **Date:** 2026-01-08

**Summary:** The post describes a project where someone counted and compiled every instance of Jensen Huang saying 'AI' (121 times) during the NVIDIA CES 2025 keynote using open-source tools. The process involved downloading the video, parsing subtitles for timestamps, and editing clips to create a compilation video.

**Key Points:**
- Jensen Huang said 'AI' 121 times during the CES 2025 keynote.
- The author used open-source tools (Dive, yt-dlp-mcp, ffmpeg-mcp-lite) to automate the video compilation.
- The process involved downloading, parsing subtitles, cutting clips, and merging them into a final video.
- The result was described as 'hypnotic' and gained significant attention on Reddit.
- Top comments included reactions to the project's popularity, humor about Jensen's attire, and mentions of tech pricing.

**Discussion Highlights:** The discussion featured a mix of appreciation for the technical achievement, humorous remarks about Jensen Huang's influence and attire, and recognition of the project's popularity within the community.

---

## 34. [16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/)

**Author:** u/ai-infos | **Upvotes:** 457 | **Comments:** 238 | **Date:** 2026-01-07

**Summary:** The post discusses running Deepseek V3.2 AWQ 4-bit on 16x AMD MI50 32GB GPUs, achieving 10 tokens/sec output and 2000 tokens/sec input with a 69000 context length. The setup aims for cost-effective local AGI and highlights power efficiency (550W idle / 2400W peak).

**Key Points:**
- Performance: 10 tok/s output, 2000 tok/s input, 69000 context length
- Power draw: 550W idle / 2400W peak inference
- Goal: Cost-effective local AGI setup using 16x AMD MI50 GPUs
- Future plans: Open-source setup for 32 AMD MI50 GPUs
- Discussion highlights power efficiency and cost justification for professional use

**Discussion Highlights:** The discussion focuses on the practicality of the setup, including power efficiency as a heating alternative, noise levels, feasibility of running 2400W from home, and cost justification for professional developers.

---

## 35. [DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.](https://reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/)

**Author:** u/Nunki08 | **Upvotes:** 665 | **Comments:** 54 | **Date:** 2026-01-07

**Summary:** DeepSeek-R1's paper was updated to expand from 22 pages to 86 pages, adding substantial detail. The update has sparked discussions about potential new architectures and research directions.

**Key Points:**
- DeepSeek-R1's paper expanded from 22 to 86 pages
- Update includes substantial additional detail
- Discussion about potential new architectures (e.g., dsv4 + r2)
- Interest in how architectural improvements work at different sizes
- Focus on linear attention and cache optimization

**Discussion Highlights:** The community is excited about the expanded paper, with discussions focusing on potential new architectures, improvements in model sizes, and advancements in linear attention and cache optimization.

---

## 36. [A 30B Qwen Model Walks Into a Raspberry Pi… and Runs in Real Time](https://reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/)

**Author:** u/ali_byteshape | **Upvotes:** 502 | **Comments:** 79 | **Date:** 2026-01-06

**Summary:** The post discusses the successful optimization of a 30B Qwen model to run efficiently on a Raspberry Pi 5, achieving 8.03 tokens per second while retaining 94.18% of BF16 quality. The optimization focuses on balancing memory usage and performance, particularly on GPUs where kernel choice significantly impacts speed. Key points include the model's performance on a Raspberry Pi 5, the optimization strategy prioritizing memory as a budget, the quirky GPU performance due to kernel choices, the request for community feedback on different setups, and a user's experience with context settings to avoid segfaults. The community showed interest in testing the model on various hardware setups and discussed potential improvements, such as combining the model with an exo-like solution for distributed processing across multiple Raspberry Pis.

---

## 37. [Performance improvements in llama.cpp over time](https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/)

**Author:** u/jacek2023 | **Upvotes:** 677 | **Comments:** 85 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses performance improvements in llama.cpp over time, with a focus on NVIDIA GPU performance gains and comparisons with other implementations.

**Key Points:**
- Performance gains are highlighted for NVIDIA GPUs
- NVIDIA's blog post is referenced for additional context
- Comparisons are made with other implementations like ik_llama.cpp
- Significant progress in token generation speed is noted

**Discussion Highlights:** The discussion highlights significant progress in token generation speed, with comparisons to other implementations and references to NVIDIA's performance improvements.

---

## 38. [For the first time in 5 years, Nvidia will not announce any new GPUs at CES — company quashes RTX 50 Super rumors as AI expected to take center stage](https://reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/)

**Author:** u/FullstackSensei | **Upvotes:** 628 | **Comments:** 196 | **Date:** 2026-01-05

**Summary:** Nvidia will not announce new GPUs at CES, focusing instead on AI. The post discusses limited supply of new GPUs, potential re-release of older models, and rising hardware prices, expressing concerns about future upgrades.

**Key Points:**
- Nvidia quashes RTX 50 Super rumors, no new GPU announcements at CES
- Limited supply of RTX 5070Ti, 5080, and 5090, with potential re-release of RTX 3060
- Rising prices of DDR5 RAM and storage, making upgrades costly
- Community frustration over corporate greed and lack of consumer-focused announcements
- Concerns about the feasibility of future hardware upgrades

**Discussion Highlights:** The discussion highlights frustration over Nvidia's shift away from consumer GPUs, with comments criticizing corporate greed and expressing concerns about the future of local computing. Some users humorously suggest alternative solutions, like China flooding the market with high-capacity GPUs.

---

## 39. [llama.cpp performance breakthrough for multi-GPU setups](https://reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/)

**Author:** u/Holiday-Injury-9397 | **Upvotes:** 569 | **Comments:** 201 | **Date:** 2026-01-05

**Summary:** The ik_llama.cpp project has achieved a significant performance breakthrough for multi-GPU setups, delivering a 3x to 4x speed improvement in local LLM inference. This advancement allows for the utilization of multiple low-cost GPUs instead of expensive high-end enterprise cards. Key points include the introduction of a new execution mode (split mode graph) for multi-GPU configurations, performance improvements on single GPU and CPU-only setups, and the project's potential to democratize access to powerful LLM inference capabilities. The community highlights the significance of these improvements and the importance of the project's open-source nature.

---

## 40. [GLM-Image model from Z.ai is coming](https://reddit.com/r/LocalLLaMA/comments/1q41bw1/glmimage_model_from_zai_is_coming/)

**Author:** u/Ravencloud007 | **Upvotes:** 325 | **Comments:** 59 | **Date:** 2026-01-04

**Summary:** The Reddit post announces the upcoming GLM-Image model from Z.ai, which has generated significant interest in the community. Users are discussing its potential size and capabilities, with some expressing excitement about its parameters and others comparing it to existing models like Z Image.

**Key Points:**
- GLM-Image model from Z.ai is being introduced
- Community interest is high, with 325 upvotes and 59 comments
- Users speculate about the model's size, with one comment mentioning 103b parameters
- Z Image is currently the community favorite, and the new model will need to compete with it
- There is a desire for a model that balances size, ease of fine-tuning, and quality

**Discussion Highlights:** The discussion highlights excitement about the new model's potential, with users speculating about its size and capabilities. There is a consensus that Z Image is currently the favorite, and the new model will need to offer significant improvements to dethrone it. Some users express a desire for a model that is smaller, easier to fine-tune, and maintains high quality.

---

## 41. [Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched](https://reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/)

**Author:** u/ubrtnk | **Upvotes:** 385 | **Comments:** 194 | **Date:** 2026-01-03

**Summary:** The post discusses the challenges faced by local LLMs in processing extreme or unlikely breaking news events, such as the US attacking Venezuela. The author shares their experience with different LLMs, highlighting how some models initially classified the event as a hoax despite credible sources.

**Key Points:**
- Local LLMs struggled to accept extreme breaking news as real, classifying it as a hoax.
- Different LLMs had varying responses, with some requiring multiple credible sources to acknowledge the event.
- The discussion highlights the bias and limitations of LLMs in processing unfamiliar geopolitical events.
- Users shared similar experiences with LLMs dismissing unlikely but real events.
- The consensus suggests that LLMs may have inherent biases that shape their output.

**Discussion Highlights:** The discussion highlights the limitations and biases of LLMs in processing extreme or unfamiliar events. Users shared similar experiences and expressed curiosity about how these biases might shape future AI interactions.

---

## 42. [LeCun Says Llama 4 results "were fudged a little bit"](https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/)

**Author:** u/MrPecunius | **Upvotes:** 368 | **Comments:** 88 | **Date:** 2026-01-02

**Summary:** Yann LeCun confirmed that Llama 4 benchmarks were manipulated, and Meta's AI division faced significant organizational changes, leading to the departure of key personnel and a lack of follow-up on promised models.

**Key Points:**
- LeCun confirmed Llama 4 benchmark manipulation
- Meta's AI division was sidelined, leading to departures
- No follow-up on the promised large Llama 4 model
- Disappointment in Meta's handling of open-source AI
- Speculation on organizational mismanagement at Meta

**Discussion Highlights:** The discussion highlights disappointment in Meta's handling of Llama and speculation on organizational issues, with some users sharing additional resources and expressing concern over the impact on open-source AI development.

---

## 43. [Qwen-Image-2512](https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/)

**Author:** u/Nunki08 | **Upvotes:** 717 | **Comments:** 122 | **Date:** 2025-12-31

**Summary:** The Reddit post announces the release of Qwen-Image-2512, a new image generation model, and provides multiple links to documentation, demos, and downloads. Users in the comments share their experiences and appreciation for the model. Key points include the model's availability on various platforms, successful runs on low-end hardware, community appreciation, support for creative tasks, and significant popularity. Discussion highlights show enthusiasm for the model's capabilities and positive community consensus.

---

## 44. [[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It’s running a raw Llama-7B instance with a 2048 token window.](https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/)

**Author:** u/simar-dmg | **Upvotes:** 745 | **Comments:** 110 | **Date:** 2025-12-30

**Summary:** A Reddit user reverse-engineered a Snapchat sextortion bot and discovered it was running a raw Llama-7B instance with a 2048 token window. The bot was vulnerable to a persona-adoption jailbreak, revealing its configuration and malicious payload.

**Key Points:**
- The bot used a Llama-7B model with a 2048 token context window and high temperature setting.
- A 'Grandma Protocol' jailbreak forced the bot to reveal its environment variables and configuration.
- The bot was likely running on minimal hardware to reduce costs.
- The bot's payload was a malicious link disguised to bypass Snapchat's URL filters.
- Scammers are increasingly using open-source models like Llama-7B to avoid API costs and censorship.

**Discussion Highlights:** The discussion highlighted skepticism about the accuracy of the bot's revealed information, with some users suggesting it could be entirely hallucinated. Others questioned the feasibility of system prompts including environment variables and debated the reliability of the findings.

---

## 45. [Llama-3.3-8B-Instruct](https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/)

**Author:** u/jacek2023 | **Upvotes:** 465 | **Comments:** 77 | **Date:** 2025-12-29

**Summary:** The post discusses the discovery and release of the Llama-3.3-8B-Instruct model, which was previously only available through Meta's API. The author details the process of obtaining the model through finetuning and shares the model on Hugging Face.

**Key Points:**
- The Llama-3.3-8B-Instruct model was previously only accessible via Meta's API.
- The author obtained the model by finetuning it through Meta's API and downloading it in HF format.
- The model is now available on Hugging Face for public use.
- The community is actively evaluating the model to confirm its authenticity and performance.
- The post has gained significant attention, with ongoing discussions and benchmarks.

**Discussion Highlights:** The community is excited about the release and is conducting various evaluations to verify the model's capabilities. Some users are running benchmarks and sanity checks to ensure it is indeed a newer version of Llama 3.3 and not a repackaged older version. The post has been well-received, with many users expressing appreciation for the author's efforts.

---

## 46. [Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.](https://reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 341 | **Comments:** 120 | **Date:** 2025-12-29

**Summary:** Z AI is set to go public with an IPO on January 8, aiming to raise $560 million, marking it as the first AI-native LLM company to list globally. The Reddit post and comments highlight concerns about the future of open-source AI and the inevitability of monetization in the AI industry.

**Key Points:**
- Z AI's IPO is scheduled for January 8 with a target of $560 million.
- Z AI is positioned as the first AI-native LLM company to go public.
- Community concerns about the impact on open-source AI.
- Debate over whether Z AI will continue releasing open weight models.
- General acceptance that companies need to monetize eventually.

**Discussion Highlights:** The discussion reflects a mix of excitement and concern, with a notable consensus that monetization is inevitable. Many users express worries about the future of open-source AI, while others argue that companies need to generate revenue to sustain development.

---

## 47. [Tencent just released WeDLM 8B Instruct on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 419 | **Comments:** 62 | **Date:** 2025-12-29

**Summary:** Tencent released WeDLM 8B Instruct on Hugging Face, a diffusion language model that runs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks. The release has generated significant interest and discussion in the community.

**Key Points:**
- WeDLM 8B Instruct is a diffusion language model released by Tencent on Hugging Face.
- It outperforms vLLM-optimized Qwen3-8B by 3-6× on math reasoning tasks.
- The model is released under the Apache 2.0 license.
- There is also a 7B version available (WeDLM-7B-Instruct).
- The community sees great potential in 7-8B models and is enthusiastic about more models in this space.

**Discussion Highlights:** The community is excited about the performance claims and the Apache 2.0 license. There is consensus on the potential of 7-8B models and a desire for more models in this size range. Some users expressed surprise at the effectiveness of diffusion models for LLMs.

---

## 48. [NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux](https://reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/)

**Author:** u/HumanDrone8721 | **Upvotes:** 446 | **Comments:** 185 | **Date:** 2025-12-27

**Summary:** NVIDIA has dropped Pascal support on Linux, causing disruptions for Arch Linux users. The community is aware of this change, with some expressing concerns about the impact on older hardware.

**Key Points:**
- NVIDIA's driver update (590) drops support for Pascal GPUs on Linux
- Arch Linux users are particularly affected as legacy drivers move to AUR
- Community members express concern but acknowledge the inevitability of the change
- Popular Pascal cards like the P40 are impacted
- The change was announced in Arch Linux news

**Discussion Highlights:** The discussion shows a mix of concern and acceptance. Users acknowledge that Arch Linux has a history of moving legacy drivers to AUR. Some express nostalgia for Pascal cards while others joke about the impact on newer hardware users.

---

## 49. [Best Local LLMs - 2025](https://reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/)

**Author:** u/rm-rf-rm | **Upvotes:** 366 | **Comments:** 198 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses the best local LLMs of 2025, highlighting models like Minimax M2.1 and GLM4.7, and categorizes them by memory footprint. Users share their favorite models and use cases, with a focus on open weights models. Key points include the categorization of models by memory footprint, specific recommendations like Qwen3-4B-instruct and LFM2-8B-A1B for small models, and discussions on creative writing and RAG for technical documentation. The discussion highlights debates on model categorization and practical applications.

---

## 50. [NVIDIA has 72GB VRAM version now](https://reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/)

**Author:** u/decentralize999 | **Upvotes:** 465 | **Comments:** 148 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses NVIDIA's new 72GB VRAM version, questioning if 96GB is too expensive and noting the AI community's lack of interest in 48GB. The discussion highlights varying opinions on the need for larger VRAM capacities and price considerations.

**Key Points:**
- NVIDIA has released a 72GB VRAM version.
- The post questions the cost of 96GB and the AI community's interest in 48GB.
- Top comments suggest a need for even larger VRAM capacities (128GB or more).
- Price comparisons are provided for different VRAM capacities.
- The price per gig is consistent across different capacities.

**Discussion Highlights:** The discussion highlights a consensus on the need for larger VRAM capacities, with some users suggesting 128GB or more. Price comparisons show that the cost per gig remains consistent, making the choice dependent on affordability. Some users express a preference for waiting for future models with higher capacities.

---

