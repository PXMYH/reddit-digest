# r/LocalLLaMA Reading Digest

**Period:** 2026-01-21 to 2026-01-21
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?](https://reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/)

**Author:** u/Adventurous-Gold6413 | **Upvotes:** 489 | **Comments:** 279 | **Date:** 2026-01-20

**Summary:** The post discusses selecting three local models to use with 64GB RAM and 16GB VRAM when internet access is unavailable. Users share their preferred models and experiences. Key points include recommendations for models like Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B, with GPT-OSS-120B being highlighted for its performance and fit within the given hardware specifications. The discussion emphasizes the importance of models that can run efficiently on the provided hardware and shows appreciation for the availability and capabilities of certain models despite their limitations. The discussion highlights a consensus around models like GPT-OSS-120B, which is praised for its performance and compatibility with the given hardware. Other models like Gemma 3 27B and GLM 4.5 Air are also mentioned as strong contenders. The overall tone is appreciative of the advancements in local model capabilities.

---

## 2. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 804 | **Comments:** 245 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, fully enclosed mobile AI system with 10 GPUs, designed for running large MoE models and supporting graphic design tasks. The build balances performance and cost, with a focus on mobility and protection from pets.

**Key Points:**
- The system features a Threadripper Pro 3995WX, 512GB DDR4, and 10 GPUs (8x 3090 + 2x 5090).
- It is designed for large MoE models, video generation, and high-detail image generation.
- The build prioritizes mobility, enclosure, and cost-effectiveness, with a total expense of ~$17k.
- The enclosure was a major challenge, with mining frames ruled out due to aesthetic and structural concerns.
- The post received significant engagement, with comments highlighting its uniqueness and practicality.

**Discussion Highlights:** The discussion highlights the system's impressive capabilities and the challenges of balancing performance, cost, and mobility. Comments also joke about the system's portability and airflow concerns.

---

## 3. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 358 | **Comments:** 60 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the official support for GLM 4.7 Flash in llama.cpp, highlighting its integration and community efforts. The discussion includes clarifications on the term 'official' and shares additional resources and performance insights. Key points include the merging of GLM 4.7 Flash support into llama.cpp, the clarification that 'official' refers to proper functionality with llama.cpp rather than endorsement by Z.ai developers, and the community-driven nature of the integration. Users also share performance insights and additional resources, contributing to a collaborative environment.

---

## 4. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 459 | **Comments:** 156 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights the effectiveness of GLM 4.7 Flash as a reliable local agent for various tasks, with users praising its performance and looking forward to its local availability. The discussion includes comparisons with other models and notes on its performance and output quality.

**Key Points:**
- GLM 4.7 Flash is praised for its reliability and performance in agentic tasks.
- Users are eager for the GGUF version to try it locally.
- Comparisons with other models like Nemotron 30B and Qwen3 are discussed.
- The model is noted for its ability to handle complex tasks like cloning repos and running commands.
- Performance benchmarks suggest it is competitive with other high-performing models.

**Discussion Highlights:** The discussion highlights a positive consensus on GLM 4.7 Flash's capabilities, with users sharing their experiences and comparisons with other models. There is anticipation for local testing and further performance evaluations.

---

## 5. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 728 | **Comments:** 227 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the release of zai-org/GLM-4.7-Flash on Hugging Face, generating significant community interest and discussion about its technical features and capabilities.

**Key Points:**
- The model is a 30B parameter model with a 3B thinking component.
- It uses MLA, which reduces KV cache memory usage, enabling longer context lengths.
- The community expresses excitement and anticipation for the release.
- The model is noted for its potential to run efficiently with a 200k context length.

**Discussion Highlights:** The discussion highlights enthusiasm for the model's technical advancements, particularly its memory efficiency and large context capabilities. Users express nostalgia for larger models and anticipation for the practical benefits of this release.

---

## 6. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 345 | **Comments:** 91 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to maximize VRAM for running large models locally. Benchmark results show impressive performance across various models, with the system costing around 9,800€ (effectively 4,900€ after refund).

**Key Points:**
- System built for running large models (120B+) locally with a focus on data privacy.
- Hardware includes 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU.
- Total cost was ~9,800€, with a 50% subsidy reducing the effective cost to ~4,900€.
- Benchmark results demonstrate strong performance across various models.
- Discussion highlights include admiration for the build and curiosity about the components and their acquisition.

**Discussion Highlights:** The discussion highlights include admiration for the build, with comments like 'HE HAS RAM GET HIM...' and 'G O D D A A A A A Y U U U U M...'. There is also curiosity about the components and their acquisition, with questions about where the cards were obtained and the author's job.

---

## 7. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 451 | **Comments:** 71 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses the potential delay in Qwen 4 development, with the lead developer indicating a focus on quality over speed. The community generally appreciates this approach, though some caution against overinterpreting the announcement.

**Key Points:**
- Qwen 4 development may be slowing down to focus on quality
- Community largely supports the focus on quality over quantity
- Some users caution against jumping to conclusions based on limited information
- The post gained significant traction with 441 upvotes and 71 comments
- Top comments highlight appreciation for quality-focused development

**Discussion Highlights:** The discussion reflects a consensus that prioritizing quality in AI development is beneficial, though there is some debate about the interpretation of the developer's statement. Many users express support for taking the necessary time to improve the Qwen series meaningfully.

---

## 8. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 532 | **Comments:** 116 | **Date:** 2026-01-17

**Summary:** The author transitioned from MI100 GPUs to R9700 GPUs for a new server build, achieving 128GB VRAM and 128GB RAM at a lower cost than an RTX 6000 Blackwell. The build includes detailed specifications and performance benchmarks.

**Key Points:**
- Transition from MI100 to R9700 GPUs due to better performance and cost efficiency
- Detailed hardware specifications including 4 R9700 GPUs, 128GB RAM, and a 1600W PSU
- Performance benchmarks showing high token processing rates
- Community appreciation and humor about financial irresponsibility

**Discussion Highlights:** The community responded positively, with top comments appreciating the build and humorously acknowledging the financial investment required.

---

## 9. [Best "End of world" model that will run on 24gb VRAM](https://reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/)

**Author:** u/gggghhhhiiiijklmnop | **Upvotes:** 338 | **Comments:** 177 | **Date:** 2026-01-17

**Summary:** The post discusses finding the best AI model to download and store for an 'end of world' scenario, with a focus on models that fit within 24GB VRAM. Users suggest various models and emphasize the importance of having a backup of essential data like Wikipedia.

**Key Points:**
- The user is looking for AI models that can run on a PC with 24GB VRAM and 64GB RAM.
- Suggestions include saving the best possible LLM and running it off an SSD if necessary.
- Specific model recommendations include gemma3:27b and Midnight Miku.
- Importance of downloading actual Wikipedia backups is highlighted.
- The post gained significant attention with 341 upvotes and 177 comments.

**Discussion Highlights:** The discussion highlights a consensus on the importance of having robust AI models and data backups for an 'end of world' scenario. Users recommend specific models like gemma3:27b and emphasize practical solutions such as running models off SSDs. There is also a strong recommendation to download comprehensive data backups like Wikipedia.

---

## 10. [GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)](https://reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/)

**Author:** u/CuriousPlatypus1881 | **Upvotes:** 378 | **Comments:** 89 | **Date:** 2026-01-16

**Summary:** The post discusses the updated SWE-bench leaderboard results from December 2025, highlighting the performance of various AI models on GitHub PR tasks. Claude Opus 4.5 leads with a 63.3% resolved rate, followed closely by GPT-5.2 (extra high effort) at 61.5%. The post also notes the strong performance of open-source models like GLM-4.7.

**Key Points:**
- Claude Opus 4.5 leads the leaderboard with a 63.3% resolved rate.
- GPT-5.2 (extra high effort) follows closely at 61.5%.
- Gemini 3 Flash Preview outperforms Gemini 3 Pro Preview despite being smaller and cheaper.
- GLM-4.7 is the strongest open-source model, ranking alongside closed models like GPT-5.1-codex.
- GPT-OSS-120B shows significant performance improvement in high-effort reasoning mode.

**Discussion Highlights:** The discussion highlights excitement around the performance of open-source models like GLM-4.7 and anticipation for future releases like DeepSeek v4. There is also a consensus that this benchmark is more believable compared to others.

---

## 11. [I fucking love this community](https://reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/)

**Author:** u/alhinai_03 | **Upvotes:** 499 | **Comments:** 54 | **Date:** 2026-01-16

**Summary:** The author expresses gratitude towards the open-source community for enabling them to run large language models on older hardware, achieving impressive performance metrics. They highlight the importance of system memory and MoE architecture for optimal performance.

**Key Points:**
- Gratitude towards the open-source community for their contributions
- Achieving 14-13.5 tokens per second on a 10-year-old PC with 4GB VRAM
- Importance of system memory and MoE architecture for running large models
- Community appreciation for optimization efforts
- Discussion on practicality of system RAM and MoE combo

**Discussion Highlights:** The community appreciates the author's achievement and discusses the practicality of using system RAM and MoE architecture for running large models on older hardware. There is a consensus on the effectiveness of these optimizations.

---

## 12. [My story of underestimating /r/LocalLLaMA's thirst for VRAM](https://reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/)

**Author:** u/EmPips | **Upvotes:** 1325 | **Comments:** 90 | **Date:** 2026-01-15

**Summary:** The post highlights the author's underestimation of the r/LocalLLaMA community's demand for VRAM, as indicated by the title and the engagement in the comments.

**Key Points:**
- The post gained significant traction with 1327 upvotes and 90 comments.
- The community appreciated the contribution, offering a special flair and featuring it on Discord.
- Discussions included hardware recommendations, such as the R9700 or 3090s, and comparisons to historical events like the gold rush.
- The post sparked conversations about the value of specific GPUs and their suitability for different tasks.

**Discussion Highlights:** The community engaged in discussions about hardware choices, with some users recommending specific GPUs like the R9700 or 3090s. There was also a humorous comparison to the gold rush, emphasizing the demand for resources.

---

## 13. [Latest upgrade…A100 40 GB](https://reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/)

**Author:** u/inserterikhere | **Upvotes:** 403 | **Comments:** 54 | **Date:** 2026-01-15

**Summary:** The user upgraded their gaming rig to an AI rig by repurposing existing parts and purchasing a faulty A100 GPU, which worked perfectly upon installation. The community reacted positively, with some expressing concerns about cooling the A100.

**Key Points:**
- User transitioned from a gaming rig to an AI rig using repurposed parts.
- Purchased a faulty A100 GPU for $1000, which worked flawlessly upon installation.
- Community expressed concerns about cooling the A100 GPU.
- Post received positive reception with 404 upvotes and 54 comments.

**Discussion Highlights:** The community praised the user's upgrade and offered technical advice, particularly regarding cooling solutions for the A100 GPU. Some users also shared humorous memes and reactions.

---

## 14. [Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M](https://reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/)

**Author:** u/eugenekwek | **Upvotes:** 327 | **Comments:** 54 | **Date:** 2026-01-14

**Summary:** The post announces Soprano 1.1, an improved version of the Soprano TTS model with significant reductions in hallucinations and audio artifacts, along with better stability and support for longer sentences. The community response is overwhelmingly positive, praising the model's performance for its size.

**Key Points:**
- Soprano 1.1 reduces hallucinations by 95% and lowers WER by 50% compared to the original model.
- The model now supports sentences up to 30 seconds long, doubling the previous limit.
- A blind study showed a 63% preference rate for Soprano 1.1 over the original model.
- The community highlights the model's impressive performance for an 80M parameter model.
- Future support for ONNX and handling of em-dashes are discussed as potential improvements.

**Discussion Highlights:** The community is highly impressed with Soprano 1.1's performance, especially given its small size. Discussions include inquiries about future features like ONNX support and minor improvements in text handling. Overall, the consensus is that the model is a significant step forward in small-scale TTS technology.

---

## 15. [NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency](https://reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/)

**Author:** u/Fear_ltself | **Upvotes:** 716 | **Comments:** 129 | **Date:** 2026-01-14

**Summary:** NVIDIA's Orchestrator-8B is an 8-billion-parameter AI designed to manage and route complex tasks to various tools, sparking discussions on its role in achieving AGI and its comparison to middle managers.

**Key Points:**
- Orchestrator-8B is a specialized AI for task management and routing.
- It is seen as a step towards AGI by integrating separate components.
- Comparisons to middle managers and existing frameworks like Claude code style agentic frameworks.
- The model's efficiency in managing tasks is highlighted.

**Discussion Highlights:** The discussion includes humorous comparisons to middle managers and mentions of advanced frameworks like Claude code style agentic frameworks, indicating a consensus on the potential of such task-managing models.

---

## 16. [GLM-Image is released!](https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/)

**Author:** u/foldl-li | **Upvotes:** 601 | **Comments:** 83 | **Date:** 2026-01-13

**Summary:** GLM-Image is a new image generation model with a hybrid autoregressive + diffusion decoder architecture. It excels in text-rendering and knowledge-intensive tasks while supporting various image-to-image tasks.

**Key Points:**
- Hybrid autoregressive + diffusion decoder architecture
- Excels in text-rendering and knowledge-intensive generation
- Supports image editing, style transfer, and multi-subject consistency
- MIT license with no restrictions
- Comparable performance to other models like nano banana 2

**Discussion Highlights:** The community appreciates the MIT license and the model's capabilities. There is interest in quantizing the model for easier use and discussions about its performance and potential applications.

---

## 17. [My wishes for 2026](https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/)

**Author:** u/jacek2023 | **Upvotes:** 647 | **Comments:** 179 | **Date:** 2026-01-13

**Summary:** The Reddit post discusses wishes and predictions for 2026, focusing on technological advancements, particularly the affordability of GPUs with more than 32GB of memory. The community engages in a mix of hopeful and skeptical comments about these predictions.

**Key Points:**
- The post asks which technological advancements will happen first in 2026.
- A significant focus is on the affordability of GPUs with more than 32GB of memory.
- The community responds with a mix of humor, skepticism, and hope.
- Specific models like Qwen 4 and Mistral are mentioned as potential advancements.
- The discussion highlights the challenges and aspirations within the tech community.

**Discussion Highlights:** The discussion is marked by a blend of humor and realism, with many users expressing skepticism about the feasibility of affordable high-memory GPUs in 2026. There is a consensus that while some advancements like Qwen 4 and Mistral are plausible, others remain aspirational.

---

## 18. [kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop—no GPU required](https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/)

**Author:** u/Nunki08 | **Upvotes:** 399 | **Comments:** 92 | **Date:** 2026-01-13

**Summary:** Kyutai introduced Pocket TTS, a 100M-parameter text-to-speech model with high-quality voice cloning that runs on a laptop without requiring a GPU. The model is available on GitHub and Hugging Face, with a blog post and arXiv paper providing more details.

**Key Points:**
- Pocket TTS is a 100M-parameter model for high-quality voice cloning
- Runs on a laptop without needing a GPU
- Available on GitHub, Hugging Face, and detailed in a blog post and arXiv paper
- Memory usage can balloon during generation, reaching up to 32 GB
- Discussion includes inquiries about language support and comparisons with other small models

**Discussion Highlights:** The discussion highlights a warning about memory usage during generation, inquiries about language support, and comparisons with other small models. Some users suggest that models below a certain size may not be worth the trouble.

---

## 19. [GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/)

**Author:** u/TKGaming_11 | **Upvotes:** 365 | **Comments:** 92 | **Date:** 2026-01-12

**Summary:** The Reddit post highlights a GitHub project by DeepSeek-AI called 'Engram,' which introduces a novel approach to memory in large language models using scalable lookup and n-gram embeddings. The discussion praises the innovation and technical depth of the project.

**Key Points:**
- DeepSeek-AI's 'Engram' project introduces a new axis of sparsity for large language models.
- The project uses n-gram embeddings and scalable lookup for memory management.
- The approach is seen as innovative and technically sound by the community.
- Comparisons are drawn to biological memory systems, suggesting natural inspiration.
- The project is noted for its originality and potential impact.

**Discussion Highlights:** The community consensus is highly positive, with users praising the originality and technical depth of the 'Engram' project. Key points of discussion include the innovative use of n-gram embeddings and the potential for this approach to complement existing methods like Mixture of Experts (MoE). Some users also draw parallels to biological memory systems, highlighting the natural inspiration behind the project.

---

## 20. [LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)](https://reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 1048 | **Comments:** 114 | **Date:** 2026-01-11

**Summary:** The post introduces TimeCapsuleLLM, a 1.2B parameter language model trained exclusively on 1800-1875 London texts to minimize modern bias. The model demonstrates period-specific knowledge and behaviors, such as unfamiliarity with post-1875 concepts like telephones. The project is open-source and aims to create synthetic Q&A pairs next.

**Key Points:**
- TimeCapsuleLLM is trained on 90GB of 1800-1875 London texts with no modern data or fine-tuning.
- The model exhibits period-specific behaviors, like arguing against the Roman Catholic Church and treating 'telephone' as an unknown term.
- The project is open-source, with code and model available on GitHub and Hugging Face.
- Future steps include creating synthetic Q&A pairs from the dataset.
- The community response is overwhelmingly positive, with users praising the project's uniqueness and potential.

**Discussion Highlights:** The community shows strong support for the project, with users expressing interest in similar historical models and sharing their own related efforts. The top comments highlight enthusiasm for the project's direction and potential applications.

---

## 21. [I bought a €9k GH200 “desktop” to save $1.27 on Claude Code (vLLM tuning notes)](https://reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/)

**Author:** u/Reddactor | **Upvotes:** 691 | **Comments:** 177 | **Date:** 2026-01-11

**Summary:** The author built a €9k GH200 'desktop' to run Claude Code locally, achieving better speeds and results than the cloud version. They shared optimized vLLM settings for dual 96GB systems and highlighted the cost savings and performance benefits of local execution.

**Key Points:**
- Author spent €9k on a GH200 setup to run Claude Code locally, achieving better performance than cloud-based Sonnet.
- Optimized vLLM settings for dual 96GB systems were shared, including tensor parallel size, context length, and sequence settings.
- The setup uses MiniMax M2.1 FP8+INT4 AWQ for full offline coding, blocking telemetry and unnecessary traffic.
- Community reactions included humor about cost savings and appreciation for the technical achievement.
- The post gained significant traction with 691 upvotes and 177 comments.

**Discussion Highlights:** The community responded with humor and appreciation, highlighting the cost savings and technical achievement. Some users expressed envy over missing out on similar deals, while others joked about the financial implications of the setup.

---

## 22. [It works! Abliteration can reduce slop without training](https://reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/)

**Author:** u/-p-e-w- | **Upvotes:** 398 | **Comments:** 125 | **Date:** 2026-01-11

**Summary:** The post discusses using abliteration to reduce 'slop' (flowery, cliched language) in LLM outputs without training. The author applied this technique to Mistral Nemo, creating a slop-reduced model using Heretic, and shared results and community feedback.

**Key Points:**
- Abliteration can reduce slop in LLM outputs without finetuning.
- Heretic was used to apply abliteration to Mistral Nemo, creating a slop-reduced model.
- The process took 2.5 hours on an A6000 but can be faster with quantization.
- Community feedback is mixed, with some preferring the reduced slop but noting potential dryness.
- GGUF versions of the model were created by another user.

**Discussion Highlights:** The community is divided on the effectiveness of slop reduction, with some appreciating the cleaner output and others finding it too dry. There is curiosity about whether the technique removes semantic meaning or just bans certain phrases.

---

## 23. [I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work](https://reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/)

**Author:** u/Ok-Pomegranate1314 | **Upvotes:** 887 | **Comments:** 146 | **Date:** 2026-01-09

**Summary:** The author successfully clustered three NVIDIA DGX Sparks, overcoming networking limitations by writing a custom NCCL plugin. This achievement allows distributed inference across all three nodes at high speeds, pushing beyond NVIDIA's officially supported configurations.

**Key Points:**
- Author clustered three DGX Sparks despite NVIDIA's official support for only two.
- Developed a custom NCCL network plugin (~1500 lines of C) to handle subnet-aware NIC selection and RDMA implementation.
- Achieved distributed inference at 8+ GB/s over RDMA, demonstrating significant performance.
- The solution is seen as a notable technical feat, with potential broader implications for clustering standalone workstations.
- The GitHub repository for the plugin is available for further exploration.

**Discussion Highlights:** The community praised the technical achievement, noting the difficulty of working with NCCL and the potential impact of the solution. Questions were raised about scalability and performance gains, indicating strong interest in the implementation details.

---

## 24. [The reason why RAM has become so expensive](https://reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/)

**Author:** u/InvadersMustLive | **Upvotes:** 4511 | **Comments:** 378 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses the significant increase in RAM prices, with comments highlighting concerns about monopolization of RAM resources by certain entities, making AI data centers economically unviable. Key points include a dramatic rise in RAM prices, concerns about monopolization by entities like OpenAI, and the economic impact on AI data centers. The discussion highlights a consensus on the monopolization of RAM resources and its impact on the economic viability of AI data centers.

---

## 25. [DeepSeek V4 Coming](https://reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/)

**Author:** u/External_Mood4719 | **Upvotes:** 495 | **Comments:** 108 | **Date:** 2026-01-09

**Summary:** DeepSeek is expected to release V4, a next-generation AI model with strong code-generation capabilities, outperforming mainstream models like Claude and GPT. The model shows improvements in handling long code prompts and overall reasoning ability.

**Key Points:**
- DeepSeek V4 focuses on strong code-generation capabilities
- Outperforms existing mainstream models in code generation
- Improved handling of very long code prompts
- Enhanced reasoning ability and reliability for complex tasks
- Positive user feedback on DeepSeek V3.2 reasoning and affordability

**Discussion Highlights:** Users express excitement and positive impressions of DeepSeek's previous models, with expectations for significant improvements in V4. Some discuss potential integrations and the model's affordability.

---

## 26. [(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability](https://reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/)

**Author:** u/Nunki08 | **Upvotes:** 490 | **Comments:** 102 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release a new flagship AI model with strong coding capabilities, generating significant interest and discussion in the community.

**Key Points:**
- DeepSeek's upcoming model focuses on strong coding ability
- The announcement has sparked excitement and anticipation
- Community members express enthusiasm for more AI models and competition
- Some comments reflect skepticism about marketing claims
- Discussion includes hopes for maintaining role-playing capabilities

**Discussion Highlights:** The community shows strong interest and excitement about DeepSeek's new model, with discussions ranging from competitive implications to specific feature requests. There's a general consensus that more AI models benefit the ecosystem, though some express typical skepticism about performance claims.

---

## 27. [The NO FAKES Act has a "Fingerprinting" Trap that kills Open Source. We need to lobby for a Safe Harbor.](https://reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/)

**Author:** u/PostEasy7183 | **Upvotes:** 619 | **Comments:** 88 | **Date:** 2026-01-08

**Summary:** The NO FAKES Act proposes a 'digital replica right' that could hold developers liable for hosting open-source AI models used to create deepfakes, potentially stifling innovation. The post urges lobbying for a 'Safe Harbor' provision to protect open-source developers.

**Key Points:**
- The NO FAKES Act targets tools used for creating digital replicas, imposing liability on developers.
- Open-source AI model hosting could become legally risky without a Safe Harbor provision.
- The post calls for action, including contacting representatives to advocate for amendments.
- Comments highlight concerns about stifling innovation and the influence of big tech corporations.
- There is skepticism about politicians' understanding of technology.

**Discussion Highlights:** The discussion emphasizes the potential negative impact on open-source development and innovation. Many commenters express concern about the influence of big tech corporations and the lack of technological literacy among politicians.

---

## 28. [Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt](https://reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/)

**Author:** u/Prior-Arm-6705 | **Upvotes:** 937 | **Comments:** 147 | **Date:** 2026-01-08

**Summary:** A Reddit user created a compilation video of every instance Jensen Huang said 'AI' during NVIDIA's CES 2025 keynote, totaling 121 times. The process involved using open-source tools to download, parse, and edit the video locally.

**Key Points:**
- Jensen Huang said 'AI' 121 times during the CES 2025 keynote.
- The user employed open-source tools like yt-dlp-mcp and ffmpeg-mcp-lite for video processing.
- The compilation video was created entirely locally without cloud services.
- The post gained significant attention with 937 upvotes and 147 comments.
- Top comments included humor and references to tech culture.

**Discussion Highlights:** The discussion highlighted the technical achievement of automating the video compilation process and included humorous remarks about Jensen Huang's frequent use of the term 'AI' and his distinctive attire.

---

## 29. [16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/)

**Author:** u/ai-infos | **Upvotes:** 459 | **Comments:** 238 | **Date:** 2026-01-07

**Summary:** The post discusses running Deepseek V3.2 AWQ 4-bit on 16x AMD MI50 32GB GPUs, achieving 10 tokens/sec output and 2000 tokens/sec input with a 69000 context length. The setup is cost-effective and aims to provide a local AGI solution without excessive spending.

**Key Points:**
- Performance: 10 tok/s output, 2000 tok/s input, 69000 context length
- Power draw: 550W idle, 2400W peak inference
- Goal: Cost-effective local AGI setup using 16x AMD MI50 32GB
- Future plans: Open-source test setup for 32 AMD MI50 32GB
- Community appreciation and cost-effectiveness highlighted in comments

**Discussion Highlights:** The discussion highlights the setup's popularity, its potential as a cost-effective alternative to CPU hardware, and practical considerations like power usage and noise levels. Users appreciate the cost-effectiveness and potential for local AGI development.

---

## 30. [DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.](https://reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/)

**Author:** u/Nunki08 | **Upvotes:** 670 | **Comments:** 54 | **Date:** 2026-01-07

**Summary:** The Reddit post discusses the recent update to DeepSeek-R1's paper, which expanded from 22 pages to 86 pages, adding significant detail. The discussion includes comments about potential new architectures, linear attention research, and the value of added implementation specifics. Key points include the paper's expansion, potential new architectures (e.g., dsv4 + r2), ongoing research in linear attention, and appreciation for added implementation details. The post received significant engagement with 670 upvotes and 54 comments.

---

## 31. [A 30B Qwen Model Walks Into a Raspberry Pi… and Runs in Real Time](https://reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/)

**Author:** u/ali_byteshape | **Upvotes:** 493 | **Comments:** 78 | **Date:** 2026-01-06

**Summary:** The post discusses the successful optimization of a 30B Qwen model to run efficiently on a Raspberry Pi 5, achieving 8.03 tokens per second while retaining 94.18% of BF16 quality. The optimization focuses on balancing memory usage and performance, highlighting differences in CPU and GPU behavior. Key points include the model's performance on a Raspberry Pi 5, the prioritization of memory budget and performance trade-offs, the differences in CPU and GPU behavior, the request for community feedback, and the discussion on practical testing results and potential for clustering Raspberry Pis. The community provided practical feedback, including successful testing on a Raspberry Pi 5 with specific settings and suggestions for further experimentation, such as clustering multiple Pis. There was also a notable comment about potential performance improvements with hybrid transformer models.

---

## 32. [Performance improvements in llama.cpp over time](https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/)

**Author:** u/jacek2023 | **Upvotes:** 682 | **Comments:** 85 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses performance improvements in llama.cpp over time, with a focus on NVIDIA GPUs and comparisons with other implementations like ik_llama.cpp. The community highlights significant progress in token generation speed.

**Key Points:**
- Performance gains are noted, particularly for NVIDIA GPUs.
- Comparisons are made with other implementations like ik_llama.cpp.
- Token generation speed has seen significant improvements.
- Prompt processing is noted to be slower compared to token generation.

**Discussion Highlights:** The discussion highlights the progress in llama.cpp performance, with a consensus on the significant improvements in token generation speed. There is also a focus on the performance gains being particularly notable for NVIDIA GPUs, as referenced in the NVIDIA developer blog.

---

## 33. [For the first time in 5 years, Nvidia will not announce any new GPUs at CES — company quashes RTX 50 Super rumors as AI expected to take center stage](https://reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/)

**Author:** u/FullstackSensei | **Upvotes:** 629 | **Comments:** 196 | **Date:** 2026-01-05

**Summary:** Nvidia will not announce new GPUs at CES, focusing instead on AI. The company faces supply issues with high-end GPUs and may reintroduce older models like the RTX 3060. Rising DDR5 and storage prices add to concerns about future hardware upgrades.

**Key Points:**
- No new GPU announcements at CES
- Limited supply of RTX 5070Ti, 5080, and 5090
- Potential reintroduction of RTX 3060
- Rising DDR5 and storage prices
- Concerns about corporate greed and future hardware upgrades

**Discussion Highlights:** The discussion highlights frustration with corporate greed and the potential impact on local computing. Users express concerns about the lack of new hardware options and rising prices, with some suggesting alternative solutions like Chinese manufacturers flooding the market with high-capacity cards.

---

## 34. [llama.cpp performance breakthrough for multi-GPU setups](https://reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/)

**Author:** u/Holiday-Injury-9397 | **Upvotes:** 567 | **Comments:** 201 | **Date:** 2026-01-05

**Summary:** The ik_llama.cpp project has achieved a significant performance breakthrough for multi-GPU setups, delivering a 3x to 4x speed improvement in local LLM inference. This advancement allows for the utilization of multiple low-cost GPUs instead of expensive high-end cards, making it a game-changer for homelabs, server rooms, or cloud setups.

**Key Points:**
- ik_llama.cpp introduces a new execution mode (split mode graph) for multi-GPU configurations.
- The breakthrough delivers a 3x to 4x speed improvement in local LLM inference.
- This advancement enables the use of multiple low-cost GPUs instead of expensive high-end cards.
- Even on a single GPU or CPU-only, ik_llama.cpp shows consistent 2x prompt processing speeds compared to llama.cpp.
- The project is seen as a significant step forward, potentially rivaling other optimized solutions like vllm.

**Discussion Highlights:** The community is highly positive about the breakthrough, with many users confirming performance improvements and sharing their experiences. There is a consensus that this development is a game-changer, especially given the high cost of GPUs and memory. Some users also noted performance gains even on single GPU or CPU-only setups.

---

## 35. [Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched](https://reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/)

**Author:** u/ubrtnk | **Upvotes:** 378 | **Comments:** 195 | **Date:** 2026-01-03

**Summary:** The post discusses challenges faced by local LLMs in processing extreme breaking news events, such as the US attacking Venezuela, where models initially classified the event as a hoax despite credible sources.

**Key Points:**
- Local LLMs struggled to accept extreme breaking news as real, classifying it as misinformation.
- Different models (Qwen Research, Spark 4.0, GPT-OSS:120B) exhibited varying degrees of skepticism.
- Providing credible sources (BBC, Reuters, NYT) helped some models acknowledge the event's reality.
- The post highlights the bias and limitations of LLMs in processing unfamiliar geopolitical events.
- Discussion consensus suggests LLMs may be inherently biased toward stability and skepticism.

**Discussion Highlights:** The discussion highlights a consensus that LLMs often exhibit bias toward skepticism and stability, making it difficult for them to process extreme or unfamiliar geopolitical events. Users shared similar experiences and noted the models' tendency to classify such events as misinformation.

---

## 36. [LeCun Says Llama 4 results "were fudged a little bit"](https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/)

**Author:** u/MrPecunius | **Upvotes:** 363 | **Comments:** 88 | **Date:** 2026-01-02

**Summary:** Yann LeCun, departing Meta AI Chief, confirmed that Llama 4 benchmark results were manipulated. This follows organizational changes at Meta, including the sidelining of the GenAI team, leading to departures and lack of progress on promised models.

**Key Points:**
- LeCun confirms Llama 4 benchmark manipulation
- Zuckerberg sidelined Meta's GenAI organization, leading to departures
- No follow-up on promised large Llama 4 model
- Community expresses disappointment in Meta's handling of open-source AI
- Shared resources include a PDF of the full article

**Discussion Highlights:** The community expresses disappointment in Meta's handling of Llama and open-source AI, with some noting the shift of AI leadership to Chinese models. A shared PDF of the full article and discussions on organizational mismanagement are highlights.

---

## 37. [Qwen-Image-2512](https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/)

**Author:** u/Nunki08 | **Upvotes:** 722 | **Comments:** 122 | **Date:** 2025-12-31

**Summary:** The Reddit post announces the release of Qwen-Image-2512, a new model available on multiple platforms with guides and GGUF files. The community has shown positive reception and practical testing of the model.

**Key Points:**
- Qwen-Image-2512 is a new model with guides and GGUF files available
- The model can be accessed on various platforms like Hugging Face, ModelScope, and GitHub
- Community members have tested the model on low-end hardware with success
- Positive feedback and appreciation from the community
- Creative use cases and image generation examples shared

**Discussion Highlights:** The community has shown enthusiasm for the new model, with users testing it on low-end hardware and sharing creative use cases. There is a general consensus of appreciation for the model's capabilities and accessibility.

---

## 38. [[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It’s running a raw Llama-7B instance with a 2048 token window.](https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/)

**Author:** u/simar-dmg | **Upvotes:** 743 | **Comments:** 109 | **Date:** 2025-12-30

**Summary:** A user reverse-engineered a Snapchat sextortion bot and discovered it was running a raw Llama-7B instance with a 2048 token window. The bot was vulnerable to a persona-adoption jailbreak, revealing its configuration and malicious payload.

**Key Points:**
- The bot used a Llama-7B model with a 2048 token context window and high temperature setting.
- A 'Grandma Protocol' jailbreak forced the bot to reveal its environment variables and configuration.
- The bot's erratic behavior was due to running on minimal hardware to cut costs.
- The bot eventually revealed a malicious link it was programmed to hide.
- Scammers are shifting to open-source models like Llama-7B to avoid API costs and censorship.

**Discussion Highlights:** The discussion included skepticism about the accuracy of the bot's revealed information, with some users suggesting it could be entirely hallucinated. Others questioned the commonality of system prompts including environment variables.

---

## 39. [Llama-3.3-8B-Instruct](https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/)

**Author:** u/jacek2023 | **Upvotes:** 470 | **Comments:** 77 | **Date:** 2025-12-29

**Summary:** The post announces the discovery and release of Llama-3.3-8B-Instruct, a previously API-exclusive model from Meta. The author successfully downloaded and shared the model, sparking community interest and verification efforts.

**Key Points:**
- Llama-3.3-8B-Instruct was previously only available via Meta's API.
- The author found a way to download the model through Meta's finetuning API.
- The model appears to be a legitimate version of Llama 3.3 8B.
- Community members are verifying the model's authenticity and performance.
- Technical details like position embeddings are being discussed.

**Discussion Highlights:** The community is actively verifying the model's authenticity and performance. Key discussions include technical details like position embeddings and ongoing evaluations to compare it with other Llama models.

---

## 40. [Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.](https://reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 341 | **Comments:** 120 | **Date:** 2025-12-29

**Summary:** Z AI is set to go public with an IPO on January 8, aiming to raise $560 million, marking it as the first AI-native LLM company to list globally. The Reddit post and comments highlight concerns about the future of open-source AI models and the company's potential shift away from open-source practices.

**Key Points:**
- Z AI's IPO is scheduled for January 8, aiming to raise $560 million.
- Concerns about the future of open-source AI models post-IPO.
- Community reactions include skepticism and calls to maintain open-source practices.
- Discussion on the cost-effectiveness of open weight models as a form of advertising.
- Mixed feelings about the company's potential shift in business model.

**Discussion Highlights:** The discussion highlights a significant concern among the community about Z AI potentially moving away from open-source practices post-IPO. While some users express skepticism and disappointment, others argue that companies need to monetize eventually. There is a consensus that open weight models are a cost-effective way to advertise AI capabilities, but the overall sentiment leans towards caution and a desire to maintain open-source principles.

---

## 41. [Tencent just released WeDLM 8B Instruct on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 426 | **Comments:** 62 | **Date:** 2025-12-29

**Summary:** Tencent released WeDLM 8B Instruct, a diffusion language model that runs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks. The model is available under the Apache 2.0 license and has generated significant interest in the community.

**Key Points:**
- WeDLM 8B Instruct is a diffusion language model
- Runs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks
- Available under Apache 2.0 license
- Community shows strong interest in diffusion models for LLMs
- 7-8B models are seen as promising

**Discussion Highlights:** The community is excited about the potential of diffusion models for LLMs, with many noting the impressive performance and open-source license. There is also interest in the 7B version of the model.

---

## 42. [NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux](https://reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/)

**Author:** u/HumanDrone8721 | **Upvotes:** 442 | **Comments:** 185 | **Date:** 2025-12-27

**Summary:** NVIDIA has dropped Pascal support on Linux, causing issues for Arch Linux users. The post highlights concerns and discussions around this change, with users expressing worry and sharing experiences.

**Key Points:**
- NVIDIA's decision to drop Pascal support affects Linux users, particularly those on Arch Linux.
- The 24GB P40, a Pascal card, is mentioned as a favored model before it became expensive.
- Users express concern and anticipation of future impacts on their systems.
- Arch Linux's practice of moving legacy drivers to AUR is noted as a long-standing policy.
- The change is documented in Arch Linux news, indicating it was expected.

**Discussion Highlights:** The discussion reflects a mix of concern and acceptance, with users acknowledging the long-standing practice of Arch Linux moving legacy drivers to AUR. Some users express nostalgia for older hardware like the P40, while others anticipate future challenges.

---

## 43. [Best Local LLMs - 2025](https://reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/)

**Author:** u/rm-rf-rm | **Upvotes:** 361 | **Comments:** 197 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses the best local LLMs of 2025, highlighting models like Minimax M2.1 and GLM4.7, and categorizes them by application and memory footprint. Users share detailed experiences and recommendations. Key points include the performance of Minimax M2.1 and GLM4.7, categorization by applications such as General, Agentic, Creative Writing, and Speciality, memory footprint classifications, and specific model recommendations like Qwen3-4B-instruct and LFM2-8B-A1B. The discussion includes debates on categorization, specific model recommendations for small memory footprints, and inquiries about RAG for technical documentation.

---

## 44. [NVIDIA has 72GB VRAM version now](https://reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/)

**Author:** u/decentralize999 | **Upvotes:** 465 | **Comments:** 148 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses NVIDIA's new 72GB VRAM version, questioning the cost of 96GB and the AI community's interest in 48GB. The discussion highlights varying opinions on the need for larger VRAM capacities and price considerations.

**Key Points:**
- NVIDIA has released a 72GB VRAM version
- Community questions the cost of 96GB and interest in 48GB
- Price per gig remains consistent across different VRAM sizes
- Some users advocate for even larger VRAM capacities (e.g., 128GB)
- Price comparisons show incremental increases with higher VRAM sizes

**Discussion Highlights:** The discussion reveals a consensus that larger VRAM capacities are desirable, with some users advocating for 128GB or more. Price per gig remains consistent, making the choice dependent on budget and need. The community shows interest in higher VRAM sizes but also considers cost-effectiveness.

---

## 45. [Hard lesson learned after a year of running large models locally](https://reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/)

**Author:** u/inboundmage | **Upvotes:** 350 | **Comments:** 146 | **Date:** 2025-12-26

**Summary:** The author shares their experience running large language models locally, highlighting challenges with VRAM limitations, model scaling, and performance trade-offs. They conclude that local inference is viable for smaller models but requires significant hardware investment for larger ones.

**Key Points:**
- Running large models locally is feasible but has hard limits with consumer-grade hardware.
- VRAM fragmentation and memory management are significant challenges.
- Quantization helps but introduces quality trade-offs and new issues.
- Cloud-based solutions offer better performance for fast iteration.
- Community suggestions include using llama.cpp for CPU offloading and adding more VRAM.

**Discussion Highlights:** The discussion highlights practical solutions like using llama.cpp for models that exceed VRAM and the general consensus that more VRAM or multiple GPUs are necessary for larger models. Some users express hope for future hardware improvements.

---

## 46. [I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA](https://reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/)

**Author:** u/CeFurkan | **Upvotes:** 1038 | **Comments:** 182 | **Date:** 2025-12-25

**Summary:** The post discusses the potential of GPU VRAM upgrade modifications to challenge NVIDIA's monopoly, highlighting their availability and popularity in China. Users share experiences with modded GPUs and discuss pricing and performance.

**Key Points:**
- GPU VRAM upgrade modifications are seen as a way to counter NVIDIA's monopoly.
- These modifications are already mainstream in China, with various models available at different price points.
- Users report successful experiences with modded GPUs, such as the 4090 with 48GB of memory.
- Pricing and availability of these modded GPUs are discussed, with some users expressing interest in purchasing.

**Discussion Highlights:** The discussion highlights the growing popularity and success of GPU VRAM upgrade modifications, particularly in China. Users share positive experiences and express interest in the technology, suggesting a potential shift in the GPU market.

---

## 47. [Why I quit using Ollama](https://reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/)

**Author:** u/SoLoFaRaDi | **Upvotes:** 487 | **Comments:** 202 | **Date:** 2025-12-25

**Summary:** The user expresses dissatisfaction with Ollama due to recent updates that introduced cloud-based models, straying from its original purpose of providing a secure platform for local AI models. The discussion highlights a shift in user preference towards alternatives like llama.cpp and LM Studio.

**Key Points:**
- User's dissatisfaction with Ollama's recent updates and shift towards cloud-based models
- Concerns about privacy implications and bloatware in Ollama
- User preference for alternatives like llama.cpp and LM Studio
- Discussion consensus favoring llama.cpp for its recent improvements
- Mention of LM Studio as a viable alternative to Ollama

**Discussion Highlights:** The discussion highlights a consensus favoring llama.cpp for its recent improvements and LM Studio as a viable alternative. Users appreciate the transparency and focus on local AI models provided by these alternatives.

---

## 48. [Exclusive: Nvidia buying AI chip startup Groq's assets for about $20 billion in largest deal on record](https://reddit.com/r/LocalLLaMA/comments/1puyq9r/exclusive_nvidia_buying_ai_chip_startup_groqs/)

**Author:** u/fallingdowndizzyvr | **Upvotes:** 672 | **Comments:** 157 | **Date:** 2025-12-24

**Summary:** Nvidia is acquiring AI chip startup Groq's assets for approximately $20 billion, marking the largest deal on record. The acquisition has sparked discussions about market competition and consolidation in the AI industry.

**Key Points:**
- Nvidia is buying Groq's assets for about $20 billion
- The deal is the largest on record
- Discussions highlight concerns about market consolidation
- Some commenters question Groq's valuation at $20 billion
- The acquisition is seen as a strategic move by Nvidia

**Discussion Highlights:** The discussion highlights a mix of optimism about market competition and concerns about industry consolidation. Some users question the valuation of Groq, while others see the acquisition as a strategic move by Nvidia to strengthen its position in the AI chip market.

---

## 49. [We asked OSS-120B and GLM 4.6 to play 1,408 Civilization V games from the Stone Age into the future. Here's what we found.](https://reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/)

**Author:** u/vox-deorum | **Upvotes:** 659 | **Comments:** 174 | **Date:** 2025-12-24

**Summary:** Researchers used open-source LLMs (GPT-OSS-120B and GLM-4.6) to play 1,408 full games of Civilization V, finding that LLMs can survive full games with a hybrid approach and develop distinct playstyles. The models showed slight improvements in best scores but minor decreases in win rates compared to baseline AI. Key points include: LLMs can survive full Civilization V games with a hybrid approach, achieving ~97.5% survival rate; OSS-120B favored a warmonger playstyle with more Domination victories, while GLM-4.6 played more balanced; Both models preferred the Order ideology (~24% more likely) over Freedom; Cost per game was ~$0.86 for OSS-120B, with input tokens scaling linearly as the game progresses; The study suggests that even smaller models (e.g., OSS-20B) can perform adequately in this setup. The community expressed excitement about the potential for LLMs to enhance gameplay, with interest in integrating them into multiplayer games. Some users speculated about future applications beyond gaming, while others inquired about the performance of smaller models.

---

## 50. [AMA With Z.AI, The Lab Behind GLM-4.7](https://reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/)

**Author:** u/zixuanlimit | **Upvotes:** 599 | **Comments:** 417 | **Date:** 2025-12-23

**Summary:** The post announces an AMA session with Z.AI, the research lab behind GLM-4.7, featuring several team members. The session is scheduled for 8 AM – 11 AM PST, with follow-ups over the next 48 hours.

**Key Points:**
- AMA session with Z.AI team members
- Scheduled for 8 AM – 11 AM PST with 48-hour follow-up
- Community questions about future releases, censorship, training challenges, and creative writing applications
- High engagement with 599 upvotes and 417 comments

**Discussion Highlights:** The community shows strong interest in future developments, ethical concerns, technical challenges, and potential applications of the GLM-4.7 model.

---

