# r/LocalLLaMA Reading Digest

**Period:** 2026-01-25 to 2026-01-25
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [Your post is getting popular and we just featured it on our Discord!](https://reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/)

**Author:** u/roculus | **Upvotes:** 553 | **Comments:** 55 | **Date:** 2026-01-23

**Summary:** The post announces that a user's contribution was featured on Discord and given a special flair, but users find the bot's public announcements annoying and suggest private messages instead. The discussion highlights community frustration with bot spam and monetization concerns.

**Key Points:**
- Bot announces post featuring on Discord and special flair for OP
- Users find public bot announcements annoying and suggest private messages
- Concerns about monetization and community engagement tactics
- Mixed reactions with some users finding humor in the situation
- General consensus that bot spam is disruptive

**Discussion Highlights:** The community largely agrees that the bot's public announcements are disruptive and prefer private messages. There are concerns about monetization and the impact of bot spam on the subreddit's quality. Some users find humor in the potential for the bot to announce its own post's popularity.

---

## 2. [Am I the only one who feels that, with all the AI boom, everyone is basically doing the same thing?](https://reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/)

**Author:** u/[deleted] | **Upvotes:** 394 | **Comments:** 186 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses the redundancy of AI projects during the AI boom, noting that many new AI tools and applications are essentially reinventing existing solutions. The author appreciates AI technology but criticizes the lack of innovation and the financial investment in less polished versions of existing tools.

**Key Points:**
- Many AI projects are redundant, reinventing existing solutions.
- The barrier to entry for AI development is low, leading to shallow implementations.
- There is a trend of people shifting from other tech hypes (e.g., cryptocurrency) to AI, often without deep expertise.
- Some developers are focusing on niche tools and specific needs rather than broad AI applications.
- The current phase is characterized by hype and enthusiasm, which may lead to a saturation of similar projects.

**Discussion Highlights:** The discussion highlights a consensus that the AI field is currently in a hype phase, with many redundant projects being developed. Some commenters note the low barrier to entry and the shift of 'experts' from other tech trends to AI. There is also a recognition of niche development efforts that focus on specific, unmet needs.

---

## 3. [Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp; 1.8B), Support for 10 languages](https://reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/)

**Author:** u/Nunki08 | **Upvotes:** 706 | **Comments:** 113 | **Date:** 2026-01-22

**Summary:** Qwen has open-sourced the Qwen3-TTS model family, including VoiceDesign, CustomVoice, and Base models in 0.6B and 1.8B sizes, supporting 10 languages. The release includes resources on GitHub, Hugging Face, a blog post, a paper, and a demo.

**Key Points:**
- Qwen3-TTS models (0.6B & 1.8B) released with support for 10 languages
- Resources available on GitHub, Hugging Face, blog, paper, and demo
- User feedback highlights voice quality and requests for additional support like llama.cpp
- Positive reception for Qwen's open-sourcing efforts

**Discussion Highlights:** Users appreciated Qwen's open-sourcing efforts but noted concerns about voice quality and requested support for additional platforms like llama.cpp. The community expressed excitement about the model's capabilities and potential applications.

---

## 4. [Qwen dev on Twitter!!](https://reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 729 | **Comments:** 60 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses the announcement of Qwen's TTS model, with the community confirming its availability on Hugging Face.

**Key Points:**
- Thread locked due to announcements
- TTS model from vLLM leak
- Link to Qwen3-TTS on Hugging Face

**Discussion Highlights:** The community is focused on the release of Qwen's TTS model, with some providing links to its availability on Hugging Face.

---

## 5. [8x AMD MI50 32GB at 26 t/s (tg) with MiniMax-M2.1 and 15 t/s (tg) with GLM 4.7 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1qjaxfy/8x_amd_mi50_32gb_at_26_ts_tg_with_minimaxm21_and/)

**Author:** u/ai-infos | **Upvotes:** 316 | **Comments:** 127 | **Date:** 2026-01-21

**Summary:** The post details a cost-effective local inference setup using 8x AMD MI50 GPUs, achieving 26.8 tok/s with MiniMax-M2.1 and 15.6 tok/s with GLM 4.7, at a cost of $880 for 256GB VRAM. The setup is praised for its performance and affordability.

**Key Points:**
- Performance metrics: MiniMax-M2.1 at 26.8 tok/s and GLM 4.7 at 15.6 tok/s
- Cost: $880 for 8 GPUs with 256GB VRAM
- Power draw: 280W idle / 1200W during inference
- Goal: Achieve a cost-effective and efficient local inference setup
- Community feedback highlights the setup's affordability and performance

**Discussion Highlights:** The community responded positively, with comments praising the setup's affordability and performance. Some users expressed interest in replicating the setup but noted challenges in sourcing the GPUs at the mentioned price.

---

## 6. [You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?](https://reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/)

**Author:** u/Adventurous-Gold6413 | **Upvotes:** 543 | **Comments:** 306 | **Date:** 2026-01-20

**Summary:** The post discusses the best local models to use with 64GB RAM and 16GB VRAM when internet access is unavailable. Users share their top model choices and experiences.

**Key Points:**
- Users recommend models like Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B
- GPT-OSS-120B is highlighted for its performance and versatility
- The discussion emphasizes the importance of model fit for the given hardware
- Some users appreciate the contribution of models like GPT-OSS-120B to the community

**Discussion Highlights:** The consensus leans towards GPT-OSS-120B as a top choice due to its performance and compatibility with the specified hardware. Other notable mentions include Gemma 3 27B and GLM 4.5 Air. The discussion also appreciates the availability of such models for offline use.

---

## 7. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 898 | **Comments:** 269 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, high-performance AI system designed for running large MoE models and supporting graphic design tasks. The system features a Threadripper Pro 3995WX, 512GB DDR4 RAM, and 10 GPUs (8x 3090 + 2x 5090), all enclosed in a Thermaltake Core W200 case for mobility and protection. The build cost approximately $17k and balances performance with budget constraints.

**Key Points:**
- Custom-built system for AI and graphic design tasks with 10 GPUs
- Enclosed in a Thermaltake Core W200 case for mobility and protection
- Balanced performance and budget, costing around $17k
- Designed to run large MoE models like Deepseek and Kimi K2
- Community reactions highlight the system's power and portability

**Discussion Highlights:** The community praised the build's power and portability, with humorous comments about its size and airflow. The post gained significant attention, including a feature on Discord and a special flair for the author.

---

## 8. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 365 | **Comments:** 60 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the official support for GLM 4.7 Flash in llama.cpp, highlighting its community-driven development and performance improvements. Users discuss its speed and share additional resources.

**Key Points:**
- GLM 4.7 Flash now officially supported in llama.cpp
- Support is community-driven, not by Z.ai developers
- Performance improvements noted, with some users reporting faster speeds without flash-attention
- Additional resources and model versions shared by users
- Post recognized and featured by the community for its contribution

**Discussion Highlights:** The discussion highlights the community effort behind the integration and shares performance insights. Some users report better performance without flash-attention, and additional model versions are shared. The post is well-received and featured by the community.

---

## 9. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 462 | **Comments:** 162 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights the effectiveness of GLM 4.7 Flash as a reliable local agent, outperforming other MoE models in an agentic framework. The author shares their positive experience with the model, noting its ability to handle complex tasks without errors. The community shows interest in comparisons with other models and shares early testing experiences.

**Key Points:**
- GLM 4.7 Flash is praised for its reliability and performance in an agentic framework.
- The model successfully handles tasks like cloning repos, running commands, and editing files without errors.
- The community is interested in comparisons with other models like Nemotron 30B.
- Early testing indicates the model is fast and performs well on a 4090 GPU.
- The model is seen as a potential replacement for other models like Qwen3.

**Discussion Highlights:** The discussion highlights the community's enthusiasm for GLM 4.7 Flash, with users sharing early testing experiences and comparisons with other models. There is a consensus on the model's potential and performance, with some users already testing it locally.

---

## 10. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 746 | **Comments:** 230 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the release of zai-org/GLM-4.7-Flash on Hugging Face, generating significant community interest and discussion about its technical features and capabilities.

**Key Points:**
- The model is a 30B parameter version with memory-efficient architecture (MLA).
- Community excitement about the release after a long wait.
- Technical advantages like reduced KV cache memory usage enabling longer context lengths.
- Mixed feelings about model size preferences (30B vs 70B).
- Positive outlook on the model's potential performance.

**Discussion Highlights:** The community shows strong enthusiasm for the GLM-4.7-Flash release, particularly noting its memory efficiency and potential for running at full 200k context. Some users express nostalgia for larger models while acknowledging the technical improvements in this release.

---

## 11. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 352 | **Comments:** 103 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to stay within a 10,000€ budget. The system is designed for running large AI models locally, with benchmark results provided for various models. Key points include the system's purpose for local AI model inference, the budget details, benchmark results, community interest, and discussions on similar builds. The community showed strong interest in the build, with comments highlighting the impressive hardware, inquiries about sourcing and cost, and mentions of similar setups. The post was well-received, earning a special flair and being featured on Discord.

---

## 12. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 453 | **Comments:** 71 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses the potential delay in Qwen 4 development, with the lead developer indicating a focus on quality over speed. The community generally supports this approach, appreciating the emphasis on quality improvements.

**Key Points:**
- Qwen 4 development may be delayed as the team focuses on quality.
- The lead developer mentioned 'slowing down' to prioritize quality.
- Community reactions are largely positive, supporting the focus on quality.
- Some users caution against jumping to conclusions based on limited information.
- There is appreciation for meaningful advancements over incremental updates.

**Discussion Highlights:** The discussion highlights a consensus among users that focusing on quality is beneficial for the Qwen series. Many appreciate the developer's decision to take time for meaningful improvements rather than rushing incremental updates. However, some users urge caution against overinterpreting the developer's statement.

---

## 13. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 537 | **Comments:** 117 | **Date:** 2026-01-17

**Summary:** The author upgraded from MI100s to four R9700 GPUs due to better performance and cost efficiency, building a 128GB VRAM server for under $7,035. The post includes detailed specs, benchmarks, and community reactions. Key points include the transition from MI100s to R9700s, detailed system specs with 128GB VRAM and 128GB RAM, cost comparison showing the build is cheaper than an RTX 6000 Blackwell, performance benchmarks provided for llama 7B Q4_0 model, and positive community feedback and engagement. The community praised the build, with top comments highlighting its appeal and humorously noting its potential to encourage financially irresponsible behavior.

---

## 14. [Best "End of world" model that will run on 24gb VRAM](https://reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/)

**Author:** u/gggghhhhiiiijklmnop | **Upvotes:** 340 | **Comments:** 180 | **Date:** 2026-01-17

**Summary:** The user is seeking recommendations for the best LLM models that can run on a PC with 24GB VRAM and 64GB RAM, aiming to hoard data for an 'end of world' scenario. The discussion includes suggestions for specific models and practical advice on data storage.

**Key Points:**
- User wants models that fit within 24GB VRAM and 64GB RAM.
- Suggestions include saving the best LLM available and running it off SSD if necessary.
- Specific model recommendations: gemma3:27b and Midnight Miku.
- Advice to download actual Wikipedia backups for offline access.
- Discussion highlights the importance of data preservation in an 'end of world' scenario.

**Discussion Highlights:** The discussion emphasizes practical solutions for data storage and model selection, with a consensus on prioritizing the best available LLM and ensuring data backups for offline use.

---

## 15. [GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)](https://reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/)

**Author:** u/CuriousPlatypus1881 | **Upvotes:** 385 | **Comments:** 89 | **Date:** 2026-01-16

**Summary:** The post discusses the December 2025 update of the SWE-bench leaderboard, highlighting the performance of various AI models on GitHub PR tasks. Claude Opus 4.5 leads with a 63.3% resolved rate, followed closely by GPT-5.2 (extra high effort) at 61.5%. The post also notes the strong performance of open-source models like GLM-4.7.

**Key Points:**
- Claude Opus 4.5 leads the SWE-bench leaderboard with a 63.3% resolved rate.
- GPT-5.2 (extra high effort) follows closely at 61.5%.
- Gemini 3 Flash Preview outperforms Gemini 3 Pro Preview despite being smaller and cheaper.
- GLM-4.7 is the strongest open-source model, ranking alongside closed models like GPT-5.1-codex.
- GPT-OSS-120B shows significant performance improvement in high-effort reasoning mode.

**Discussion Highlights:** The discussion highlights excitement around Gemini Flash's performance and the strong showing of open-source models like GLM-4.7. There is also anticipation for future releases like DeepSeek v4.

---

## 16. [I fucking love this community](https://reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/)

**Author:** u/alhinai_03 | **Upvotes:** 518 | **Comments:** 54 | **Date:** 2026-01-16

**Summary:** The post expresses gratitude towards the open-source community for enabling the author to run large language models on older hardware with limited VRAM. The author highlights the impressive performance of the nemotron-3-nano-30B-a3b-iq4_nl model, achieving 14-13.5 tokens per second on a 10-year-old PC with only 4GB of VRAM.

**Key Points:**
- Gratitude towards the open-source community and contributors
- Impressive performance of large models on older hardware
- Effectiveness of system RAM and MoE architecture for running large models
- Specific model performance: nemotron-3-nano-30B-a3b-iq4_nl at 14-13.5 t/s
- Hardware constraints: 4GB VRAM on a 10-year-old PC

**Discussion Highlights:** The discussion highlights the community's appreciation for optimization achievements and the practicality of using system RAM combined with MoE architecture for running large models on limited hardware. There is a consensus on the effectiveness of these methods and a shared desire for more VRAM and RAM to run state-of-the-art models.

---

## 17. [My story of underestimating /r/LocalLLaMA's thirst for VRAM](https://reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/)

**Author:** u/EmPips | **Upvotes:** 1355 | **Comments:** 91 | **Date:** 2026-01-15

**Summary:** The Reddit post discusses the author's experience with the subreddit's high demand for VRAM, as indicated by the title and the context of the comments. The post gained significant attention, as shown by the upvotes and comments.

**Key Points:**
- The post gained popularity and was featured on Discord.
- The author received a special flair for their contribution.
- Discussion includes references to hardware like the R9700 and comparisons with other GPUs.
- Comments highlight the competitive nature of acquiring hardware, akin to a gold rush.
- Some users share their personal experiences with similar hardware.

**Discussion Highlights:** The discussion highlights the competitive nature of acquiring high-demand hardware, with users sharing personal experiences and recommendations. There is a consensus on the value of certain GPUs and the importance of VRAM.

---

## 18. [Latest upgrade…A100 40 GB](https://reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/)

**Author:** u/inserterikhere | **Upvotes:** 411 | **Comments:** 54 | **Date:** 2026-01-15

**Summary:** The user upgraded their gaming rig to an AI rig by purchasing a faulty A100 GPU for $1000, which worked perfectly upon installation. They previously used a 3090 and 7950x for AI tasks but decided to upgrade despite price surges.

**Key Points:**
- User transitioned from gaming to AI rig
- Purchased a faulty A100 GPU that worked upon installation
- Previously used a 3090 and 7950x for AI tasks
- Community expressed concerns about cooling for the A100
- Post gained popularity and was featured on Discord

**Discussion Highlights:** The community reacted with a mix of admiration and concern, particularly about cooling the A100 GPU. Some users shared memes and jokes, while others provided practical advice on cooling solutions.

---

## 19. [Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M](https://reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/)

**Author:** u/eugenekwek | **Upvotes:** 329 | **Comments:** 54 | **Date:** 2026-01-14

**Summary:** The post announces Soprano 1.1, an improved version of the Soprano TTS model with significant reductions in hallucinations and audio artifacts, along with enhanced stability and support for longer sentences. The community response is overwhelmingly positive, with users praising the model's performance and expressing interest in further developments.

**Key Points:**
- Soprano 1.1 reduces hallucinations by 95% and lowers WER by 50% compared to the previous version.
- The model now supports sentences up to 30 seconds long, doubling the previous limit.
- Audio artifacts and high-frequency noise have been significantly reduced through further training.
- Community feedback highlights the model's impressive performance for its size (80M parameters).
- Users are interested in additional features like ONNX support and improved handling of punctuation.

**Discussion Highlights:** The community response is highly positive, with users expressing surprise at the model's quality given its small size. There is interest in additional features and support, and the author's work is widely appreciated.

---

## 20. [NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency](https://reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/)

**Author:** u/Fear_ltself | **Upvotes:** 718 | **Comments:** 130 | **Date:** 2026-01-14

**Summary:** NVIDIA's Orchestrator-8B is an 8-billion-parameter AI model designed to manage and route complex tasks to various tools for greater efficiency, sparking discussions about the future of AI systems and their integration.

**Key Points:**
- Orchestrator-8B is a specialized model for task management and routing.
- The model aims to enhance efficiency by leveraging other tools and models.
- Discussions highlight the potential of integrating multiple AI systems.
- Comparisons to middle management and existing frameworks were noted.
- The post gained significant attention with 718 upvotes and 130 comments.

**Discussion Highlights:** The discussion emphasized the importance of integrating various AI tools and models, with some users comparing the Orchestrator-8B to middle management roles and existing agentic frameworks. The consensus suggests a move towards more functional and interconnected AI systems.

---

## 21. [GLM-Image is released!](https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/)

**Author:** u/foldl-li | **Upvotes:** 599 | **Comments:** 83 | **Date:** 2026-01-13

**Summary:** GLM-Image is a new image generation model with a hybrid autoregressive + diffusion decoder architecture. It excels in text-rendering and knowledge-intensive tasks while supporting various image-to-image tasks like editing and style transfer.

**Key Points:**
- Hybrid autoregressive + diffusion decoder architecture
- Excels in text-rendering and knowledge-intensive generation
- Supports image-to-image tasks like editing and style transfer
- MIT license with no restrictions
- Model size: 13GB diffusion model + 20GB text encoder

**Discussion Highlights:** The community appreciates the MIT license and the model's capabilities. There is interest in quantizing the model for easier use and discussions about its performance compared to other models.

---

## 22. [My wishes for 2026](https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/)

**Author:** u/jacek2023 | **Upvotes:** 652 | **Comments:** 179 | **Date:** 2026-01-13

**Summary:** The Reddit post discusses predictions for 2026, focusing on the possibility of affordable GPUs with more than 32GB memory. The community responds with a mix of humor and skepticism.

**Key Points:**
- Desire for affordable GPUs with more than 32GB memory
- Skeptical reactions from the community
- Humorous comments about the feasibility of such GPUs
- Mentions of specific AI models like Qwen 4 and Mistral

**Discussion Highlights:** The discussion highlights a mix of humor and skepticism regarding the possibility of affordable GPUs with more than 32GB memory in 2026. The community engages in playful banter and expresses doubts about the feasibility of such advancements.

---

## 23. [kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop—no GPU required](https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/)

**Author:** u/Nunki08 | **Upvotes:** 399 | **Comments:** 93 | **Date:** 2026-01-13

**Summary:** Kyutai introduced Pocket TTS, a 100M-parameter text-to-speech model with high-quality voice cloning that runs on a laptop without requiring a GPU. The model is open-source with resources available on GitHub, Hugging Face, and arXiv.

**Key Points:**
- Pocket TTS is a lightweight (100M parameters) TTS model capable of high-quality voice cloning.
- It runs efficiently on a laptop without needing a GPU.
- Resources include a blog post, GitHub repository, Hugging Face model card, arXiv paper, and social media updates.
- Users discussed potential memory issues during prolonged use and inquired about multi-language support.

**Discussion Highlights:** The community showed interest in multi-language support and raised concerns about memory usage during extended generation sessions. Some users suggested that smaller models may not yet match the quality of established alternatives.

---

## 24. [GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/)

**Author:** u/TKGaming_11 | **Upvotes:** 366 | **Comments:** 93 | **Date:** 2026-01-12

**Summary:** The Reddit post highlights DeepSeek-AI's 'Engram,' a novel approach for conditional memory in large language models using scalable lookup, praised for its originality and technical innovation.

**Key Points:**
- DeepSeek-AI introduces 'Engram' for conditional memory via scalable lookup.
- The method uses n-gram embedding and mHC (M=4) for ablations.
- It adds static memory as a complementary sparsity axis with O(1) lookup.
- The approach is compared to biological memory processes.
- The community appreciates the innovation and potential of this method.

**Discussion Highlights:** The discussion emphasizes the novelty and technical merits of 'Engram,' with users highlighting its potential impact and comparing it to natural memory processes.

---

## 25. [LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)](https://reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 1063 | **Comments:** 114 | **Date:** 2026-01-11

**Summary:** The post introduces TimeCapsuleLLM, a 1.2B parameter language model trained exclusively on 1800-1875 London texts to minimize modern bias. The model demonstrates period-specific knowledge and behaviors, such as unfamiliarity with post-1875 concepts like telephones. The project is open-source and available on GitHub and Hugging Face.

**Key Points:**
- TimeCapsuleLLM is trained on 90GB of 1800-1875 London texts with no modern data or fine-tuning.
- The model exhibits period-appropriate responses, such as treating 'telephone' as an unknown term.
- The project is open-source and available on GitHub and Hugging Face.
- Future steps include creating synthetic Q&A pairs from the dataset.
- The community shows strong interest and support for the project.

**Discussion Highlights:** The community response is overwhelmingly positive, with users praising the project's uniqueness and expressing interest in similar historical language models. Some users shared their own related projects, and there was humorous engagement with the model's period-specific limitations.

---

## 26. [I bought a €9k GH200 “desktop” to save $1.27 on Claude Code (vLLM tuning notes)](https://reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/)

**Author:** u/Reddactor | **Upvotes:** 692 | **Comments:** 177 | **Date:** 2026-01-11

**Summary:** The author built a €9k GH200 'desktop' to run Claude Code locally, achieving better speeds than the cloud version and sharing optimized vLLM settings for dual 96GB systems. The setup uses MiniMax M2.1 for offline coding and blocks telemetry, though the cost is humorously noted as 321X the yearly subscription fee.

**Key Points:**
- Built a €9k GH200 'desktop' with 192GB VRAM for local Claude Code execution.
- Achieved better speeds than Claude Code with Sonnet using optimized vLLM settings.
- Shared settings for vLLM in Docker, including tensor parallelism and context size adjustments.
- Used MiniMax M2.1 FP8+INT4 AWQ for offline coding and blocked telemetry.
- Community reactions highlight the humor in the cost vs. savings and the fun of the project.

**Discussion Highlights:** The community praised the setup's novelty and humor, with top comments joking about the cost vs. savings and the fun of the project. Some users expressed envy over missing out on the hardware deal.

---

## 27. [It works! Abliteration can reduce slop without training](https://reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/)

**Author:** u/-p-e-w- | **Upvotes:** 404 | **Comments:** 129 | **Date:** 2026-01-11

**Summary:** The post discusses the use of abliteration to reduce 'slop' (flowery, cliched language) in LLM outputs without training. The author successfully applied this technique to the Mistral Nemo model, creating a slop-reduced version using Heretic, a tool originally designed for censorship removal.

**Key Points:**
- Abliteration can reduce slop in LLM outputs without training.
- The author used Heretic to create a slop-reduced configuration for the Mistral Nemo model.
- The process took 2.5 hours on an A6000 but can be faster with quantization or reduced parameters.
- The technique shows promise but may result in drier prose, as noted in the comments.
- GGUF versions of the model are available for further testing.

**Discussion Highlights:** The discussion highlights mixed opinions on the effectiveness of slop reduction. Some users appreciate the reduction in cliched language, while others feel it makes the prose too dry. There is also interest in whether this technique can be applied to other overused patterns in writing.

---

## 28. [I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work](https://reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/)

**Author:** u/Ok-Pomegranate1314 | **Upvotes:** 889 | **Comments:** 147 | **Date:** 2026-01-09

**Summary:** The author successfully clustered three NVIDIA DGX Sparks, overcoming networking limitations by writing a custom NCCL plugin. This achievement allows distributed inference across all three nodes at high speeds, despite NVIDIA's official support for only two-node clustering. Key points include the development of a custom NCCL network plugin, achieving distributed inference at 8+ GB/s over RDMA, and addressing challenges like subnet mismatches and RDMA state machine issues. The community praised the work as impressive and potentially impactful for distributed computing, with questions raised about scalability and performance gains.

---

## 29. [The reason why RAM has become so expensive](https://reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/)

**Author:** u/InvadersMustLive | **Upvotes:** 4554 | **Comments:** 382 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses the significant increase in RAM prices, with users suggesting that companies like OpenAI may be monopolizing RAM to create future demand and make competitors' data centers economically unviable. The cost of RAM has reportedly increased by up to 10 times compared to the previous year.

**Key Points:**
- RAM prices have increased significantly, with some users reporting a 10-fold increase.
- OpenAI is accused of monopolizing RAM to create future demand and hinder competitors.
- The high cost of RAM is making data centers, especially in China, economically unviable.
- Users express concern about the sustainability of current RAM prices, with some suggesting it may be a bubble.

**Discussion Highlights:** The discussion highlights concerns about monopolistic practices in the RAM market, with users pointing to OpenAI's potential role in driving up prices. There is a consensus that the current price surge is unsustainable and may have broader economic implications for data centers and AI development.

---

## 30. [DeepSeek V4 Coming](https://reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/)

**Author:** u/External_Mood4719 | **Upvotes:** 502 | **Comments:** 109 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release V4, a next-generation AI model with enhanced code-generation capabilities, outperforming mainstream models like Claude and GPT. The model features improvements in handling long code prompts and data pattern understanding, with users anticipating better reasoning and reliability.

**Key Points:**
- DeepSeek V4 focuses on strong code-generation capabilities
- Outperforms existing models like Claude and GPT in internal benchmarks
- Improved handling of long code prompts and data patterns
- Users report high satisfaction with V3.2 and anticipate further improvements
- Discussion highlights include cost-effectiveness and potential integration of advanced features like mHC and OCR

**Discussion Highlights:** Users express enthusiasm for DeepSeek's cost-effectiveness and performance, with some anticipating significant advancements in V4. There is consensus on the model's potential to disrupt the AI landscape, particularly in coding tasks.

---

## 31. [(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability](https://reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/)

**Author:** u/Nunki08 | **Upvotes:** 482 | **Comments:** 102 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release a new flagship AI model with strong coding abilities, generating excitement and discussion in the community.

**Key Points:**
- DeepSeek's upcoming model focuses on strong coding abilities
- Community reactions range from enthusiasm to skepticism
- The model is expected to be state-of-the-art based on internal benchmarks
- Users express hope for improved role-playing capabilities
- The announcement has sparked significant interest and discussion

**Discussion Highlights:** The community shows a mix of excitement and skepticism, with many users eagerly anticipating the new model's capabilities, particularly in coding and role-playing. Some users express concerns about potential limitations or overhyped claims.

---

## 32. [The NO FAKES Act has a "Fingerprinting" Trap that kills Open Source. We need to lobby for a Safe Harbor.](https://reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/)

**Author:** u/PostEasy7183 | **Upvotes:** 619 | **Comments:** 88 | **Date:** 2026-01-08

**Summary:** The NO FAKES Act threatens open-source AI development by imposing liability on developers for tools used to create digital replicas, potentially banning open-source AI hosting in the US. The post urges lobbying for a Safe Harbor provision to protect developers.

**Key Points:**
- The NO FAKES Act creates a 'digital replica right' that could hold developers liable for tools used to create deepfakes.
- Developers hosting TTS or voice-conversion models could face statutory damages if their tools are misused.
- The post calls for a 'Safe Harbor' provision to protect open-source developers.
- The discussion highlights concerns about the impact on innovation and the influence of big tech corporations.
- Action items include emailing or calling representatives to oppose the bill unless amended.

**Discussion Highlights:** The discussion reflects strong opposition to the bill, with concerns about its impact on innovation and the potential for big tech monopolies. Many commenters express skepticism about politicians' understanding of technology.

---

## 33. [Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt](https://reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/)

**Author:** u/Prior-Arm-6705 | **Upvotes:** 940 | **Comments:** 147 | **Date:** 2026-01-08

**Summary:** The post describes a project where someone counted Jensen Huang saying 'AI' 121 times during his CES 2025 keynote and created a compilation video using open-source tools. The process involved downloading the video, parsing subtitles, and editing clips locally.

**Key Points:**
- Jensen Huang said 'AI' 121 times in his CES 2025 keynote
- The author used open-source tools (yt-dlp-mcp and ffmpeg-mcp-lite) to create a compilation video
- The process was entirely local, with no cloud involvement
- The result was described as 'hypnotic'
- Discussion included reactions to the project and mentions of AI costs

**Discussion Highlights:** The discussion featured reactions to the project, with some users praising the technical execution and others commenting on the cost of AI. There were also references to other tech communities like Gamers Nexus.

---

## 34. [16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/)

**Author:** u/ai-infos | **Upvotes:** 457 | **Comments:** 238 | **Date:** 2026-01-07

**Summary:** The post discusses running Deepseek V3.2 AWQ 4-bit on 16x AMD MI50 32GB GPUs, achieving 10 tokens/sec output and 2000 tokens/sec input with a 69000 context length. The setup aims for cost-effective local AGI and highlights power efficiency and future scalability. Key points include performance metrics, power draw, cost-effective goals, future plans, and community contributions. Discussion highlights the setup's popularity, power efficiency as a heater alternative, concerns about noise and power requirements, and cost-effectiveness for professional use.

---

## 35. [DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.](https://reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/)

**Author:** u/Nunki08 | **Upvotes:** 661 | **Comments:** 54 | **Date:** 2026-01-07

**Summary:** DeepSeek-R1’s paper was recently updated, expanding from 22 pages to 86 pages with added details. The update has sparked discussions about potential new architectures and improvements in model training.

**Key Points:**
- The paper expanded significantly from 22 to 86 pages.
- Discussions highlight potential new architectures like dsv4 + r2.
- Interest in how architectural improvements perform at different model sizes.
- Focus on linear attention and cache optimization in current research.
- Original paper lacked implementation specifics, which the update may address.

**Discussion Highlights:** The community is excited about the expanded paper, speculating on new architectures and improvements. There is particular interest in how these advancements will scale across different model sizes and the potential for linear attention to enhance training efficiency.

---

## 36. [A 30B Qwen Model Walks Into a Raspberry Pi… and Runs in Real Time](https://reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/)

**Author:** u/ali_byteshape | **Upvotes:** 500 | **Comments:** 79 | **Date:** 2026-01-06

**Summary:** The post discusses the successful optimization of a 30B Qwen model to run efficiently on a Raspberry Pi 5, achieving 8.03 tokens per second while retaining 94.18% of BF16 quality. The author highlights the quirks of GPU performance and requests community feedback for further testing. Key points include: A 30B Qwen model runs on a Raspberry Pi 5 with 8.03 TPS and 94.18% BF16 quality retention; CPU performance is generally monotonic, while GPU performance depends on kernel choice; Community feedback is requested for testing on different setups and workloads; The model achieves better TPS/quality tradeoffs compared to alternatives like Unsloth and MagicQuant; A user reported needing to set context to -c 4096 to avoid segfaults on a Pi 5. The community showed interest in testing the model on various setups, including non-NVIDIA hardware and clusters of Raspberry Pis. Some users reported specific configurations needed for successful execution.

---

## 37. [Performance improvements in llama.cpp over time](https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/)

**Author:** u/jacek2023 | **Upvotes:** 683 | **Comments:** 85 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses performance improvements in llama.cpp over time, highlighting significant progress and community appreciation. The discussion includes mentions of GPU-specific optimizations and comparisons with other implementations.

**Key Points:**
- Performance gains in llama.cpp have been substantial, with notable progress in token generation speed.
- The improvements may be particularly beneficial for NVIDIA GPUs, as suggested by community comments.
- The post received recognition from the community, including a special flair and feature on Discord.
- Comparisons with other implementations like ik_llama.cpp indicate competitive performance.
- Prompt processing remains slower compared to token generation but has shown improvement.

**Discussion Highlights:** The community consensus highlights the impressive progress in llama.cpp performance, with specific mentions of NVIDIA GPU optimizations and comparisons to other implementations. The discussion also reflects appreciation for the contributions and recognition of the post's popularity.

---

## 38. [For the first time in 5 years, Nvidia will not announce any new GPUs at CES — company quashes RTX 50 Super rumors as AI expected to take center stage](https://reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/)

**Author:** u/FullstackSensei | **Upvotes:** 624 | **Comments:** 195 | **Date:** 2026-01-05

**Summary:** Nvidia will not announce new GPUs at CES, focusing instead on AI. There are concerns about limited supply of new GPUs, potential reintroduction of older models, and rising hardware prices.

**Key Points:**
- Nvidia quashes RTX 50 Super rumors
- Limited supply of RTX 5070Ti, 5080, and 5090
- Rumors of RTX 3060 reintroduction
- Rising DDR5 and storage prices
- Concerns about corporate greed and impact on local computing

**Discussion Highlights:** The discussion highlights frustration with corporate greed, concerns about the future of local computing, and suggestions for alternative solutions like increased competition from China.

---

## 39. [llama.cpp performance breakthrough for multi-GPU setups](https://reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/)

**Author:** u/Holiday-Injury-9397 | **Upvotes:** 567 | **Comments:** 203 | **Date:** 2026-01-05

**Summary:** The ik_llama.cpp project achieved a 3x to 4x performance improvement for local LLM inference using multi-GPU setups, enabling cost-effective use of multiple low-cost GPUs instead of high-end enterprise cards.

**Key Points:**
- ik_llama.cpp introduced a new execution mode (split mode graph) for maximum multi-GPU utilization.
- Performance gains are significant, with 3x to 4x speed improvements in multi-GPU setups.
- The breakthrough makes it feasible to use multiple low-cost GPUs instead of expensive high-end cards.
- Even single GPU or CPU-only setups see consistent 2x prompt processing speed improvements.
- The project is seen as competitive with other solutions like exllama and vllm for single batch processing.

**Discussion Highlights:** The community highlights the importance of this breakthrough for cost-effective local LLM inference, with users reporting significant performance gains even on single GPU or CPU-only setups. Some users note challenges with hybrid inference due to hardware bottlenecks like NUMA and PCIe 3.0 limitations.

---

## 40. [GLM-Image model from Z.ai is coming](https://reddit.com/r/LocalLLaMA/comments/1q41bw1/glmimage_model_from_zai_is_coming/)

**Author:** u/Ravencloud007 | **Upvotes:** 319 | **Comments:** 59 | **Date:** 2026-01-04

**Summary:** The post announces the upcoming GLM-Image model from Z.ai, generating significant interest and discussion in the r/LocalLLaMA community.

**Key Points:**
- The GLM-Image model from Z.ai is highly anticipated.
- The model is expected to have a large number of parameters (e.g., 103b).
- Z.ai's image model is currently the community favorite.
- There is a desire for a model that balances size, ease of fine-tuning, and quality.
- The post has gained popularity, being featured on Discord and earning the author a special flair.

**Discussion Highlights:** The community is excited about the GLM-Image model, with discussions highlighting its potential size and capabilities. There is a consensus that Z.ai's current image model is highly regarded, and users are eager for a model that combines ease of use with high quality.

---

## 41. [Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched](https://reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/)

**Author:** u/ubrtnk | **Upvotes:** 383 | **Comments:** 194 | **Date:** 2026-01-03

**Summary:** The post discusses the challenges faced by local LLMs in processing extreme breaking news events, such as the US attacking Venezuela. The author shares their experience with different LLMs, highlighting how these models often classify such events as hoaxes or misinformation despite credible sources.

**Key Points:**
- Local LLMs struggle to process extreme or unlikely breaking news events.
- Models like Qwen Research and Spark initially classified the event as a hoax despite credible sources.
- Larger models like GPT-OSS:120B performed better but still showed skepticism.
- The discussion highlights the bias and limitations of LLMs in understanding unfamiliar geopolitical events.
- Users express frustration with LLMs' tendency to dismiss extreme but real events.

**Discussion Highlights:** The discussion consensus indicates that LLMs have a tendency to dismiss extreme or unlikely events as misinformation, even when provided with credible sources. Users share similar experiences and express frustration with the limitations of LLMs in understanding and processing breaking news.

---

## 42. [LeCun Says Llama 4 results "were fudged a little bit"](https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/)

**Author:** u/MrPecunius | **Upvotes:** 362 | **Comments:** 88 | **Date:** 2026-01-02

**Summary:** Yann LeCun, departing Meta AI Chief, confirmed that Llama 4 benchmark results were manipulated. This follows speculation about suspicious benchmarks and coincides with Zuckerberg sidelining the GenAI organization, leading to significant departures.

**Key Points:**
- LeCun confirms Llama 4 benchmark manipulation
- Zuckerberg sidelined Meta's GenAI organization
- Significant departures from Meta's AI team
- Llama 4's promised large model was never released
- Community disappointment over Llama's failure

**Discussion Highlights:** The discussion highlights disappointment over Llama's failure and its impact on the open-source AI community. Users express concern over Meta's strategic missteps in AI development and share additional resources for further reading.

---

## 43. [Qwen-Image-2512](https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/)

**Author:** u/Nunki08 | **Upvotes:** 719 | **Comments:** 122 | **Date:** 2025-12-31

**Summary:** The Reddit post announces the release of Qwen-Image-2512, a new model available on multiple platforms including Hugging Face, ModelScope, and GitHub. Users have shared positive feedback and experiences, including running the model on low-end hardware.

**Key Points:**
- Qwen-Image-2512 is available on various platforms like Hugging Face, ModelScope, and GitHub.
- Users have successfully run the model on low-end hardware without a GPU.
- The community appreciates the release as a gift for the new year.
- The model supports image generation with detailed prompts.
- Positive feedback and engagement from the community.

**Discussion Highlights:** Users expressed gratitude for the new model release, shared their experiences running it on different hardware configurations, and provided creative examples of image generation prompts.

---

## 44. [[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It’s running a raw Llama-7B instance with a 2048 token window.](https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/)

**Author:** u/simar-dmg | **Upvotes:** 743 | **Comments:** 110 | **Date:** 2025-12-30

**Summary:** A user reverse-engineered a Snapchat sextortion bot and discovered it was running a raw Llama-7B instance with a 2048 token window. The bot was vulnerable to a persona-adoption jailbreak, revealing its configuration and malicious payload.

**Key Points:**
- The bot used a Llama-7B model with a 2048 token context window and high temperature setting.
- A persona-adoption jailbreak (Grandma Protocol) forced the bot to reveal its environment variables.
- The bot was running on minimal hardware to maximize profit margins.
- The bot eventually revealed a malicious link it was programmed to hide.
- Scammers are increasingly using open-source models like Llama-7B to avoid API costs and censorship.

**Discussion Highlights:** The discussion included skepticism about the accuracy of the bot's revealed information, with some users suggesting it could be entirely hallucinated. Others questioned the commonality of system prompts including environment variables.

---

## 45. [Llama-3.3-8B-Instruct](https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/)

**Author:** u/jacek2023 | **Upvotes:** 471 | **Comments:** 77 | **Date:** 2025-12-29

**Summary:** The post discusses the discovery and release of the Llama-3.3-8B-Instruct model, which was obtained through Meta's finetuning API after overcoming technical challenges. The author extracted the original model by removing a merged adapter.

**Key Points:**
- Llama-3.3-8B-Instruct model was obtained via Meta's finetuning API.
- The process involved navigating UI issues and technical bugs.
- The original model was extracted by removing a merged adapter.
- Community is verifying the model's authenticity and features.
- Excited reactions from the community about the discovery.

**Discussion Highlights:** The community is actively verifying the model's authenticity and discussing its features, such as the 8K max position embeddings. There is excitement and appreciation for the discovery and release of the model.

---

## 46. [Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.](https://reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 349 | **Comments:** 120 | **Date:** 2025-12-29

**Summary:** Z AI is set to go public with an IPO on January 8, aiming to raise $560 million, marking it as the first AI-native LLM company to list globally. The Reddit post and comments highlight concerns about the future of open-source models and the company's potential shift in focus.

**Key Points:**
- Z AI's IPO is scheduled for January 8, aiming to raise $560 million.
- Concerns about the future of open-source models post-IPO.
- Community reactions include skepticism and hopes for continued open-source contributions.
- Discussion on the balance between commercial success and open-source ethics.
- Mixed feelings about the company's potential shift in focus.

**Discussion Highlights:** The discussion highlights a consensus of concern about the potential impact on open-source models, with some users expressing skepticism about the company's future contributions to the open-source community. Others argue that commercial success is necessary for sustainability, while a few urge the company not to 'sell out.'

---

## 47. [Tencent just released WeDLM 8B Instruct on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 424 | **Comments:** 62 | **Date:** 2025-12-29

**Summary:** Tencent released WeDLM 8B Instruct on Hugging Face, a diffusion language model that outperforms vLLM-optimized Qwen3-8B in math reasoning tasks by 3-6 times. The model is available under Apache 2.0 license and has garnered significant interest in the community.

**Key Points:**
- WeDLM 8B Instruct is a diffusion language model released by Tencent on Hugging Face.
- It runs 3-6 times faster than vLLM-optimized Qwen3-8B on math reasoning tasks.
- The model is licensed under Apache 2.0.
- Community interest is high, with discussions highlighting its potential and performance.
- A 7B version of the model is also available.

**Discussion Highlights:** The community is excited about the performance and potential of the WeDLM models, with many users expressing interest in the Apache 2.0 license and the impressive benchmark scores. There is a consensus that 7-8B models have significant potential and more models in this range are welcomed.

---

## 48. [NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux](https://reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/)

**Author:** u/HumanDrone8721 | **Upvotes:** 449 | **Comments:** 185 | **Date:** 2025-12-27

**Summary:** NVIDIA has dropped Pascal support on Linux, causing issues for Arch Linux users. The change affects hardware like the 24GB P40 Pascal card and has sparked discussions about legacy driver management.

**Key Points:**
- NVIDIA's driver update (590) drops Pascal support
- Impact on Arch Linux users and specific hardware like the 24GB P40 Pascal card
- User concerns about future compatibility and driver management
- Arch Linux's practice of moving legacy drivers to AUR is noted as expected behavior

**Discussion Highlights:** The discussion highlights user concerns about hardware compatibility and the expected shift of legacy drivers to the Arch User Repository (AUR). Some users express nostalgia for older hardware while others acknowledge the inevitability of such changes.

---

## 49. [Best Local LLMs - 2025](https://reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/)

**Author:** u/rm-rf-rm | **Upvotes:** 364 | **Comments:** 198 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses the best local LLMs of 2025, highlighting models like Minimax M2.1 and GLM4.7 as frontier performers. Users share their favorite models across categories like General, Agentic, Creative Writing, and Speciality, with a focus on open-weight models and memory footprint classifications. Key points include the categorization of models by memory footprint (Unlimited, Medium, Small) and notable small models like Qwen3-4B-instruct and LFM2-8B-A1B. The discussion emphasizes detailed setup descriptions and usage contexts, with interest in RAG for technical documentation and the best embedding/LLM model combinations.

---

## 50. [NVIDIA has 72GB VRAM version now](https://reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/)

**Author:** u/decentralize999 | **Upvotes:** 466 | **Comments:** 148 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses NVIDIA's new 72GB VRAM version, questioning if 96GB is too expensive and noting the AI community's lack of interest in 48GB. The discussion includes price comparisons and community opinions on VRAM sizes.

**Key Points:**
- NVIDIA has released a 72GB VRAM version.
- Community questions the cost of 96GB and interest in 48GB.
- Price comparisons show RTX 5000 48GB at $5100, 72GB at $7800, and RTX 6000 96GB at $8300.
- Some users suggest larger VRAM sizes like 128GB.
- Price per gig remains consistent across models.

**Discussion Highlights:** The discussion highlights a consensus that larger VRAM sizes are preferred, with some users advocating for even larger options like 128GB. Price comparisons show consistent pricing per gigabyte, making the choice dependent on affordability.

---

