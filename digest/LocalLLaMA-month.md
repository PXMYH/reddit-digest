# r/LocalLLaMA Reading Digest

**Period:** 2026-01-20 to 2026-01-20
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 610 | **Comments:** 172 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, fully enclosed mobile AI system with 10 GPUs, designed for running large MoE models and supporting graphic design tasks. The build cost around $17k and prioritizes mobility, enclosure, and performance within budget constraints.

**Key Points:**
- The system features a Threadripper Pro 3995WX, 512GB DDR4, and 10 GPUs (8x 3090 + 2x 5090).
- It is designed for large MoE models, video generation, and high-detail image generation.
- The enclosure was a major challenge, solved using a Thermaltake Core W200 case.
- Budget constraints led to a mix of GPUs to balance cost and performance.
- The post received significant engagement, with comments highlighting its uniqueness and humor.

**Discussion Highlights:** The discussion includes humorous comments about the system's portability and power, with one user joking about plugging it into a McDonald's socket. Other comments praise the build's uniqueness and the creative solution to the enclosure problem.

---

## 2. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 362 | **Comments:** 61 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the official support for GLM 4.7 Flash in llama.cpp, highlighting its community-driven development and performance improvements. Users discuss its efficiency and share additional resources. Key points include: GLM 4.7 Flash now officially supported in llama.cpp, support is community-driven, performance improvements noted, additional resources and model versions shared by community members, and the post recognized with special flair for contribution. The discussion highlights the community effort behind the implementation and shares performance insights, with some users noting that disabling flash-attention can lead to faster execution. Additional model versions and resources are also shared.

---

## 3. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 442 | **Comments:** 152 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights the author's positive experience with GLM 4.7 Flash, an MoE model that performed reliably in an agentic framework, handling extensive tasks without errors. The community discussion includes comparisons with other models, performance notes, and enthusiasm for its local use.

**Key Points:**
- GLM 4.7 Flash performed reliably in an agentic framework, handling extensive tasks without errors.
- The model is praised for its ability to clone repos, run commands, edit files, and commit changes seamlessly.
- Community members compare it favorably to other models like Nemotron 30B and Qwen3.
- Performance benchmarks suggest it is as smart as SEED OSS 36B but with better performance due to MoE.
- GGUFs for local use are highly anticipated.

**Discussion Highlights:** The discussion highlights enthusiasm for GLM 4.7 Flash's performance and reliability, with comparisons to other models and anticipation for local use via GGUFs. Some users note its deep thinking capabilities and decent speed on high-end GPUs.

---

## 4. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 727 | **Comments:** 225 | **Date:** 2026-01-19

**Summary:** The post announces the release of GLM-4.7-Flash model on Hugging Face, generating significant community interest with 720 upvotes and 225 comments. Users express excitement about the model's capabilities and features.

**Key Points:**
- GLM-4.7-Flash model released on Hugging Face
- Model uses MLA, reducing KV cache memory usage
- Supports full 200k context, making it accessible to more users
- Community shows strong interest with high upvotes and comments
- Mixed feelings about model size (30b vs 70b)

**Discussion Highlights:** The community is enthusiastic about the new model's efficiency and context length capabilities. There's notable discussion about the model's architecture (MLA) and its memory efficiency. Some users express nostalgia for larger models while appreciating the current release's accessibility.

---

## 5. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 346 | **Comments:** 91 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to stay within a ~10,000€ budget. The system is designed for running large AI models locally, with benchmark results showing strong performance across various models. Key points include the system's purpose for local AI model inference, the budget and subsidy details, strong benchmark performance, community praise for the hardware setup, and the existence of similar builds in the community. The discussion highlights the community's positive reaction, with comments focusing on the impressive specifications and cost, as well as inquiries about sourcing and job context.

---

## 6. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 445 | **Comments:** 71 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses the potential delay in Qwen 4 development, with the lead developer indicating a focus on quality over speed. The community generally supports this approach, appreciating the emphasis on quality improvements.

**Key Points:**
- Qwen 4 development may be delayed as the team focuses on quality.
- The lead developer mentioned 'slowing down' to prioritize quality.
- Community reactions are largely positive, valuing quality over frequent releases.
- Some users caution against jumping to conclusions based on limited information.

**Discussion Highlights:** The discussion highlights a consensus that focusing on quality is beneficial for the Qwen series. Users appreciate the developer's approach and hope for meaningful improvements rather than incremental updates.

---

## 7. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 526 | **Comments:** 111 | **Date:** 2026-01-17

**Summary:** The author upgraded from MI100s to four R9700 GPUs, building a 128GB VRAM server for under $7,035, achieving better prompt processing performance at a lower cost compared to alternatives like the RTX 6000 Blackwell. Key points include the cost efficiency, high performance benchmarks, and positive community reactions. The discussion highlights praise for the build and humorous acknowledgment of the financial commitment.

---

## 8. [GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)](https://reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/)

**Author:** u/CuriousPlatypus1881 | **Upvotes:** 374 | **Comments:** 89 | **Date:** 2026-01-16

**Summary:** The Reddit post discusses the updated SWE-bench leaderboard results from December 2025, highlighting the performance of various AI models on GitHub PR tasks. Claude Opus 4.5 leads with a 63.3% resolved rate, followed closely by GPT-5.2 (extra high effort) at 61.5%. The post also notes the strong performance of open-source models like GLM-4.7.

**Key Points:**
- Claude Opus 4.5 leads the leaderboard with a 63.3% resolved rate.
- GPT-5.2 (extra high effort) follows closely at 61.5%.
- Gemini 3 Flash Preview outperforms Gemini 3 Pro Preview despite being smaller and cheaper.
- GLM-4.7 is the strongest open-source model, ranking alongside closed models like GPT-5.1-codex.
- GPT-OSS-120B shows significant performance improvement in high-effort reasoning mode.

**Discussion Highlights:** The discussion highlights excitement around the performance of open-source models like GLM-4.7 and anticipation for future releases like DeepSeek v4. Users also appreciate the benchmark's credibility compared to others.

---

## 9. [I fucking love this community](https://reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/)

**Author:** u/alhinai_03 | **Upvotes:** 498 | **Comments:** 54 | **Date:** 2026-01-16

**Summary:** The author expresses gratitude towards the open-source community for enabling them to run large language models on older hardware, highlighting the efficiency of MoE architectures and system memory.

**Key Points:**
- Gratitude towards the open-source community for their contributions
- Running large models on a 10-year-old PC with 4GB VRAM
- Achieving 14-13.5 tokens per second with a 30B parameter model
- Importance of system memory and MoE architecture for performance
- Community appreciation for optimization efforts

**Discussion Highlights:** The community appreciates the author's achievement and emphasizes the importance of system memory and MoE architectures for running large models on limited hardware.

---

## 10. [My story of underestimating /r/LocalLLaMA's thirst for VRAM](https://reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/)

**Author:** u/EmPips | **Upvotes:** 1308 | **Comments:** 88 | **Date:** 2026-01-15

**Summary:** The post highlights the author's underestimation of the r/LocalLLaMA community's demand for VRAM, with discussions including hardware recommendations and humorous commentary.

**Key Points:**
- Author underestimated community's VRAM demand
- Discussion includes hardware recommendations (e.g., 3090s, R9700)
- Gold rush analogy used to describe the situation
- Mention of Discord feature and special flair for the author

**Discussion Highlights:** The discussion features hardware advice, a humorous gold rush analogy, and community engagement through Discord and special flairs.

---

## 11. [Latest upgrade…A100 40 GB](https://reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/)

**Author:** u/inserterikhere | **Upvotes:** 405 | **Comments:** 54 | **Date:** 2026-01-15

**Summary:** The user upgraded their gaming rig to an AI rig by purchasing a faulty A100 GPU for $1000, which worked perfectly upon installation. They previously used a 3090 and a 7950x CPU, and the community reacted positively with some concerns about cooling.

**Key Points:**
- User transitioned from a gaming rig to an AI rig using existing parts.
- Purchased an A100 GPU listed as faulty for $1000, which worked upon installation.
- Community expressed concerns about cooling the A100 GPU.
- Post received positive reception with 404 upvotes and 54 comments.

**Discussion Highlights:** The community celebrated the user's successful upgrade, with some offering technical advice about cooling the A100 GPU. The post was well-received, earning a special flair and being featured on Discord.

---

## 12. [NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency](https://reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/)

**Author:** u/Fear_ltself | **Upvotes:** 714 | **Comments:** 129 | **Date:** 2026-01-14

**Summary:** NVIDIA's Orchestrator-8B is an 8-billion-parameter AI designed to manage and route complex tasks to various tools for greater efficiency. The post discusses its potential as a step towards AGI by integrating different models and tools effectively.

**Key Points:**
- Orchestrator-8B is a specialized 8B model for task management and routing.
- It aims to integrate various tools and models for efficient task handling.
- The post suggests this approach could be a step towards AGI.
- Top comments highlight its role as a 'middle manager' and its potential in agentic frameworks.

**Discussion Highlights:** The discussion highlights the model's role as a 'middle manager' LLM and its potential in creating hierarchical systems of models managing other models. Some comments also mention similar existing frameworks and the future of agentic systems.

---

## 13. [GLM-Image is released!](https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/)

**Author:** u/foldl-li | **Upvotes:** 602 | **Comments:** 83 | **Date:** 2026-01-13

**Summary:** GLM-Image is a new image generation model with a hybrid autoregressive + diffusion decoder architecture. It excels in text-rendering and knowledge-intensive tasks while maintaining high-fidelity image generation capabilities. The model supports various image-to-image tasks and has been released under an MIT license.

**Key Points:**
- Hybrid autoregressive + diffusion decoder architecture
- Excels in text-rendering and knowledge-intensive generation
- Supports image editing, style transfer, and multi-subject consistency
- Released under MIT license
- Model size: 13GB diffusion model + 20GB text encoder

**Discussion Highlights:** The community appreciates the MIT license and the model's capabilities. There is excitement about its performance compared to other models and its potential for various image generation tasks. Some users are waiting for optimized versions for easier use.

---

## 14. [My wishes for 2026](https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/)

**Author:** u/jacek2023 | **Upvotes:** 650 | **Comments:** 179 | **Date:** 2026-01-13

**Summary:** The Reddit post discusses predictions for 2026, focusing on the feasibility of affordable GPUs with more than 32GB of memory. The community engages in a mix of hopeful and skeptical comments about this possibility.

**Key Points:**
- The post asks which predictions for 2026 are likely to happen first.
- A major focus is on the availability of affordable GPUs with >32GB memory.
- Comments range from humorous skepticism to outright dismissal of the idea.
- Some users mention specific AI models like Qwen 4 and Mistral as more realistic expectations.

**Discussion Highlights:** The discussion highlights a consensus of skepticism around the idea of affordable high-memory GPUs in 2026, with many users expressing humor or disbelief. Some comments suggest that advancements in AI models are more plausible than hardware affordability.

---

## 15. [kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop—no GPU required](https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/)

**Author:** u/Nunki08 | **Upvotes:** 398 | **Comments:** 92 | **Date:** 2026-01-13

**Summary:** Kyutai introduced Pocket TTS, a 100M-parameter text-to-speech model with high-quality voice cloning that runs on a laptop without requiring a GPU. The model is available on GitHub and Hugging Face, with a blog post and arXiv paper providing more details.

**Key Points:**
- Pocket TTS is a 100M-parameter TTS model with high-quality voice cloning.
- It runs on a laptop without needing a GPU.
- Available on GitHub, Hugging Face, and detailed in a blog post and arXiv paper.
- Concerns about memory usage during generation.
- Interest in multilingual support and comparisons with other small models.

**Discussion Highlights:** The discussion highlights concerns about memory usage ballooning during generation, interest in finetuning for different languages, and comparisons with other small models. Some users noted that models below a certain size may not be worth the trouble.

---

## 16. [GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/)

**Author:** u/TKGaming_11 | **Upvotes:** 366 | **Comments:** 92 | **Date:** 2026-01-12

**Summary:** The Reddit post highlights a GitHub project by DeepSeek-AI called 'Engram,' which introduces a novel approach to memory in large language models using scalable lookup and n-gram embeddings. The discussion praises the originality of the work and its potential to complement existing sparsity methods.

**Key Points:**
- DeepSeek-AI's 'Engram' project introduces a new memory mechanism for LLMs
- The approach uses n-gram embeddings and scalable lookup for O(1) complexity
- The method is seen as a complementary sparsity axis to existing MoE techniques
- The community appreciates the originality and potential impact of the work
- Comparisons are drawn to biological memory systems in animals and humans

**Discussion Highlights:** The discussion is overwhelmingly positive, with users praising DeepSeek's consistent innovation. Key technical aspects like the n-gram embedding approach and O(1) lookup complexity are highlighted as significant advancements. Some users draw parallels to biological memory systems, suggesting this approach aligns with natural cognitive processes.

---

## 17. [LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)](https://reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 1040 | **Comments:** 114 | **Date:** 2026-01-11

**Summary:** The post introduces TimeCapsuleLLM, an open-source project training language models on 1800s London texts to reduce modern bias. The model, with 1.2B parameters and a 90GB dataset, generates contextually relevant outputs based on historical data.

**Key Points:**
- TimeCapsuleLLM is trained on texts from London between 1800-1875 to minimize modern bias.
- The model has 1.2B parameters and uses a 90GB dataset of historical texts.
- Example outputs show the model's ability to generate historically relevant content, such as arguments against the Roman Catholic Church and unfamiliarity with post-1875 inventions like the telephone.
- Future steps include creating synthetic Q&A pairs using the dataset.
- The project has received positive feedback and recognition in the community.

**Discussion Highlights:** The community has shown strong support for the project, with comments praising its uniqueness and potential. Some users shared similar projects or ideas, indicating a broader interest in historical language models.

---

## 18. [I bought a €9k GH200 “desktop” to save $1.27 on Claude Code (vLLM tuning notes)](https://reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/)

**Author:** u/Reddactor | **Upvotes:** 692 | **Comments:** 178 | **Date:** 2026-01-11

**Summary:** The author built a high-end GH200 desktop system to run Claude Code locally, achieving better performance and cost savings compared to cloud-based solutions. They shared optimized vLLM settings for dual 96GB systems and highlighted the benefits of local model usage. Key points include the €9k investment, performance improvements, and community reactions. The discussion highlights the community's appreciation and humorous comments about the high initial cost.

---

## 19. [It works! Abliteration can reduce slop without training](https://reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/)

**Author:** u/-p-e-w- | **Upvotes:** 401 | **Comments:** 123 | **Date:** 2026-01-11

**Summary:** The post discusses using abliteration to reduce 'slop' (flowery, cliched language) in LLM outputs without training. The author successfully applied this technique to Mistral Nemo, creating a slop-reduced model in 2.5 hours. The method shows promise but has mixed community feedback regarding its effectiveness and impact on output quality.

**Key Points:**
- Abliteration can reduce slop in LLM outputs without finetuning
- Technique applied to Mistral Nemo, creating a slop-reduced model
- Process took 2.5 hours on an A6000 at full precision
- Community feedback is mixed, with some praising the reduction in slop while others find the output too dry
- GGUF versions of the model have been created by community members

**Discussion Highlights:** The community is divided on the effectiveness of the technique. Some appreciate the reduction in slop, while others feel it makes the prose too dry and lacks imagination. There is also interest in whether this technique can be applied to other overused patterns in LLM outputs.

---

## 20. [I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work](https://reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/)

**Author:** u/Ok-Pomegranate1314 | **Upvotes:** 885 | **Comments:** 147 | **Date:** 2026-01-09

**Summary:** The author successfully clustered three NVIDIA DGX Sparks, overcoming networking limitations by writing a custom NCCL plugin. This achievement allows distributed inference across all three nodes at high speeds, pushing the boundaries of what NVIDIA officially supports.

**Key Points:**
- Author clustered three DGX Sparks despite NVIDIA's official support for only two.
- Developed a custom NCCL network plugin (~1500 lines of C) to handle subnet-aware NIC selection and raw RDMA implementation.
- Achieved distributed inference at 8+ GB/s over RDMA, demonstrating significant performance.
- The solution is considered highly advanced, placing the author in NVIDIA's 'How did you even...' support tier.
- The GitHub repository for the plugin is available for further exploration.

**Discussion Highlights:** The community praised the technical achievement, noting the complexity of working with NCCL and the potential impact of this solution. Questions were raised about scalability and performance gains, indicating strong interest in the implementation details.

---

## 21. [The reason why RAM has become so expensive](https://reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/)

**Author:** u/InvadersMustLive | **Upvotes:** 4480 | **Comments:** 377 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses the significant increase in RAM prices, with users noting a rise of up to 10 times compared to previous years. The discussion highlights concerns about market manipulation and monopolization of key resources by major players like OpenAI. Key points include the dramatic price increase, concerns about monopolization, the strategic nature of the price surge, and the potential for a market bubble. The discussion centers around the economic implications of rising RAM prices, with a consensus that major players may be strategically controlling the market.

---

## 22. [DeepSeek V4 Coming](https://reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/)

**Author:** u/External_Mood4719 | **Upvotes:** 498 | **Comments:** 108 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release V4, a next-generation AI model with enhanced code-generation capabilities, outperforming mainstream models like Claude and GPT. The model shows improvements in handling long code prompts and overall reasoning ability.

**Key Points:**
- DeepSeek V4 focuses on strong code-generation capabilities.
- V4 outperforms existing models like Claude and GPT in code generation.
- Improved handling of long code prompts and data pattern understanding.
- Users anticipate more logically rigorous and clear outputs.
- Discussion highlights include praise for DeepSeek's affordability and potential for significant advancements.

**Discussion Highlights:** Users express excitement and anticipation for V4, with some noting DeepSeek's cost-effectiveness and potential for disruption. There is consensus on the model's improved reasoning and reliability, though some speculate on the timeline and technical details of the release.

---

## 23. [(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability](https://reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/)

**Author:** u/Nunki08 | **Upvotes:** 485 | **Comments:** 102 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release a new flagship AI model with strong coding capabilities, generating excitement and discussion in the community.

**Key Points:**
- DeepSeek's upcoming model focuses on strong coding ability
- Community excitement and anticipation for the new model
- Discussion about potential performance and benchmarks
- Mixed reactions including enthusiasm and skepticism
- Community interest in retaining role-playing (RP) abilities

**Discussion Highlights:** The community shows strong interest and excitement, with some expressing enthusiasm for more models and competition, while others are skeptical about performance claims and hope for retained features like role-playing abilities.

---

## 24. [The NO FAKES Act has a "Fingerprinting" Trap that kills Open Source. We need to lobby for a Safe Harbor.](https://reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/)

**Author:** u/PostEasy7183 | **Upvotes:** 614 | **Comments:** 88 | **Date:** 2026-01-08

**Summary:** The NO FAKES Act proposes a 'digital replica right' that could make developers liable for hosting open-source AI models used to create deepfakes, potentially stifling innovation. The post urges lobbying for a 'Safe Harbor' provision to protect open-source developers.

**Key Points:**
- The NO FAKES Act creates liability for developers hosting tools used for digital replicas.
- Developers could face statutory damages of $5k-$25k per violation.
- The post calls for a 'Safe Harbor' provision to protect open-source developers.
- Action items include emailing or calling representatives to oppose the bill.
- Comments highlight concerns about the impact on innovation and the influence of big tech.

**Discussion Highlights:** The discussion highlights concerns about the potential negative impact on innovation and the influence of big tech corporations. There is a consensus that the current language of the bill could stifle open-source development and give a monopoly to large tech companies.

---

## 25. [Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt](https://reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/)

**Author:** u/Prior-Arm-6705 | **Upvotes:** 931 | **Comments:** 147 | **Date:** 2026-01-08

**Summary:** A Reddit user created a compilation video of every instance Jensen Huang said 'AI' during the NVIDIA CES 2025 keynote, totaling 121 times, using open-source tools. The process involved downloading the video, parsing subtitles for timestamps, and editing the clips together.

**Key Points:**
- Jensen Huang said 'AI' 121 times during the CES 2025 keynote.
- The user utilized open-source tools like yt-dlp-mcp and ffmpeg-mcp-lite for video processing.
- The compilation video was created locally without cloud services.
- The result was described as 'hypnotic' and gained significant attention on Reddit.
- Top comments included discussions about the video's popularity, Jensen's influence on tech prices, and his distinctive attire.

**Discussion Highlights:** The discussion highlighted the video's viral nature, with comments praising the technical execution and humorously critiquing Jensen Huang's impact on tech pricing and fashion choices.

---

## 26. [16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/)

**Author:** u/ai-infos | **Upvotes:** 458 | **Comments:** 238 | **Date:** 2026-01-07

**Summary:** The post discusses running Deepseek V3.2 AWQ 4-bit on 16x AMD MI50 32GB GPUs, achieving 10 tokens/sec output and 2000 tokens/sec input with a 69000 context length. The setup aims for cost-effective local AGI, with future plans for a 32-GPU configuration.

**Key Points:**
- Performance: 10 tok/s output, 2000 tok/s input, 69000 context length
- Power draw: 550W idle, 2400W peak inference
- Goal: Cost-effective local AGI without high hardware costs
- Future plans: 32 AMD MI50 setup for Kimi K2 Thinking
- Community appreciation and open-source setup details provided

**Discussion Highlights:** The discussion highlights the setup's popularity, its potential as a cost-effective alternative to CPU hardware, and practical considerations like power usage as heating and noise levels. Users also noted the affordability for professional developers.

---

## 27. [DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.](https://reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/)

**Author:** u/Nunki08 | **Upvotes:** 662 | **Comments:** 54 | **Date:** 2026-01-07

**Summary:** DeepSeek-R1's paper was recently updated, expanding from 22 pages to 86 pages with added details. The update has sparked discussions about potential new architectures and improvements in the model.

**Key Points:**
- DeepSeek-R1's paper expanded from 22 to 86 pages with substantial new details
- Discussion about potential new architectures (e.g., dsv4 + r2)
- Interest in how architectural improvements perform at different model sizes
- Focus on linear attention and cache optimization in current research
- Original paper lacked implementation specifics, which the update may address

**Discussion Highlights:** The community is excited about the expanded paper, with discussions focusing on potential new architectures, improvements in model performance, and the inclusion of more implementation details. There is also interest in how these updates will impact smaller model sizes.

---

## 28. [A 30B Qwen Model Walks Into a Raspberry Pi… and Runs in Real Time](https://reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/)

**Author:** u/ali_byteshape | **Upvotes:** 495 | **Comments:** 78 | **Date:** 2026-01-06

**Summary:** The post discusses running a 30B Qwen model on a Raspberry Pi 5 with real-time performance, achieving 8.03 TPS at 2.70 BPW while retaining 94.18% of BF16 quality. The discussion highlights user feedback, technical challenges, and potential improvements. Key points include the model's performance on Raspberry Pi 5, quality retention, GPU performance dependencies, community feedback on hardware testing, and user-reported adjustments to avoid segfaults. The community provided feedback on performance, shared experiences with different hardware setups, and discussed potential improvements like combining the model with exo solutions for cluster computing.

---

## 29. [Performance improvements in llama.cpp over time](https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/)

**Author:** u/jacek2023 | **Upvotes:** 679 | **Comments:** 85 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses performance improvements in llama.cpp over time, with a focus on NVIDIA GPU performance gains and comparisons with other implementations.

**Key Points:**
- Performance gains are highlighted for NVIDIA GPUs
- NVIDIA's blog post is referenced for further details
- Comparisons are made with other implementations like ik_llama.cpp
- Significant progress is noted in token generation speed

**Discussion Highlights:** The discussion highlights significant progress in token generation speed, with comparisons to other implementations and references to NVIDIA's blog post for further details.

---

## 30. [For the first time in 5 years, Nvidia will not announce any new GPUs at CES — company quashes RTX 50 Super rumors as AI expected to take center stage](https://reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/)

**Author:** u/FullstackSensei | **Upvotes:** 627 | **Comments:** 196 | **Date:** 2026-01-05

**Summary:** Nvidia will not announce new GPUs at CES, focusing instead on AI. The company faces supply issues with high-end GPUs and may reintroduce older models like the RTX 3060. Rising DDR5 and storage prices add to concerns about future upgrades.

**Key Points:**
- No new GPU announcements from Nvidia at CES
- Limited supply of RTX 5070Ti, 5080, and 5090 GPUs
- Rumors of RTX 3060 reintroduction to meet demand
- DDR5 and storage prices are rising significantly
- Concerns about corporate greed and the future of local computing

**Discussion Highlights:** The discussion highlights frustration with corporate greed and the potential decline of local computing. Users express concerns about the lack of new GPU releases and the rising costs of hardware, making future upgrades difficult.

---

## 31. [llama.cpp performance breakthrough for multi-GPU setups](https://reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/)

**Author:** u/Holiday-Injury-9397 | **Upvotes:** 565 | **Comments:** 201 | **Date:** 2026-01-05

**Summary:** The ik_llama.cpp project achieved a 3x to 4x performance improvement in local LLM inference for multi-GPU setups, enabling cost-effective use of multiple low-cost GPUs instead of high-end enterprise cards.

**Key Points:**
- ik_llama.cpp introduced a new execution mode (split mode graph) for maximum utilization of multiple GPUs.
- Performance improvements are significant, with 2x prompt processing speeds even on single GPU or CPU-only setups.
- This breakthrough makes high-performance LLM inference more accessible and cost-effective.
- The project is open-source and details are available on GitHub.
- Users report performance comparable to other optimized frameworks like vllm.

**Discussion Highlights:** The community highlights the cost-effectiveness and accessibility of this breakthrough, with users reporting consistent performance improvements across various setups. There is a consensus on the significance of this development for homelabs, server rooms, and cloud environments.

---

## 32. [Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched](https://reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/)

**Author:** u/ubrtnk | **Upvotes:** 377 | **Comments:** 195 | **Date:** 2026-01-03

**Summary:** The post discusses the challenges faced by local LLMs in processing extreme breaking news events, such as the US attacking Venezuela. The author shares their experience with different LLMs, highlighting how these models initially classified the event as a hoax despite credible sources.

**Key Points:**
- Local LLMs struggled to accept extreme breaking news as real, classifying it as a hoax.
- Different LLMs (Qwen Research, Spark 4.0, GPT-OSS:120B) had varying responses to the news.
- Providing credible sources helped some LLMs acknowledge the event's reality.
- Commenters shared similar experiences with LLMs dismissing unlikely events.
- Discussion highlights the bias and limitations of LLMs in processing unfamiliar geopolitical events.

**Discussion Highlights:** The discussion consensus suggests that LLMs have inherent biases and struggle with processing extreme or unfamiliar events, often requiring credible sources to accept such news as real. Commenters shared similar experiences and expressed curiosity about the future of AI in handling such scenarios.

---

## 33. [LeCun Says Llama 4 results "were fudged a little bit"](https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/)

**Author:** u/MrPecunius | **Upvotes:** 361 | **Comments:** 88 | **Date:** 2026-01-02

**Summary:** Yann LeCun confirmed that Llama 4 benchmarks were manipulated, and Meta's GenAI organization was sidelined, leading to departures and lack of follow-up on promised models. The community expressed disappointment and shared resources for further reading.

**Key Points:**
- LeCun confirms Llama 4 benchmark manipulation
- Meta's GenAI organization was sidelined by Zuckerberg
- Many employees left or are leaving Meta
- Community disappointment over Llama's lack of progress
- Shared resources for further reading (e.g., PDF of the article)

**Discussion Highlights:** The community expressed strong interest in Llama's success and disappointment over its stagnation. Some users shared additional resources, while others critiqued Meta's organizational decisions.

---

## 34. [Qwen-Image-2512](https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/)

**Author:** u/Nunki08 | **Upvotes:** 713 | **Comments:** 122 | **Date:** 2025-12-31

**Summary:** The Reddit post introduces Qwen-Image-2512, a new image generation model, and provides links to guides, GGUF files, and various platforms for accessing the model. The community response is positive, with users sharing their experiences and creative outputs.

**Key Points:**
- Qwen-Image-2512 is a new image generation model.
- Links to guides, GGUF files, and platforms like Hugging Face and ModelScope are provided.
- The model can be tried in Qwen Chat and other demos.
- Users have successfully run the model on low-end hardware.
- Creative use cases and positive feedback are highlighted in the comments.

**Discussion Highlights:** Users appreciate the model's capabilities and accessibility, with some sharing their experiences running it on low-end hardware and generating creative images.

---

## 35. [[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It’s running a raw Llama-7B instance with a 2048 token window.](https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/)

**Author:** u/simar-dmg | **Upvotes:** 744 | **Comments:** 109 | **Date:** 2025-12-30

**Summary:** A user reverse-engineered a Snapchat sextortion bot and discovered it uses a Llama-7B model with a 2048 token window and high temperature setting, making it susceptible to jailbreaks. The bot's responses were erratic and it eventually revealed a malicious link.

**Key Points:**
- The bot uses a Llama-7B model with a 2048 token window and high temperature setting.
- A persona-adoption jailbreak (Grandma Protocol) was used to reveal the bot's configuration.
- The bot's responses were erratic due to its high temperature setting.
- The bot eventually revealed a malicious link it was programmed to hide.
- Scammers are using open-source models to avoid API costs and censorship filters.

**Discussion Highlights:** The discussion highlights skepticism about the bot's responses, with some users suggesting that the details provided by the bot may be hallucinated. There is a consensus that the bot is powered by an LLM, but other details may not be accurate.

---

## 36. [Llama-3.3-8B-Instruct](https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/)

**Author:** u/jacek2023 | **Upvotes:** 466 | **Comments:** 77 | **Date:** 2025-12-29

**Summary:** The post announces the discovery and release of the Llama-3.3-8B-Instruct model, which was previously only accessible via Meta's API. The author managed to download and share the model in GGUF format after navigating Meta's finetuning API.

**Key Points:**
- Llama-3.3-8B-Instruct model is now available in GGUF format.
- The model was previously only accessible through Meta's Llama API.
- The author discovered a way to download the model via Meta's finetuning API.
- The community is verifying the model's authenticity through benchmarks.
- There are discussions about the model's max position embeddings being limited to 8K.

**Discussion Highlights:** The community is excited about the release, with ongoing benchmarks to confirm the model's authenticity. Some users are concerned about the 8K max position embeddings, and there is general appreciation for the author's efforts in making the model available.

---

## 37. [Tencent just released WeDLM 8B Instruct on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 423 | **Comments:** 62 | **Date:** 2025-12-29

**Summary:** Tencent has released WeDLM 8B Instruct, a diffusion language model that outperforms vLLM-optimized Qwen3-8B in math reasoning tasks by running 3-6 times faster. The model is available on Hugging Face under an Apache 2.0 license.

**Key Points:**
- WeDLM 8B Instruct is a diffusion language model released by Tencent.
- It runs 3-6 times faster than vLLM-optimized Qwen3-8B on math reasoning tasks.
- The model is available on Hugging Face with an Apache 2.0 license.
- There is also a 7B version of the model available.
- The community finds the model promising and appreciates its performance and open-source license.

**Discussion Highlights:** The community is excited about the release, highlighting the model's impressive benchmark scores and open-source license. There is consensus on the potential of 7-8B models and a call for more such models.

---

## 38. [NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux](https://reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/)

**Author:** u/HumanDrone8721 | **Upvotes:** 450 | **Comments:** 185 | **Date:** 2025-12-27

**Summary:** NVIDIA has dropped Pascal support on Linux, causing disruptions for Arch Linux users. The community is aware of this change, with some expressing concern and others noting it was expected.

**Key Points:**
- NVIDIA's Linux drivers no longer support Pascal GPUs
- Arch Linux users are particularly affected by this change
- The 24GB P40, a Pascal card, is mentioned as a popular but now expensive option
- Community reactions range from concern to acceptance, with some noting this was expected
- Arch Linux has a history of moving legacy drivers to AUR (Arch User Repository)

**Discussion Highlights:** The discussion highlights a mix of concern and acceptance among users. Some express worry about the impact on their systems, while others note that this change was anticipated and aligns with Arch Linux's practice of moving legacy drivers to AUR. The community seems generally informed and prepared for this transition.

---

## 39. [Best Local LLMs - 2025](https://reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/)

**Author:** u/rm-rf-rm | **Upvotes:** 363 | **Comments:** 197 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses the best local LLMs of 2025, highlighting models like Minimax M2.1 and GLM4.7 as frontier performers. Users share their favorite models and usage details, categorized by application and memory footprint. Key points include the categorization of models by applications like General, Agentic, Creative Writing, and Speciality, and memory footprint categories such as Unlimited (>128GB VRAM), Medium (8-128GB VRAM), and Small (<8GB VRAM). The discussion highlights a debate on the memory footprint categories, with specific model recommendations like Qwen3-4B-instruct and LFM2-8B-A1B praised for their performance.

---

## 40. [NVIDIA has 72GB VRAM version now](https://reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/)

**Author:** u/decentralize999 | **Upvotes:** 460 | **Comments:** 148 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses NVIDIA's new 72GB VRAM version, questioning the cost of 96GB and the AI community's interest in 48GB. The discussion includes pricing details and community opinions on the need for larger VRAM versions.

**Key Points:**
- NVIDIA has released a 72GB VRAM version.
- Pricing details for RTX 5000 48GB, 72GB, and RTX 6000 96GB are provided.
- Community suggests a need for 128GB or larger versions.
- Price per gig remains consistent across versions.
- Community opinions vary on the value of different VRAM sizes.

**Discussion Highlights:** The discussion highlights a consensus on the need for larger VRAM versions, with some users emphasizing the consistent price per gig and others expressing interest in future releases like the 5090 with 48GB.

---

## 41. [Hard lesson learned after a year of running large models locally](https://reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/)

**Author:** u/inboundmage | **Upvotes:** 351 | **Comments:** 146 | **Date:** 2025-12-26

**Summary:** The author shares their experience running large language models locally, highlighting challenges with VRAM limitations, model scaling, and performance trade-offs. They discuss the viability of local inference for smaller models but note significant hurdles for larger models without high-end hardware.

**Key Points:**
- Running large models locally is feasible for smaller models but faces hard limits with larger models due to VRAM constraints.
- VRAM fragmentation and inefficient offloading to system RAM are significant issues when working with larger models.
- Quantization helps but introduces trade-offs in quality and new bugs.
- Cloud-based solutions offer better performance for fast iteration, but local setups are preferred for privacy-sensitive tasks.
- Community suggestions include using llama.cpp for CPU offloading and considering additional GPUs for better performance.

**Discussion Highlights:** The discussion highlights practical solutions like using llama.cpp for CPU offloading and suggests hardware upgrades such as adding more GPUs. There is a consensus that while local inference is viable for smaller models, larger models require significant hardware investments or alternative approaches.

---

## 42. [I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA](https://reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/)

**Author:** u/CeFurkan | **Upvotes:** 1034 | **Comments:** 182 | **Date:** 2025-12-25

**Summary:** The post discusses the potential of GPU VRAM upgrade modifications to challenge NVIDIA's monopoly, highlighting their availability and popularity in China. Users share experiences and pricing details of these modified GPUs.

**Key Points:**
- GPU VRAM upgrade modifications are seen as a way to counter NVIDIA's monopoly.
- These modifications are already mainstream in China, with Alibaba offering upgraded versions of various GPUs.
- Pricing ranges from $300 for a 2080Ti 22GB to $4000 for a 5090 96GB.
- Users report successful usage of modified GPUs, such as a 4090 with 48GB of memory.
- There is interest in the cost-effectiveness and performance of these modifications.

**Discussion Highlights:** The discussion highlights the availability and success of GPU VRAM modifications in China, with users expressing interest in their cost-effectiveness and performance. There is a consensus on the potential of these modifications to disrupt NVIDIA's monopoly.

---

## 43. [Why I quit using Ollama](https://reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/)

**Author:** u/SoLoFaRaDi | **Upvotes:** 487 | **Comments:** 202 | **Date:** 2025-12-25

**Summary:** The Reddit post discusses the author's decision to stop using Ollama due to a perceived shift away from its original purpose of providing a secure platform for local AI models, citing concerns over recent updates and the introduction of cloud-based models. The discussion highlights a consensus among users favoring alternatives like llama.cpp and LM Studio. Key points include the author's dissatisfaction with Ollama's recent updates, concerns over privacy implications and bloatware, user preference for alternatives like llama.cpp and LM Studio, discussion consensus favoring llama.cpp for its efficiency and recent improvements, and mention of LM Studio as a viable alternative to Ollama. The discussion reflects a general consensus that alternatives like llama.cpp and LM Studio are preferred over Ollama due to their focus on local model inference and recent improvements in functionality.

---

## 44. [Exclusive: Nvidia buying AI chip startup Groq's assets for about $20 billion in largest deal on record](https://reddit.com/r/LocalLLaMA/comments/1puyq9r/exclusive_nvidia_buying_ai_chip_startup_groqs/)

**Author:** u/fallingdowndizzyvr | **Upvotes:** 673 | **Comments:** 157 | **Date:** 2025-12-24

**Summary:** Nvidia is acquiring AI chip startup Groq's assets for approximately $20 billion, marking the largest deal on record. The acquisition has sparked discussions about market competition and consolidation in the AI industry.

**Key Points:**
- Nvidia is buying Groq's assets for about $20 billion
- The deal is the largest on record
- Discussions highlight concerns about market consolidation
- Some commenters question Groq's valuation
- The acquisition is seen as a strategic move by Nvidia

**Discussion Highlights:** The discussion highlights a mix of opinions, with some seeing the acquisition as beneficial for market competition, while others express concerns about further consolidation in the AI industry. There is also skepticism about Groq's valuation and the nature of the deal.

---

## 45. [We asked OSS-120B and GLM 4.6 to play 1,408 Civilization V games from the Stone Age into the future. Here's what we found.](https://reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/)

**Author:** u/vox-deorum | **Upvotes:** 654 | **Comments:** 174 | **Date:** 2025-12-24

**Summary:** Researchers used open-source LLMs (GPT-OSS-120B and GLM-4.6) to play 1,408 full games of Civilization V with a hybrid approach, achieving survival rates comparable to the in-game AI. The LLMs developed distinct playstyles, with OSS-120B favoring warmonger strategies and GLM-4.6 adopting a more balanced approach. The cost per game was approximately $0.86, with input tokens scaling linearly as the game progressed. Key points include: LLMs can now play full Civilization V games end-to-end using a hybrid approach; OSS-120B and GLM-4.6 developed different playstyles: warmonger vs. balanced; Both models preferred the Order ideology over Freedom; Cost per game was around $0.86 with linear scaling of input tokens; LLMs achieved survival rates similar to the in-game AI (~97.5% vs. ~97.3%). The discussion highlights enthusiasm for integrating LLMs into multiplayer games and curiosity about the potential of smaller models. Comments also reflect interest in the broader implications of AI in gaming and the uniqueness of the approach.

---

## 46. [AMA With Z.AI, The Lab Behind GLM-4.7](https://reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/)

**Author:** u/zixuanlimit | **Upvotes:** 594 | **Comments:** 417 | **Date:** 2025-12-23

**Summary:** The post announces an AMA session with Z.AI, the research lab behind GLM-4.7, featuring several team members. The session aims to answer community questions directly and will run from 8 AM to 11 AM PST, with follow-ups over the next 48 hours.

**Key Points:**
- AMA session with Z.AI team members
- Focus on answering community questions about GLM-4.7
- Session duration: 8 AM – 11 AM PST with 48-hour follow-up
- Top comments include questions about future releases, censorship concerns, training challenges, and creative writing instruction sets
- High engagement with 594 upvotes and 417 comments

**Discussion Highlights:** The discussion highlights include questions about future releases (e.g., 'when Air?'), concerns over potential censorship, inquiries about training challenges, and interest in creative writing instruction sets. The community shows strong engagement and curiosity about the development and future directions of GLM-4.7.

---

## 47. [DGX Spark: an unpopular opinion](https://reddit.com/r/LocalLLaMA/comments/1ptdtmz/dgx_spark_an_unpopular_opinion/)

**Author:** u/emdblc | **Upvotes:** 746 | **Comments:** 223 | **Date:** 2025-12-22

**Summary:** The author, a doctoral student, shares their positive experience with the DGX Spark, highlighting its benefits for small research groups with limited resources. They emphasize its all-in-one design and massive memory, which enable them to compete with groups having access to high-performance GPUs.

**Key Points:**
- DGX Spark is beneficial for small research groups with limited funding and resources.
- It enables prototyping and training of foundation models, competing with high-performance GPU groups.
- The Spark is not faster than high-end GPUs like H100s but offers a large amount of VRAM in an all-in-one design.
- The intended use case for the Spark is acknowledged by the community, though it is slower than some consumer GPUs.
- The post received significant engagement, indicating broad agreement with the author's perspective.

**Discussion Highlights:** The discussion highlights broad agreement with the author's perspective, acknowledging the Spark's intended use case for small research groups. Some comments note its limitations in speed compared to other GPUs but appreciate its VRAM capacity and power efficiency.

---

## 48. [GLM 4.7 is out on HF!](https://reddit.com/r/LocalLLaMA/comments/1pt5heq/glm_47_is_out_on_hf/)

**Author:** u/KvAk_AKPlaysYT | **Upvotes:** 593 | **Comments:** 123 | **Date:** 2025-12-22

**Summary:** The Reddit post announces the release of GLM 4.7 on Hugging Face, garnering significant attention with 593 upvotes and 123 comments. The community appreciates the contribution, with the author receiving a special flair.

**Key Points:**
- GLM 4.7 is now available on Hugging Face
- Post received 593 upvotes and 123 comments
- Author recognized with a special flair for their contribution
- Community compares GLM 4.7 with other models like Minimax and Gemma 4
- Unique feature: diagrams in the reasoning/planning stage

**Discussion Highlights:** The community is engaged and appreciative, with discussions highlighting comparisons to other models and noting unique features like diagrams in the reasoning stage. There is also anticipation for other model releases like Gemma 4.

---

## 49. [I made Soprano-80M: Stream ultra-realistic TTS in &lt;15ms, up to 2000x realtime, and &lt;1 GB VRAM, released under Apache 2.0!](https://reddit.com/r/LocalLLaMA/comments/1pt3sco/i_made_soprano80m_stream_ultrarealistic_tts_in/)

**Author:** u/eugenekwek | **Upvotes:** 649 | **Comments:** 104 | **Date:** 2025-12-22

**Summary:** Eugene introduces Soprano-80M, a state-of-the-art TTS model designed for ultra-low latency and high-speed audio generation, achieving <15ms latency and up to 2000x realtime performance. The model uses a 32 kHz sample rate and a vocoder-based decoder for superior audio quality and speed.

**Key Points:**
- Soprano-80M achieves <15ms latency and up to 2000x realtime performance.
- Uses a 32 kHz sample rate for clearer audio.
- Employs a vocoder-based decoder for faster audio generation.
- Can generate a 10-hour audiobook in under 20 seconds.
- Released under Apache 2.0 license.

**Discussion Highlights:** Users praised the model's speed and performance, with one user noting it spends minimal time on GPU before generating long audio outputs quickly. There were inquiries about finetuning code and hardware specifications used for benchmarking.

---

## 50. [major open-source releases this year](https://reddit.com/r/LocalLLaMA/comments/1pstlas/major_opensource_releases_this_year/)

**Author:** u/sahilypatel | **Upvotes:** 700 | **Comments:** 100 | **Date:** 2025-12-22

**Summary:** The Reddit post discusses major open-source releases this year, highlighting the dominance of China in the open-source space and high expectations for future releases like Deepseek. The post has gained significant popularity and sparked discussions on the best small-sized models.

**Key Points:**
- The post is popular and featured on Discord
- China is dominating the open-source space
- High expectations for future releases like Deepseek
- Discussion on the best small-sized model, Mistral

**Discussion Highlights:** The discussion highlights the popularity of the post, the dominance of China in open-source, high expectations for future releases, and opinions on the best small-sized model.

---

