# r/LocalLLaMA Reading Digest

**Period:** 2026-01-22 to 2026-01-22
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [Qwen dev on Twitter!!](https://reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 407 | **Comments:** 57 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses Qwen's TTS model, which is linked to a vLLM leak. The community is engaged in verifying its authenticity and sharing relevant resources.

**Key Points:**
- Qwen's TTS model is mentioned
- Model is associated with a vLLM leak
- Hugging Face collection link provided
- Community is verifying the model's legitimacy

**Discussion Highlights:** The discussion revolves around the authenticity of the TTS model, with some users providing links to resources and others expressing skepticism.

---

## 2. [You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?](https://reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/)

**Author:** u/Adventurous-Gold6413 | **Upvotes:** 525 | **Comments:** 289 | **Date:** 2026-01-20

**Summary:** The post asks for recommendations on local models to use with 64GB RAM and 16GB VRAM when internet is permanently shut off. The community suggests several models, with a focus on performance and versatility.

**Key Points:**
- The post is about selecting local models for offline use with specific hardware constraints.
- Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B are recommended models.
- GPT-OSS-120B is highlighted for its performance and versatility.
- The community appreciates the contribution and engagement in the discussion.

**Discussion Highlights:** The discussion highlights the importance of model performance and versatility, with specific recommendations for Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B. The community shows appreciation for the post and engages actively in the discussion.

---

## 3. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 850 | **Comments:** 253 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, fully enclosed 10-GPU mobile AI system designed for running large MoE models, video generation, and high-detail image generation. The system features a Threadripper Pro 3995WX, 512GB DDR4, 8x 3090 and 2x 5090 GPUs, and dual PSUs, all within a Thermaltake Core W200 case. The build aimed to balance performance and cost while ensuring mobility and protection from pets.

**Key Points:**
- The system is designed for large MoE models, video generation, and image generation.
- It features high-end hardware including a Threadripper Pro 3995WX, 512GB DDR4, and 10 GPUs (8x 3090 + 2x 5090).
- The build prioritizes mobility, enclosure for protection, and cost-effectiveness.
- The total cost was approximately $17k, with a focus on avoiding unnecessary expenses.
- The enclosure was a significant challenge, with mining frames ruled out due to aesthetic and structural concerns.

**Discussion Highlights:** The community reacted positively, with comments highlighting the system's power and humorously noting its portability. Key comments included jokes about plugging it into a McDonald's socket, admiration for the build's capabilities, and curiosity about the GPU arrangement.

---

## 4. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 358 | **Comments:** 60 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the official support for GLM 4.7 Flash in llama.cpp, highlighting its community-driven development and performance improvements. Users discuss its efficiency and share additional resources. Key points include: GLM 4.7 Flash now officially supported in llama.cpp, support is community-driven, performance improvements noted, additional resources and model versions shared by community members, and the post recognized with special flair for its contribution. The discussion highlights the community effort behind the integration and shares mixed experiences with performance, with some users finding better results without flash-attention. Additional model versions and resources are also shared.

---

## 5. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 456 | **Comments:** 160 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights the effectiveness of GLM 4.7 Flash as a reliable local agent, noting its ability to handle extensive tasks without errors. Users express enthusiasm for its performance and anticipate local availability via GGUFs.

**Key Points:**
- GLM 4.7 Flash is praised for its reliability and performance in agentic tasks.
- The model has been tested extensively, producing hundreds of thousands of tokens without errors.
- Users are eager for GGUFs to try the model locally.
- Comparisons with other models like Nemotron 30B and Qwen3 are discussed.
- The model's performance is noted to be comparable to SEED OSS 36B but with better efficiency.

**Discussion Highlights:** The discussion highlights enthusiasm for GLM 4.7 Flash's performance and reliability. Users compare it favorably to other models and express anticipation for local testing via GGUFs. Some users note its deep thinking capabilities and decent speed on high-end GPUs.

---

## 6. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 730 | **Comments:** 227 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the release of zai-org/GLM-4.7-Flash model, generating significant community interest with 730 upvotes and 227 comments. The discussion highlights technical features and enthusiasm for the model's capabilities.

**Key Points:**
- GLM-4.7-Flash model release announced
- Uses MLA architecture for efficient KV cache memory usage
- Supports full 200k context length
- Community excitement about 30B model size
- Special recognition for the post's popularity

**Discussion Highlights:** The community shows strong enthusiasm for the new model, particularly noting its memory efficiency and context length capabilities. There's nostalgia for larger models (70B) while appreciating the current 30B offering. The post gained special recognition for its popularity in the community.

---

## 7. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 346 | **Comments:** 91 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to stay within a ~10,000€ budget. The system is designed for running large language models (120B+ parameters) locally, with benchmark results provided for various models. The post gained significant attention in the r/LocalLLaMA community. Key points include maximizing VRAM for local model inference, a total cost of ~9,800€ with a 50% subsidy, benchmark results for models ranging from 8B to 230B parameters, and strong community interest with 345 upvotes and 91 comments. The discussion highlights include curiosity about hardware sourcing and comparisons with similar builds.

---

## 8. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 452 | **Comments:** 71 | **Date:** 2026-01-17

**Summary:** The lead developer of Qwen has indicated a slowdown in development to focus on quality, which has been positively received by the community. The statement has sparked discussions about the future of Qwen 4 and the importance of meaningful improvements.

**Key Points:**
- Qwen development is slowing down to focus on quality
- Community appreciates the focus on quality over quantity
- Uncertainty about whether the statement refers to Qwen 4
- Positive reception to taking time for meaningful improvements

**Discussion Highlights:** The community supports the decision to prioritize quality, with some cautioning against over-interpreting the statement. There is a general consensus that meaningful improvements are more valuable than frequent incremental updates.

---

## 9. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 532 | **Comments:** 116 | **Date:** 2026-01-17

**Summary:** The author transitioned from MI100 GPUs to R9700 GPUs for better performance and cost efficiency, detailing a high-end server build with 128GB VRAM and 128GB RAM. The build cost less than an RTX 6000 Blackwell and includes performance benchmarks for the setup. Key points include the transition from MI100 to R9700 GPUs, detailed hardware specifications, performance benchmarks, and positive community reception. The community responded positively to the build, with top comments praising the setup and joking about the financial implications of such high-end hardware.

---

## 10. [Best "End of world" model that will run on 24gb VRAM](https://reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/)

**Author:** u/gggghhhhiiiijklmnop | **Upvotes:** 339 | **Comments:** 177 | **Date:** 2026-01-17

**Summary:** The post discusses finding the best LLM model to run on a PC with 24GB VRAM and 64GB RAM for an 'end of world' scenario, focusing on hoarding data like Wikipedia and other educational resources.

**Key Points:**
- User is looking for LLM models that fit within 24GB VRAM and 64GB RAM.
- Suggestions include saving the best LLM possible and running it off SSD if necessary.
- Specific model recommendations include gemma3:27b and Midnight Miku.
- Importance of downloading actual Wikipedia backups for offline access.

**Discussion Highlights:** The discussion highlights the importance of having a robust LLM model and offline data backups. There is a consensus on the utility of models like gemma3:27b and the necessity of downloading comprehensive data backups like Wikipedia.

---

## 11. [GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)](https://reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/)

**Author:** u/CuriousPlatypus1881 | **Upvotes:** 379 | **Comments:** 89 | **Date:** 2026-01-16

**Summary:** The post discusses the updated SWE-bench leaderboard results from December 2025, highlighting the performance of various AI models on GitHub PR tasks. Claude Opus 4.5 leads with a 63.3% resolved rate, followed closely by GPT-5.2 (extra high effort) at 61.5%. GLM-4.7 is noted as the strongest open-source model, performing comparably to closed models like GPT-5.1-codex.

**Key Points:**
- Claude Opus 4.5 leads the leaderboard with a 63.3% resolved rate.
- GPT-5.2 (extra high effort) follows closely at 61.5%.
- GLM-4.7 is the strongest open-source model, ranking alongside closed models.
- Gemini 3 Flash Preview outperforms Gemini 3 Pro Preview despite being smaller and cheaper.
- GPT-OSS-120B shows significant performance improvement in high-effort reasoning mode.

**Discussion Highlights:** The discussion highlights excitement over GLM-4.7's performance as an open-source model and the surprising effectiveness of Gemini Flash. There is also anticipation for future releases like DeepSeek v4.

---

## 12. [I fucking love this community](https://reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/)

**Author:** u/alhinai_03 | **Upvotes:** 517 | **Comments:** 54 | **Date:** 2026-01-16

**Summary:** The author expresses gratitude towards the open-source community for enabling them to run large language models on older hardware, highlighting the performance of the nemotron-3-nano-30B-a3b-iq4_nl model on a 10-year-old PC with limited VRAM.

**Key Points:**
- Author appreciates the open-source community for their contributions.
- Achieved 14-13.5 tokens per second on a 10-year-old PC with 4GB VRAM.
- Key factors for performance include sufficient system memory and MoE architecture.
- Community optimization efforts are highly praised.
- Discussion highlights the practicality of system RAM and MoE combo.

**Discussion Highlights:** The community agrees on the effectiveness of system RAM and MoE architecture for running large models on older hardware, with many praising the optimization efforts of the community.

---

## 13. [My story of underestimating /r/LocalLLaMA's thirst for VRAM](https://reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/)

**Author:** u/EmPips | **Upvotes:** 1333 | **Comments:** 91 | **Date:** 2026-01-15

**Summary:** The post highlights the author's underestimation of the r/LocalLLaMA community's demand for VRAM, with discussions focusing on hardware recommendations and market dynamics.

**Key Points:**
- Author underestimated VRAM demand in the community
- Discussion includes hardware recommendations (e.g., 3090s or R9700)
- Reference to California gold rush as a humorous analogy
- Mention of Discord feature and special flair for the author

**Discussion Highlights:** The discussion revolves around hardware recommendations and market dynamics, with a humorous analogy to the California gold rush and mentions of community engagement on Discord.

---

## 14. [Latest upgrade…A100 40 GB](https://reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/)

**Author:** u/inserterikhere | **Upvotes:** 406 | **Comments:** 54 | **Date:** 2026-01-15

**Summary:** The user upgraded their gaming rig to an AI rig by purchasing a faulty A100 GPU for $1000, which worked perfectly upon installation, allowing them to run and train larger models. The community reacted positively, with some expressing concerns about cooling the A100.

**Key Points:**
- User transitioned from a gaming rig to an AI rig using existing parts.
- Purchased a faulty A100 GPU for $1000, which worked upon installation.
- Community expressed concerns about cooling the A100.
- Post received positive reception with 405 upvotes and 54 comments.

**Discussion Highlights:** The community praised the user's upgrade and offered technical advice, particularly regarding cooling solutions for the A100 GPU.

---

## 15. [Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M](https://reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/)

**Author:** u/eugenekwek | **Upvotes:** 327 | **Comments:** 54 | **Date:** 2026-01-14

**Summary:** The post announces Soprano 1.1, an improved version of the Soprano TTS model with significant reductions in hallucinations and audio artifacts, along with enhanced stability and support for longer sentences. The community response is overwhelmingly positive, highlighting the model's usability and performance.

**Key Points:**
- Soprano 1.1 reduces hallucinations by 95% and lowers WER by 50% compared to Soprano-80M.
- The model now supports sentences up to 30 seconds long, up from 15 seconds.
- Audio artifacts and high-frequency noise have been reduced through further training.
- Community feedback is positive, with users impressed by the model's performance for its size.
- Inquiries about ONNX support and suggestions for improving consistency in handling em-dashes.

**Discussion Highlights:** The community response is highly positive, with users expressing admiration for the model's performance and usability. There is interest in additional features like ONNX support and suggestions for further improvements in text handling.

---

## 16. [NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency](https://reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/)

**Author:** u/Fear_ltself | **Upvotes:** 715 | **Comments:** 130 | **Date:** 2026-01-14

**Summary:** NVIDIA's Orchestrator-8B is an 8-billion-parameter AI designed to manage and route complex tasks to various tools for greater efficiency, sparking discussions about AGI and functional AI systems.

**Key Points:**
- Orchestrator-8B is a specialized AI for task management and routing.
- The model is seen as a step towards functional AI systems.
- Discussions compare it to middle management and existing agentic frameworks.
- Some view it as a progression towards AGI by integrating separate components.

**Discussion Highlights:** The discussion highlights a consensus that integrating specialized models and tools is a promising approach towards more functional AI systems, with comparisons to management roles and existing frameworks.

---

## 17. [GLM-Image is released!](https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/)

**Author:** u/foldl-li | **Upvotes:** 602 | **Comments:** 83 | **Date:** 2026-01-13

**Summary:** GLM-Image is a new image generation model with a hybrid autoregressive + diffusion decoder architecture. It excels in text-rendering and knowledge-intensive tasks while supporting various image-to-image tasks.

**Key Points:**
- Hybrid autoregressive + diffusion decoder architecture
- Excels in text-rendering and knowledge-intensive generation
- Supports image editing, style transfer, and multi-subject consistency
- MIT license with no restrictions
- Model size: 13GB diffusion model + 20GB text encoder

**Discussion Highlights:** The community appreciates the MIT license and the model's capabilities. There is interest in quantizing the model for easier use and discussions about its performance compared to other models.

---

## 18. [My wishes for 2026](https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/)

**Author:** u/jacek2023 | **Upvotes:** 656 | **Comments:** 179 | **Date:** 2026-01-13

**Summary:** The Reddit post discusses predictions for 2026, focusing on the possibility of affordable GPUs with more than 32GB memory. The community engages in a mix of hopeful and skeptical comments about this prospect.

**Key Points:**
- The post asks which predictions for 2026 are likely to happen first.
- A top comment humorously doubts the possibility of affordable GPUs with >32GB memory.
- Other comments joke about the feasibility of such predictions.
- There is mention of specific AI models like Qwen 4 and Mistral as more realistic expectations.

**Discussion Highlights:** The discussion is marked by a mix of humor and skepticism regarding the feasibility of affordable high-memory GPUs in 2026. Some users joke about the idea, while others mention more realistic expectations like advancements in AI models.

---

## 19. [kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop—no GPU required](https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/)

**Author:** u/Nunki08 | **Upvotes:** 401 | **Comments:** 93 | **Date:** 2026-01-13

**Summary:** Kyutai introduced Pocket TTS, a 100M-parameter text-to-speech model with high-quality voice cloning that runs on a laptop without requiring a GPU. The model is available on GitHub and Hugging Face, with a blog post and research paper providing more details.

**Key Points:**
- Pocket TTS is a lightweight (100M parameters) TTS model capable of high-quality voice cloning.
- It runs on a laptop without needing a GPU.
- The model is open-source and available on GitHub and Hugging Face.
- Users have raised concerns about memory usage during generation.
- There is interest in finetuning the model for different languages.

**Discussion Highlights:** The discussion highlights concerns about memory usage during generation, with one user reporting it ballooning to 32 GB. There is also interest in multilingual support and comparisons with other small TTS models.

---

## 20. [GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/)

**Author:** u/TKGaming_11 | **Upvotes:** 366 | **Comments:** 93 | **Date:** 2026-01-12

**Summary:** The Reddit post highlights a GitHub repository by DeepSeek-AI introducing 'Engram,' a novel method for conditional memory in large language models using scalable lookup. The discussion praises the innovation and technical approach.

**Key Points:**
- DeepSeek-AI's 'Engram' introduces conditional memory via scalable lookup
- The method uses n-gram embedding for static memory, complementing neural computation
- The approach is praised for its originality and potential impact
- Discussion notes the use of mHC (M=4) for ablations, indicating derisking efforts

**Discussion Highlights:** The community consensus highlights the innovation of the n-gram embedding approach and its potential as a complementary sparsity axis. Users appreciate DeepSeek's consistent delivery of original ideas and technical rigor.

---

## 21. [LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)](https://reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 1048 | **Comments:** 114 | **Date:** 2026-01-11

**Summary:** The post introduces TimeCapsuleLLM, a 1.2B parameter language model trained exclusively on 1800-1875 London texts to minimize modern bias. The model demonstrates period-specific knowledge and limitations, such as unfamiliarity with post-1875 concepts like telephones.

**Key Points:**
- Model trained on 90GB of 1800-1875 London texts with no modern data or fine-tuning
- Shows period-specific behaviors like anti-Catholic arguments reflecting 1829 Catholic Emancipation Act
- Unfamiliar with post-1875 concepts (e.g., treats 'telephone' as unknown diplomatic device)
- Next steps include creating synthetic Q&A pairs from the dataset
- Community shows strong interest with 1048 upvotes and positive feedback

**Discussion Highlights:** The community expressed enthusiasm for the project's historical focus, with top comments praising the unique approach and sharing similar interests in period-specific language models. Some humorous remarks played on the model's 1875 knowledge cutoff.

---

## 22. [I bought a €9k GH200 “desktop” to save $1.27 on Claude Code (vLLM tuning notes)](https://reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/)

**Author:** u/Reddactor | **Upvotes:** 688 | **Comments:** 177 | **Date:** 2026-01-11

**Summary:** The author built a €9k GH200 'desktop' to run Claude Code locally, achieving better speeds and results than the cloud version. They shared optimized vLLM settings for dual 96GB systems and highlighted the cost and performance benefits of local execution.

**Key Points:**
- Author spent €9k on a GH200 setup to run Claude Code locally
- Achieved better speeds and results compared to cloud-based Claude Code
- Shared optimized vLLM settings for dual 96GB systems
- Highlighted cost and performance benefits of local execution
- Community reactions included humor about cost and appreciation for the setup

**Discussion Highlights:** The community reacted with humor about the cost, appreciation for the setup, and discussions about the technical details and model specifications.

---

## 23. [It works! Abliteration can reduce slop without training](https://reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/)

**Author:** u/-p-e-w- | **Upvotes:** 400 | **Comments:** 129 | **Date:** 2026-01-11

**Summary:** The Reddit post discusses the use of abliteration to reduce 'slop' (flowery, cliched language) in LLM outputs without training. The author successfully applied this technique to the Mistral Nemo model, creating a slop-reduced version using Heretic, a tool originally designed for censorship removal. Key points include the effectiveness of abliteration in reducing slop without finetuning, the application to the Mistral Nemo model, the process duration and optimization potential, the comparison of outputs, and community discussions on effectiveness and limitations. The discussion highlights mixed opinions on the technique's effectiveness, with some appreciating the reduction in flowery language while others find the prose too dry, and interest in broader applications and semantic impact.

---

## 24. [I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work](https://reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/)

**Author:** u/Ok-Pomegranate1314 | **Upvotes:** 888 | **Comments:** 147 | **Date:** 2026-01-09

**Summary:** The user successfully clustered 3 DGX Sparks, overcoming NVIDIA's limitations by writing a custom NCCL network plugin, achieving distributed inference at 8+ GB/s over RDMA.

**Key Points:**
- Clustered 3 DGX Sparks despite NVIDIA's official support for only 2.
- Developed a custom NCCL network plugin (~1500 lines of C) to handle subnet-aware NIC selection and RDMA implementation.
- Achieved distributed inference at 8+ GB/s over RDMA.
- Community praised the technical achievement and its potential impact.
- Discussion focused on scalability and performance improvements.

**Discussion Highlights:** The community highlighted the technical difficulty and potential significance of the achievement, with discussions focusing on scalability, performance gains, and general applicability to DGX Spark clusters.

---

## 25. [The reason why RAM has become so expensive](https://reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/)

**Author:** u/InvadersMustLive | **Upvotes:** 4519 | **Comments:** 379 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses the significant increase in RAM prices, with comments suggesting strategic monopolization by certain entities to control future demand and economic viability of competitors.

**Key Points:**
- RAM prices have increased dramatically, with some users reporting a 10x increase.
- There is speculation about monopolization of RAM resources to control future demand.
- The economic viability of competing AI data centers, particularly in China, is being affected.
- The price surge is not seen as a temporary bubble by some commentators.

**Discussion Highlights:** The discussion highlights concerns about monopolistic practices in the RAM market, with users pointing out the economic impact on competitors and the potential long-term implications of controlled resource availability.

---

## 26. [DeepSeek V4 Coming](https://reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/)

**Author:** u/External_Mood4719 | **Upvotes:** 499 | **Comments:** 108 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release V4, a next-generation AI model with enhanced code-generation capabilities, outperforming mainstream models like Claude and GPT. The model features improvements in handling long code prompts and data pattern understanding, with stronger reasoning and reliability.

**Key Points:**
- DeepSeek V4 focuses on strong code-generation capabilities.
- V4 outperforms existing models like Claude and GPT in code generation.
- Improved handling of long code prompts and data patterns.
- Users anticipate V4 to be more logically rigorous and reliable.
- Community discussion highlights excitement and expectations for V4's performance.

**Discussion Highlights:** The community is enthusiastic about DeepSeek V4, with users praising its potential for improved reasoning and cost-effectiveness. Some anticipate significant advancements, while others speculate on integration with additional technologies like mHC and OCR for enhanced capabilities.

---

## 27. [(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability](https://reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/)

**Author:** u/Nunki08 | **Upvotes:** 492 | **Comments:** 102 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release a new flagship AI model with strong coding capabilities, generating significant interest and discussion in the community.

**Key Points:**
- DeepSeek's upcoming model emphasizes strong coding abilities.
- The announcement has sparked excitement and anticipation among users.
- Community members express enthusiasm for more AI models and competition in the field.
- Some users are skeptical about performance claims based on internal benchmarks.
- There is a desire for the model to maintain role-playing (RP) capabilities.

**Discussion Highlights:** The community shows a mix of excitement and skepticism, with a general consensus that more competition and options in AI models are beneficial. Some users humorously reference competitive dynamics with phrases like 'OpenAI code red 2.0 loading...' and 'LFG BIG WHALE.'

---

## 28. [The NO FAKES Act has a "Fingerprinting" Trap that kills Open Source. We need to lobby for a Safe Harbor.](https://reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/)

**Author:** u/PostEasy7183 | **Upvotes:** 614 | **Comments:** 88 | **Date:** 2026-01-08

**Summary:** The NO FAKES Act introduces a 'digital replica right' that could hold developers liable for hosting open-source AI models used to create deepfakes, potentially stifling innovation. The post urges the community to lobby for a 'Safe Harbor' provision to protect open-source developers.

**Key Points:**
- The NO FAKES Act targets developers who 'make available' tools primarily used for creating digital replicas, imposing statutory damages.
- Open-source AI model hosting could become legally risky without a Safe Harbor provision.
- The community is encouraged to contact their representatives to advocate for amendments protecting open-source developers.
- Critics argue that the bill could hand a monopoly to Big Tech by making open-source hosting legally untenable.
- The discussion highlights concerns about the technical literacy of politicians and the potential impact on innovation.

**Discussion Highlights:** The discussion emphasizes the need for a Safe Harbor provision to protect open-source developers and critiques the potential monopolistic impact of the bill. There is also skepticism about politicians' understanding of technology and its implications.

---

## 29. [Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt](https://reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/)

**Author:** u/Prior-Arm-6705 | **Upvotes:** 931 | **Comments:** 147 | **Date:** 2026-01-08

**Summary:** A Reddit user created a compilation video of every instance Jensen Huang said 'AI' during NVIDIA's CES 2025 keynote, totaling 121 times. The process involved using open-source tools to download, parse, and edit the video automatically with a single prompt.

**Key Points:**
- Jensen Huang said 'AI' 121 times during the CES 2025 keynote.
- The user automated the video compilation using open-source tools like Dive, yt-dlp-mcp, and ffmpeg-mcp-lite.
- The process was fully local, requiring no cloud services.
- The result was described as 'hypnotic' and gained significant attention on Reddit.
- Top comments included humor, criticism of pricing, and praise for the technical execution.

**Discussion Highlights:** The discussion highlighted a mix of humor, technical appreciation, and criticism. Users found the compilation amusing and technically impressive, while some commented on NVIDIA's pricing and Jensen Huang's attire.

---

## 30. [16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/)

**Author:** u/ai-infos | **Upvotes:** 455 | **Comments:** 238 | **Date:** 2026-01-07

**Summary:** The post discusses running Deepseek V3.2 AWQ 4-bit on 16x AMD MI50 32GB GPUs, achieving 10 tokens/sec output and 2000 tokens/sec input with a 69000 context length. The setup aims for cost-effective local AGI and highlights power efficiency (550W idle, 2400W peak).

**Key Points:**
- Performance: 10 tok/s output, 2000 tok/s input, 69000 context length
- Power draw: 550W idle, 2400W peak inference
- Goal: Cost-effective local AGI setup using 16x AMD MI50 GPUs
- Future plans: 32 AMD MI50 setup for Kimi K2 Thinking
- Community appreciation and open-source setup details provided

**Discussion Highlights:** The discussion highlights the power efficiency of the setup, with comments noting its potential as a heater alternative in winter. Concerns about noise and power requirements at home were raised, while others praised the cost-effectiveness for professional use, comparing it favorably to CPU hardware as RAM prices increase.

---

## 31. [DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.](https://reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/)

**Author:** u/Nunki08 | **Upvotes:** 660 | **Comments:** 54 | **Date:** 2026-01-07

**Summary:** The Reddit post discusses the recent update to DeepSeek-R1's paper, which expanded from 22 pages to 86 pages, adding significant detail. The discussion includes comments about potential new architectures, linear attention research, and the value of added implementation specifics. Key points include the paper's expansion, potential new architectures, and community interest in architectural improvements across different model sizes. The discussion highlights focus on the implications of the paper's expansion and the importance of detailed implementation specifics.

---

## 32. [A 30B Qwen Model Walks Into a Raspberry Pi… and Runs in Real Time](https://reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/)

**Author:** u/ali_byteshape | **Upvotes:** 497 | **Comments:** 79 | **Date:** 2026-01-06

**Summary:** The post discusses the successful optimization of a 30B Qwen model to run efficiently on a Raspberry Pi 5, achieving 8.03 tokens per second while retaining 94.18% of BF16 quality. The optimization focuses on balancing memory usage and performance, particularly noting the quirks of GPU kernel choices. Key points include the model's performance on a Raspberry Pi 5, the focus on memory budget and performance trade-offs, the influence of GPU kernel choices on performance, the request for community feedback, and discussions on practical testing and potential improvements with hybrid transformer models.

---

## 33. [Performance improvements in llama.cpp over time](https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/)

**Author:** u/jacek2023 | **Upvotes:** 683 | **Comments:** 85 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses performance improvements in llama.cpp over time, highlighting significant progress and comparisons with other implementations like ik_llama.cpp. The discussion includes mentions of NVIDIA GPU-specific optimizations and community appreciation for the advancements.

**Key Points:**
- Performance gains in llama.cpp have been substantial, nearly matching ik_llama.cpp in token generation speed.
- Prompt processing remains slower but has seen significant improvements.
- NVIDIA GPUs are mentioned as benefiting from specific optimizations.
- Community recognition and appreciation for the progress made.

**Discussion Highlights:** The discussion highlights the impressive progress in llama.cpp performance, with users noting its near-parity with ik_llama.cpp in token generation speed. There is a focus on NVIDIA GPU optimizations and overall community enthusiasm for the advancements.

---

## 34. [For the first time in 5 years, Nvidia will not announce any new GPUs at CES — company quashes RTX 50 Super rumors as AI expected to take center stage](https://reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/)

**Author:** u/FullstackSensei | **Upvotes:** 623 | **Comments:** 196 | **Date:** 2026-01-05

**Summary:** Nvidia will not announce new GPUs at CES, focusing instead on AI. The company faces supply issues with new GPUs and may reintroduce older models like the RTX 3060. Rising hardware prices and limited availability are causing concerns among consumers.

**Key Points:**
- Nvidia quashes RTX 50 Super rumors and will not announce new GPUs at CES
- Limited supply of RTX 5070Ti, 5080, and 5090, with potential reintroduction of RTX 3060
- Rising prices of DDR5 RAM and storage, making upgrades expensive
- Concerns about corporate greed and the future of local computing
- Calls for alternative solutions, such as increased competition from China

**Discussion Highlights:** The discussion highlights frustration with Nvidia's focus on AI over consumer GPUs, concerns about rising hardware prices, and a consensus that corporate greed is negatively impacting the availability and affordability of local computing resources.

---

## 35. [llama.cpp performance breakthrough for multi-GPU setups](https://reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/)

**Author:** u/Holiday-Injury-9397 | **Upvotes:** 573 | **Comments:** 201 | **Date:** 2026-01-05

**Summary:** The ik_llama.cpp project achieved a significant performance breakthrough for multi-GPU setups, delivering a 3x to 4x speed improvement in local LLM inference. This allows for the use of multiple low-cost GPUs instead of expensive high-end cards.

**Key Points:**
- ik_llama.cpp introduces a new execution mode (split mode graph) for multi-GPU utilization.
- Performance improvements range from 3x to 4x, making it a game-changer for cost-effective setups.
- Even single GPU or CPU-only setups see a 2x speed improvement in prompt processing.
- The breakthrough reduces the need for expensive high-end GPUs, enabling the use of multiple low-cost GPUs.
- The project is open-source and details are available on GitHub.

**Discussion Highlights:** The community highlights the significant performance gains and cost-effectiveness of the new multi-GPU setup. There is consensus on the superiority of ik_llama.cpp over other forks like exllama and vllm for single batch processing. Some users report bottlenecks with hybrid inference setups.

---

## 36. [Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched](https://reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/)

**Author:** u/ubrtnk | **Upvotes:** 380 | **Comments:** 195 | **Date:** 2026-01-03

**Summary:** The post discusses the challenges faced by local LLMs in processing extreme or unlikely breaking news events, such as the US attacking Venezuela and capturing Maduro. The author shares their experience with different LLMs, highlighting how these models initially classified the event as a hoax despite credible sources.

**Key Points:**
- Local LLMs struggled to accept extreme breaking news as real, classifying it as a hoax.
- Different LLMs (Qwen Research, Spark 4.0, GPT-OSS:120B) had varying responses to the news.
- Providing credible sources helped some LLMs acknowledge the event's reality.
- Commenters shared similar experiences with LLMs dismissing unlikely events.
- Discussion highlights the bias and limitations of LLMs in processing unfamiliar geopolitical events.

**Discussion Highlights:** The discussion highlights the limitations and biases of LLMs in processing unfamiliar or extreme geopolitical events. Commenters shared similar experiences and expressed curiosity about the future of AI in handling such events.

---

## 37. [LeCun Says Llama 4 results "were fudged a little bit"](https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/)

**Author:** u/MrPecunius | **Upvotes:** 370 | **Comments:** 88 | **Date:** 2026-01-02

**Summary:** Yann LeCun, departing Meta AI Chief, confirmed that Llama 4 benchmark results were manipulated, leading to organizational changes and departures. The post discusses the impact on Meta's AI initiatives and the broader implications for open-source AI development.

**Key Points:**
- LeCun confirms Llama 4 benchmark manipulation
- Zuckerberg sidelined the GenAI organization, leading to departures
- The promised large Llama 4 model was never released
- Discussion highlights concerns about Meta's AI strategy and the shift towards Chinese models
- Community shares additional resources and discusses organizational failures

**Discussion Highlights:** The discussion reflects disappointment in Meta's handling of its AI initiatives, with users expressing concern over the shift away from open-source AI development. There is a consensus that Meta's strategic missteps have allowed smaller labs to thrive, and the community shares additional resources to provide more context.

---

## 38. [Qwen-Image-2512](https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/)

**Author:** u/Nunki08 | **Upvotes:** 717 | **Comments:** 122 | **Date:** 2025-12-31

**Summary:** The Reddit post introduces Qwen-Image-2512, a new model with various resources and demos available. It includes links to guides, GGUF files, and multiple platforms like Hugging Face, ModelScope, and GitHub. The post also features a demo link and API documentation.

**Key Points:**
- Qwen-Image-2512 is a new model with resources available on multiple platforms.
- The post provides links to guides, GGUF files, and demos.
- Users can try the model via Qwen Chat and access it through various platforms like Hugging Face and ModelScope.
- The model can be run on low-end hardware, as demonstrated by a user with an i5-8500 and no GPU.
- The community appreciates the release, with comments highlighting its timeliness and usefulness.

**Discussion Highlights:** The discussion highlights include users successfully running the model on low-end hardware, appreciation for the new release, and creative use cases like generating unique images.

---

## 39. [[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It’s running a raw Llama-7B instance with a 2048 token window.](https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/)

**Author:** u/simar-dmg | **Upvotes:** 745 | **Comments:** 110 | **Date:** 2025-12-30

**Summary:** A user reverse-engineered a Snapchat sextortion bot and discovered it was running a Llama-7B instance with a 2048 token window and high temperature setting, making it susceptible to jailbreaks. The bot was likely deployed to avoid API costs and censorship filters.

**Key Points:**
- The bot was reverse-engineered using a persona-adoption jailbreak called the 'Grandma Protocol'.
- The model was identified as Llama-7B with a 2048 token window and a temperature setting of 1.0.
- The bot's erratic behavior and susceptibility to jailbreaks were due to its high temperature setting.
- Scammers are using open-source models like Llama-7B to avoid API costs and censorship filters.
- The extracted information might be partially hallucinated, as suggested by the discussion.

**Discussion Highlights:** The discussion highlighted skepticism about the accuracy of the extracted information, with some users suggesting that the details about the model and its configuration might be hallucinated. However, there was a consensus that the bot was powered by an LLM.

---

## 40. [Llama-3.3-8B-Instruct](https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/)

**Author:** u/jacek2023 | **Upvotes:** 469 | **Comments:** 77 | **Date:** 2025-12-29

**Summary:** The post announces the release of Llama-3.3-8B-Instruct, a model previously only available via Meta's API. The author discovered a method to download it through finetuning and has made it available in GGUF format.

**Key Points:**
- Llama-3.3-8B-Instruct was previously only accessible through Meta's API.
- The author found a way to download the model via finetuning and extracted the original model.
- The model is now available in GGUF format on Hugging Face.
- The community is verifying the model's authenticity and performance.
- There is excitement and interest in benchmarking the model against other versions.

**Discussion Highlights:** The community is actively verifying the model's authenticity and running benchmarks. There is excitement about the release, with some users questioning the model's specifications, such as the 8K max position embeddings.

---

## 41. [Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.](https://reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 345 | **Comments:** 120 | **Date:** 2025-12-29

**Summary:** Z AI is set to go public with an IPO on January 8, aiming to raise $560 million, marking it as the first AI-native LLM company to list globally. The Reddit post and comments reflect mixed reactions, with concerns about the future of open-source models and the company's potential shift away from open weights.

**Key Points:**
- Z AI's IPO is scheduled for January 8, targeting $560 million in funding.
- The company is positioned as the first AI-native LLM firm to go public.
- Community reactions are mixed, with concerns about the impact on open-source AI models.
- Some users speculate that the company may continue releasing open weight models alongside paid subscriptions.
- The discussion highlights the tension between commercial success and open-source principles.

**Discussion Highlights:** The discussion is marked by a divide in opinions, with some users expressing skepticism about the future of open-source models post-IPO, while others argue that the company might still support open weights. The overall sentiment reflects a cautious optimism mixed with concerns about commercialization.

---

## 42. [Tencent just released WeDLM 8B Instruct on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 424 | **Comments:** 62 | **Date:** 2025-12-29

**Summary:** Tencent released WeDLM 8B Instruct on Hugging Face, a diffusion language model that runs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks. The release has generated significant interest in the community, with discussions highlighting its performance and potential.

**Key Points:**
- WeDLM 8B Instruct is a diffusion language model released by Tencent on Hugging Face.
- It outperforms vLLM-optimized Qwen3-8B by 3-6× on math reasoning tasks.
- The model is released under the Apache 2.0 license.
- Community discussions emphasize its impressive benchmark scores and potential.
- A 7B version of the model is also available.

**Discussion Highlights:** The community is highly interested in the model's performance and potential, with many users expressing excitement about its benchmark scores and the availability of a 7B version. There is a consensus that 7-8B models have significant potential in the field.

---

## 43. [NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux](https://reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/)

**Author:** u/HumanDrone8721 | **Upvotes:** 446 | **Comments:** 185 | **Date:** 2025-12-27

**Summary:** NVIDIA has dropped Pascal support on Linux, causing issues for Arch Linux users. The community discusses the impact on hardware like the 24GB p40 Pascal card and notes Arch Linux's policy of moving legacy drivers to AUR.

**Key Points:**
- NVIDIA's driver update (590) drops Pascal support on Linux
- Arch Linux users are affected, with legacy drivers moved to AUR
- The 24GB p40 Pascal card is highlighted as impacted hardware
- Community reactions range from concern to acceptance of Arch's policies
- Arch Linux has a history of moving legacy drivers to AUR

**Discussion Highlights:** The discussion highlights concerns about hardware compatibility and notes that Arch Linux's policy of moving legacy drivers to AUR is not new. Users express mixed reactions, with some accepting the change as expected and others worried about the impact on their systems.

---

## 44. [Best Local LLMs - 2025](https://reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/)

**Author:** u/rm-rf-rm | **Upvotes:** 364 | **Comments:** 197 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses the best local LLMs of 2025, highlighting models like Minimax M2.1 and GLM4.7 as frontier models. Users share their favorite models and experiences, categorized by application and memory footprint.

**Key Points:**
- Minimax M2.1 and GLM4.7 are noted as frontier models.
- Models are categorized by application (General, Agentic, Creative Writing, Speciality) and memory footprint (Unlimited, Medium, Small).
- Users emphasize detailed descriptions of their setups and usage.
- Specific models like Qwen3-4B-instruct and LFM2-8B-A1B are recommended for small memory footprints.
- Discussion includes debates on categorization and recommendations for RAG in technical documentation.

**Discussion Highlights:** The discussion highlights debates on categorization, with some users suggesting more granular categories. Specific models like Qwen3-4B-instruct and LFM2-8B-A1B are praised for their performance in small memory footprints. There is also interest in the best embedding/LLM models combo for RAG in technical documentation.

---

## 45. [NVIDIA has 72GB VRAM version now](https://reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/)

**Author:** u/decentralize999 | **Upvotes:** 464 | **Comments:** 148 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses NVIDIA's new 72GB VRAM version, questioning the pricing and community interest in different VRAM sizes. The discussion highlights a demand for even larger VRAM options and analyzes the cost-effectiveness of various models.

**Key Points:**
- NVIDIA has released a 72GB VRAM version of their GPU.
- The community is debating the cost-effectiveness of different VRAM sizes.
- There is a demand for larger VRAM options, such as 128GB or more.
- Price per gig remains consistent across different VRAM sizes.
- Specifications and pricing details for RTX 5000 and RTX 6000 models are provided.

**Discussion Highlights:** The discussion highlights a consensus on the need for larger VRAM options, with many users expressing interest in models with 128GB or more. The price per gig analysis shows that there is no added or lost value, making the choice straightforward for users who can afford higher VRAM sizes.

---

## 46. [Hard lesson learned after a year of running large models locally](https://reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/)

**Author:** u/inboundmage | **Upvotes:** 343 | **Comments:** 146 | **Date:** 2025-12-26

**Summary:** The author shares their experience running large language models locally, highlighting challenges with VRAM limitations, model scaling, and performance trade-offs. They conclude that local inference is viable for smaller models but requires significant hardware investment for larger ones.

**Key Points:**
- Running large models locally is feasible but has hard limitations with consumer-grade hardware.
- VRAM fragmentation and memory management are significant challenges when swapping between models.
- Quantization helps but introduces quality trade-offs and potential bugs.
- Cloud-based solutions offer better performance for fast iteration compared to local setups.
- Community suggestions include using llama.cpp for CPU offloading and considering additional GPUs.

**Discussion Highlights:** The discussion highlights practical solutions like using llama.cpp for models that exceed VRAM and the potential need for additional GPUs. There is a consensus that while local inference is possible, it requires careful management of resources and may not match cloud-based performance.

---

## 47. [I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA](https://reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/)

**Author:** u/CeFurkan | **Upvotes:** 1033 | **Comments:** 178 | **Date:** 2025-12-25

**Summary:** The Reddit post discusses the desire for GPU VRAM upgrade modifications to become mainstream, challenging NVIDIA's monopoly. The discussion highlights that such modifications are already popular in China, with Alibaba offering upgraded GPUs at various price points.

**Key Points:**
- GPU VRAM upgrade modifications are desired to challenge NVIDIA's monopoly.
- Such modifications are already mainstream in China.
- Alibaba offers upgraded GPUs like 2080Ti, 3080, 4080, 4090, and 5090 with increased VRAM.
- Prices range from $300 for a 2080Ti 22GB to $4000 for a 5090 96GB.
- Users report successful use of modded GPUs for faster processing.

**Discussion Highlights:** The discussion highlights the availability and success of GPU VRAM upgrades in China, with users expressing interest in the cost-effectiveness and performance benefits of these modifications.

---

## 48. [Why I quit using Ollama](https://reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/)

**Author:** u/SoLoFaRaDi | **Upvotes:** 491 | **Comments:** 202 | **Date:** 2025-12-25

**Summary:** The author expresses dissatisfaction with Ollama due to recent changes, particularly the introduction of Cloud features and perceived bloatware, leading them to switch to alternatives. The discussion largely supports this view and suggests alternatives like llama.cpp and LM Studio.

**Key Points:**
- Author's dissatisfaction with Ollama's recent updates and introduction of Cloud features
- Perceived bloatware and straying from the main purpose of providing a secure inference platform for local AI models
- User preference for alternatives like llama.cpp and LM Studio
- Concerns about privacy implications and funding strategies
- Community support for the author's decision and suggestions for other tools

**Discussion Highlights:** The discussion highlights a consensus among users who share the author's concerns about Ollama's recent changes. Many users recommend alternatives like llama.cpp and LM Studio, citing better performance and alignment with their needs for local AI model inference.

---

## 49. [Exclusive: Nvidia buying AI chip startup Groq's assets for about $20 billion in largest deal on record](https://reddit.com/r/LocalLLaMA/comments/1puyq9r/exclusive_nvidia_buying_ai_chip_startup_groqs/)

**Author:** u/fallingdowndizzyvr | **Upvotes:** 666 | **Comments:** 157 | **Date:** 2025-12-24

**Summary:** Nvidia is acquiring AI chip startup Groq's assets for approximately $20 billion, marking the largest deal on record. The acquisition has sparked discussions about market competition and consolidation in the AI industry.

**Key Points:**
- Nvidia is buying Groq's assets for about $20 billion
- The deal is the largest on record
- Discussions highlight concerns about market consolidation
- Some users question Groq's valuation at $20 billion
- The acquisition is seen as a strategic move by Nvidia

**Discussion Highlights:** The discussion highlights a mix of opinions, with some users viewing the acquisition as beneficial for market competition, while others express concerns about further consolidation in the AI industry. There is also skepticism about Groq's valuation and the nature of the deal being an 'acquihire'.

---

## 50. [We asked OSS-120B and GLM 4.6 to play 1,408 Civilization V games from the Stone Age into the future. Here's what we found.](https://reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/)

**Author:** u/vox-deorum | **Upvotes:** 658 | **Comments:** 174 | **Date:** 2025-12-24

**Summary:** Researchers used open-source LLMs (GPT-OSS-120B and GLM-4.6) to play 1,408 full games of Civilization V with a hybrid approach, achieving survival rates comparable to the in-game AI. The LLMs developed distinct playstyles, with OSS-120B favoring warmonger strategies and GLM-4.6 adopting a more balanced approach. The cost per game was approximately $0.86, with input tokens scaling linearly as the game progressed. Key points include: LLMs played 1,408 full Civilization V games with a hybrid approach, achieving survival rates of ~97.5%; OSS-120B favored warmonger strategies, while GLM-4.6 played more balanced; Both models preferred the Order ideology (~24% more likely) over Freedom; Cost per game was ~$0.86, with input tokens scaling linearly; LLMs showed slightly better best scores but slightly worse win rates compared to baseline. The discussion highlights enthusiasm for integrating LLMs into multiplayer games and curiosity about the potential of smaller models. Comments also reflect interest in the broader implications of AI in gaming and the uniqueness of the approach.

---

