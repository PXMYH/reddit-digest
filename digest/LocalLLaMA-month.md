# r/LocalLLaMA Reading Digest

**Period:** 2026-01-26 to 2026-01-26
**Posts Summarized:** 35
**Total Posts Analyzed:** 50

---

## 1. [I just won an Nvidia DGX Spark GB10 at an Nvidia hackathon. What do I do with it?](https://reddit.com/r/LocalLLaMA/comments/1qn3xig/i_just_won_an_nvidia_dgx_spark_gb10_at_an_nvidia/)

**Author:** u/brandon-i | **Upvotes:** 373 | **Comments:** 119 | **Date:** 2026-01-25

**Summary:** The author won an Nvidia DGX Spark GB10 at a hackathon and seeks advice on how to utilize it, mentioning potential use for running multiple NextJS applications.

**Key Points:**
- The user is new to fine-tuning models and previously used the system for inferencing with a large model.
- The user suggests running multiple NextJS applications due to high memory usage.
- Top comments recommend exploring Nvidia's playbooks and inquire about the hackathon project.

**Discussion Highlights:** The discussion highlights suggestions for running multiple NextJS applications and recommendations to explore Nvidia's resources for guidance.

---

## 2. [Your post is getting popular and we just featured it on our Discord!](https://reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/)

**Author:** u/roculus | **Upvotes:** 556 | **Comments:** 56 | **Date:** 2026-01-23

**Summary:** The Reddit post announces that a user's post is popular and has been featured on Discord, with the user receiving a special flair. The community discusses the annoyance of bot spam and questions the monetization of the Discord server.

**Key Points:**
- The bot announces the popularity of a user's post and its feature on Discord.
- The user receives a special flair for their contribution.
- The community finds the bot spam annoying and questions the monetization of the Discord server.
- There is a pinned thread about the Discord server that has been there for 5 months.
- The community humorously suggests that the bot might announce the post's popularity on Discord.

**Discussion Highlights:** The community expresses annoyance at the bot spam and questions the monetization of the Discord server. There is a consensus that private messages would be preferable to public bot posts.

---

## 3. [Qwen dev on Twitter!!](https://reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 736 | **Comments:** 60 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses Qwen's TTS model announcement, with community reactions and links to the model on Hugging Face.

**Key Points:**
- Qwen's TTS model announcement
- Community reaction and discussion
- Link to the model on Hugging Face
- Thread locked due to announcements being out
- Mention of vLLM leak related to the TTS model

**Discussion Highlights:** The community is excited about the Qwen TTS model, with some users sharing links to the model on Hugging Face. There is also mention of a vLLM leak related to the TTS model, and the thread was locked as announcements are out.

---

## 4. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 915 | **Comments:** 271 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, high-performance AI system designed for running large MoE models and supporting graphic design tasks. The system features a Threadripper Pro 3995WX, 512GB DDR4, 10 GPUs (8x 3090 + 2x 5090), and dual PSUs, all enclosed in a Thermaltake Core W200 case for mobility and protection. The build cost approximately $17k and balances performance with budget constraints.

**Key Points:**
- The system is designed for large MoE models and graphic design tasks.
- It features a Threadripper Pro 3995WX, 512GB DDR4, and 10 GPUs (8x 3090 + 2x 5090).
- The build is enclosed in a Thermaltake Core W200 case for mobility and protection.
- The total cost was approximately $17k, balancing performance and budget.
- The post highlights the challenges of enclosure and mobility, especially in a household with pets.

**Discussion Highlights:** The discussion includes humorous comments about the system's portability and power requirements, as well as appreciation for the build's capabilities and aesthetics. Some users expressed curiosity about the physical arrangement of the GPUs.

---

## 5. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 363 | **Comments:** 60 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the official support for GLM 4.7 Flash in llama.cpp, highlighting its community-driven development and performance improvements. Key points include the community effort behind the integration, performance improvements noted by users, and mixed feedback on performance with certain configurations. The discussion highlights the community effort and shares additional resources, with users reporting varying performance results.

---

## 6. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 463 | **Comments:** 162 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights GLM 4.7 Flash as a reliable local agent, outperforming other MoE models in an agentic framework. The author reports successful long sessions with extensive token generation and error-free tool calling. The discussion includes comparisons with other models and notes on performance benchmarks.

**Key Points:**
- GLM 4.7 Flash is praised for its reliability and performance in an agentic framework.
- The model has been tested extensively with successful long sessions and error-free tool calling.
- GGUFs for local use are anticipated, with some already available for testing.
- Comparisons with other models like Nemotron 30B and Qwen3 are discussed.
- Performance benchmarks suggest it is competitive with models like SEED OSS 36B but with better performance due to MoE.

**Discussion Highlights:** The discussion highlights the model's reliability and performance, with users sharing their experiences and comparisons with other models. There is anticipation for local use with GGUFs, and some benchmarks suggest it performs well compared to other large models.

---

## 7. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 744 | **Comments:** 230 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights the release of GLM-4.7-Flash, a 30B model on Hugging Face, with community excitement about its features like efficient memory usage and long context length.

**Key Points:**
- GLM-4.7-Flash is a 30B model
- Uses MLA for efficient KV cache memory
- Supports 200k context length
- Community anticipates the release
- Some users miss larger models like 70B

**Discussion Highlights:** The community is enthusiastic about the new model, particularly its memory efficiency and context length capabilities, though some express nostalgia for larger models.

---

## 8. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 352 | **Comments:** 104 | **Date:** 2026-01-18

**Summary:** The author built a high-end system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to maximize VRAM for running large AI models locally. Benchmark results show performance metrics for various models, highlighting the system's capability to handle models up to 230B parameters. Key points include the system's specifications, the use of a subsidy, benchmark results, total cost, and discussion highlights such as admiration for the build and questions about sourcing components. The discussion highlights admiration for the build, with comments expressing awe and questions about sourcing the components and comparisons to similar builds.

---

## 9. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 456 | **Comments:** 71 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses the potential delay in Qwen 4 development, with the lead developer indicating a focus on quality over speed. The community generally supports this approach, appreciating the emphasis on quality improvements.

**Key Points:**
- Qwen 4 development may be slowing down to focus on quality
- Community appreciates the focus on quality over quantity
- Uncertainty about whether the statement specifically refers to Qwen 4
- Support for taking time to make meaningful advancements
- Mixed reactions with some cautioning against speculative rumors

**Discussion Highlights:** The discussion highlights a consensus that prioritizing quality in AI development is beneficial. Many users express support for the decision to slow down, emphasizing the importance of meaningful advancements over frequent incremental updates. Some users also caution against jumping to conclusions based on limited information.

---

## 10. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 540 | **Comments:** 117 | **Date:** 2026-01-17

**Summary:** The author transitioned from MI100 GPUs to R9700 GPUs for a new server build, achieving 128GB VRAM and 128GB RAM at a lower cost than an RTX 6000 Blackwell. The build includes detailed specifications and performance benchmarks. Key points include the transition from MI100 to R9700 GPUs due to better performance and cost efficiency, detailed specifications of the server build, performance benchmarks showing high token processing rates, and positive community feedback and engagement. The community responded positively, with comments appreciating the build and expressing interest in similar setups. Some users humorously noted the financial irresponsibility of such high-end builds.

---

## 11. [Best "End of world" model that will run on 24gb VRAM](https://reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/)

**Author:** u/gggghhhhiiiijklmnop | **Upvotes:** 346 | **Comments:** 180 | **Date:** 2026-01-17

**Summary:** The user is seeking recommendations for the best LLM models that can run on a PC with 24GB VRAM and 64GB RAM, suitable for an 'end of world' scenario. The discussion includes suggestions for specific models and practical advice on data storage.

**Key Points:**
- User wants models that fit within 24GB VRAM and 64GB RAM.
- Suggestions include saving the best LLM possible and running it off SSD if necessary.
- Specific model recommendations: gemma3:27b and Midnight Miku.
- Advice to download actual Wikipedia backups for offline use.
- Mention of Discord feature and special flair for the post.

**Discussion Highlights:** The discussion highlights practical advice on model selection and data storage, with a consensus leaning towards saving the best possible LLM and using it offline. Specific model recommendations include gemma3:27b for its capabilities and Midnight Miku for entertainment purposes.

---

## 12. [GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)](https://reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/)

**Author:** u/CuriousPlatypus1881 | **Upvotes:** 379 | **Comments:** 89 | **Date:** 2026-01-16

**Summary:** The post discusses the December 2025 SWE-bench leaderboard results, highlighting the performance of various AI models on GitHub PR tasks. Claude Opus 4.5 leads with a 63.3% resolved rate, followed closely by GPT-5.2 (extra high effort) at 61.5%. The post also notes the strong performance of open-source models like GLM-4.7. Key points include Claude Opus 4.5 leading the leaderboard, GPT-5.2 following closely, Gemini 3 Flash Preview outperforming Gemini 3 Pro Preview, GLM-4.7 being the strongest open-source model, and GPT-OSS-120B showing significant performance improvement in high-effort reasoning mode. The discussion highlights excitement around Gemini Flash's performance and the strong showing of open-source models like GLM-4.7, with anticipation for future releases like DeepSeek v4.

---

## 13. [I fucking love this community](https://reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/)

**Author:** u/alhinai_03 | **Upvotes:** 524 | **Comments:** 54 | **Date:** 2026-01-16

**Summary:** The author expresses gratitude towards the open-source community for enabling them to run large language models on a 10-year-old PC with limited VRAM. They highlight the effectiveness of using system memory and MoE architecture for decent performance.

**Key Points:**
- Author runs nemotron-3-nano-30B-a3b-iq4_nl at 14-13.5 tokens per second with 65k context on a 10-year-old PC with 4GB VRAM.
- Key factors for performance include good system memory and MoE architecture.
- Community appreciates the optimization efforts and practicality of the system RAM + MoE combo.
- Author's post gains popularity and recognition within the community.

**Discussion Highlights:** The community is impressed by the performance achieved on older hardware and emphasizes the underrated practicality of using system RAM combined with MoE architecture. There is also interest in learning more about running large models on limited equipment.

---

## 14. [Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M](https://reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/)

**Author:** u/eugenekwek | **Upvotes:** 327 | **Comments:** 54 | **Date:** 2026-01-14

**Summary:** The post announces Soprano 1.1, an improved version of the Soprano TTS model with significant reductions in hallucinations and audio artifacts, along with better performance metrics. The community response is overwhelmingly positive, with users praising the model's quality and expressing interest in further developments.

**Key Points:**
- Soprano 1.1 reduces hallucinations by 95% and lowers WER by 50% compared to the previous version.
- The model now supports sentences up to 30 seconds long, doubling the previous limit.
- A blind study showed a 63% preference rate for Soprano 1.1 over the original model.
- Community feedback highlights the model's impressive performance for its size (80M parameters).
- Users are inquiring about additional features like ONNX support.

**Discussion Highlights:** The community is highly impressed with Soprano 1.1's performance, particularly given its small size. There is strong interest in further improvements and additional features, with users expressing appreciation for the author's contributions.

---

## 15. [NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency](https://reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/)

**Author:** u/Fear_ltself | **Upvotes:** 713 | **Comments:** 130 | **Date:** 2026-01-14

**Summary:** NVIDIA's Orchestrator-8B is an 8-billion-parameter AI designed to manage and route complex tasks to various tools, sparking discussions on its potential in creating functional systems and comparisons to middle managers.

**Key Points:**
- Orchestrator-8B is a specialized AI for task management and routing.
- It aims to create functional systems by connecting with other tools and models.
- Discussions compare it humorously to a 'Middle manager LLM'.
- Mentions of Claude code style agentic frameworks as a potential next leap.

**Discussion Highlights:** The discussion highlights humorous comparisons to middle managers and mentions of advanced frameworks like Claude code style agentic frameworks as potential future developments.

---

## 16. [GLM-Image is released!](https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/)

**Author:** u/foldl-li | **Upvotes:** 597 | **Comments:** 83 | **Date:** 2026-01-13

**Summary:** GLM-Image is a new image generation model with a hybrid autoregressive + diffusion decoder architecture. It excels in text-rendering and knowledge-intensive tasks while supporting various image-to-image tasks.

**Key Points:**
- Hybrid autoregressive + diffusion decoder architecture
- Excels in text-rendering and knowledge-intensive generation
- Supports image editing, style transfer, and multi-subject consistency
- MIT license with no restrictions
- High benchmark scores comparable to other models

**Discussion Highlights:** The community appreciates the MIT license and the model's capabilities. There is excitement about its performance and potential for quantization and optimization.

---

## 17. [My wishes for 2026](https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/)

**Author:** u/jacek2023 | **Upvotes:** 661 | **Comments:** 179 | **Date:** 2026-01-13

**Summary:** The Reddit post discusses predictions for 2026, focusing on the feasibility of affordable GPUs with more than 32GB memory. The community expresses skepticism and humor about the likelihood of this happening soon.

**Key Points:**
- The post asks which predictions for 2026 are likely to happen first.
- A top comment humorously doubts the possibility of affordable GPUs with >32GB memory.
- Other comments joke about the unrealistic nature of the prediction.
- Some users mention specific AI models like Qwen 4 and Mistral as more plausible developments.

**Discussion Highlights:** The discussion is marked by skepticism and humor regarding the feasibility of affordable high-memory GPUs in 2026. The community seems to agree that such hardware advancements are unlikely in the near term, with some suggesting that progress in AI models is more plausible.

---

## 18. [kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop—no GPU required](https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/)

**Author:** u/Nunki08 | **Upvotes:** 397 | **Comments:** 93 | **Date:** 2026-01-13

**Summary:** Kyutai introduced Pocket TTS, a 100M-parameter text-to-speech model with high-quality voice cloning that runs on a laptop without requiring a GPU. The model is available on GitHub and Hugging Face, with a blog post and arXiv paper providing more details.

**Key Points:**
- Pocket TTS is a 100M-parameter model for high-quality voice cloning.
- It runs on a laptop without needing a GPU.
- Available on GitHub, Hugging Face, and detailed in a blog post and arXiv paper.
- Memory usage can balloon during generation, reaching up to 32 GB.
- Interest in finetuning for different languages and comparisons with other small models.

**Discussion Highlights:** The discussion highlights concerns about memory usage during generation, interest in multilingual support, and comparisons with other small models. Users are cautious about the practicality of small models and suggest alternatives for specific use cases.

---

## 19. [GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/)

**Author:** u/TKGaming_11 | **Upvotes:** 368 | **Comments:** 93 | **Date:** 2026-01-12

**Summary:** The Reddit post highlights a GitHub repository by DeepSeek-AI introducing 'Engram,' a method for conditional memory via scalable lookup in large language models. The discussion praises the innovation and technical aspects of the approach.

**Key Points:**
- DeepSeek team praised for original ideas
- Introduces n-gram embedding and static memory as a complementary sparsity axis
- O(1) lookup efficiency noted
- Community discusses potential impact and novelty

**Discussion Highlights:** The community appreciates the innovation and discusses the technical aspects and potential implications of the 'Engram' method.

---

## 20. [LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)](https://reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 1072 | **Comments:** 114 | **Date:** 2026-01-11

**Summary:** The post introduces TimeCapsuleLLM, a 1.2B parameter language model trained exclusively on 1800-1875 London texts to minimize modern bias. The model demonstrates period-specific knowledge and behaviors, such as unfamiliarity with post-1875 concepts like telephones. The project is open-source and available on GitHub and Hugging Face.

**Key Points:**
- TimeCapsuleLLM is trained on 90GB of 1800-1875 London texts with no modern data or fine-tuning.
- The model exhibits period-specific behaviors, like generating arguments relevant to the Catholic Emancipation Act of 1829.
- The model treats post-1875 concepts (e.g., telephones) as unfamiliar, aligning with its training data cutoff.
- Future work includes creating synthetic Q&A pairs from the dataset.
- The project has garnered significant community interest and support.

**Discussion Highlights:** The community response is overwhelmingly positive, with users praising the project's uniqueness and expressing interest in similar historical language models. Some users shared their own related projects, while others humorously referenced the model's 1875 knowledge cutoff.

---

## 21. [It works! Abliteration can reduce slop without training](https://reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/)

**Author:** u/-p-e-w- | **Upvotes:** 404 | **Comments:** 129 | **Date:** 2026-01-11

**Summary:** The post discusses the use of abliteration to reduce 'slop' (flowery, cliched language) in LLM outputs, specifically for the Mistral Nemo model. The author successfully applied this technique using a configuration file and the Heretic tool, resulting in a slop-reduced model without fine-tuning. Key points include: Abliteration can reduce slop in LLM outputs without training; the technique was applied to Mistral Nemo, showing semantic separation between layers 7 and 10; the process took 2.5 hours on an A6000 but can be faster with quantization; community feedback includes both positive and critical views on the effectiveness and impact on output quality; GGUF versions of the model were created and shared by a community member. The discussion highlights mixed opinions on the effectiveness of the technique, with some users appreciating the reduction in slop but noting a lack of imagination in the output, while others question whether the technique bans all synonyms or truly reduces semantic meaning. There is also practical interest in the GGUF versions of the model.

---

## 22. [I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work](https://reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/)

**Author:** u/Ok-Pomegranate1314 | **Upvotes:** 895 | **Comments:** 147 | **Date:** 2026-01-09

**Summary:** The author successfully clustered three NVIDIA DGX Sparks, overcoming networking limitations by writing a custom NCCL plugin. This achievement allows distributed inference across all three nodes at high speeds, despite NVIDIA's official support only covering two-node clusters.

**Key Points:**
- Author clustered three DGX Sparks, exceeding NVIDIA's official support for two-node clusters.
- Developed a custom NCCL network plugin (~1500 lines of C) to handle subnet-aware NIC selection and RDMA implementation.
- Achieved distributed inference at 8+ GB/s over RDMA, demonstrating significant performance.
- The solution involves low-level debugging and custom protocols to avoid deadlocks.
- The community recognizes this as a notable achievement, with potential broader implications for DGX Spark clusters.

**Discussion Highlights:** The community praised the technical achievement, noting the complexity of working with NCCL and the potential impact of this solution. Questions were raised about scalability and performance gains, indicating strong interest in the implementation details.

---

## 23. [The reason why RAM has become so expensive](https://reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/)

**Author:** u/InvadersMustLive | **Upvotes:** 4587 | **Comments:** 382 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses the significant increase in RAM prices, with users suggesting that this may be a strategic move to monopolize resources and create future demand. The discussion highlights concerns about economic viability and potential market manipulation.

**Key Points:**
- RAM prices have increased dramatically, with some users reporting a 10x increase.
- There are concerns about monopolization of key resources like RAM by major players like OpenAI.
- The price hike is seen as a strategy to make other AI data centers, particularly Chinese ones, economically unviable.
- Users express skepticism about the sustainability of such price increases, with some calling it a bubble.

**Discussion Highlights:** The discussion is centered around the economic implications of rising RAM prices, with a consensus that this could be a deliberate strategy to control the market and limit competition. Users are divided on whether this trend is sustainable or indicative of a bubble.

---

## 24. [DeepSeek V4 Coming](https://reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/)

**Author:** u/External_Mood4719 | **Upvotes:** 500 | **Comments:** 110 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release V4, a next-generation AI model with enhanced code-generation capabilities, outperforming mainstream models like Claude and GPT. The model shows improvements in handling long code prompts and overall reasoning ability.

**Key Points:**
- DeepSeek V4 focuses on strong code-generation capabilities.
- V4 outperforms existing models like Claude and GPT in code generation.
- Improved handling of long code prompts and data pattern understanding.
- Users anticipate more logically rigorous and clear outputs.
- Community discussion highlights enthusiasm and technical expectations.

**Discussion Highlights:** Users express excitement about V4's potential, with some noting its cost-effectiveness and reliability. There is speculation about technical advancements, such as heavier pre-training and post-training RL, and potential integration of features like mHC and deepseek-ocr.

---

## 25. [(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability](https://reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/)

**Author:** u/Nunki08 | **Upvotes:** 487 | **Comments:** 102 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release a new flagship AI model with strong coding capabilities, generating significant interest and discussion in the community.

**Key Points:**
- DeepSeek's upcoming AI model focuses on strong coding abilities
- The announcement has sparked excitement and anticipation in the community
- Users are looking forward to more details and benchmarks
- There is a consensus that more models benefit the AI ecosystem
- Some users express concerns about potential limitations in role-playing abilities

**Discussion Highlights:** The community is largely excited about the new model, with many users expressing anticipation for its release and potential capabilities. There is a general consensus that more models are beneficial for the AI ecosystem. Some users have raised concerns about potential limitations in role-playing abilities.

---

## 26. [Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt](https://reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/)

**Author:** u/Prior-Arm-6705 | **Upvotes:** 940 | **Comments:** 147 | **Date:** 2026-01-08

**Summary:** A Reddit user created a compilation video of every instance Jensen Huang said 'AI' during NVIDIA's CES 2025 keynote, totaling 121 times. The process involved using open-source tools to download, parse, and edit the video locally.

**Key Points:**
- Jensen Huang said 'AI' 121 times during the CES 2025 keynote.
- The user employed open-source tools (Dive, yt-dlp-mcp, ffmpeg-mcp-lite) to automate the video compilation.
- The process was entirely local, with no cloud dependency.
- The result was described as 'hypnotic' and gained significant attention.
- Top comments included humor, criticism of pricing, and praise for the technical execution.

**Discussion Highlights:** The discussion featured a mix of humor, criticism of NVIDIA's pricing, and appreciation for the technical execution of the video compilation. Some comments were removed, and others highlighted the uniqueness of Jensen Huang's attire.

---

## 27. [16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/)

**Author:** u/ai-infos | **Upvotes:** 460 | **Comments:** 238 | **Date:** 2026-01-07

**Summary:** The post discusses running Deepseek V3.2 AWQ 4-bit on 16 AMD MI50 GPUs, achieving 10 tokens/sec output and 2000 tokens/sec input with a 69000 context length. The setup draws 550W idle and 2400W peak power, aiming for cost-effective local AGI solutions.

**Key Points:**
- Performance: 10 tok/s output, 2000 tok/s input, 69000 context length
- Power draw: 550W idle / 2400W peak
- Goal: Cost-effective local AGI without high hardware costs
- Future plans: 32 AMD MI50 setup for Kimi K2 Thinking
- Community appreciation and open-source collaboration

**Discussion Highlights:** The discussion highlights the setup's popularity, its potential as a heater alternative, concerns about noise and power requirements, and the cost-effectiveness for professional developers.

---

## 28. [DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.](https://reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/)

**Author:** u/Nunki08 | **Upvotes:** 669 | **Comments:** 54 | **Date:** 2026-01-07

**Summary:** The Reddit post discusses the recent update to DeepSeek-R1's paper, which expanded from 22 pages to 86 pages, adding significant detail. The community is engaged in discussing potential new architectures and improvements.

**Key Points:**
- DeepSeek-R1's paper was updated from 22 pages to 86 pages.
- The update includes substantial additional details.
- Community speculation about new architectures (e.g., dsv4 + r2).
- Interest in how architectural improvements perform at different model sizes.
- Focus on linear attention and cache optimization in current research.

**Discussion Highlights:** The discussion highlights community excitement about potential new architectures and improvements in model training, with a focus on linear attention and cache optimization. There is also interest in seeing how these improvements scale across different model sizes.

---

## 29. [Performance improvements in llama.cpp over time](https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/)

**Author:** u/jacek2023 | **Upvotes:** 682 | **Comments:** 85 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses performance improvements in llama.cpp over time, with a focus on NVIDIA GPU optimizations and community feedback.

**Key Points:**
- Performance gains are highlighted, particularly for NVIDIA GPUs.
- Comparison with other implementations like ik_llama.cpp is mentioned.
- Community appreciation and engagement are evident through upvotes and comments.
- Prompt processing speed is noted to be slower compared to token generation speed.

**Discussion Highlights:** The discussion emphasizes the significant progress in llama.cpp performance, especially for NVIDIA GPUs, and compares it favorably with other implementations. Community feedback is positive, with appreciation for the improvements and active engagement in the comments.

---

## 30. [LeCun Says Llama 4 results "were fudged a little bit"](https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/)

**Author:** u/MrPecunius | **Upvotes:** 366 | **Comments:** 88 | **Date:** 2026-01-02

**Summary:** Yann LeCun confirmed that Llama 4 benchmarks were manipulated, and Meta's AI organization faced significant changes, leading to a lack of follow-up on the promised model. The community expressed disappointment and shared resources for further reading.

**Key Points:**
- LeCun confirmed Llama 4 benchmark manipulation
- Meta's AI organization was sidelined, leading to departures
- No follow-up on the promised large Llama 4 model
- Community disappointment over Meta's handling of open-source AI
- Shared resources for further reading on the topic

**Discussion Highlights:** The community expressed disappointment in Meta's handling of Llama 4 and shared resources for further reading. There was a consensus on the missed opportunity for open-source AI advancement.

---

## 31. [[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It’s running a raw Llama-7B instance with a 2048 token window.](https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/)

**Author:** u/simar-dmg | **Upvotes:** 747 | **Comments:** 110 | **Date:** 2025-12-30

**Summary:** A user reverse-engineered a Snapchat sextortion bot and discovered it was running a raw Llama-7B instance with a 2048 token window. The bot was vulnerable to a persona-adoption jailbreak, revealing its configuration and malicious payload.

**Key Points:**
- The bot used a Llama-7B model with a 2048 token context window and high temperature setting.
- A 'Grandma Protocol' jailbreak forced the bot to reveal its environment variables and configuration.
- The bot was likely running on minimal hardware to reduce costs.
- The post sparked discussion about the reliability of the bot's responses and the prevalence of such scams.
- Some commenters questioned the authenticity of the bot's revealed information.

**Discussion Highlights:** The discussion highlighted skepticism about the bot's responses, with some users suggesting the information could be hallucinated. Others appreciated the detailed analysis and the insights into how scammers are using open-source models to avoid costs.

---

## 32. [Llama-3.3-8B-Instruct](https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/)

**Author:** u/jacek2023 | **Upvotes:** 469 | **Comments:** 77 | **Date:** 2025-12-29

**Summary:** The post discusses the discovery and release of the Llama-3.3-8B-Instruct model, obtained through Meta's finetuning API after overcoming technical challenges. The community is actively verifying its authenticity and performance.

**Key Points:**
- Llama-3.3-8B-Instruct model obtained via Meta's finetuning API
- Process involved overcoming UI and technical issues
- Model's authenticity and performance are being verified by the community
- Community shows strong interest in benchmarking and evaluating the model
- Questions raised about model specifications like position embeddings

**Discussion Highlights:** The community is actively engaged in verifying the model's authenticity and performance through benchmarks. There is positive reception and interest in comparing it with other models, though some questions remain about its specifications.

---

## 33. [Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.](https://reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 343 | **Comments:** 120 | **Date:** 2025-12-29

**Summary:** Z AI is set to go public with an IPO on January 8, aiming to raise $560 million, marking it as the first AI-native LLM company to list globally. The announcement has sparked discussions about the future of open-source AI models and the company's potential shift in business strategy.

**Key Points:**
- Z AI's IPO is scheduled for January 8 with a target of raising $560 million.
- Concerns about the future of open-source AI models post-IPO.
- Debate on whether Z AI will continue releasing open weight models.
- Mixed reactions from the community, with some expressing concerns about commercialization.
- The post gained significant traction with 343 upvotes and 120 comments.

**Discussion Highlights:** The community discussion highlights a divide in opinions, with some users expressing concerns about the potential end of open-source contributions from Z AI, while others argue that the company might continue releasing open weight models alongside their subscription services. There is a general acknowledgment that companies need to monetize eventually, but sentiments are mixed about the implications for the AI community.

---

## 34. [Tencent just released WeDLM 8B Instruct on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 417 | **Comments:** 62 | **Date:** 2025-12-29

**Summary:** Tencent released WeDLM 8B Instruct on Hugging Face, a diffusion language model that runs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks. The release has garnered significant attention and positive feedback from the community.

**Key Points:**
- WeDLM 8B Instruct is a diffusion language model released by Tencent on Hugging Face.
- It outperforms vLLM-optimized Qwen3-8B by 3-6× on math reasoning tasks.
- The model is released under the Apache 2.0 license.
- Community feedback highlights the potential of 7-8B models and interest in diffusion models for LLMs.
- A 7B version of the model is also available.

**Discussion Highlights:** The community is excited about the performance and potential of diffusion models for LLMs, with many expressing interest in the Apache 2.0 license and the broader implications for 7-8B models.

---

## 35. [NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux](https://reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/)

**Author:** u/HumanDrone8721 | **Upvotes:** 443 | **Comments:** 185 | **Date:** 2025-12-27

**Summary:** NVIDIA has dropped Pascal support on Linux, causing disruptions for Arch Linux users. The community is aware of this change, with some expressing concerns about the impact on older hardware.

**Key Points:**
- NVIDIA's decision to drop Pascal support affects Linux users, particularly those on Arch Linux.
- The 24GB P40, a Pascal card, is mentioned as a popular choice before becoming expensive.
- Arch Linux has a history of moving legacy drivers to AUR, making this change less surprising to some users.
- Community reactions range from concern to acceptance, with some users noting the inevitability of the change.

**Discussion Highlights:** The discussion highlights a mix of concern and acceptance among users. Some appreciate the heads-up from Arch Linux's news updates, while others express worry about the impact on their hardware. The community seems generally aware of Arch's practice of moving legacy drivers to AUR.

---

