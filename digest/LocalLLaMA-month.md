# r/LocalLLaMA Reading Digest

**Period:** 2026-01-26 to 2026-01-26
**Posts Summarized:** 25
**Total Posts Analyzed:** 50

---

## 1. [I just won an Nvidia DGX Spark GB10 at an Nvidia hackathon. What do I do with it?](https://reddit.com/r/LocalLLaMA/comments/1qn3xig/i_just_won_an_nvidia_dgx_spark_gb10_at_an_nvidia/)

**Author:** u/brandon-i | **Upvotes:** 312 | **Comments:** 100 | **Date:** 2026-01-25

**Summary:** A user won an Nvidia DGX Spark GB10 at a hackathon and is seeking advice on how to utilize it, given their limited experience with fine-tuning models. The community suggests various uses, including running multiple NextJS applications and exploring Nvidia's playbooks.

**Key Points:**
- User won an Nvidia DGX Spark GB10 at a hackathon
- User is inexperienced with fine-tuning models
- Current use involves inferencing with a large model
- Community suggests running multiple NextJS applications
- Recommendations to explore Nvidia's playbooks

**Discussion Highlights:** The discussion highlights include suggestions to run multiple NextJS applications simultaneously and to explore Nvidia's official playbooks for guidance on utilizing the DGX Spark GB10 effectively.

---

## 2. [Your post is getting popular and we just featured it on our Discord!](https://reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/)

**Author:** u/roculus | **Upvotes:** 555 | **Comments:** 56 | **Date:** 2026-01-23

**Summary:** The Reddit post announces that a user's contribution has been featured on Discord and they have received a special flair. The community expresses annoyance at the bot's public posts, suggesting private messages would be more appropriate.

**Key Points:**
- The bot announces a user's post being featured on Discord and awards a special flair.
- The community finds the bot's public posts annoying and suggests private messages instead.
- There is speculation about monetization through Discord.
- The subreddit already has a pinned thread about the Discord.
- Some users find the bot's behavior disruptive and question the subreddit's management.

**Discussion Highlights:** The community consensus is that the bot's public announcements are disruptive and should be sent as private messages. There is also skepticism about the motives behind the bot's actions, with some users suggesting monetization efforts.

---

## 3. [Am I the only one who feels that, with all the AI boom, everyone is basically doing the same thing?](https://reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/)

**Author:** u/[deleted] | **Upvotes:** 396 | **Comments:** 188 | **Date:** 2026-01-22

**Summary:** The post discusses the redundancy of AI tools and applications during the AI boom, highlighting that many new tools are less polished versions of existing ones. The discussion reflects on the early days of AI technology and the enthusiasm driving shallow implementations. Key points include the low barrier to entry for AI development, the 'hype stage' with many self-proclaimed AI experts, and a consensus that while AI is exciting, the market is saturated with repetitive ideas. The discussion highlights the enthusiasm and low barrier to entry in AI development, leading to many similar tools, and indicates a potential shift towards more specialized applications.

---

## 4. [Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp; 1.8B), Support for 10 languages](https://reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/)

**Author:** u/Nunki08 | **Upvotes:** 714 | **Comments:** 117 | **Date:** 2026-01-22

**Summary:** Qwen has open-sourced the Qwen3-TTS model family, including 5 models (0.6B & 1.8B) with support for 10 languages. The release includes resources like GitHub, Hugging Face, a blog post, a paper, and a demo.

**Key Points:**
- Open-sourcing of Qwen3-TTS model family
- 5 models available (0.6B & 1.8B)
- Support for 10 languages
- Multiple resources provided (GitHub, Hugging Face, blog, paper, demo)
- Positive community reception with some concerns about English voice quality

**Discussion Highlights:** The community appreciates Qwen's open-source contributions, with positive feedback on the model's capabilities. Some users noted concerns about the English voice quality and requested compatibility with tools like llama.cpp and mistral.rs.

---

## 5. [Qwen dev on Twitter!!](https://reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 735 | **Comments:** 60 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses Qwen's TTS model announcement, with community reactions and links to the model on Hugging Face.

**Key Points:**
- The post is a link to a Twitter announcement by Qwen dev.
- The top comment clarifies it's the TTS model from the vLLM leak.
- A link to the Qwen3-TTS model on Hugging Face is provided.
- The thread was locked as announcements are out.

**Discussion Highlights:** The community is discussing the TTS model release, with some clarifying it's from a previous leak and others sharing relevant links.

---

## 6. [8x AMD MI50 32GB at 26 t/s (tg) with MiniMax-M2.1 and 15 t/s (tg) with GLM 4.7 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1qjaxfy/8x_amd_mi50_32gb_at_26_ts_tg_with_minimaxm21_and/)

**Author:** u/ai-infos | **Upvotes:** 317 | **Comments:** 128 | **Date:** 2026-01-21

**Summary:** The post discusses a cost-effective local inference setup using 8x AMD MI50 GPUs, achieving high token generation speeds with MiniMax-M2.1 and GLM 4.7 models. The setup is praised for its performance and affordability.

**Key Points:**
- MiniMax-M2.1 achieves 26.8 tokens per second (output) and 3000 tokens per second (input) with a context length of 196,608.
- GLM 4.7 achieves 15.6 tokens per second (output) and 3000 tokens per second (input) with a context length of 95,000.
- The total cost for 8 GPUs is $880, providing 256GB VRAM, with power draw ranging from 280W (idle) to 1200W (inference).
- The setup is considered one of the most cost-effective solutions for fast, intelligent local inference.
- Community feedback highlights the impressive performance and affordability of the setup.

**Discussion Highlights:** The community is highly positive about the setup, praising its performance and cost-effectiveness. Some users express interest in replicating the setup but note that current prices for the GPUs have increased.

---

## 7. [You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?](https://reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/)

**Author:** u/Adventurous-Gold6413 | **Upvotes:** 540 | **Comments:** 306 | **Date:** 2026-01-20

**Summary:** The post discusses the selection of local models for use with 64GB RAM and 16GB VRAM without internet access. Users share their preferred models and experiences.

**Key Points:**
- The post asks for recommendations on local models to use with specific hardware constraints.
- Popular models mentioned include Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B.
- GPT-OSS-120B is highlighted for its performance and versatility.
- The discussion emphasizes the importance of model performance and compatibility with the given hardware.

**Discussion Highlights:** The consensus among users is that GPT-OSS-120B is a strong choice due to its performance and compatibility with the specified hardware. Other models like Gemma 3 27B and GLM 4.5 Air are also recommended.

---

## 8. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 907 | **Comments:** 271 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, fully enclosed 10-GPU mobile AI system designed for running large MoE models and supporting graphic design tasks. The system, built with a Threadripper Pro 3995WX, 512GB DDR4, and a mix of 3090 and 5090 GPUs, cost approximately $17k and was designed to be movable and enclosed to protect against pets.

**Key Points:**
- The system is built for running large MoE models and supporting graphic design tasks.
- It features a Threadripper Pro 3995WX, 512GB DDR4, and a mix of 3090 and 5090 GPUs.
- The total cost was approximately $17k, with a focus on balancing performance and budget.
- The enclosure was a critical requirement to protect the hardware from pets.
- The build was designed to be easily movable and fully enclosed.

**Discussion Highlights:** The discussion highlights include humor about the system's portability and power requirements, as well as appreciation for the innovative and powerful build. Some comments focus on the practicality and aesthetics of the enclosure solution.

---

## 9. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 366 | **Comments:** 60 | **Date:** 2026-01-19

**Summary:** The post announces official support for GLM 4.7 Flash in llama.cpp, highlighting its successful integration and community efforts. The discussion includes performance comparisons and additional resources.

**Key Points:**
- Official support for GLM 4.7 Flash merged in llama.cpp
- Community-driven effort, not by Z.ai developers
- Performance comparisons with other implementations like VLLm
- Availability of additional versions on Hugging Face
- Mixed feedback on flash-attention performance

**Discussion Highlights:** The discussion highlights the community's role in the integration, performance benchmarks, and additional resources for users. Some users report mixed results with flash-attention, suggesting further optimizations may be needed.

---

## 10. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 463 | **Comments:** 162 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights the author's positive experience with GLM 4.7 Flash, an MoE model that performed reliably in an agentic framework, handling extensive tasks without errors. The discussion includes comparisons with other models and mentions of available GGUFs for local testing.

**Key Points:**
- GLM 4.7 Flash is praised for its reliability and performance in agentic tasks.
- The model handled extensive token generation and tool calls without errors.
- GGUFs for local testing are anticipated and already available.
- Comparisons with other models like Nemotron 30B and Qwen3 are discussed.
- Performance benchmarks suggest it is competitive with larger models like SEED OSS 36B.

**Discussion Highlights:** The discussion highlights enthusiasm for GLM 4.7 Flash, with users sharing their experiences and comparisons with other models. There is a consensus on its strong performance and potential for local use, with some users already testing it via GGUFs.

---

## 11. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 742 | **Comments:** 230 | **Date:** 2026-01-19

**Summary:** The post announces the release of GLM-4.7-Flash model on Hugging Face, generating significant community interest with 741 upvotes and 230 comments. Users express excitement about the model's capabilities and potential for running at full 200k context with efficient memory usage.

**Key Points:**
- GLM-4.7-Flash model released on Hugging Face
- Model uses MLA architecture for efficient KV cache memory usage
- Capable of running at full 200k context length
- Community expresses excitement about 30B model size
- Special recognition given to the poster in the subreddit community

**Discussion Highlights:** The community shows strong enthusiasm for the new model release, particularly noting its memory efficiency and context length capabilities. Several users express appreciation for the model size (30B) and its potential applications. The post received special recognition from moderators, indicating its significance to the community.

---

## 12. [Best "End of world" model that will run on 24gb VRAM](https://reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/)

**Author:** u/gggghhhhiiiijklmnop | **Upvotes:** 339 | **Comments:** 180 | **Date:** 2026-01-17

**Summary:** The post discusses finding the best LLM model to download and store for an 'end of world' scenario, with a focus on models that fit within 24GB VRAM and 64GB RAM. The discussion includes suggestions for specific models and practical advice on data storage.

**Key Points:**
- User is looking for the best LLM model to fit within 24GB VRAM and 64GB RAM.
- Suggestions include saving a copy of the best LLM available and running it off SSD if necessary.
- Specific model recommendations include gemma3:27b and Midnight Miku.
- Advice to download actual Wikipedia backups for offline access.
- Discussion highlights the importance of data hoarding and offline accessibility.

**Discussion Highlights:** The discussion highlights a consensus on prioritizing the best available LLM model, even if it requires running off SSD. Specific model recommendations and practical advice on data storage are provided, emphasizing the importance of offline accessibility in an 'end of world' scenario.

---

## 13. [My story of underestimating /r/LocalLLaMA's thirst for VRAM](https://reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/)

**Author:** u/EmPips | **Upvotes:** 1359 | **Comments:** 91 | **Date:** 2026-01-15

**Summary:** The post highlights the author's underestimation of the subreddit's demand for VRAM, sparking a discussion with hardware recommendations and community engagement.

**Key Points:**
- Author underestimated VRAM demand
- Community engagement via Discord and special flair
- Hardware recommendations (e.g., 3090s or R9700)
- Gold rush analogy for demand surge

**Discussion Highlights:** The discussion includes hardware advice, community engagement, and analogies to historical events like the gold rush.

---

## 14. [GLM-Image is released!](https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/)

**Author:** u/foldl-li | **Upvotes:** 601 | **Comments:** 83 | **Date:** 2026-01-13

**Summary:** GLM-Image is a new image generation model with a hybrid autoregressive + diffusion decoder architecture. It excels in text-rendering and knowledge-intensive tasks while supporting various image-to-image tasks.

**Key Points:**
- Hybrid autoregressive + diffusion decoder architecture
- Excels in text-rendering and knowledge-intensive generation
- Supports image editing, style transfer, and multi-subject consistency
- MIT license with no restrictions
- Comparable performance to other models like nano banana 2

**Discussion Highlights:** The community appreciates the MIT license and the model's capabilities. There is interest in quantizing the model for easier use and discussions about its performance compared to other models.

---

## 15. [My wishes for 2026](https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/)

**Author:** u/jacek2023 | **Upvotes:** 655 | **Comments:** 179 | **Date:** 2026-01-13

**Summary:** The Reddit post discusses predictions for 2026, focusing on the possibility of affordable GPUs with more than 32GB of memory. The community expresses skepticism and humor about the feasibility of such advancements in the near future.

**Key Points:**
- The post asks which predictions for 2026 are likely to happen first.
- A major focus is on the availability of affordable GPUs with >32GB memory.
- The community responds with skepticism and humor about the feasibility of affordable high-memory GPUs.
- Comments highlight specific AI models like Qwen 4 and Mistral as more realistic advancements.
- The discussion reflects a mix of optimism and realism about technological progress in 2026.

**Discussion Highlights:** The discussion is marked by a mix of humor and skepticism regarding the possibility of affordable high-memory GPUs in 2026. While some users joke about the idea, others point to specific AI models as more plausible advancements. The overall consensus leans towards cautious optimism about technological progress.

---

## 16. [kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop—no GPU required](https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/)

**Author:** u/Nunki08 | **Upvotes:** 400 | **Comments:** 93 | **Date:** 2026-01-13

**Summary:** Kyutai introduced Pocket TTS, a 100M-parameter text-to-speech model with high-quality voice cloning that runs on a laptop without requiring a GPU. The model is available on GitHub and Hugging Face, with a blog post and arXiv paper providing more details.

**Key Points:**
- Pocket TTS is a lightweight (100M parameters) TTS model capable of high-quality voice cloning.
- It runs on a laptop without needing a GPU.
- The model is open-source and available on GitHub and Hugging Face.
- Users have raised concerns about memory usage during generation.
- There is interest in fine-tuning the model for different languages.

**Discussion Highlights:** The discussion highlights concerns about memory usage during generation, with one user reporting it ballooning to 32 GB. There is also interest in multilingual support and comparisons with other small TTS models.

---

## 17. [GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/)

**Author:** u/TKGaming_11 | **Upvotes:** 369 | **Comments:** 93 | **Date:** 2026-01-12

**Summary:** The Reddit post highlights a GitHub project by DeepSeek-AI called Engram, which introduces a novel approach to memory in large language models using scalable lookup and n-gram embeddings. The discussion praises the innovation and technical depth of the project.

**Key Points:**
- DeepSeek-AI's Engram project introduces a new memory axis for LLMs via scalable lookup.
- The n-gram embedding approach adds static memory as a complementary sparsity axis with O(1) lookup.
- The project is praised for its originality and technical depth.
- Comparisons are drawn to biological memory systems in animals and humans.

**Discussion Highlights:** The discussion highlights the innovation of the Engram project, with users praising DeepSeek's originality and the technical approach of using n-gram embeddings. There is a consensus on the potential impact of this new memory axis for LLMs, with some users drawing parallels to biological memory systems.

---

## 18. [The reason why RAM has become so expensive](https://reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/)

**Author:** u/InvadersMustLive | **Upvotes:** 4577 | **Comments:** 382 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses the significant increase in RAM prices, with users highlighting concerns about monopolization of RAM resources by companies like OpenAI, making AI data centers economically unviable, especially in China.

**Key Points:**
- RAM prices have increased dramatically, with some users reporting a 10x increase.
- OpenAI is accused of monopolizing RAM resources to create future demand and stifle competition.
- The high cost of RAM is making AI data centers, particularly in China, economically unviable.
- Users express skepticism about the sustainability of the current pricing trend.

**Discussion Highlights:** The discussion centers around the economic implications of rising RAM prices, with a consensus that monopolization by key players like OpenAI is driving costs up and potentially stifling competition in the AI industry.

---

## 19. [(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability](https://reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/)

**Author:** u/Nunki08 | **Upvotes:** 484 | **Comments:** 102 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release a new flagship AI model with strong coding capabilities, generating excitement and anticipation in the community.

**Key Points:**
- DeepSeek's upcoming model focuses on strong coding ability
- The announcement has generated significant interest and discussion
- Community members express excitement and anticipation
- Some users highlight the importance of model performance and capabilities
- There is a call for more diversity in AI models

**Discussion Highlights:** The community shows enthusiasm for DeepSeek's new model, with comments reflecting excitement, anticipation, and a desire for high performance and diverse capabilities in AI models.

---

## 20. [16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/)

**Author:** u/ai-infos | **Upvotes:** 463 | **Comments:** 238 | **Date:** 2026-01-07

**Summary:** The post discusses running Deepseek v3.2 on 16 AMD MI50 GPUs, achieving 10 tokens/sec output and 2000 tokens/sec input with a 69000 context length. The setup aims for cost-effective local AGI readiness, drawing 550W idle and 2400W peak power.

**Key Points:**
- Deepseek v3.2 runs at 10 tok/s output and 2000 tok/s input on 16 AMD MI50 GPUs
- Power draw is 550W idle and 2400W peak during inference
- Goal is cost-effective local AGI setup without high hardware costs
- Community appreciates the power efficiency and potential as a heater alternative
- Setup details are open-sourced on GitHub

**Discussion Highlights:** The community highlights the power efficiency (comparing it to a heater), questions about noise levels, and discusses the cost-effectiveness for professional developers. There is general excitement about the setup's potential.

---

## 21. [Performance improvements in llama.cpp over time](https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/)

**Author:** u/jacek2023 | **Upvotes:** 677 | **Comments:** 85 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses performance improvements in llama.cpp over time, with a focus on NVIDIA GPU enhancements and comparisons to other implementations.

**Key Points:**
- Performance gains are highlighted, particularly for NVIDIA GPUs.
- A reference to NVIDIA's blog post on open-source AI tool upgrades.
- Comparisons with ik_llama.cpp, noting llama.cpp's progress in token generation speed.
- Prompt processing in llama.cpp is noted to be slower but improving.

**Discussion Highlights:** The discussion emphasizes the significant performance improvements in llama.cpp, especially for NVIDIA GPUs, and compares it favorably to other implementations like ik_llama.cpp. The community appreciates the progress and shares additional resources like NVIDIA's blog post.

---

## 22. [llama.cpp performance breakthrough for multi-GPU setups](https://reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/)

**Author:** u/Holiday-Injury-9397 | **Upvotes:** 573 | **Comments:** 203 | **Date:** 2026-01-05

**Summary:** The ik_llama.cpp project achieved a 3x to 4x performance improvement for multi-GPU setups, enabling cost-effective use of multiple low-cost GPUs for local LLM inference.

**Key Points:**
- 3x to 4x speed improvement in multi-GPU configurations
- New execution mode (split mode graph) for maximum GPU utilization
- Cost-effective alternative to high-end enterprise GPUs
- Performance gains also observed in single GPU and CPU-only setups
- Potential bottlenecks in hybrid inference setups noted

**Discussion Highlights:** The community highlights significant performance gains even on single GPUs and CPU-only setups, with some users noting potential bottlenecks in hybrid inference configurations. There is consensus on the value of the breakthrough for cost-effective local LLM inference.

---

## 23. [Qwen-Image-2512](https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/)

**Author:** u/Nunki08 | **Upvotes:** 719 | **Comments:** 122 | **Date:** 2025-12-31

**Summary:** The Reddit post announces the release of Qwen-Image-2512, a new model available through various platforms like Hugging Face, ModelScope, and GitHub. It includes guides, GGUF files, and demos for users to explore and utilize the model.

**Key Points:**
- Qwen-Image-2512 is a new model with guides and GGUF files available.
- The model can be accessed via multiple platforms including Hugging Face, ModelScope, and GitHub.
- Users have successfully run the model on low-end hardware without a GPU.
- The community has shown positive reception and creative applications of the model.

**Discussion Highlights:** The discussion highlights include successful usage on low-end hardware, positive feedback on the model's release, and creative applications such as generating unique images.

---

## 24. [Llama-3.3-8B-Instruct](https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/)

**Author:** u/jacek2023 | **Upvotes:** 471 | **Comments:** 77 | **Date:** 2025-12-29

**Summary:** The post discusses the discovery and release of the Llama-3.3-8B-Instruct model, which was previously only accessible via Facebook's API. The author managed to download and share the model by reversing a finetuned version.

**Key Points:**
- Llama-3.3-8B-Instruct was previously only available through Facebook's API.
- The author found a way to download the model by reversing a finetuned version.
- The model is now available on Hugging Face in GGUF format.
- Community members are verifying the model's authenticity and performance.
- There are questions about the model's specifications, such as its max position embeddings.

**Discussion Highlights:** The community is actively engaged in verifying the model's authenticity and performance. There is excitement about the discovery, with some users running benchmarks and evaluations. Technical questions about the model's specifications are also being discussed.

---

## 25. [NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux](https://reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/)

**Author:** u/HumanDrone8721 | **Upvotes:** 442 | **Comments:** 185 | **Date:** 2025-12-27

**Summary:** NVIDIA has dropped Pascal support on Linux, causing issues for Arch Linux users. The post highlights concerns and discussions around this change, with users expressing worry and sharing experiences.

**Key Points:**
- NVIDIA's decision to drop Pascal support affects Linux users, particularly those on Arch Linux.
- The 24GB P40, a Pascal card, is mentioned as a favored model before price increases.
- Users express concern and anticipation of this change.
- Arch Linux's practice of moving legacy drivers to AUR is noted as not surprising.

**Discussion Highlights:** The discussion highlights user concerns about the impact of NVIDIA's decision, with some noting the expected nature of Arch Linux's driver management practices. There is a mix of worry and acceptance among users.

---

