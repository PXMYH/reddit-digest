# r/LocalLLaMA Reading Digest

**Period:** 2026-01-23 to 2026-01-23
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp; 1.8B), Support for 10 languages](https://reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/)

**Author:** u/Nunki08 | **Upvotes:** 644 | **Comments:** 91 | **Date:** 2026-01-22

**Summary:** Qwen has open-sourced the Qwen3-TTS model family, including VoiceDesign, CustomVoice, and Base models in 0.6B and 1.8B sizes, supporting 10 languages. The release includes resources on GitHub, Hugging Face, and a demo, with mixed feedback on voice quality and requests for additional support.

**Key Points:**
- Qwen3-TTS models (0.6B & 1.8B) released with support for 10 languages
- Resources available on GitHub, Hugging Face, blog, paper, and demo
- Mixed feedback on voice quality, with some noting anime-like English voices
- Requests for support in compiled languages like llama.cpp
- Overall positive reception for open-sourcing efforts

**Discussion Highlights:** The discussion highlights mixed feedback on voice quality, with some users noting anime-like English voices. There are requests for additional support in compiled languages like llama.cpp, and overall appreciation for Qwen's open-sourcing efforts.

---

## 2. [Qwen dev on Twitter!!](https://reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 680 | **Comments:** 60 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses Qwen's TTS model announcement, with community reactions and links to the model on Hugging Face.

**Key Points:**
- Qwen's TTS model announcement
- Community reaction and discussion
- Link to the model on Hugging Face
- Thread locked due to announcements being out
- Mention of vLLM leak related to the TTS model

**Discussion Highlights:** The community is excited about the Qwen TTS model, with some users sharing links to the model on Hugging Face. There is also mention of a vLLM leak related to the model, and the thread was locked as announcements were already out.

---

## 3. [You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?](https://reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/)

**Author:** u/Adventurous-Gold6413 | **Upvotes:** 529 | **Comments:** 298 | **Date:** 2026-01-20

**Summary:** The Reddit post discusses the selection of local models for use with 64GB RAM and 16GB VRAM when internet access is unavailable. The community shares their preferences and recommendations for the best models to use in this scenario. Key points include recommendations for models like Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B, with GPT-OSS-120B being praised for its performance and versatility. The discussion highlights a consensus around these models and shows appreciation for the post and engagement in the discussion.

---

## 4. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 858 | **Comments:** 257 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, fully enclosed mobile AI system with 10 GPUs, designed for running large MoE models and supporting graphic design tasks. The build, costing around $17k, uses a mix of 3090 and 5090 GPUs to balance performance and budget, and addresses challenges like mobility and protection from pets.

**Key Points:**
- Custom-built system with Threadripper Pro 3995WX, 512GB DDR4, and 10 GPUs (8x 3090 + 2x 5090)
- Designed for large MoE models, video generation, and high-detail image generation
- Fully enclosed and mobile, addressing challenges like pet protection and airflow
- Budget-conscious build, avoiding unnecessary expenses for diminishing returns
- Community reactions highlight the impressive nature of the build and its practicality

**Discussion Highlights:** The community praised the build for its innovation and practicality, with humorous comments about its portability and power requirements. The post gained significant attention, including a special flair and feature on Discord.

---

## 5. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 361 | **Comments:** 60 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the official support for GLM 4.7 Flash in llama.cpp, highlighting its community-driven development and performance improvements.

**Key Points:**
- GLM 4.7 Flash now officially supported in llama.cpp
- Support is community-driven, not by Z.ai developers
- Performance improvements noted, with some users reporting faster execution with specific settings
- Additional versions and resources shared by community members
- Mixed feedback on performance, with some users experiencing slower speeds with flash-attention

**Discussion Highlights:** The discussion highlights the community effort behind the integration, performance comparisons, and additional resources shared by users. There is a consensus on the significance of the update but mixed experiences regarding performance.

---

## 6. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 459 | **Comments:** 161 | **Date:** 2026-01-19

**Summary:** The post highlights GLM 4.7 Flash as a reliable local agent for GPU users, praised for its performance in agentic tasks and token generation. Users are eager for GGUF versions to test locally.

**Key Points:**
- GLM 4.7 Flash is reliable for agentic tasks
- It generates hundreds of thousands of tokens without errors
- Users are excited for local GGUF versions
- Comparisons with other models like Nemotron 30B are of interest
- Performance benchmarks suggest it is competitive with larger models

**Discussion Highlights:** The discussion focuses on the model's performance, comparisons with other models, and the anticipation for local testing with GGUF versions. Users are optimistic about its capabilities and speed.

---

## 7. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 735 | **Comments:** 230 | **Date:** 2026-01-19

**Summary:** The Reddit post discusses the release of the zai-org/GLM-4.7-Flash model on Hugging Face, highlighting its popularity and technical features like memory efficiency and large context support.

**Key Points:**
- The model release was highly anticipated
- Users appreciate 30b models and miss larger ones like 70b
- The model uses MLA, reducing KV cache memory usage
- It supports a full 200k context, making it accessible to more users

**Discussion Highlights:** The community is excited about the release, particularly noting its memory efficiency and large context support, with some users expressing nostalgia for larger models.

---

## 8. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 348 | **Comments:** 93 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to maximize VRAM for running large AI models locally. Benchmark results show impressive performance across various models, with the system costing around 9,800€ (effectively 4,900€ after refund).

**Key Points:**
- Built a system with 4x AMD R9700 GPUs (128GB VRAM) and Threadripper 9955WX CPU
- Leveraged a 50% subsidy to maximize VRAM for large AI models
- Total cost was ~9,800€ (effectively ~4,900€ after refund)
- Benchmark results show strong performance across various models
- Community reaction includes admiration and curiosity about the build

**Discussion Highlights:** The community reacted with admiration and humor, with comments highlighting the impressive hardware and curiosity about the build process and cost. Some users noted similar builds, indicating a trend in high-VRAM setups for local AI model inference.

---

## 9. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 450 | **Comments:** 71 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses the potential delay in Qwen 4 development, with the lead developer indicating a focus on quality over speed. The community generally supports this approach, appreciating the emphasis on quality improvements. Key points include the potential slowdown in development, community appreciation for quality focus, uncertainty about the specificity of the statement regarding Qwen 4, and the post's significant attention with 453 upvotes and 71 comments. The discussion highlights a general consensus supporting the focus on quality, with some users expressing appreciation for the developer's approach, but also skepticism about the specificity of the statement regarding Qwen 4.

---

## 10. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 538 | **Comments:** 117 | **Date:** 2026-01-17

**Summary:** The author upgraded their server from a dual MI100 setup to a quad R9700 configuration, achieving 128GB VRAM and 128GB RAM for a cost-effective price. They detailed the hardware components and provided benchmarks for performance. Key points include the upgrade to quad R9700 GPUs, achieving 128GB VRAM and RAM, cost-effectiveness compared to RTX 6000 Blackwell, detailed hardware specifications and benchmarks, and positive community feedback. The community appreciated the build, with comments highlighting its popularity, financial irresponsibility humor, and overall admiration for the setup.

---

## 11. [Best "End of world" model that will run on 24gb VRAM](https://reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/)

**Author:** u/gggghhhhiiiijklmnop | **Upvotes:** 342 | **Comments:** 176 | **Date:** 2026-01-17

**Summary:** The user is seeking recommendations for the best LLM model that can run on a PC with 24GB VRAM and 64GB RAM, in preparation for a hypothetical 'end of world' scenario where they have downloaded extensive data like Wikipedia and Khan Academy.

**Key Points:**
- User is hoarding data like Wikipedia, Wiktionary, and Khan Academy.
- Seeking LLM models that fit within 24GB VRAM and 64GB RAM.
- Top comment suggests saving the best LLM possible and running it off SSD if necessary.
- Gemma3:27b is recommended for its capabilities, including vision.
- Actual Wikipedia backups are suggested for long-term data preservation.

**Discussion Highlights:** The discussion highlights a focus on practicality, with recommendations for specific models like Gemma3:27b and suggestions for data preservation methods such as downloading Wikipedia backups. There is a consensus on prioritizing functionality and data accessibility in an 'end of world' scenario.

---

## 12. [GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)](https://reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/)

**Author:** u/CuriousPlatypus1881 | **Upvotes:** 377 | **Comments:** 89 | **Date:** 2026-01-16

**Summary:** The post discusses the December 2025 update to the SWE-bench leaderboard, highlighting the performance of various AI models on GitHub PR tasks. Claude Opus 4.5 leads with a 63.3% resolved rate, followed closely by GPT-5.2 (extra high effort) at 61.5%. The post also notes the strong performance of open-source models like GLM-4.7.

**Key Points:**
- Claude Opus 4.5 leads the SWE-bench leaderboard with a 63.3% resolved rate.
- GPT-5.2 (extra high effort) follows closely at 61.5%.
- Gemini 3 Flash Preview outperforms Gemini 3 Pro Preview despite being smaller and cheaper.
- GLM-4.7 is the strongest open-source model, ranking alongside closed models like GPT-5.1-codex.
- GPT-OSS-120B shows significant performance improvement in high-effort reasoning mode.

**Discussion Highlights:** The discussion highlights excitement around Gemini Flash's performance and the strong showing of open-source models like GLM-4.7. There is also anticipation for future releases like DeepSeek v4.

---

## 13. [I fucking love this community](https://reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/)

**Author:** u/alhinai_03 | **Upvotes:** 524 | **Comments:** 54 | **Date:** 2026-01-16

**Summary:** The author expresses gratitude towards the open-source community for enabling them to run large language models on older hardware, highlighting the performance of a 30B parameter model on a 10-year-old PC with limited VRAM.

**Key Points:**
- Author thanks the community for their contributions and support.
- Achieved 14-13.5 tokens per second with a 30B parameter model on a 10-year-old PC with 4GB VRAM.
- Key factors for performance include sufficient system memory and using Mixture of Experts (MoE) architecture models.
- Community members praise the optimization efforts and practicality of the system RAM + MoE combo.
- Discussion includes requests for more details on running large models on limited hardware.

**Discussion Highlights:** The community appreciates the author's contribution and shares insights on optimizing model performance on older hardware. There is a consensus on the effectiveness of using system RAM and MoE models for better performance.

---

## 14. [My story of underestimating /r/LocalLLaMA's thirst for VRAM](https://reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/)

**Author:** u/EmPips | **Upvotes:** 1341 | **Comments:** 91 | **Date:** 2026-01-15

**Summary:** The post highlights the author's underestimation of the r/LocalLLaMA community's demand for VRAM, sparking discussions on hardware recommendations and market dynamics.

**Key Points:**
- Author underestimated VRAM demand
- Community engagement via Discord
- Hardware recommendations (3090s, R9700)
- Comparison to California gold rush

**Discussion Highlights:** The discussion includes hardware advice, market insights, and humorous analogies, with a consensus leaning toward specific GPU recommendations.

---

## 15. [Latest upgrade…A100 40 GB](https://reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/)

**Author:** u/inserterikhere | **Upvotes:** 404 | **Comments:** 54 | **Date:** 2026-01-15

**Summary:** The user upgraded their gaming rig to an AI rig by purchasing a faulty A100 GPU for $1000, which worked perfectly upon installation, allowing them to run and train larger models. The post gained significant attention in the r/LocalLLaMA community.

**Key Points:**
- User transitioned from a gaming rig to an AI rig using existing parts and new purchases like a 3090 and 7950x.
- Purchased an A100 GPU listed as faulty for $1000, which worked without issues.
- The A100 was successfully integrated and used for running and training models.
- Community reactions included a meme reference and concerns about cooling the A100.
- The post was featured on Discord and received a special flair for its contribution.

**Discussion Highlights:** The discussion highlighted concerns about cooling the A100, with suggestions for active cooling methods. The community generally reacted positively to the upgrade, with some humorous comments and a meme reference.

---

## 16. [NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency](https://reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/)

**Author:** u/Fear_ltself | **Upvotes:** 713 | **Comments:** 130 | **Date:** 2026-01-14

**Summary:** NVIDIA's new 8B model, Orchestrator-8B, is designed to intelligently manage and route complex tasks to different tools for greater efficiency. The post discusses the potential of integrating separate AI components to achieve functional systems, with comments highlighting its managerial role and comparisons to existing frameworks.

**Key Points:**
- Orchestrator-8B is an 8-billion-parameter AI designed to route tasks to various tools.
- The model emphasizes efficiency through task delegation rather than direct execution.
- Discussion includes comparisons to middle management and existing agentic frameworks.
- The post suggests that combining separate AI components could lead to AGI-like functionality.
- Comments highlight the potential of hierarchical AI systems managing other models.

**Discussion Highlights:** The discussion highlights the model's role as a 'middle manager' for AI tasks, with comparisons to existing frameworks like Claude's agentic systems. There is a consensus on the potential of integrating specialized models to create more functional and efficient AI systems.

---

## 17. [GLM-Image is released!](https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/)

**Author:** u/foldl-li | **Upvotes:** 597 | **Comments:** 83 | **Date:** 2026-01-13

**Summary:** GLM-Image is a new image generation model with a hybrid autoregressive + diffusion decoder architecture. It excels in text-rendering and knowledge-intensive tasks while supporting various image-to-image tasks like editing and style transfer.

**Key Points:**
- Hybrid autoregressive + diffusion decoder architecture
- Strong performance in text-rendering and knowledge-intensive generation
- Supports image-to-image tasks like editing and style transfer
- MIT license with no restrictions
- Community interest in quantization and optimization for accessibility

**Discussion Highlights:** The community highlights the MIT license as a major advantage, compares performance to other models like nano banana 2, and expresses interest in optimizing the model for broader use. There is also curiosity about the model's capabilities in generating specific types of content.

---

## 18. [My wishes for 2026](https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/)

**Author:** u/jacek2023 | **Upvotes:** 653 | **Comments:** 179 | **Date:** 2026-01-13

**Summary:** The Reddit post discusses predictions for 2026, focusing on the possibility of affordable GPUs with more than 32GB memory. The community engages in a mix of hopeful and skeptical comments about this prospect.

**Key Points:**
- The post asks which predictions for 2026 are likely to happen first.
- A top comment humorously doubts the affordability of GPUs with more than 32GB memory.
- Another comment references 'Qwen 4' and 'Mistral' as potential developments, while others are seen as unlikely.
- The community shows a mix of optimism and skepticism about technological advancements in 2026.

**Discussion Highlights:** The discussion highlights a divide in opinions, with some users humorously dismissing the idea of affordable high-memory GPUs, while others reference specific models like 'Qwen 4' and 'Mistral' as plausible developments. The overall tone is a mix of hope and skepticism about technological progress in 2026.

---

## 19. [kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop—no GPU required](https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/)

**Author:** u/Nunki08 | **Upvotes:** 396 | **Comments:** 93 | **Date:** 2026-01-13

**Summary:** Kyutai introduced Pocket TTS, a 100M-parameter text-to-speech model with high-quality voice cloning that runs on a laptop without requiring a GPU. The model is open-source with resources available on GitHub, Hugging Face, and arXiv.

**Key Points:**
- Pocket TTS is a lightweight (100M parameters) TTS model capable of high-quality voice cloning.
- It runs efficiently on a laptop without needing a GPU.
- Resources include a blog post, GitHub repository, Hugging Face model card, arXiv paper, and X (Twitter) announcement.
- Users discussed potential for fine-tuning on different languages and noted memory usage concerns during generation.
- Some comments suggest smaller models may not yet match the quality of established alternatives.

**Discussion Highlights:** The discussion highlighted interest in multilingual fine-tuning, warnings about memory usage during generation, and comparisons with other small TTS models. Users appreciated the model's accessibility but noted potential limitations in quality for very small models.

---

## 20. [GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/)

**Author:** u/TKGaming_11 | **Upvotes:** 370 | **Comments:** 93 | **Date:** 2026-01-12

**Summary:** The Reddit post highlights a GitHub repository by DeepSeek-AI introducing 'Engram,' a novel method for conditional memory in large language models using scalable lookup. The community praises the originality and technical approach of the paper.

**Key Points:**
- DeepSeek-AI's 'Engram' introduces conditional memory via scalable lookup as a new sparsity axis for LLMs.
- The method uses n-gram embeddings, complementing existing approaches like Mixture of Experts (MoE).
- The community appreciates the originality and technical depth of the paper.
- The approach is noted for its O(1) lookup efficiency.

**Discussion Highlights:** The discussion emphasizes the technical novelty of the n-gram embedding approach and its potential to complement existing methods. The community consensus is highly positive, with praise for DeepSeek's consistent innovation.

---

## 21. [LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)](https://reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 1053 | **Comments:** 114 | **Date:** 2026-01-11

**Summary:** The post introduces TimeCapsuleLLM, a 1.2B parameter language model trained exclusively on 1800-1875 London texts to minimize modern bias. The model demonstrates period-specific knowledge and behaviors, such as unfamiliarity with post-1875 concepts like telephones. The project is open-source and available on GitHub and Hugging Face.

**Key Points:**
- TimeCapsuleLLM is trained on 90GB of 1800-1875 London texts with no modern data or fine-tuning.
- The model exhibits period-specific behaviors, like generating arguments relevant to the Catholic Emancipation Act of 1829.
- The model treats post-1875 concepts (e.g., telephones) as unfamiliar, aligning with its training data cutoff.
- Future work includes creating synthetic Q&A pairs from the dataset.
- The project has garnered significant community interest and support.

**Discussion Highlights:** The community response is overwhelmingly positive, with users praising the project's uniqueness and expressing interest in similar historical language models. Some users shared their own related projects, indicating a broader trend in historical LLM development. The top comments highlight enthusiasm for the project's progress and potential applications.

---

## 22. [I bought a €9k GH200 “desktop” to save $1.27 on Claude Code (vLLM tuning notes)](https://reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/)

**Author:** u/Reddactor | **Upvotes:** 690 | **Comments:** 177 | **Date:** 2026-01-11

**Summary:** The author built a high-end GH200 desktop setup to run Claude Code locally, achieving better speeds and results than the cloud version. Despite the high cost, the setup allows for offline coding and code reviews with optimized vLLM settings.

**Key Points:**
- Author spent €9k on a dual GH200 96GB setup to run Claude Code locally.
- Achieved better speeds and results than Claude Code with Sonnet.
- Shared optimized vLLM settings for dual 96GB systems in a blog post.
- Setup includes blocking telemetry and unnecessary traffic for full offline coding.
- Community reactions highlight the high cost but acknowledge the fun and novelty of the setup.

**Discussion Highlights:** The community praised the setup's novelty and the author's dedication, though some joked about the high cost and energy consumption. There was also interest in the specific model and settings used.

---

## 23. [It works! Abliteration can reduce slop without training](https://reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/)

**Author:** u/-p-e-w- | **Upvotes:** 398 | **Comments:** 129 | **Date:** 2026-01-11

**Summary:** The post discusses the use of abliteration to reduce 'slop' (flowery, cliched language) in LLM outputs, specifically applied to the Mistral Nemo model. The author successfully created a slop-reduced model using Heretic, a tool originally designed for censorship removal, without any fine-tuning. Key points include: Abliteration can reduce 'slop' in LLM outputs without training; Heretic was repurposed for slop reduction; the process took 2.5 hours on an A6000 at full precision; community feedback is mixed, with some appreciating the reduction in slop while others find the output too dry; and the technique shows promise but may need refinement to balance slop reduction with creativity. The community discussion highlights mixed opinions on the effectiveness of slop reduction, with some users appreciating the cleaner output while others feel it lacks imagination, and there is interest in applying this technique to other overused patterns and further refining the approach.

---

## 24. [I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work](https://reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/)

**Author:** u/Ok-Pomegranate1314 | **Upvotes:** 886 | **Comments:** 147 | **Date:** 2026-01-09

**Summary:** The author successfully clustered three NVIDIA DGX Sparks, overcoming networking limitations by writing a custom NCCL plugin. This achievement allows distributed inference across all three nodes at high speeds, pushing beyond NVIDIA's officially supported configurations.

**Key Points:**
- NVIDIA officially supports clustering only two DGX Sparks, but the author achieved clustering three.
- The solution involved writing a custom NCCL network plugin from scratch, addressing subnet-aware NIC selection and RDMA implementation.
- The plugin enables distributed inference at 8+ GB/s over RDMA, demonstrating significant performance.
- The community praised the technical achievement, noting the complexity of working with NCCL and RDMA.
- Questions from the community focused on scalability and performance improvements with additional nodes.

**Discussion Highlights:** The community expressed strong interest in the technical details and potential scalability of the solution. Many praised the author's achievement, highlighting the complexity of working with NCCL and RDMA. Questions centered around performance gains and whether the solution could scale beyond three nodes.

---

## 25. [The reason why RAM has become so expensive](https://reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/)

**Author:** u/InvadersMustLive | **Upvotes:** 4521 | **Comments:** 381 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses the significant increase in RAM prices, with comments suggesting strategic monopolization by certain entities to control future demand and economic viability of competitors.

**Key Points:**
- RAM prices have increased dramatically, with some users reporting a 10x increase.
- There is speculation about monopolization of RAM resources to control future demand.
- The economic viability of competing AI data centers, particularly in China, is being affected.
- The price surge is seen as a potential bubble by some commentators.

**Discussion Highlights:** The discussion highlights concerns about monopolistic practices and the economic impact on competitors, with some users sharing personal experiences of the price surge.

---

## 26. [DeepSeek V4 Coming](https://reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/)

**Author:** u/External_Mood4719 | **Upvotes:** 499 | **Comments:** 109 | **Date:** 2026-01-09

**Summary:** DeepSeek is expected to release V4, a next-generation AI model with strong code-generation capabilities, outperforming mainstream models like Claude and GPT. The model shows improvements in handling long code prompts and overall reasoning ability.

**Key Points:**
- DeepSeek V4 focuses on strong code-generation capabilities
- Outperforms existing models like Claude and GPT in code generation
- Improved handling of long code prompts and data patterns
- Enhanced logical rigor and clarity in outputs
- Users anticipate significant improvements and reliability

**Discussion Highlights:** Users express excitement and high expectations for DeepSeek V4, with some noting its potential to disrupt the AI landscape. Many appreciate DeepSeek's cost-effectiveness and performance, while others speculate on further advancements in the model's architecture and training methods.

---

## 27. [(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability](https://reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/)

**Author:** u/Nunki08 | **Upvotes:** 482 | **Comments:** 102 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release a new flagship AI model with strong coding capabilities, generating significant interest and discussion in the community.

**Key Points:**
- DeepSeek's upcoming AI model focuses on strong coding abilities
- The announcement has generated excitement and anticipation
- Community members express both enthusiasm and skepticism
- The model is expected to be state-of-the-art based on internal benchmarks
- There is a call for more models to benefit the broader community

**Discussion Highlights:** The discussion highlights a mix of excitement and skepticism, with users anticipating the model's release and its potential impact on the AI landscape. Some users express enthusiasm for more models, while others are cautious about the claims of outperforming benchmarks.

---

## 28. [The NO FAKES Act has a "Fingerprinting" Trap that kills Open Source. We need to lobby for a Safe Harbor.](https://reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/)

**Author:** u/PostEasy7183 | **Upvotes:** 614 | **Comments:** 88 | **Date:** 2026-01-08

**Summary:** The Reddit post discusses the NO FAKES Act, which aims to create a 'digital replica right' for voices and likenesses but poses significant legal risks for open-source developers. The author urges the community to lobby for a 'Safe Harbor' provision to protect developers from liability. Key points include the Act targeting tools used for creating digital replicas, potential liability for developers, calls for action to advocate for a Safe Harbor provision, concerns about the impact on innovation, and a consensus that the current legislation could stifle open-source development. The discussion highlights strong opposition to the NO FAKES Act as currently written, with many users expressing concern about its potential to stifle innovation and benefit large tech corporations. There is a call for collective action to lobby for amendments that protect open-source developers.

---

## 29. [Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt](https://reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/)

**Author:** u/Prior-Arm-6705 | **Upvotes:** 939 | **Comments:** 147 | **Date:** 2026-01-08

**Summary:** A Reddit user created a compilation video of every instance Jensen Huang said 'AI' during the NVIDIA CES 2025 keynote, totaling 121 times, using open-source tools. The process involved downloading the video, parsing subtitles, and editing clips locally.

**Key Points:**
- Jensen Huang said 'AI' 121 times during the CES 2025 keynote.
- The user utilized open-source tools like yt-dlp-mcp and ffmpeg-mcp-lite for video processing.
- The process was entirely local, with no cloud involvement.
- The resulting video was described as 'hypnotic'.
- Top comments included discussions about the post's popularity and Jensen's attire.

**Discussion Highlights:** The discussion highlighted the post's popularity, with comments ranging from appreciation for the technical achievement to humorous remarks about Jensen's attire and the cost of NVIDIA products.

---

## 30. [16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/)

**Author:** u/ai-infos | **Upvotes:** 463 | **Comments:** 238 | **Date:** 2026-01-07

**Summary:** The post discusses running Deepseek v3.2 on 16 AMD MI50 GPUs, achieving 10 tokens/sec output and 2000 tokens/sec input with a 69000 context length. The setup aims for cost-effective local AGI with 550W idle and 2400W peak power draw.

**Key Points:**
- 16 AMD MI50 GPUs used for Deepseek v3.2
- Performance: 10 t/s output, 2000 t/s input
- Power draw: 550W idle, 2400W peak
- Goal: cost-effective local AGI setup
- Future plan: 32 MI50 setup for Kimi K2

**Discussion Highlights:** Comments highlight the post's popularity, potential heating benefits, concerns about noise and power, and cost justification for professional use.

---

## 31. [DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.](https://reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/)

**Author:** u/Nunki08 | **Upvotes:** 663 | **Comments:** 54 | **Date:** 2026-01-07

**Summary:** The Reddit post discusses the recent update to DeepSeek-R1's paper, which expanded from 22 pages to 86 pages, adding significant detail. The discussion includes speculation about new architectures and the potential impact of linear attention research.

**Key Points:**
- DeepSeek-R1's paper was updated from 22 pages to 86 pages.
- The update includes substantial additional detail.
- Discussion highlights potential new architectures and linear attention research.
- Community interest in how architectural improvements perform at different model sizes.
- Original paper lacked implementation specifics, which the update may address.

**Discussion Highlights:** The community is excited about the expanded paper, with speculation about new architectures and the impact of linear attention. There is interest in seeing how architectural improvements scale across different model sizes.

---

## 32. [A 30B Qwen Model Walks Into a Raspberry Pi… and Runs in Real Time](https://reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/)

**Author:** u/ali_byteshape | **Upvotes:** 493 | **Comments:** 79 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses the release of the Qwen3-30B-A3B-Instruct-2507 model, optimized for performance on small hardware like the Raspberry Pi 5. The model achieves 8.03 tokens per second (TPS) at 2.70 bits per weight (BPW) while retaining 94.18% of BF16 quality. The post highlights the trade-offs between model size, speed, and quality, particularly on GPUs where kernel choice significantly impacts performance. Key points include the model's performance on a Raspberry Pi 5, the retention of BF16 quality, the influence of kernel choice on GPU performance, community feedback on testing different hardware and batch sizes, and discussions on clustering Raspberry Pis and comparisons with other models. The community discussion includes feedback on performance, potential for clustering Raspberry Pis, and comparisons with other models like Mamba2 hybrid transformers. Notable comments mention the need for specific context settings to avoid segfaults and the potential for combining the model with exo-like solutions for distributed computing.

---

## 33. [Performance improvements in llama.cpp over time](https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/)

**Author:** u/jacek2023 | **Upvotes:** 685 | **Comments:** 85 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses performance improvements in llama.cpp over time, with a focus on NVIDIA GPUs and comparisons with other implementations like ik_llama.cpp. The community highlights significant progress in token generation speed.

**Key Points:**
- Performance gains are noted, particularly for NVIDIA GPUs.
- Comparisons are made with other implementations like ik_llama.cpp.
- Token generation speed has seen significant improvements.
- Prompt processing remains slower compared to token generation.

**Discussion Highlights:** The discussion highlights the impressive progress in llama.cpp's performance, especially in token generation speed, which is now close to ik_llama.cpp. However, prompt processing is noted to be about twice as slow. The community appreciates the contributions and shares additional resources related to NVIDIA's optimizations.

---

## 34. [For the first time in 5 years, Nvidia will not announce any new GPUs at CES — company quashes RTX 50 Super rumors as AI expected to take center stage](https://reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/)

**Author:** u/FullstackSensei | **Upvotes:** 623 | **Comments:** 196 | **Date:** 2026-01-05

**Summary:** Nvidia will not announce new GPUs at CES, focusing instead on AI. The company faces supply issues with high-end GPUs and may reintroduce older models like the RTX 3060. Rising hardware prices and limited availability are causing concerns among consumers.

**Key Points:**
- No new GPU announcements from Nvidia at CES
- Limited supply of high-end GPUs (RTX 5070Ti, 5080, 5090)
- Potential reintroduction of older models like the RTX 3060
- Rising prices for DDR5 RAM and storage
- Consumer concerns about corporate greed and future upgrades

**Discussion Highlights:** The discussion highlights frustration with corporate greed and the impact on local computing. Users express concerns about the future of hardware upgrades and suggest alternatives like increased competition from China.

---

## 35. [llama.cpp performance breakthrough for multi-GPU setups](https://reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/)

**Author:** u/Holiday-Injury-9397 | **Upvotes:** 572 | **Comments:** 201 | **Date:** 2026-01-05

**Summary:** The ik_llama.cpp project has achieved a significant performance breakthrough for multi-GPU setups, delivering a 3x to 4x speed improvement in local LLM inference. This advancement allows for the utilization of multiple low-cost GPUs instead of expensive high-end cards, making it a game-changer for homelabs, server rooms, and cloud setups.

**Key Points:**
- ik_llama.cpp introduces a new execution mode (split mode graph) for multi-GPU configurations.
- The breakthrough delivers a 3x to 4x speed improvement in local LLM inference.
- This advancement makes it feasible to use multiple low-cost GPUs instead of expensive high-end cards.
- Even on a single GPU or CPU-only, ik_llama.cpp shows consistent 2x prompt processing speeds compared to llama.cpp.
- The performance improvements are significant enough to rival other optimized frameworks like exllama and vllm.

**Discussion Highlights:** The community is highly enthusiastic about the performance gains, with many users confirming the speed improvements on various setups. There is a consensus that this development is a game-changer, especially given the high cost of GPUs and memory. Some users have noted specific hardware configurations and potential bottlenecks, but overall, the feedback is overwhelmingly positive.

---

## 36. [Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched](https://reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/)

**Author:** u/ubrtnk | **Upvotes:** 376 | **Comments:** 195 | **Date:** 2026-01-03

**Summary:** The post discusses the challenges faced by local LLMs in processing and verifying extreme breaking news events, such as the US attacking Venezuela. Users reported that models initially dismissed such events as hoaxes despite credible sources, highlighting the struggle of LLMs to adapt to rapidly unfolding, unlikely scenarios.

**Key Points:**
- Local LLMs struggled to accept extreme breaking news as real, even with credible sources.
- Models like Qwen Research and Spark initially dismissed the event as a hoax or misinformation.
- Larger models (e.g., GPT-OSS:120B) performed better but still showed skepticism.
- Users noted similar issues with other unlikely events, such as hypothetical corporate deals.
- Discussion highlighted biases in LLMs' internal models of unfamiliar geopolitical events.

**Discussion Highlights:** The discussion emphasized the limitations of LLMs in processing rapidly unfolding, extreme events. Users shared similar experiences, noting that models often default to skepticism or dismissal of unlikely scenarios. There was a consensus that LLMs have inherent biases and struggle with events that deviate significantly from their training data.

---

## 37. [LeCun Says Llama 4 results "were fudged a little bit"](https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/)

**Author:** u/MrPecunius | **Upvotes:** 361 | **Comments:** 88 | **Date:** 2026-01-02

**Summary:** Yann LeCun confirmed that Llama 4 benchmarks were manipulated, and Meta's AI division faced significant restructuring, leading to departures and lack of progress. The community expressed disappointment and shared additional resources for context.

**Key Points:**
- LeCun confirmed Llama 4 benchmark manipulation
- Meta's AI division was restructured, leading to departures
- Llama 4's promised large model was never released
- Community disappointment in Meta's handling of Llama
- Additional resources shared for deeper context

**Discussion Highlights:** The discussion highlighted disappointment in Meta's strategic decisions, with users sharing additional resources and questioning how a well-positioned company could falter while smaller labs thrived.

---

## 38. [Qwen-Image-2512](https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/)

**Author:** u/Nunki08 | **Upvotes:** 721 | **Comments:** 122 | **Date:** 2025-12-31

**Summary:** The Reddit post introduces Qwen-Image-2512, a new model with various resources and demos available. It includes links to guides, GGUF files, and multiple platforms like Hugging Face, ModelScope, and GitHub. The post also features a demo link and API documentation.

**Key Points:**
- Qwen-Image-2512 is a new model with extensive documentation and resources.
- Multiple platforms host the model, including Hugging Face, ModelScope, and GitHub.
- The model can be tested via a demo link and API.
- Users have successfully run the model on low-end hardware without a GPU.
- The community appreciates the model as a gift and has created unique images with it.

**Discussion Highlights:** The discussion highlights include users successfully running the model on low-end hardware, appreciation for the model as a gift, and creative use cases like generating unique images. The community is engaged and positive about the model's capabilities.

---

## 39. [[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It’s running a raw Llama-7B instance with a 2048 token window.](https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/)

**Author:** u/simar-dmg | **Upvotes:** 737 | **Comments:** 110 | **Date:** 2025-12-30

**Summary:** A user reverse-engineered a Snapchat sextortion bot and discovered it was running a raw Llama-7B instance with a 2048 token window. The bot was vulnerable to a persona-adoption jailbreak, revealing its configuration and malicious payload.

**Key Points:**
- The bot used a Llama-7B model with a 2048 token context window and high temperature setting.
- A 'Grandma Protocol' jailbreak forced the bot to reveal its environment variables and configuration.
- The bot was likely running on minimal hardware to reduce costs.
- The payload included a malicious link that the bot attempted to hide.
- Scammers are shifting to open-source models like Llama-7B to avoid API costs and censorship.

**Discussion Highlights:** The discussion included skepticism about the accuracy of the bot's revealed information, with some users suggesting it could be entirely hallucinated. Others questioned the commonality of system prompts including environment variables.

---

## 40. [Llama-3.3-8B-Instruct](https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/)

**Author:** u/jacek2023 | **Upvotes:** 465 | **Comments:** 77 | **Date:** 2025-12-29

**Summary:** The post discusses the discovery and release of the Llama-3.3-8B-Instruct model, which was previously only available through Meta's API. The author details their process of obtaining the model by exploiting a finetuning feature in the API, despite initial difficulties and bugs.

**Key Points:**
- The Llama-3.3-8B-Instruct model was previously only accessible via Meta's API.
- The author discovered a finetuning feature in the API that allowed them to download the model.
- The process involved overcoming UI bugs and CORS issues to obtain the model.
- The model includes an adapter that can be removed to retrieve the original model.
- Community members are verifying the model's authenticity and performance.

**Discussion Highlights:** The community is actively verifying the model's authenticity and performance through benchmarks and evaluations. There is excitement about the discovery, with some users running private evaluations to compare it against other Llama models. Concerns about the model's max position embeddings being limited to 8K were also raised.

---

## 41. [Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.](https://reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 342 | **Comments:** 120 | **Date:** 2025-12-29

**Summary:** Z AI is set to go public on January 8 with an IPO aiming to raise $560 million, marking it as the first AI-native LLM company to list globally. The Reddit discussion highlights concerns about the future of open-source AI models and the company's potential shift away from open-source practices.

**Key Points:**
- Z AI's IPO is scheduled for January 8, aiming to raise $560 million.
- Concerns about the future of open-source AI models post-IPO.
- Debate on whether Z AI will continue releasing open weight models.
- Mixed reactions from the community, with some expressing disappointment.
- The IPO is seen as a necessary step for the company's financial growth.

**Discussion Highlights:** The discussion reflects a divide in the community, with some users expressing concerns about the potential end of open-source contributions from Z AI, while others see the IPO as a natural progression for the company's financial sustainability. The top comments highlight skepticism about continued open-source support and the financial motivations behind the IPO.

---

## 42. [Tencent just released WeDLM 8B Instruct on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 425 | **Comments:** 62 | **Date:** 2025-12-29

**Summary:** Tencent released WeDLM 8B Instruct on Hugging Face, a diffusion language model that runs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks. The community response highlights its impressive performance and potential in the 7-8B model space.

**Key Points:**
- WeDLM 8B Instruct is a diffusion language model released by Tencent on Hugging Face.
- It runs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks.
- The model has an Apache 2.0 license.
- Community sees potential in 7-8B models and appreciates the release.
- A 7B version (WeDLM-7B-Instruct) is also available.

**Discussion Highlights:** The community is excited about the performance improvements and the potential of 7-8B models. There is a consensus that diffusion models like WeDLM are promising and could be a significant advancement in the field.

---

## 43. [NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux](https://reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/)

**Author:** u/HumanDrone8721 | **Upvotes:** 447 | **Comments:** 185 | **Date:** 2025-12-27

**Summary:** NVIDIA has dropped Pascal support on Linux, causing disruptions for Arch Linux users who rely on these legacy drivers. The change has sparked discussions about the impact on older hardware and the broader implications for Linux users.

**Key Points:**
- NVIDIA's decision affects Pascal-based GPUs like the P40.
- Arch Linux users are particularly impacted due to the distribution's handling of legacy drivers.
- The change has been anticipated by some users but still caused concern.
- Discussions highlight the broader implications for users with older hardware.

**Discussion Highlights:** The discussion reflects a mix of concern and resignation, with users acknowledging the inevitability of hardware obsolescence. Some users express nostalgia for Pascal cards, while others highlight the practical challenges of maintaining older hardware on modern systems.

---

## 44. [Best Local LLMs - 2025](https://reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/)

**Author:** u/rm-rf-rm | **Upvotes:** 363 | **Comments:** 197 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses the best local LLMs of 2025, highlighting models like Minimax M2.1 and GLM4.7, and categorizes them by application and memory footprint. Users share detailed experiences and recommendations. Key points include the categorization of LLMs by applications such as General, Agentic, Creative Writing, and Speciality, and by memory footprint: Unlimited (>128GB VRAM), Medium (8-128GB VRAM), and Small (<8GB VRAM). Specific recommendations include Qwen3-4B-instruct and LFM2-8B-A1B for small models. The discussion includes debates on categorization and highlights specific model recommendations and their strengths.

---

## 45. [NVIDIA has 72GB VRAM version now](https://reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/)

**Author:** u/decentralize999 | **Upvotes:** 458 | **Comments:** 148 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses NVIDIA's new 72GB VRAM version, questioning the pricing and community interest in different VRAM sizes. The discussion highlights pricing comparisons and community opinions on the need for larger VRAM options.

**Key Points:**
- NVIDIA has released a 72GB VRAM version.
- Pricing comparisons between 48GB, 72GB, and 96GB models are discussed.
- Community opinions vary, with some advocating for even larger VRAM options like 128GB.
- Price per gig remains consistent across models, making the choice dependent on budget.
- The post gained significant traction with 458 upvotes and 148 comments.

**Discussion Highlights:** The discussion highlights a consensus on the need for larger VRAM options, with some users advocating for 128GB or more. Pricing comparisons show that the price per gig is consistent, making the choice dependent on individual budget and needs. The community is engaged, with significant upvotes and comments indicating strong interest in the topic.

---

## 46. [Hard lesson learned after a year of running large models locally](https://reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/)

**Author:** u/inboundmage | **Upvotes:** 351 | **Comments:** 146 | **Date:** 2025-12-26

**Summary:** The author shares their experience running large language models locally, highlighting challenges with VRAM limitations, model scaling, and performance trade-offs. They conclude that local inference is viable for smaller models but requires significant hardware investment for larger ones.

**Key Points:**
- Running large models locally is feasible but has hard limits with consumer-grade hardware.
- VRAM fragmentation and memory management are significant challenges.
- Quantization helps but introduces quality trade-offs and new issues.
- Cloud-based solutions offer better performance for fast iteration.
- Community suggestions include using llama.cpp for CPU offloading and adding more VRAM.

**Discussion Highlights:** The discussion highlights practical solutions like using llama.cpp for CPU offloading and the need for more VRAM. There is a consensus that local inference is viable for smaller models but requires significant hardware upgrades for larger models.

---

## 47. [I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA](https://reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/)

**Author:** u/CeFurkan | **Upvotes:** 1035 | **Comments:** 178 | **Date:** 2025-12-25

**Summary:** The Reddit post discusses the potential for GPU VRAM upgrade modifications to become mainstream, which could challenge NVIDIA's monopoly. The discussion highlights that such modifications are already prevalent in China, with various models being upgraded and sold at different price points. Key points include: GPU VRAM upgrade modifications could disrupt NVIDIA's monopoly, such modifications are already mainstream in China, Alibaba offers upgraded GPUs like 2080Ti, 3080, 4080, 4090, and 5090 with varying VRAM capacities, prices range from $300 for a 2080Ti 22GB to $4000 for a 5090 96GB, and users report successful usage of modded GPUs like the 4090 with 48GB VRAM. The discussion highlights that GPU VRAM upgrade modifications are already mainstream in China, with Alibaba offering a range of upgraded GPUs. Users share their positive experiences with modded GPUs, and there is interest in the cost-effectiveness and performance benefits of these modifications.

---

## 48. [Why I quit using Ollama](https://reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/)

**Author:** u/SoLoFaRaDi | **Upvotes:** 492 | **Comments:** 202 | **Date:** 2025-12-25

**Summary:** The Reddit post discusses the author's decision to stop using Ollama due to recent updates that introduced cloud-based models, which they feel stray from the original purpose of providing a secure platform for local AI models. The discussion highlights a shift in user preference towards alternatives like llama.cpp and LM Studio.

**Key Points:**
- Author's dissatisfaction with Ollama's recent updates and shift towards cloud-based models
- Concerns about privacy implications and bloatware in Ollama
- User preference for alternatives like llama.cpp and LM Studio
- Criticism of Ollama's funding strategy through cloud options
- Discussion about the decline in update quality and frequency

**Discussion Highlights:** The discussion reveals a consensus among users that Ollama's recent updates have been disappointing, with many users switching to alternatives like llama.cpp and LM Studio. The main concerns revolve around privacy, bloatware, and the shift away from the original purpose of providing a secure platform for local AI models.

---

## 49. [Exclusive: Nvidia buying AI chip startup Groq's assets for about $20 billion in largest deal on record](https://reddit.com/r/LocalLLaMA/comments/1puyq9r/exclusive_nvidia_buying_ai_chip_startup_groqs/)

**Author:** u/fallingdowndizzyvr | **Upvotes:** 666 | **Comments:** 157 | **Date:** 2025-12-24

**Summary:** Nvidia is acquiring AI chip startup Groq's assets for approximately $20 billion, marking the largest deal on record. The discussion highlights mixed reactions, with some seeing it as beneficial for market competition while others express concerns about consolidation and regulatory implications.

**Key Points:**
- Nvidia is buying Groq's assets for about $20 billion
- The deal is seen as the largest on record
- Mixed reactions: some view it as positive for competition, others as consolidation
- Skepticism about Groq's valuation at $20 billion
- Regulatory concerns and potential 'acquihire' implications

**Discussion Highlights:** The discussion reflects a divide in opinions, with some users optimistic about the deal fostering a competitive market, while others are concerned about increasing consolidation in the AI chip industry. There is also skepticism about Groq's valuation and regulatory challenges surrounding the acquisition.

---

## 50. [We asked OSS-120B and GLM 4.6 to play 1,408 Civilization V games from the Stone Age into the future. Here's what we found.](https://reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/)

**Author:** u/vox-deorum | **Upvotes:** 661 | **Comments:** 174 | **Date:** 2025-12-24

**Summary:** The post discusses an experiment where open-source LLMs (GPT-OSS-120B and GLM-4.6) were used to play 1,408 full games of Civilization V. The LLMs showed slightly better performance in best scores but slightly worse in win rates compared to the baseline. The models developed distinct playstyles and could survive full games, marking a significant achievement in AI gaming. Key points include: LLMs played 1,408 full Civilization V games with slight performance improvements in best scores; OSS-120B and GLM-4.6 developed different playstyles, with OSS-120B being more warmonger-like; Both models preferred the Order ideology over Freedom; The cost per game was approximately $0.86 for OSS-120B; The hybrid approach allowed LLMs to survive full games, a feat not achieved by pure-LLM or pure-RL approaches. The discussion highlights enthusiasm for the potential of LLMs in gaming, with users expressing interest in playing against local models and exploring multiplayer integration. There was also curiosity about the impact of model size on performance and the possibility of treating the game as quasi-multi-level ABMs.

---

