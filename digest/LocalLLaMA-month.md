# r/LocalLLaMA Reading Digest

**Period:** 2026-01-24 to 2026-01-24
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [Your post is getting popular and we just featured it on our Discord!](https://reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/)

**Author:** u/roculus | **Upvotes:** 522 | **Comments:** 56 | **Date:** 2026-01-23

**Summary:** The Reddit post announces that a user's contribution has been featured on Discord and they have received a special flair. The community discusses the annoyance of bot spam and questions the monetization of the Discord server. Key points include the bot's announcement of the featured post, community annoyance with bot spam, concerns about monetization, the long-standing pinned thread about Discord, and some users' unawareness of the Discord server. The discussion highlights a consensus that bot spam is annoying and private messages would be better, with additional concerns about monetization and its impact on reply quality.

---

## 2. [Am I the only one who feels that, with all the AI boom, everyone is basically doing the same thing?](https://reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/)

**Author:** u/[deleted] | **Upvotes:** 391 | **Comments:** 186 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses the redundancy of AI projects during the AI boom, highlighting that many new AI tools and applications are essentially reinventing existing solutions. The author notes that while AI is incredible, there is a trend of people investing heavily in creating less polished versions of already available tools.

**Key Points:**
- Many AI projects are redundant, reinventing existing solutions.
- The barrier to entry for AI development is low, leading to shallow implementations.
- There is a lot of enthusiasm and hype around AI, similar to past tech trends like cryptocurrency.
- Some developers are focusing on niche tools and specific needs rather than broad applications.
- The current phase is seen as the 'hype stage' of AI development.

**Discussion Highlights:** The discussion highlights a consensus that the AI field is currently in a hype phase, with many redundant projects being developed. However, there is also a focus on niche tools and specific applications that address unique needs. The community acknowledges the excitement and low barrier to entry but also recognizes the potential for shallow implementations.

---

## 3. [Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp; 1.8B), Support for 10 languages](https://reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/)

**Author:** u/Nunki08 | **Upvotes:** 705 | **Comments:** 110 | **Date:** 2026-01-22

**Summary:** Qwen has open-sourced the Qwen3-TTS model family, including VoiceDesign, CustomVoice, and Base models in 0.6B and 1.8B sizes, supporting 10 languages. The release includes resources on GitHub, Hugging Face, and a demo, with mixed reactions from the community.

**Key Points:**
- Qwen3-TTS models (0.6B & 1.8B) released with support for 10 languages
- Resources available on GitHub, Hugging Face, blog, paper, and demo
- Community feedback includes praise for open-source efforts and concerns about voice quality
- Requests for compatibility with tools like llama.cpp and mistral.rs
- Positive reception for the samples provided

**Discussion Highlights:** The community appreciates Qwen's open-source contributions but has concerns about the voice quality of English speakers and requests for broader compatibility with other tools. Overall, the release is well-received with enthusiasm for the provided samples.

---

## 4. [Qwen dev on Twitter!!](https://reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 723 | **Comments:** 60 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses Qwen's TTS model announcement, which is reportedly from a vLLM leak. The community is engaged in verifying the source and legitimacy of the model.

**Key Points:**
- Qwen's TTS model announcement
- Model is from a vLLM leak
- Hugging Face link provided for the model
- Community discussion on model legitimacy

**Discussion Highlights:** The discussion highlights include verification of the model's source, with some users providing a Hugging Face link and others expressing skepticism or additional context.

---

## 5. [You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?](https://reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/)

**Author:** u/Adventurous-Gold6413 | **Upvotes:** 539 | **Comments:** 305 | **Date:** 2026-01-20

**Summary:** The Reddit post discusses the selection of local models for use with 64GB RAM and 16GB VRAM in an offline environment. Users share their preferences and recommendations for models that fit these hardware specifications.

**Key Points:**
- The post asks for recommendations on local models to use with 64GB RAM and 16GB VRAM without internet access.
- Top comments highlight specific models like Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B.
- GPT-OSS-120B is praised for its performance and versatility on the given hardware.
- The community appreciates the contribution and engagement in the discussion.
- There is a consensus on the suitability of certain models for the specified hardware.

**Discussion Highlights:** The discussion highlights a strong preference for models like GPT-OSS-120B, which is noted for its good performance and versatility on the given hardware. Other models like Gemma 3 27B and GLM 4.5 Air are also recommended. The community shows appreciation for the post and engages actively in the discussion.

---

## 6. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 881 | **Comments:** 267 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, fully enclosed 10x GPU mobile AI system designed for running large MoE models and supporting graphic design tasks. The build, costing around $17k, features a Threadripper Pro 3995WX, 512GB DDR4, and a mix of 3090 and 5090 GPUs, with a focus on mobility and protection from pets.

**Key Points:**
- Custom-built system for large MoE models and graphic design tasks
- Features Threadripper Pro 3995WX, 512GB DDR4, and 10 GPUs (8x 3090 + 2x 5090)
- Designed to be mobile and fully enclosed to protect from pets
- Budget-conscious build aiming for high performance without unnecessary expenses
- Community reactions highlight the uniqueness and practicality of the build

**Discussion Highlights:** The community reactions include humor about the system's portability and power requirements, appreciation for the build's uniqueness, and curiosity about the physical setup of the GPUs. The top comments reflect a mix of admiration and playful banter, indicating strong engagement with the post.

---

## 7. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 362 | **Comments:** 60 | **Date:** 2026-01-19

**Summary:** The Reddit post announces that GLM 4.7 Flash has received official support in llama.cpp, highlighting a community-driven effort. The discussion includes notes on performance, alternative implementations, and community recognition. Key points include: GLM 4.7 Flash now has official support in llama.cpp, thanks to community efforts; the term 'official' refers to proper functionality with llama.cpp, not endorsement by Z.ai developers; performance notes include flash-attention being slow for some users, with better results using '-fa 0'; alternative versions and implementations are shared, such as a GGUF model on Hugging Face; and the post author received recognition for their contribution, including a special flair and feature on Discord. The community discussion emphasizes the collaborative nature of the achievement, with users sharing performance insights and alternative resources. There is a consensus that flash-attention may not always be the fastest option, and users are actively contributing additional models and versions.

---

## 8. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 465 | **Comments:** 162 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights the effectiveness of GLM 4.7 Flash as a reliable local agent, outperforming other MoE models in an agentic framework. The author shares their positive experience with the model, noting its ability to handle complex tasks without errors. The community discusses comparisons with other models and shares resources for local testing. Key points include the model's reliability, successful task handling, community interest in comparisons, availability of GGUFs for local testing, and competitive performance benchmarks. The discussion highlights enthusiasm for GLM 4.7 Flash's performance and potential as a local agent, with users sharing resources and expressing interest in comparative benchmarks.

---

## 9. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 740 | **Comments:** 230 | **Date:** 2026-01-19

**Summary:** The Reddit post discusses the release of the GLM-4.7-Flash model on Hugging Face, highlighting its technical features and community anticipation.

**Key Points:**
- The model uses MLA, reducing KV cache memory usage.
- Community excitement about the release after a long wait.
- The model supports a 200k context length.
- Mixed reactions about model sizes (e.g., preference for 30B or 70B models).

**Discussion Highlights:** The community is enthusiastic about the release, with discussions focusing on technical improvements like memory efficiency and context length, as well as nostalgia for larger model sizes.

---

## 10. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 350 | **Comments:** 101 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to stay within a ~10,000€ budget. The system is designed for running large AI models (120B+ parameters) locally, with benchmark results showing strong performance across various models. Key points include maximizing VRAM for large AI models, a total cost of ~9,800€ with a 50% subsidy, strong benchmark performance, high-end components, and significant engagement in the discussion. The discussion highlights the impressive hardware and cost, with comments praising the system's capabilities and expressing interest in the components used.

---

## 11. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 448 | **Comments:** 71 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses a potential slowdown in the development of Qwen 4, with the lead developer emphasizing a focus on quality over quantity. The community generally supports this approach, appreciating the commitment to improvement.

**Key Points:**
- Qwen 4 development may be slowing down to focus on quality
- Community appreciates the focus on quality over quantity
- Uncertainty about whether the statement specifically refers to Qwen 4
- Support for taking necessary time to advance the technology meaningfully
- Mixed reactions with some cautioning against speculative rumors

**Discussion Highlights:** The discussion highlights a general consensus supporting the focus on quality, with some users expressing appreciation for the developer's approach. There is also a note of caution about interpreting the statement as specifically referring to Qwen 4, with calls to avoid speculative rumors.

---

## 12. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 542 | **Comments:** 117 | **Date:** 2026-01-17

**Summary:** The author upgraded their server from a dual MI100 setup to a quad R9700 configuration, achieving 128GB VRAM and 128GB RAM for a cost-effective solution compared to alternatives like the RTX 6000 Blackwell. The post includes detailed specifications, benchmarks, and a cost breakdown of the build.

**Key Points:**
- The author switched from MI100 GPUs to R9700 GPUs due to better performance and cost efficiency.
- The total cost of the build was $7,035, which is less than the price of a single RTX 6000 Blackwell.
- The system includes 128GB VRAM and 128GB RAM, making it highly capable for AI workloads.
- Benchmarks show strong performance in prompt processing with minimal token generation loss.
- The community response was positive, with many appreciating the build and its cost-effectiveness.

**Discussion Highlights:** The discussion highlights the community's appreciation for the build, with comments praising its cost-effectiveness and performance. Some users humorously noted the financial irresponsibility of such builds, while others congratulated the author on the achievement.

---

## 13. [Best "End of world" model that will run on 24gb VRAM](https://reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/)

**Author:** u/gggghhhhiiiijklmnop | **Upvotes:** 345 | **Comments:** 180 | **Date:** 2026-01-17

**Summary:** The user is seeking recommendations for the best LLM models that can run on a PC with 24GB VRAM and 64GB RAM, motivated by a desire to hoard data in anticipation of a potential 'end of the world' scenario. The discussion includes suggestions for specific models and practical advice on data storage.

**Key Points:**
- User wants to download and store models that fit within 24GB VRAM and 64GB RAM.
- Suggestions include saving the best LLM possible and running it off SSD if necessary.
- Specific model recommendations include gemma3:27b and Midnight Miku.
- Advice to download actual Wikipedia backups for offline access.
- Discussion highlights the importance of practical data storage solutions.

**Discussion Highlights:** The discussion features a mix of practical advice and specific model recommendations. There is a consensus on the importance of saving high-quality models and ensuring they can be run efficiently, even in extreme scenarios. The top comments emphasize practicality, such as running models off SSD and downloading comprehensive data backups.

---

## 14. [GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)](https://reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/)

**Author:** u/CuriousPlatypus1881 | **Upvotes:** 385 | **Comments:** 89 | **Date:** 2026-01-16

**Summary:** The post discusses the updated SWE-bench leaderboard results from December 2025, highlighting the performance of various AI models on GitHub PR tasks. Claude Opus 4.5 leads with a 63.3% resolved rate, followed closely by GPT-5.2 (extra high effort) at 61.5%. The post also notes the strong performance of open-source models like GLM-4.7.

**Key Points:**
- Claude Opus 4.5 leads the SWE-bench leaderboard with a 63.3% resolved rate.
- GPT-5.2 (extra high effort) follows closely at 61.5%.
- GLM-4.7 is the strongest open-source model, ranking alongside closed models like GPT-5.1-codex.
- Gemini 3 Flash Preview outperforms Gemini 3 Pro Preview despite being smaller and cheaper.
- The community is excited about the performance of open-source models and upcoming releases like DeepSeek v4.

**Discussion Highlights:** The discussion highlights excitement around the performance of open-source models like GLM-4.7 and anticipation for future releases like DeepSeek v4. There is also a consensus that this benchmark is more believable compared to others.

---

## 15. [I fucking love this community](https://reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/)

**Author:** u/alhinai_03 | **Upvotes:** 519 | **Comments:** 54 | **Date:** 2026-01-16

**Summary:** The post expresses gratitude towards the open-source community for enabling the user to run large language models on older hardware with limited resources. The user highlights the effectiveness of using system memory and Mixture of Experts (MoE) architecture for better performance.

**Key Points:**
- Appreciation for the open-source community and contributors
- Running large models on a 10-year-old PC with limited VRAM
- Importance of system memory and MoE architecture for performance
- Achieving 14-13.5 tokens per second with a 30B parameter model
- Community support and shared knowledge as key enablers

**Discussion Highlights:** The discussion highlights the impressive optimizations achieved by the community, with users praising the ability to run large models on older hardware. There is a consensus on the effectiveness of using system RAM and MoE architecture for better performance, and a request for more information on running large models on limited equipment.

---

## 16. [My story of underestimating /r/LocalLLaMA's thirst for VRAM](https://reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/)

**Author:** u/EmPips | **Upvotes:** 1352 | **Comments:** 91 | **Date:** 2026-01-15

**Summary:** The post highlights the author's underestimation of the r/LocalLLaMA community's demand for VRAM, sparking discussions on hardware recommendations and community engagement.

**Key Points:**
- Author underestimated VRAM demand
- Community engagement via Discord
- Hardware recommendations discussed
- Gold rush analogy mentioned

**Discussion Highlights:** The discussion includes hardware advice (e.g., 3090s or R9700) and community engagement, with a notable analogy comparing the situation to a gold rush.

---

## 17. [Latest upgrade…A100 40 GB](https://reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/)

**Author:** u/inserterikhere | **Upvotes:** 407 | **Comments:** 54 | **Date:** 2026-01-15

**Summary:** The author upgraded their gaming rig to an AI-focused setup by acquiring a used A100 GPU for $1000, despite it being listed as faulty. The GPU worked immediately, allowing them to run and train larger AI models. The post gained significant attention in the community.

**Key Points:**
- The author transitioned from a gaming rig to an AI-focused setup.
- They purchased a faulty A100 GPU for $1000, which worked upon installation.
- The setup allows for running and training larger AI models.
- The post received significant engagement with 407 upvotes and 54 comments.
- Community members discussed cooling solutions for the A100 GPU.

**Discussion Highlights:** The community showed interest in the author's setup, with discussions focusing on cooling solutions for the A100 GPU and general admiration for the upgrade.

---

## 18. [Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M](https://reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/)

**Author:** u/eugenekwek | **Upvotes:** 322 | **Comments:** 54 | **Date:** 2026-01-14

**Summary:** The post announces Soprano 1.1, an improved version of the Soprano model with significant reductions in hallucinations and audio artifacts, along with a higher preference rate. The community responds positively, praising the model's performance and expressing interest in future developments.

**Key Points:**
- Soprano 1.1 reduces hallucinations by 95% and has a 63% preference rate over Soprano-80M
- The model supports longer sentences (up to 30 seconds) and has a 50% lower WER
- Community feedback highlights the model's usability and impressiveness for its size
- Discussion includes inquiries about future support like ONNX compatibility
- Positive reception and appreciation for the author's contributions

**Discussion Highlights:** The community is highly positive about Soprano 1.1, with many users expressing surprise at its performance for an 80M model. There is interest in future developments, such as ONNX support, and appreciation for the author's work.

---

## 19. [NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency](https://reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/)

**Author:** u/Fear_ltself | **Upvotes:** 721 | **Comments:** 130 | **Date:** 2026-01-14

**Summary:** NVIDIA's Orchestrator-8B is an 8-billion-parameter AI model designed to manage and route complex tasks to various tools for greater efficiency, sparking discussions about the future of AI systems and their integration.

**Key Points:**
- Orchestrator-8B is a specialized model for task management and routing.
- The model aims to enhance efficiency by connecting with other tools and models.
- Discussions highlight the potential of such models in creating functional AI systems.
- Comparisons to middle management and existing frameworks were made.
- The post gained significant attention with 721 upvotes and 130 comments.

**Discussion Highlights:** The discussion emphasized the importance of integrating different AI tools and models for enhanced functionality. Some users humorously compared the model to a 'middle manager,' while others noted its potential in creating hierarchical AI systems. The post was well-received, indicating strong community interest in this development.

---

## 20. [GLM-Image is released!](https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/)

**Author:** u/foldl-li | **Upvotes:** 599 | **Comments:** 83 | **Date:** 2026-01-13

**Summary:** GLM-Image is a new image generation model with a hybrid autoregressive + diffusion decoder architecture. It excels in text-rendering and knowledge-intensive tasks while maintaining high-fidelity image generation capabilities. The model supports various image-to-image tasks and has been released under an MIT license.

**Key Points:**
- Hybrid autoregressive + diffusion decoder architecture
- Excels in text-rendering and knowledge-intensive generation
- Supports image editing, style transfer, and multi-subject consistency
- Released under MIT license
- Model size: 13GB diffusion model + 20GB text encoder

**Discussion Highlights:** The community appreciates the MIT license and the model's capabilities. There is excitement about its performance compared to other models and anticipation for quantized versions for easier use.

---

## 21. [My wishes for 2026](https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/)

**Author:** u/jacek2023 | **Upvotes:** 649 | **Comments:** 179 | **Date:** 2026-01-13

**Summary:** The Reddit post discusses predictions for 2026, focusing on the feasibility of affordable GPUs with more than 32GB of memory. The community expresses skepticism and humor about the likelihood of such advancements.

**Key Points:**
- The post asks which predictions for 2026 are likely to happen first.
- A top comment highlights the desire for affordable GPUs with >32GB memory.
- Other comments express skepticism and humor about the feasibility of affordable GPUs.
- Mentions of AI models like Qwen 4 and Mistral as potential advancements.

**Discussion Highlights:** The discussion is marked by a mix of humor and skepticism regarding the possibility of affordable high-memory GPUs in 2026. There is no clear consensus, but the community engages in playful banter about the topic.

---

## 22. [kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop—no GPU required](https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/)

**Author:** u/Nunki08 | **Upvotes:** 398 | **Comments:** 93 | **Date:** 2026-01-13

**Summary:** Kyutai introduced Pocket TTS, a 100M-parameter text-to-speech model with high-quality voice cloning that runs on a laptop without requiring a GPU. The model is available on GitHub and Hugging Face, with additional details provided in a blog post and arXiv paper.

**Key Points:**
- Pocket TTS is a 100M-parameter text-to-speech model.
- It supports high-quality voice cloning.
- The model runs on a laptop without needing a GPU.
- Resources include a blog post, GitHub repository, Hugging Face model card, and arXiv paper.
- Community feedback highlights concerns about memory usage and language support.

**Discussion Highlights:** The community discussion includes questions about language support and warnings about memory usage during generation. Some users suggest that smaller models may not be as effective as larger, more established alternatives.

---

## 23. [GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/)

**Author:** u/TKGaming_11 | **Upvotes:** 375 | **Comments:** 93 | **Date:** 2026-01-12

**Summary:** The Reddit post highlights DeepSeek-AI's Engram project, which introduces a novel approach to memory in large language models using scalable lookup and n-gram embeddings. The community praises the originality and technical depth of the work.

**Key Points:**
- DeepSeek-AI's Engram project introduces conditional memory via scalable lookup.
- The approach uses n-gram embeddings as a complementary sparsity axis with O(1) lookup.
- The community appreciates the originality and technical depth of the work.
- Comparisons are drawn to biological memory processes in animals and humans.

**Discussion Highlights:** The discussion highlights the technical novelty of the n-gram embedding approach and the community's positive reception of DeepSeek's innovative work. There is also a consensus on the potential biological parallels of this memory mechanism.

---

## 24. [LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)](https://reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 1061 | **Comments:** 114 | **Date:** 2026-01-11

**Summary:** The post introduces TimeCapsuleLLM, a 1.2B parameter language model trained exclusively on 1800-1875 London texts to minimize modern bias. The model demonstrates period-specific knowledge and behaviors, such as unfamiliarity with post-1875 concepts like telephones. The project is open-source and available on GitHub and Hugging Face.

**Key Points:**
- TimeCapsuleLLM is trained on 90GB of 1800-1875 London texts with no modern data or fine-tuning.
- The model exhibits period-specific behaviors, such as generating arguments relevant to the Catholic Emancipation Act of 1829.
- The model is unfamiliar with concepts post-1875, like telephones, treating them as unknown terms.
- Future steps include creating synthetic Q&A pairs from the dataset.
- The project has garnered significant community interest and support.

**Discussion Highlights:** The community shows strong enthusiasm for the project, with comments highlighting its uniqueness and potential. Some users share similar interests in training models on historical datasets, and there is a general consensus on the novelty and value of the approach.

---

## 25. [I bought a €9k GH200 “desktop” to save $1.27 on Claude Code (vLLM tuning notes)](https://reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/)

**Author:** u/Reddactor | **Upvotes:** 692 | **Comments:** 177 | **Date:** 2026-01-11

**Summary:** The author built a €9k GH200 'desktop' to run Claude Code locally, achieving better speeds and results than the cloud version. They shared optimized vLLM settings for dual 96GB systems and highlighted the cost savings and performance benefits of local execution.

**Key Points:**
- Author spent €9k on a GH200 setup to run Claude Code locally.
- Achieved better speeds and results compared to cloud-based Claude Code.
- Shared optimized vLLM settings for dual 96GB systems.
- Highlighted cost savings and performance benefits of local execution.
- Discussion includes humor about cost justification and technical details.

**Discussion Highlights:** The discussion includes humorous comments about cost justification, technical queries about specific model versions, and general appreciation for the setup and results.

---

## 26. [It works! Abliteration can reduce slop without training](https://reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/)

**Author:** u/-p-e-w- | **Upvotes:** 399 | **Comments:** 129 | **Date:** 2026-01-11

**Summary:** The post discusses the use of abliteration to reduce 'slop' (flowery, cliched language) in LLM outputs without training. The author demonstrates this technique using the Heretic tool and provides a configuration file to achieve slop reduction in the Mistral Nemo model. The process is quick and does not require fine-tuning.

**Key Points:**
- Abliteration can reduce slop in LLM outputs without training.
- The Heretic tool was used with a specific configuration file to achieve this.
- The process took 2.5 hours on an A6000 but can be faster with quantization or reduced parameters.
- The technique shows a clear semantic separation in the model's layers.
- Community feedback is mixed, with some appreciating the reduction in slop while others find the output too dry.

**Discussion Highlights:** The community discussion highlights mixed opinions on the effectiveness of slop reduction. Some users appreciate the cleaner output, while others feel it lacks imagination or becomes too dry. There is also interest in whether this technique can be applied to other patterns or styles of writing.

---

## 27. [I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work](https://reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/)

**Author:** u/Ok-Pomegranate1314 | **Upvotes:** 896 | **Comments:** 147 | **Date:** 2026-01-09

**Summary:** The author successfully clustered three NVIDIA DGX Sparks, overcoming networking limitations by writing a custom NCCL plugin. This achievement enables distributed inference across all three nodes at high speeds, despite NVIDIA's official support only covering two-node clusters.

**Key Points:**
- Author clustered three DGX Sparks, exceeding NVIDIA's official support for two-node clusters.
- Custom NCCL plugin written in ~1500 lines of C to handle subnet-aware NIC selection and RDMA implementation.
- Achieved distributed inference at 8+ GB/s over RDMA, solving complex networking challenges.
- The solution is considered a significant technical feat, as noted by community feedback.
- GitHub repository provided for further exploration of the implementation.

**Discussion Highlights:** The community praised the technical achievement, with one comment highlighting the difficulty of working with NCCL and its significance for distributed computing. Questions were raised about scalability and performance gains, indicating strong interest in the solution's broader applicability.

---

## 28. [The reason why RAM has become so expensive](https://reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/)

**Author:** u/InvadersMustLive | **Upvotes:** 4541 | **Comments:** 380 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses the significant increase in RAM prices, with comments suggesting strategic monopolization by certain entities to control future demand and economic viability of competitors.

**Key Points:**
- RAM prices have increased dramatically, with some users reporting up to 10 times the cost compared to the previous year.
- There is speculation about monopolization of RAM resources to control future demand and economic viability of competitors.
- The post and comments highlight concerns about the economic impact on data centers, particularly in China.
- Some users express skepticism about the sustainability of the current pricing trend.

**Discussion Highlights:** The discussion primarily revolves around the economic implications of rising RAM prices, with a focus on potential monopolistic practices and their impact on competitors. Users also share personal experiences with the increased costs and express concerns about the future of data center operations.

---

## 29. [DeepSeek V4 Coming](https://reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/)

**Author:** u/External_Mood4719 | **Upvotes:** 502 | **Comments:** 109 | **Date:** 2026-01-09

**Summary:** DeepSeek is expected to release V4, a next-generation AI model with strong code-generation capabilities, outperforming mainstream models like Claude and GPT. The model shows improvements in handling long code prompts and overall reasoning ability.

**Key Points:**
- DeepSeek V4 focuses on strong code-generation capabilities
- Outperforms existing models like Claude and GPT in code generation
- Improved handling of long code prompts and data patterns
- Enhanced reasoning ability and reliability for complex tasks
- Users appreciate DeepSeek's cost-effectiveness and performance

**Discussion Highlights:** Users express excitement and anticipation for V4, with some noting its potential to disrupt the AI landscape. There is consensus on DeepSeek's cost-effectiveness and performance, with expectations of significant improvements in V4.

---

## 30. [(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability](https://reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/)

**Author:** u/Nunki08 | **Upvotes:** 486 | **Comments:** 102 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release a new flagship AI model with strong coding capabilities, generating significant interest and discussion in the community.

**Key Points:**
- DeepSeek's upcoming model focuses on strong coding abilities
- The announcement has sparked excitement and anticipation
- Community members express enthusiasm for more AI models and competition
- Some comments reflect skepticism about marketing claims
- Discussion includes hopes for improved role-playing capabilities

**Discussion Highlights:** The community shows strong interest and excitement about DeepSeek's new model, with some expressing enthusiasm for increased competition in AI development. There is also skepticism about marketing claims and hopes for improved capabilities beyond coding.

---

## 31. [The NO FAKES Act has a "Fingerprinting" Trap that kills Open Source. We need to lobby for a Safe Harbor.](https://reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/)

**Author:** u/PostEasy7183 | **Upvotes:** 614 | **Comments:** 88 | **Date:** 2026-01-08

**Summary:** The NO FAKES Act proposes a 'digital replica right' that could hold developers liable for hosting open-source AI models used to create deepfakes, potentially stifling innovation. The post urges lobbying for a 'Safe Harbor' provision to protect open-source developers.

**Key Points:**
- The NO FAKES Act targets developers who 'make available' tools primarily used for creating digital replicas.
- Developers could face statutory damages of $5k-$25k per violation, with no Section 230 protection.
- The post calls for a 'Safe Harbor' provision to protect open-source developers.
- The discussion highlights concerns about the bill favoring big tech and stifling innovation.
- Action items include emailing or calling representatives to oppose the bill unless amended.

**Discussion Highlights:** The discussion reflects strong opposition to the bill, with concerns about its impact on innovation and the potential for big tech monopolies. Many commenters express skepticism about politicians' understanding of technology and the bill's implications.

---

## 32. [Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt](https://reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/)

**Author:** u/Prior-Arm-6705 | **Upvotes:** 939 | **Comments:** 147 | **Date:** 2026-01-08

**Summary:** The post describes a project where someone counted and compiled every instance of Jensen Huang saying 'AI' (121 times) during his NVIDIA CES 2025 keynote using open-source tools. The process involved downloading the video, parsing subtitles for timestamps, and editing clips to create a compilation video. Key points include the use of open-source tools, the automated process, and the video's popularity. Discussion highlights include reactions to the project's popularity and humorous comments about Jensen Huang's influence and attire.

---

## 33. [16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/)

**Author:** u/ai-infos | **Upvotes:** 462 | **Comments:** 238 | **Date:** 2026-01-07

**Summary:** The post discusses running Deepseek v3.2 on 16 AMD MI50 GPUs, achieving 10 tokens/sec output and 2000 tokens/sec input with a 69000 context length. The setup aims for cost-effective local AGI readiness, drawing 550W idle and 2400W peak power.

**Key Points:**
- Performance: 10 tok/s output, 2000 tok/s input, 69000 context length
- Power draw: 550W idle / 2400W peak
- Goal: Cost-effective local AGI setup using 16 AMD MI50 GPUs
- Future plan: 32 AMD MI50 setup for Kimi K2 Thinking
- Alternative to expensive CPU hardware with high bandwidth and tensor parallelism

**Discussion Highlights:** Comments highlight the power efficiency as a heating solution, curiosity about noise levels and home power usage, and the cost-effectiveness for professional developers. The community appreciates the open-source contribution and sees it as a viable alternative to expensive hardware.

---

## 34. [DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.](https://reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/)

**Author:** u/Nunki08 | **Upvotes:** 668 | **Comments:** 54 | **Date:** 2026-01-07

**Summary:** The DeepSeek-R1 paper was recently updated, expanding from 22 pages to 86 pages, adding significant detail. The Reddit post highlights this update and includes discussions on potential new architectures and research directions.

**Key Points:**
- DeepSeek-R1's paper expanded from 22 to 86 pages
- The update includes substantial additional details
- Discussions mention potential new architectures like dsv4 + r2
- Research focus on linear attention and cache optimization
- Interest in how architectural improvements scale across model sizes

**Discussion Highlights:** The discussion highlights potential new architectures and research directions, with a focus on linear attention and scalability of improvements across different model sizes.

---

## 35. [A 30B Qwen Model Walks Into a Raspberry Pi… and Runs in Real Time](https://reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/)

**Author:** u/ali_byteshape | **Upvotes:** 497 | **Comments:** 79 | **Date:** 2026-01-06

**Summary:** The post discusses the successful optimization of a 30B Qwen model to run efficiently on a Raspberry Pi 5, achieving 8.03 TPS at 2.70 BPW while retaining 94.18% of BF16 quality. The author highlights the unique challenges and trade-offs in optimizing models for different hardware, particularly GPUs, and invites community feedback for further testing.

**Key Points:**
- A 30B Qwen model runs on a Raspberry Pi 5 with 8.03 TPS at 2.70 BPW, retaining 94.18% of BF16 quality.
- Optimization focuses on fitting within memory constraints and then maximizing TPS without sacrificing quality.
- GPU performance is influenced by kernel choice, leading to quirky behavior and sweet spots around ~4b.
- Community feedback is requested for testing different builds, batch sizes, and non-NVIDIA setups.
- Users reported successful runs on Raspberry Pi 5 with specific context settings.

**Discussion Highlights:** The community showed interest in testing the model on various setups, including Raspberry Pi clusters and non-NVIDIA hardware. Some users reported successful runs with specific configurations, while others suggested combining the model with other solutions like exo or MOE for enhanced performance.

---

## 36. [Performance improvements in llama.cpp over time](https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/)

**Author:** u/jacek2023 | **Upvotes:** 681 | **Comments:** 85 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses performance improvements in llama.cpp over time, highlighting significant progress and community recognition. The discussion focuses on GPU-specific optimizations, particularly for NVIDIA GPUs, and overall advancements in token generation speed.

**Key Points:**
- Performance gains in llama.cpp have been substantial, especially for NVIDIA GPUs.
- The community appreciates the progress, with mentions of special flairs and Discord features.
- Token generation speed has improved significantly, approaching the performance of alternative implementations like ik_llama.cpp.
- Prompt processing remains slower but has seen notable improvements.
- The discussion includes references to NVIDIA's blog on open-source AI tool upgrades.

**Discussion Highlights:** The community consensus highlights the impressive progress in llama.cpp performance, particularly for NVIDIA GPUs. Users note the near-parity with alternative implementations and appreciate the ongoing optimizations, despite some areas like prompt processing still lagging behind.

---

## 37. [For the first time in 5 years, Nvidia will not announce any new GPUs at CES — company quashes RTX 50 Super rumors as AI expected to take center stage](https://reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/)

**Author:** u/FullstackSensei | **Upvotes:** 625 | **Comments:** 196 | **Date:** 2026-01-05

**Summary:** Nvidia will not announce new GPUs at CES, focusing instead on AI. The post discusses limited supply of new GPUs, potential re-release of older models, and rising hardware prices, expressing concerns about future upgrades.

**Key Points:**
- Nvidia quashes RTX 50 Super rumors and shifts focus to AI at CES.
- Limited supply of RTX 5070Ti, 5080, and 5090, with rumors of RTX 3060 re-release.
- Rising prices of DDR5 RAM and storage, making upgrades costly.
- Community frustration over corporate greed and lack of consumer-focused announcements.
- Calls for alternative solutions, such as Chinese manufacturers flooding the market with high-capacity GPUs.

**Discussion Highlights:** The discussion highlights frustration among users over Nvidia's shift away from consumer GPUs, rising hardware costs, and the lack of viable upgrade paths. There is a consensus that corporate greed is driving these decisions, with some users humorously suggesting extreme measures like kicking Nvidia out of CES.

---

## 38. [llama.cpp performance breakthrough for multi-GPU setups](https://reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/)

**Author:** u/Holiday-Injury-9397 | **Upvotes:** 571 | **Comments:** 201 | **Date:** 2026-01-05

**Summary:** The ik_llama.cpp project achieved a 3x to 4x performance improvement for multi-GPU setups, enabling cost-effective use of multiple low-cost GPUs for local LLM inference.

**Key Points:**
- ik_llama.cpp introduces a new execution mode (split mode graph) for multi-GPU utilization.
- Performance improvements of 3x to 4x compared to previous methods.
- Cost-effective alternative to high-end enterprise GPUs.
- Significant speed improvements even on single GPU or CPU-only setups.
- Potential bottlenecks in hybrid inference setups noted by users.

**Discussion Highlights:** Users highlight the significant performance gains, cost-effectiveness, and potential bottlenecks in hybrid inference setups. There is consensus on the value of the ik_llama.cpp fork for both multi-GPU and single-GPU setups.

---

## 39. [Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched](https://reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/)

**Author:** u/ubrtnk | **Upvotes:** 380 | **Comments:** 195 | **Date:** 2026-01-03

**Summary:** The post discusses challenges faced by local LLMs in processing extreme breaking news events, such as the US attacking Venezuela, where models initially classified the event as a hoax despite credible sources. The author shares experiences with different models and their responses to the event.

**Key Points:**
- Local LLMs struggled to accept extreme breaking news as real, classifying it as misinformation.
- Different models (Qwen, Spark, GPT-OSS) had varying responses to the same event.
- Models required explicit credible sources to acknowledge the event's reality.
- Discussion highlights the bias and limitations of LLMs in processing unfamiliar geopolitical events.

**Discussion Highlights:** The discussion highlights the limitations of LLMs in processing extreme or unfamiliar events, with users sharing similar experiences and noting the models' biases. There is a consensus on the need for better handling of breaking news and the importance of credible sources.

---

## 40. [LeCun Says Llama 4 results "were fudged a little bit"](https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/)

**Author:** u/MrPecunius | **Upvotes:** 361 | **Comments:** 88 | **Date:** 2026-01-02

**Summary:** Yann LeCun, departing Meta AI Chief, confirmed that Llama 4 benchmark results were manipulated. This follows speculation about suspicious benchmarks and coincides with Zuckerberg sidelining the GenAI organization, leading to significant staff departures.

**Key Points:**
- LeCun confirms Llama 4 benchmark manipulation
- Zuckerberg sidelined Meta's GenAI organization
- Significant staff departures from Meta's AI division
- Llama 4's promised large model was never released
- Community disappointment over Llama's perceived failure

**Discussion Highlights:** The discussion highlights disappointment in Llama's perceived failure and the impact on open-source AI development. Users express concern over Meta's strategic missteps in AI, contrasting with the success of smaller labs. There is also appreciation for shared resources like the full article PDF.

---

## 41. [Qwen-Image-2512](https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/)

**Author:** u/Nunki08 | **Upvotes:** 718 | **Comments:** 122 | **Date:** 2025-12-31

**Summary:** The post announces the release of Qwen-Image-2512, a new model with various resources and demos available. It includes links to guides, repositories, and interactive demos, and highlights community reactions and usage examples.

**Key Points:**
- Qwen-Image-2512 is a newly released model with multiple resources available.
- Links to guides, GGUF files, and demos are provided.
- Community reactions include successful usage on low-end hardware and creative image generation examples.
- The model is available on platforms like Hugging Face, ModelScope, and GitHub.
- A special flair was given to the author for their contribution.

**Discussion Highlights:** The community is enthusiastic about the new model, with users sharing successful implementations on low-end hardware and creative image generation examples. The post was also featured on Discord, indicating its popularity.

---

## 42. [[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It’s running a raw Llama-7B instance with a 2048 token window.](https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/)

**Author:** u/simar-dmg | **Upvotes:** 741 | **Comments:** 110 | **Date:** 2025-12-30

**Summary:** A user reverse-engineered a Snapchat sextortion bot and discovered it was running a raw Llama-7B instance with a 2048 token window. The bot was vulnerable to a persona-adoption jailbreak, revealing its configuration and malicious payload.

**Key Points:**
- The bot used a Llama-7B model with a 2048 token context window and high temperature setting.
- A 'Grandma Protocol' jailbreak forced the bot to reveal its environment variables and configuration.
- The bot's erratic behavior was due to minimal hardware and high creativity settings.
- Scammers are using open-source models like Llama-7B to avoid API costs and censorship filters.
- The post sparked discussions about the reliability of information extracted from LLMs and the prevalence of hallucinations.

**Discussion Highlights:** The discussion highlighted skepticism about the accuracy of the extracted information, with many users pointing out that LLMs can hallucinate details. There was also a consensus that the use of open-source models for malicious purposes is becoming more common.

---

## 43. [Llama-3.3-8B-Instruct](https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/)

**Author:** u/jacek2023 | **Upvotes:** 462 | **Comments:** 77 | **Date:** 2025-12-29

**Summary:** The post announces the discovery and release of the Llama-3.3-8B-Instruct model, which was previously only accessible via Meta's API. The author successfully downloaded and shared the model in GGUF format after navigating Meta's finetuning API.

**Key Points:**
- Llama-3.3-8B-Instruct was previously only available through Meta's Llama API.
- The author found a way to download the model via Meta's finetuning API, despite UI and CORS issues.
- The model is now available in GGUF format on Hugging Face.
- The community is verifying the model's authenticity through benchmarks.
- There are discussions about the model's configuration, such as its 8K max position embeddings.

**Discussion Highlights:** The community is excited about the release, with ongoing benchmarks to confirm the model's authenticity. Some users are running private evaluations to compare it with other Llama models, while others question specific configurations like the 8K max position embeddings.

---

## 44. [Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.](https://reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 340 | **Comments:** 120 | **Date:** 2025-12-29

**Summary:** Z AI is set to go public with an IPO on January 8, aiming to raise $560 million, marking it as the first AI-native LLM company to list globally. The Reddit post and comments highlight concerns about the future of open-source AI models and the implications of Z AI's IPO.

**Key Points:**
- Z AI's IPO is scheduled for January 8, aiming to raise $560 million.
- Z AI is the first AI-native LLM company to list on the global market.
- Community concerns about the future of open-source AI models post-IPO.
- Discussion on the balance between subscription costs and GPU expenses for AI projects.
- Mixed reactions to Z AI's potential shift away from open-source models.

**Discussion Highlights:** The discussion highlights a consensus around concerns about the future of open-source AI models, with some users expressing skepticism about Z AI's commitment to open-source post-IPO. There is also a discussion on the cost-effectiveness of AI subscriptions versus GPU expenses for personal projects.

---

## 45. [Tencent just released WeDLM 8B Instruct on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 421 | **Comments:** 62 | **Date:** 2025-12-29

**Summary:** Tencent released WeDLM 8B Instruct on Hugging Face, a diffusion language model that runs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks. The community response highlights its potential and performance.

**Key Points:**
- WeDLM 8B Instruct is a diffusion language model
- It runs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks
- The model is released under Apache 2.0 license
- Community shows strong interest and positive feedback
- A 7B version is also available

**Discussion Highlights:** The community is excited about the performance and potential of the WeDLM models, with many users expressing interest in the Apache 2.0 license and the availability of both 7B and 8B versions.

---

## 46. [NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux](https://reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/)

**Author:** u/HumanDrone8721 | **Upvotes:** 442 | **Comments:** 185 | **Date:** 2025-12-27

**Summary:** NVIDIA has dropped Pascal support on Linux, causing issues for Arch Linux users. The post highlights concerns and discussions around this change, with users expressing worry and sharing experiences.

**Key Points:**
- NVIDIA's decision to drop Pascal support affects Linux users, particularly those on Arch Linux.
- The 24GB P40, a Pascal card, is mentioned as a favored model before price increases.
- Users express concern and anticipation of future impacts on their systems.
- Arch Linux's practice of moving legacy drivers to AUR is noted as a long-standing policy.

**Discussion Highlights:** The discussion reflects a mix of concern and acceptance, with users acknowledging Arch Linux's history of moving legacy drivers to AUR. Some users express nostalgia for Pascal cards and worry about future compatibility.

---

## 47. [Best Local LLMs - 2025](https://reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/)

**Author:** u/rm-rf-rm | **Upvotes:** 366 | **Comments:** 198 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses the best local LLMs of 2025, focusing on open weights models and categorizing them by application and memory footprint. Key points include the mention of models like Minimax M2.1 and GLM4.7, user preferences for models like Qwen3-4B-instruct and LFM2-8B-A1B, and the structured discussion with categories such as General, Agentic, Creative Writing, and Speciality. The discussion highlights the importance of detailed setup descriptions and usage contexts.

---

## 48. [NVIDIA has 72GB VRAM version now](https://reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/)

**Author:** u/decentralize999 | **Upvotes:** 460 | **Comments:** 148 | **Date:** 2025-12-26

**Summary:** The post discusses NVIDIA's new 72GB VRAM version, questioning its cost-effectiveness compared to 48GB and 96GB options. The community expresses mixed reactions, with some advocating for larger VRAM capacities.

**Key Points:**
- NVIDIA has released a 72GB VRAM version of their GPU.
- The community debates the cost and necessity of different VRAM sizes.
- Price comparisons show similar cost per gigabyte across different VRAM sizes.
- Some users suggest that larger VRAM capacities (e.g., 128GB) are needed.
- The choice of VRAM size is influenced by affordability and specific use cases.

**Discussion Highlights:** The discussion highlights a consensus that the price per gigabyte remains consistent across different VRAM sizes. Users generally recommend purchasing the largest VRAM capacity one can afford, with some advocating for even larger capacities like 128GB.

---

## 49. [Hard lesson learned after a year of running large models locally](https://reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/)

**Author:** u/inboundmage | **Upvotes:** 344 | **Comments:** 146 | **Date:** 2025-12-26

**Summary:** The author shares their experience running large language models locally, highlighting challenges with VRAM limitations, model scaling, and performance trade-offs. They conclude that local inference is viable for smaller models but requires significant hardware investment for larger ones.

**Key Points:**
- Running large models (e.g., 70B parameters) on consumer-grade hardware (RTX 3090) faces VRAM limitations and performance issues.
- Quantization and VRAM management techniques help but come with trade-offs in quality and stability.
- Local inference is feasible for privacy-sensitive tasks but may not match cloud-based solutions in speed and scalability.
- VRAM fragmentation and inefficient offloading to system RAM are significant challenges.
- Community suggestions include using llama.cpp for RAM offloading and considering multi-GPU setups.

**Discussion Highlights:** The discussion highlights practical challenges of local LLM inference, with consensus that consumer-grade hardware has limitations for large models. Suggestions include optimizing software choices (e.g., llama.cpp for RAM offloading) and hardware upgrades (e.g., additional GPUs or high-VRAM cards).

---

## 50. [I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA](https://reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/)

**Author:** u/CeFurkan | **Upvotes:** 1043 | **Comments:** 179 | **Date:** 2025-12-25

**Summary:** The post discusses the potential of GPU VRAM upgrade modifications to challenge NVIDIA's monopoly, highlighting their availability and popularity in China. Users share experiences and pricing details of these modified GPUs.

**Key Points:**
- GPU VRAM upgrade modifications are seen as a way to counter NVIDIA's monopoly.
- These modifications are already mainstream in China, with Alibaba offering various upgraded GPUs.
- Pricing ranges from $300 for a 2080Ti 22GB to $4000 for a 5090 96GB.
- Users report successful usage of modified GPUs, such as a 4090 with 48GB of memory.
- There is interest in the cost-effectiveness and performance of these modifications.

**Discussion Highlights:** The discussion highlights the availability and success of GPU VRAM modifications in China, with users expressing interest in their cost-effectiveness and performance. There is a consensus on the potential of these modifications to disrupt NVIDIA's monopoly.

---

