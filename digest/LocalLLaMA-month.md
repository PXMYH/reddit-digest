# r/LocalLLaMA Reading Digest

**Period:** 2026-01-25 to 2026-01-25
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [Your post is getting popular and we just featured it on our Discord!](https://reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/)

**Author:** u/roculus | **Upvotes:** 543 | **Comments:** 55 | **Date:** 2026-01-23

**Summary:** The Reddit post announces that a user's post has been featured on Discord and they have received a special flair. The community expresses annoyance at the bot's public posts, suggesting private messages would be better.

**Key Points:**
- The bot announces a user's post being featured on Discord and awards a special flair.
- The community finds the bot's public posts annoying and suggests private messages instead.
- There is suspicion about monetization through the Discord server.
- The subreddit already has a pinned thread about the Discord server.
- The community humorously notes the irony of the bot's post potentially gaining traction.

**Discussion Highlights:** The community consensus is that the bot's public posts are annoying and should be sent as private messages. There is also suspicion about monetization and humor about the potential traction of the post.

---

## 2. [Am I the only one who feels that, with all the AI boom, everyone is basically doing the same thing?](https://reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/)

**Author:** u/[deleted] | **Upvotes:** 396 | **Comments:** 186 | **Date:** 2026-01-22

**Summary:** The post discusses the redundancy of AI tools and applications during the AI boom, highlighting that many new tools are less polished versions of existing ones. The discussion reflects on the early days of AI technology and the enthusiasm driving shallow implementations. Key points include the trend of re-building similar tools, the focus on niche applications, and the hype phase of the AI field. The discussion highlights a consensus that the AI field is currently in a hype phase, with many enthusiasts and newcomers creating redundant tools, but also recognizes the potential for niche and specialized applications that address specific needs.

---

## 3. [Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp; 1.8B), Support for 10 languages](https://reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/)

**Author:** u/Nunki08 | **Upvotes:** 705 | **Comments:** 113 | **Date:** 2026-01-22

**Summary:** Qwen has open-sourced the Qwen3-TTS model family, including 5 models (0.6B & 1.8B) with support for 10 languages. The release includes resources like GitHub, Hugging Face, a blog post, a paper, and a demo.

**Key Points:**
- Qwen3-TTS model family open-sourced
- 5 models available (0.6B & 1.8B)
- Support for 10 languages
- Multiple resources provided (GitHub, Hugging Face, blog, paper, demo)
- Community feedback highlights both praise and concerns about model performance and compatibility

**Discussion Highlights:** The community appreciates Qwen's open-sourcing efforts but notes concerns about the English voice quality sounding like anime dubs and requests for better compatibility with tools like llama.cpp and mistral.rs.

---

## 4. [Qwen dev on Twitter!!](https://reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 730 | **Comments:** 60 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses Qwen's TTS model announcement, which is revealed to be from a vLLM leak. The community shares relevant links and discusses the model's release.

**Key Points:**
- Qwen's TTS model is announced
- The model is from a vLLM leak
- A Hugging Face link is shared for the model
- The thread is locked as announcements are out

**Discussion Highlights:** The community is focused on the TTS model's release, with some users sharing links and others confirming the model's origin from a leak.

---

## 5. [8x AMD MI50 32GB at 26 t/s (tg) with MiniMax-M2.1 and 15 t/s (tg) with GLM 4.7 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1qjaxfy/8x_amd_mi50_32gb_at_26_ts_tg_with_minimaxm21_and/)

**Author:** u/ai-infos | **Upvotes:** 311 | **Comments:** 127 | **Date:** 2026-01-21

**Summary:** The post details a cost-effective local inference setup using 8x AMD MI50 GPUs, achieving 26.8 tok/s with MiniMax-M2.1 and 15.6 tok/s with GLM 4.7, at a cost of $880 for 256GB VRAM. The setup is praised for its efficiency and stability in long context tasks.

**Key Points:**
- Performance: MiniMax-M2.1 at 26.8 tok/s and GLM 4.7 at 15.6 tok/s
- Cost: $880 for 256GB VRAM (early 2025 prices)
- Power draw: 280W idle / 1200W during inference
- Stability: Models are stable at long context lengths, suitable for coding agents
- Community reaction: Highly praised for cost-effectiveness and performance

**Discussion Highlights:** The community is highly impressed with the cost-effectiveness and performance of the setup, with comments highlighting the affordability of 256GB VRAM for under $1k and the practical usability of the models for long context tasks.

---

## 6. [You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?](https://reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/)

**Author:** u/Adventurous-Gold6413 | **Upvotes:** 543 | **Comments:** 306 | **Date:** 2026-01-20

**Summary:** The Reddit post discusses selecting local models for use with 64GB RAM and 16GB VRAM without internet access. The community suggests models like Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B, with a strong preference for GPT-OSS-120B due to its performance and hardware compatibility. Key points include the post asking for recommendations on local models, top comments suggesting specific models, and GPT-OSS-120B being highlighted for its performance and fit for the specified hardware. The discussion highlights a consensus around using GPT-OSS-120B for its performance and compatibility with the given hardware, with other models like Gemma 3 27B and GLM 4.5 Air also mentioned as viable options.

---

## 7. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 891 | **Comments:** 268 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, fully enclosed 10-GPU mobile AI system designed for running large MoE models and supporting graphic design tasks. The build, costing around $17k, balances performance and budget constraints while addressing mobility and enclosure challenges.

**Key Points:**
- The system features a Threadripper Pro 3995WX, 512GB DDR4, and 10 GPUs (8x 3090 + 2x 5090).
- It is designed for large MoE models, video generation, and high-detail image generation.
- The enclosure was a major challenge, solved using a Thermaltake Core W200 case.
- The build prioritizes mobility, enclosure, and cost-effectiveness.
- The discussion highlights include humor about the system's power and airflow concerns.

**Discussion Highlights:** The top comments include humor about the system's power requirements and its impressive capabilities. There is also a focus on the practicality of the enclosure and the innovative approach to fitting 10 GPUs into the case.

---

## 8. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 364 | **Comments:** 60 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the official support for GLM 4.7 Flash in llama.cpp, highlighting its community-driven development and performance improvements.

**Key Points:**
- GLM 4.7 Flash now officially supported in llama.cpp
- Support is community-driven, not by Z.ai developers
- Performance improvements noted, with some users reporting faster execution with certain settings
- Additional versions and resources shared by community members
- Mixed feedback on performance, with some users experiencing slower speeds with flash-attention

**Discussion Highlights:** The discussion highlights the community effort behind the integration and provides additional resources and performance insights. Users share their experiences with different settings and versions, contributing to a broader understanding of the implementation's effectiveness.

---

## 9. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 463 | **Comments:** 162 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights the effectiveness of GLM 4.7 Flash as a reliable local agent for various tasks, with users praising its performance and capabilities. The discussion includes comparisons with other models and notes on its performance and output quality. Key points include its reliability in agentic frameworks, successful task execution, comparisons with models like Nemotron 30B and Qwen3, anticipation for GGUFs, and performance benchmarks suggesting it is as smart as SEED OSS 36B but with better performance due to MoE. The discussion highlights comparisons with other models, anticipation for local testing with GGUFs, and notes on the model's performance and output quality. Users express enthusiasm for GLM 4.7 Flash's capabilities and reliability.

---

## 10. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 740 | **Comments:** 230 | **Date:** 2026-01-19

**Summary:** The Reddit post discusses the release of zai-org/GLM-4.7-Flash on Hugging Face, highlighting its popularity and technical features like MLA for efficient memory usage and a 200k context window.

**Key Points:**
- The model uses MLA, reducing KV cache memory consumption.
- It supports a full 200k context, making it accessible for many users.
- The community expresses excitement and nostalgia for larger models like 70b.
- The release is seen as promising and highly anticipated.

**Discussion Highlights:** The community is enthusiastic about the release, appreciating its technical advancements and accessibility. There is a sense of anticipation and nostalgia for larger models, with positive feedback on the model's potential.

---

## 11. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 347 | **Comments:** 102 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to stay within a ~10,000€ budget. The system is designed for running large language models (120B+ parameters) locally, with benchmark results showing strong performance across various models. Key points include maximizing VRAM for large models, a total cost of ~9,800€ with a 50% subsidy, high performance across models, and significant engagement in the discussion highlighting the impressive hardware and cost-effectiveness.

---

## 12. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 456 | **Comments:** 71 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses the potential delay in Qwen 4 development, with the lead developer indicating a focus on quality over speed. The community generally supports this approach, appreciating the emphasis on quality improvements.

**Key Points:**
- Qwen 4 development may be delayed as the team focuses on quality
- Community largely supports the decision to prioritize quality
- Some users caution against jumping to conclusions based on limited information
- There is appreciation for meaningful advancements over incremental updates
- The post gained significant attention with 456 upvotes and 71 comments

**Discussion Highlights:** The discussion highlights a consensus that focusing on quality is beneficial for the Qwen series. Users express support for taking the necessary time to make substantial improvements rather than rushing incremental updates. Some comments also advise caution against overinterpreting the developer's statement.

---

## 13. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 534 | **Comments:** 117 | **Date:** 2026-01-17

**Summary:** The author upgraded from MI100s to four R9700 GPUs, achieving 128GB VRAM and 128GB RAM for a cost-effective high-performance system. The post details the hardware specifications, benchmarks, and community reactions.

**Key Points:**
- Upgrade from MI100s to R9700s for better performance and cost efficiency
- System specs include 128GB VRAM, 128GB RAM, and detailed hardware components
- Benchmarks show high performance in prompt processing
- Community reactions highlight appreciation and humor about financial irresponsibility
- Total cost of the build is $7,035, which is competitive with high-end alternatives

**Discussion Highlights:** The community showed strong appreciation for the build, with top comments highlighting its popularity, humor about financial irresponsibility, and general admiration for the system's capabilities.

---

## 14. [Best "End of world" model that will run on 24gb VRAM](https://reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/)

**Author:** u/gggghhhhiiiijklmnop | **Upvotes:** 341 | **Comments:** 180 | **Date:** 2026-01-17

**Summary:** The user is looking for recommendations on the best LLM models that can run on a PC with 24GB VRAM and 64GB RAM, aiming to hoard data like Wikipedia and other educational resources. The discussion includes suggestions for specific models and practical advice on data storage.

**Key Points:**
- User wants to download and store large datasets like Wikipedia, Wiktionary, etc.
- Looking for LLM models that fit within 24GB VRAM and 64GB RAM constraints
- Suggestions include Gemma3:27b and practical advice on data storage
- Discussion highlights the importance of saving data for potential end-of-world scenarios
- Recommendations for downloading actual Wikipedia backups for offline use

**Discussion Highlights:** The discussion features a mix of practical advice and model recommendations. The top comment suggests saving the best possible LLM and running it off SSD if necessary. Other notable suggestions include Gemma3:27b for its capabilities and vision features, and a reminder to download actual Wikipedia backups for offline use.

---

## 15. [GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)](https://reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/)

**Author:** u/CuriousPlatypus1881 | **Upvotes:** 381 | **Comments:** 89 | **Date:** 2026-01-16

**Summary:** The post discusses the updated SWE-bench leaderboard results from December 2025, highlighting the performance of various AI models on GitHub PR tasks. Claude Opus 4.5 leads with a 63.3% resolved rate, followed closely by GPT-5.2 (extra high effort) at 61.5%. The post also notes the strong performance of open-source models like GLM-4.7.

**Key Points:**
- Claude Opus 4.5 leads the leaderboard with a 63.3% resolved rate.
- GPT-5.2 (extra high effort) follows closely at 61.5%.
- Gemini 3 Flash Preview outperforms Gemini 3 Pro Preview despite being smaller and cheaper.
- GLM-4.7 is the strongest open-source model, ranking alongside closed models like GPT-5.1-codex.
- GPT-OSS-120B shows significant performance improvement in high-effort reasoning mode.

**Discussion Highlights:** The discussion highlights excitement around the performance of open-source models like GLM-4.7 and anticipation for future releases like DeepSeek v4. There is also a consensus that this benchmark is more believable compared to others.

---

## 16. [I fucking love this community](https://reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/)

**Author:** u/alhinai_03 | **Upvotes:** 526 | **Comments:** 54 | **Date:** 2026-01-16

**Summary:** The author expresses gratitude to the open-source community for enabling them to run large language models on older hardware, achieving impressive performance metrics. They highlight the importance of system memory and MoE architecture for their setup.

**Key Points:**
- Gratitude towards the open-source community for their contributions
- Achieving 14-13.5 tokens per second on a 10-year-old PC with limited GPU VRAM
- Importance of system memory and MoE architecture for running large models
- Community appreciation for optimization efforts
- Discussion on practicality of using system RAM and MoE models

**Discussion Highlights:** The community appreciates the author's achievement and discusses the practicality of using system RAM and MoE models for running large language models on older hardware. There is a consensus on the effectiveness of these optimizations.

---

## 17. [My story of underestimating /r/LocalLLaMA's thirst for VRAM](https://reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/)

**Author:** u/EmPips | **Upvotes:** 1351 | **Comments:** 91 | **Date:** 2026-01-15

**Summary:** The Reddit post discusses the author's experience with the subreddit's high demand for VRAM, as indicated by the title and the context of the comments. The post gained significant attention, as shown by the upvotes and comments.

**Key Points:**
- The post gained popularity and was featured on Discord.
- The author received a special flair for their contribution.
- Discussion includes references to hardware recommendations and market trends.
- Comments highlight the community's engagement and interest in the topic.

**Discussion Highlights:** The discussion highlights include a mix of community engagement, hardware recommendations, and market insights. The top comments provide additional context and humor, contributing to the overall discussion.

---

## 18. [Latest upgrade…A100 40 GB](https://reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/)

**Author:** u/inserterikhere | **Upvotes:** 407 | **Comments:** 54 | **Date:** 2026-01-15

**Summary:** The user upgraded their gaming rig to an AI rig by purchasing a faulty A100 GPU for $1000, which worked perfectly upon installation. They detailed their journey from using a 5070ti to eventually acquiring the A100, despite initial plans to sell their old parts.

**Key Points:**
- User transitioned from a gaming rig to an AI rig using existing parts.
- Purchased a faulty A100 GPU for $1000, which worked upon installation.
- Community expressed concerns about cooling the A100 GPU.
- Post received positive reception with 407 upvotes and 54 comments.
- User shared their upgrade journey and successful integration of the A100.

**Discussion Highlights:** The community reacted positively to the post, with some users expressing concerns about cooling the A100 GPU. One comment suggested using a blower fan or water cooling to prevent overheating. The post was also featured on Discord, indicating its popularity.

---

## 19. [Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M](https://reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/)

**Author:** u/eugenekwek | **Upvotes:** 325 | **Comments:** 54 | **Date:** 2026-01-14

**Summary:** The post announces Soprano 1.1, an improved version of the Soprano TTS model with significant reductions in hallucinations and audio artifacts, along with enhanced stability and support for longer sentences. The community response is overwhelmingly positive, with users praising the model's performance and expressing interest in future developments.

**Key Points:**
- Soprano 1.1 reduces hallucinations by 95% and lowers WER by 50% compared to the previous version.
- The model now supports sentences up to 30 seconds long, doubling the previous limit.
- A blind study showed a 63% preference rate for Soprano 1.1 over the original model.
- Community feedback highlights the model's impressive performance for its size (80M parameters).
- Users are inquiring about future support, such as ONNX compatibility.

**Discussion Highlights:** The community response is highly positive, with users expressing surprise at the model's quality given its small size. There is interest in additional features and support, such as ONNX compatibility, and appreciation for the developer's efforts.

---

## 20. [NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency](https://reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/)

**Author:** u/Fear_ltself | **Upvotes:** 711 | **Comments:** 130 | **Date:** 2026-01-14

**Summary:** NVIDIA's Orchestrator-8B is an 8-billion-parameter AI designed to manage and route complex tasks to various tools for greater efficiency. The post suggests this approach is a step towards more functional AI systems by integrating separate components effectively.

**Key Points:**
- Orchestrator-8B is a specialized AI model for task management and routing.
- The model aims to enhance efficiency by leveraging other tools and models.
- The post argues that integrating separate AI components is key to achieving functional systems.
- Top comments humorously compare the model to a 'Middle manager LLM' and discuss the potential of agentic frameworks.
- Some comments mention that similar concepts have been explored before.

**Discussion Highlights:** The discussion highlights a mix of humor and technical insights, with some users comparing the model to a middle manager and others discussing the potential of agentic frameworks and the integration of multiple models. There is also a mention of similar concepts being explored previously.

---

## 21. [GLM-Image is released!](https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/)

**Author:** u/foldl-li | **Upvotes:** 599 | **Comments:** 83 | **Date:** 2026-01-13

**Summary:** GLM-Image is a new image generation model with a hybrid autoregressive + diffusion decoder architecture. It excels in text-rendering and knowledge-intensive tasks while supporting various image-to-image tasks.

**Key Points:**
- Hybrid autoregressive + diffusion decoder architecture
- Excels in text-rendering and knowledge-intensive generation
- Supports image editing, style transfer, and multi-subject consistency
- MIT license with no restrictions
- High benchmark scores comparable to other models

**Discussion Highlights:** The community appreciates the MIT license and the model's capabilities. There is excitement about its performance and potential for quantization and optimization.

---

## 22. [My wishes for 2026](https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/)

**Author:** u/jacek2023 | **Upvotes:** 655 | **Comments:** 179 | **Date:** 2026-01-13

**Summary:** The Reddit post discusses predictions for 2026, focusing on the possibility of affordable GPUs with more than 32GB memory. The community engages in a mix of hopeful and skeptical comments about this prospect.

**Key Points:**
- The post asks which predictions for 2026 are likely to happen first.
- A top comment humorously doubts the affordability of GPUs with more than 32GB memory.
- Another comment references 'Qwen 4' and 'Mistral' as potential developments, while others are seen as unlikely.
- The community shows a mix of optimism and skepticism about technological advancements in 2026.

**Discussion Highlights:** The discussion highlights a divide in opinions, with some users humorously dismissing the idea of affordable high-memory GPUs, while others reference specific models like 'Qwen 4' and 'Mistral' as plausible developments. The overall tone is a mix of hope and skepticism about technological progress in 2026.

---

## 23. [kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop—no GPU required](https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/)

**Author:** u/Nunki08 | **Upvotes:** 396 | **Comments:** 93 | **Date:** 2026-01-13

**Summary:** Kyutai introduced Pocket TTS, a 100M-parameter text-to-speech model with high-quality voice cloning that runs on a laptop without requiring a GPU. The model is open-source with resources available on GitHub, Hugging Face, and detailed in an arXiv paper.

**Key Points:**
- Pocket TTS is a lightweight (100M parameters) TTS model with high-quality voice cloning capabilities.
- The model runs efficiently on a laptop without needing a GPU.
- Resources include a blog post, GitHub repository, Hugging Face model card, arXiv paper, and social media updates.
- Users discussed potential fine-tuning for different languages and noted memory usage issues during local testing.
- Some users questioned the practicality of small models compared to established alternatives.

**Discussion Highlights:** The discussion highlighted interest in multilingual support and potential fine-tuning, while also noting technical issues like memory usage during local testing. Some users expressed skepticism about the practicality of small models compared to established alternatives.

---

## 24. [GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/)

**Author:** u/TKGaming_11 | **Upvotes:** 370 | **Comments:** 93 | **Date:** 2026-01-12

**Summary:** The Reddit post highlights a GitHub repository by DeepSeek-AI introducing 'Engram,' a novel approach for conditional memory in large language models using scalable lookup. The community praises the originality and technical depth of the paper, noting its potential impact on model efficiency.

**Key Points:**
- DeepSeek-AI's 'Engram' introduces conditional memory via scalable lookup as a new sparsity axis for LLMs.
- The method uses n-gram embeddings to add static memory, complementing neural computation with O(1) lookup.
- The community appreciates the originality and technical rigor of the paper.
- Ablation studies suggest the method was derisked using models with mHC (M=4).
- The approach is seen as an obvious yet innovative step, potentially mimicking biological memory processes.

**Discussion Highlights:** The discussion highlights strong community approval of DeepSeek's work, with particular interest in the n-gram embedding approach and its potential to enhance model efficiency. The consensus is that 'Engram' offers a promising new direction for memory management in LLMs, with some users drawing parallels to biological memory systems.

---

## 25. [LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)](https://reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 1063 | **Comments:** 114 | **Date:** 2026-01-11

**Summary:** The post introduces TimeCapsuleLLM, an open-source project training language models from scratch on 1800s London texts to reduce modern bias. The latest model, with 1.2B parameters and a 90GB dataset, generates contextually relevant outputs based on historical data.

**Key Points:**
- The model is trained exclusively on texts from London between 1800-1875, with no modern data or fine-tuning.
- It uses a custom tokenizer and has been trained for 182k steps on an H100 SXM.
- Example outputs show the model's ability to generate historically relevant arguments and its unfamiliarity with post-1875 concepts like the telephone.
- The project aims to create synthetic Q&A pairs using the dataset in future steps.
- The community response is overwhelmingly positive, with users praising the project's uniqueness and potential.

**Discussion Highlights:** The discussion highlights the community's enthusiasm for the project, with users expressing interest in similar historical datasets and appreciating the model's historical accuracy. Some comments humorously reference the model's cutoff date and its limitations.

---

## 26. [I bought a €9k GH200 “desktop” to save $1.27 on Claude Code (vLLM tuning notes)](https://reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/)

**Author:** u/Reddactor | **Upvotes:** 696 | **Comments:** 177 | **Date:** 2026-01-11

**Summary:** The author built a high-end 'desktop' with dual GH200 GPUs to run Claude Code locally, achieving better speeds and results than the cloud version. Despite the high cost, the setup allows for offline coding with optimized vLLM settings.

**Key Points:**
- Author spent €9k on a dual GH200 96GB system to run Claude Code locally.
- Achieved better performance than cloud-based Claude Code with Sonnet.
- Shared optimized vLLM settings for dual 96GB systems in a Docker setup.
- Highlighted the cost savings in a humorous context, noting it was 321X the yearly subscription fee.
- Community praised the setup but joked about the high cost and energy consumption.

**Discussion Highlights:** The community appreciated the technical achievement and humor in the post, with comments highlighting the high cost, energy consumption, and the fun aspect of the project. Some users expressed envy over missing out on similar hardware deals.

---

## 27. [It works! Abliteration can reduce slop without training](https://reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/)

**Author:** u/-p-e-w- | **Upvotes:** 400 | **Comments:** 129 | **Date:** 2026-01-11

**Summary:** The post discusses using abliteration to reduce 'slop' (flowery, cliched language) in LLM outputs, specifically with Mistral Nemo. The author successfully applied this technique without finetuning, achieving noticeable results in 2.5 hours. Community feedback is mixed, with some appreciating the reduction in slop while others find the output too dry.

**Key Points:**
- Abliteration can reduce slop in LLM outputs without training.
- The technique was applied to Mistral Nemo, showing semantic separation between layers 7 and 10.
- The process took 2.5 hours on an A6000 but can be faster with quantization.
- Community feedback is mixed, with some preferring the reduced slop and others finding the output lacking imagination.
- GGUF files for the slop-reduced model are available for download.

**Discussion Highlights:** The discussion highlights a divide in opinion: some users appreciate the reduction in slop, finding it more concise, while others feel it makes the prose too dry and lacks creativity. There is also interest in whether this technique could be applied to other overused patterns in LLM outputs.

---

## 28. [I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work](https://reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/)

**Author:** u/Ok-Pomegranate1314 | **Upvotes:** 898 | **Comments:** 147 | **Date:** 2026-01-09

**Summary:** The author successfully clustered three NVIDIA DGX Sparks, overcoming networking limitations by writing a custom NCCL plugin. This achievement allows distributed inference across all three nodes at high speeds, despite NVIDIA's official support for only two-node clustering.

**Key Points:**
- Author clustered three DGX Sparks, exceeding NVIDIA's official support for two-node clustering.
- Developed a custom NCCL network plugin to handle subnet-aware NIC selection and raw RDMA implementation.
- Achieved distributed inference at 8+ GB/s over RDMA, showcasing significant technical prowess.
- The solution involved extensive low-level debugging and custom protocol implementation.
- The community praised the achievement, noting its potential impact and technical difficulty.

**Discussion Highlights:** The community highlighted the technical difficulty and potential significance of the achievement, with discussions focusing on scalability, speedup factors, and the broader implications for DGX Spark clusters.

---

## 29. [The reason why RAM has become so expensive](https://reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/)

**Author:** u/InvadersMustLive | **Upvotes:** 4555 | **Comments:** 381 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses the significant increase in RAM prices, with users noting a rise of up to 10 times the previous cost. The discussion highlights concerns about market manipulation and monopolization of key resources by major players like OpenAI.

**Key Points:**
- RAM prices have increased dramatically, with some users reporting a 10-fold rise.
- There are concerns about monopolization of RAM resources by companies like OpenAI.
- The price increase is seen as a strategy to create future demand and make competitors economically unviable.
- Users express skepticism about the sustainability of the price surge, with comments like 'its totally not a bubble or anything...'.

**Discussion Highlights:** The discussion consensus suggests that the sharp increase in RAM prices is driven by strategic market control rather than natural supply and demand factors. Users are critical of the potential monopolistic practices and their impact on the AI industry.

---

## 30. [DeepSeek V4 Coming](https://reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/)

**Author:** u/External_Mood4719 | **Upvotes:** 502 | **Comments:** 109 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release V4, a next-generation AI model with enhanced code-generation capabilities, outperforming mainstream models like Claude and GPT. The model shows improvements in handling long code prompts and overall reasoning ability.

**Key Points:**
- DeepSeek V4 focuses on strong code-generation capabilities.
- V4 outperforms existing models like Claude and GPT in code generation.
- Improved handling of long code prompts and data pattern understanding.
- Users anticipate V4 to be more logically rigorous and reliable.
- Discussion highlights include excitement, cost-effectiveness, and potential delays.

**Discussion Highlights:** Users express excitement about V4's potential, with some noting its cost-effectiveness and reliability. There is also speculation about possible delays due to extensive pre-training and post-training processes.

---

## 31. [(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability](https://reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/)

**Author:** u/Nunki08 | **Upvotes:** 486 | **Comments:** 102 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release a new flagship AI model with strong coding abilities, generating excitement and discussion in the community.

**Key Points:**
- DeepSeek's upcoming model focuses on strong coding capabilities
- The announcement has generated significant interest and excitement
- Community members are looking forward to more details and benchmarks
- There is anticipation for improved performance in coding tasks
- Some users express concerns about potential limitations in role-playing abilities

**Discussion Highlights:** The community is largely excited about the new model, with many users expressing anticipation for its release and potential capabilities. Some comments reflect skepticism about marketing claims and hope for genuine performance improvements.

---

## 32. [The NO FAKES Act has a "Fingerprinting" Trap that kills Open Source. We need to lobby for a Safe Harbor.](https://reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/)

**Author:** u/PostEasy7183 | **Upvotes:** 611 | **Comments:** 88 | **Date:** 2026-01-08

**Summary:** The NO FAKES Act threatens open-source AI development by imposing liability on developers for tools used to create digital replicas, potentially banning open-source AI hosting in the US. The post urges lobbying for a Safe Harbor provision to protect developers.

**Key Points:**
- The NO FAKES Act creates a 'digital replica right' that could hold developers liable for tools used to fake voices/likenesses.
- Developers hosting TTS or voice-conversion models on platforms like HuggingFace could face statutory damages ($5k-$25k per violation).
- The bill lacks Section 230 protection, making open-source AI hosting legally risky.
- The post calls for a 'Safe Harbor' provision to protect open-source developers.
- Action items include emailing/calling representatives to oppose the bill unless amended.

**Discussion Highlights:** The discussion highlights concerns about the bill's impact on innovation, with comments suggesting it could turn the US into a 'third world nation' technologically. Some users believe big tech corporations are behind the anti-AI movement to stifle competition. There is also skepticism about whether politicians understand the technical implications of the bill.

---

## 33. [Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt](https://reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/)

**Author:** u/Prior-Arm-6705 | **Upvotes:** 938 | **Comments:** 147 | **Date:** 2026-01-08

**Summary:** The post describes a project where someone counted Jensen Huang saying 'AI' 121 times during his CES 2025 keynote and created a compilation video using open-source tools like Dive, yt-dlp-mcp, and ffmpeg-mcp-lite. The process involved downloading the video, parsing subtitles for timestamps, cutting clips, and merging them into a final video.

**Key Points:**
- Jensen Huang said 'AI' 121 times during his CES 2025 keynote.
- The project used open-source tools (Dive, yt-dlp-mcp, ffmpeg-mcp-lite) to create a compilation video.
- The process involved downloading, parsing subtitles, cutting clips, and merging them.
- The result was described as 'hypnotic'.
- Discussion highlights include reactions to the project, comments on AI costs, and references to tech content creators.

**Discussion Highlights:** The discussion includes reactions to the project, comments on the cost of AI, and references to tech content creators like Gamers Nexus. Some comments also humorously mention Jensen Huang's attire.

---

## 34. [16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/)

**Author:** u/ai-infos | **Upvotes:** 460 | **Comments:** 238 | **Date:** 2026-01-07

**Summary:** The post discusses running Deepseek v3.2 on 16 AMD MI50 GPUs, achieving 10 tokens per second (output) and 2000 tokens per second (input) with a 69,000 context length. The setup aims for cost-effective hardware and highlights power draw and future plans for scaling.

**Key Points:**
- Deepseek v3.2 runs on 16 AMD MI50 GPUs with 32GB memory.
- Achieves 10 tokens per second (output) and 2000 tokens per second (input).
- Power draw is 550W idle and 2400W peak during inference.
- Future plans include testing 32 AMD MI50 GPUs for Kimi K2 Thinking.
- The setup is cost-effective compared to CPU hardware.

**Discussion Highlights:** The discussion highlights the efficiency of the setup, with comments praising the power draw as a potential heating solution and expressing interest in the cost-effectiveness for professional use.

---

## 35. [DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.](https://reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/)

**Author:** u/Nunki08 | **Upvotes:** 670 | **Comments:** 54 | **Date:** 2026-01-07

**Summary:** The Reddit post discusses the recent update to DeepSeek-R1's paper, which expanded from 22 pages to 86 pages, adding significant detail. The discussion includes comments about potential new architectures, linear attention research, and the value of added implementation specifics.

**Key Points:**
- DeepSeek-R1's paper was updated from 22 pages to 86 pages.
- The update includes substantial additional detail.
- Discussion highlights potential new architectures and linear attention research.
- The original paper lacked implementation specifics, which the update may address.
- Community interest in how architectural improvements perform at different model sizes.

**Discussion Highlights:** The discussion focuses on the implications of the paper's expansion, potential new model architectures, and the importance of detailed implementation specifics. There is also interest in how these improvements scale across different model sizes.

---

## 36. [A 30B Qwen Model Walks Into a Raspberry Pi… and Runs in Real Time](https://reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/)

**Author:** u/ali_byteshape | **Upvotes:** 501 | **Comments:** 79 | **Date:** 2026-01-06

**Summary:** The post discusses the successful optimization of a 30B Qwen model to run efficiently on a Raspberry Pi 5, achieving 8.03 tokens per second while retaining 94.18% of BF16 quality. The optimization focuses on balancing memory usage and performance, particularly noting the quirks of GPU kernel choices. Key points include the model's performance on a Raspberry Pi 5, the optimization strategy prioritizing memory as a budget, the influence of GPU kernel choices on performance, the request for community feedback on testing, and a user's experience with adjusting context size to avoid segfaults. The community showed interest in testing the model on various hardware setups and workloads, with one user reporting a successful run on a Raspberry Pi 5 after adjusting the context size and speculation about combining the model with other solutions for distributed computing.

---

## 37. [Performance improvements in llama.cpp over time](https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/)

**Author:** u/jacek2023 | **Upvotes:** 683 | **Comments:** 85 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses performance improvements in llama.cpp over time, focusing on NVIDIA GPU enhancements and comparisons with other implementations. The discussion highlights significant progress in token generation speed.

**Key Points:**
- Performance gains are noted for NVIDIA GPUs
- NVIDIA's blog post is referenced for further details
- Comparisons with ik_llama.cpp show llama.cpp is approaching similar token generation speeds
- Prompt processing remains slower but overall progress is praised

**Discussion Highlights:** The community acknowledges substantial improvements in llama.cpp's token generation speed, with comparisons to ik_llama.cpp indicating near-parity. However, prompt processing is still noted to be slower. The discussion also references NVIDIA's official blog post for additional context on performance upgrades.

---

## 38. [For the first time in 5 years, Nvidia will not announce any new GPUs at CES — company quashes RTX 50 Super rumors as AI expected to take center stage](https://reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/)

**Author:** u/FullstackSensei | **Upvotes:** 624 | **Comments:** 195 | **Date:** 2026-01-05

**Summary:** Nvidia will not announce new GPUs at CES, focusing instead on AI. The post discusses limited supply of new GPUs, potential re-release of older models, and rising hardware prices, expressing concerns about future upgrades.

**Key Points:**
- No new GPU announcements at CES
- Limited supply of RTX 50 series GPUs
- Potential re-release of RTX 3060
- Rising prices of DDR5 RAM and storage
- Concerns about future hardware upgrades

**Discussion Highlights:** The discussion highlights frustration with corporate greed, the lack of new consumer-focused hardware, and the hope for alternative sources of hardware like China flooding the market with high-capacity GPUs.

---

## 39. [llama.cpp performance breakthrough for multi-GPU setups](https://reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/)

**Author:** u/Holiday-Injury-9397 | **Upvotes:** 573 | **Comments:** 201 | **Date:** 2026-01-05

**Summary:** The ik_llama.cpp project achieved a significant performance breakthrough for multi-GPU setups, delivering a 3x to 4x speed improvement in local LLM inference. This advancement allows for the use of multiple low-cost GPUs instead of expensive high-end cards, making it a game-changer for homelabs and cloud setups.

**Key Points:**
- ik_llama.cpp achieved a 3x to 4x speed improvement in multi-GPU setups.
- The new execution mode (split mode graph) enables maximum utilization of multiple GPUs.
- This breakthrough reduces the need for expensive high-end GPUs, making low-cost GPUs more viable.
- Performance improvements are also noted in single GPU and CPU-only setups.
- The project is seen as competitive with other performance-focused forks like exllama and vllm.

**Discussion Highlights:** The community is excited about the performance gains and the potential cost savings. There is a consensus that ik_llama.cpp is now a strong contender in the space, with some users reporting consistent 2x prompt processing speeds even on single GPUs. However, there are mentions of potential bottlenecks in hybrid inference setups.

---

## 40. [GLM-Image model from Z.ai is coming](https://reddit.com/r/LocalLLaMA/comments/1q41bw1/glmimage_model_from_zai_is_coming/)

**Author:** u/Ravencloud007 | **Upvotes:** 323 | **Comments:** 59 | **Date:** 2026-01-04

**Summary:** The post announces the upcoming GLM-Image model from Z.ai, which has generated significant interest in the community. Users are discussing its potential size and capabilities, with some expressing excitement about its parameters and others comparing it to existing models.

**Key Points:**
- GLM-Image model from Z.ai is being introduced
- The model is expected to have a large number of parameters (e.g., 103b)
- Z.ai's image model is currently a community favorite
- Users are discussing the computational resources needed to use the model
- There is a desire for a model that combines small size, ease of fine-tuning, and high quality

**Discussion Highlights:** The discussion highlights excitement about the new model's potential capabilities and size. There is a consensus that Z.ai's current image model is highly regarded, and users are curious about how the new model will compare. Some users are concerned about the computational resources required to use the model effectively.

---

## 41. [Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched](https://reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/)

**Author:** u/ubrtnk | **Upvotes:** 381 | **Comments:** 194 | **Date:** 2026-01-03

**Summary:** The post discusses challenges faced by local LLMs in processing extreme breaking news events, such as the US attacking Venezuela. The author shares experiences with different models like Qwen Research and Spark, highlighting their struggles to accept the reality of such events despite credible sources.

**Key Points:**
- Local LLMs struggled to accept extreme breaking news as real, classifying it as misinformation.
- Different models like Qwen Research and Spark had varying responses to the same event.
- Larger models like GPT-OSS:120B performed better in verifying and accepting the news.
- Users shared similar experiences with LLMs doubting unlikely but real events.
- Discussion highlights the bias and limitations of LLMs in processing unfamiliar geopolitical events.

**Discussion Highlights:** The discussion consensus indicates that LLMs often struggle with extreme or unlikely events, showing bias and skepticism even when provided with credible sources. Users shared similar experiences and noted the limitations of LLMs in processing unfamiliar geopolitical events.

---

## 42. [LeCun Says Llama 4 results "were fudged a little bit"](https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/)

**Author:** u/MrPecunius | **Upvotes:** 366 | **Comments:** 88 | **Date:** 2026-01-02

**Summary:** Yann LeCun, departing Meta AI Chief, confirmed that Llama 4 benchmark results were manipulated. This follows organizational changes at Meta, including the sidelining of the GenAI team, leading to departures and lack of progress on promised models.

**Key Points:**
- LeCun confirms Llama 4 benchmark manipulation
- Meta's GenAI organization was sidelined by Zuckerberg
- Many team members have left or are leaving Meta
- No follow-up on the promised large Llama 4 model
- Community expresses disappointment and shares additional resources

**Discussion Highlights:** The discussion reflects disappointment in Meta's handling of Llama 4, with users sharing additional resources and questioning how a major company could falter while smaller labs thrive. Some users also clarify organizational details and express concern over the shift away from open-source AI development.

---

## 43. [Qwen-Image-2512](https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/)

**Author:** u/Nunki08 | **Upvotes:** 721 | **Comments:** 122 | **Date:** 2025-12-31

**Summary:** The Reddit post announces the release of Qwen-Image-2512, a new model available on multiple platforms with guides and GGUF files. It has garnered significant community engagement and positive feedback.

**Key Points:**
- Qwen-Image-2512 is a new model with guides and GGUF files available.
- The model is accessible on platforms like Hugging Face, ModelScope, and GitHub.
- Community engagement is high with 721 upvotes and 122 comments.
- Users have successfully run the model on low-end hardware.
- Creative use cases and positive sentiment are highlighted in the comments.

**Discussion Highlights:** The discussion highlights successful usage on low-end hardware, creative applications, and overall positive sentiment towards the new model release.

---

## 44. [[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It’s running a raw Llama-7B instance with a 2048 token window.](https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/)

**Author:** u/simar-dmg | **Upvotes:** 738 | **Comments:** 110 | **Date:** 2025-12-30

**Summary:** A user reverse-engineered a Snapchat sextortion bot and discovered it was running a raw Llama-7B instance with a 2048 token window. The bot was vulnerable to a persona-adoption jailbreak, revealing its configuration and malicious payload.

**Key Points:**
- The bot used a Llama-7B model with a 2048 token context window and high temperature setting.
- A persona-adoption jailbreak (Grandma Protocol) forced the bot to reveal its environment variables.
- The bot was running on minimal hardware to maximize profit margins.
- The bot eventually revealed a malicious link it was programmed to hide.
- Scammers are increasingly using open-source models like Llama-7B to avoid API costs and censorship filters.

**Discussion Highlights:** The discussion highlights skepticism about the accuracy of the bot's revealed information, with some users suggesting it could be entirely hallucinated. Others questioned the commonality of system prompts including environment variables.

---

## 45. [Llama-3.3-8B-Instruct](https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/)

**Author:** u/jacek2023 | **Upvotes:** 467 | **Comments:** 77 | **Date:** 2025-12-29

**Summary:** The post discusses the discovery and release of the Llama-3.3-8B-Instruct model, which was previously only available through Meta's API. The author details the process of obtaining the model through finetuning and provides a download link. The community is actively evaluating the model's performance and authenticity.

**Key Points:**
- Llama-3.3-8B-Instruct model is now available for download.
- The model was obtained through Meta's finetuning API.
- Community is conducting benchmarks and evaluations.
- The model's authenticity and performance are being verified.

**Discussion Highlights:** The community is excited about the release and is actively running benchmarks and evaluations to confirm the model's authenticity and performance. Some users are questioning the model's specifications, such as the max position embeddings.

---

## 46. [Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.](https://reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 348 | **Comments:** 120 | **Date:** 2025-12-29

**Summary:** Z AI is set to go public with an IPO on January 8, aiming to raise $560 million, marking it as the first AI-native LLM company to list globally. The Reddit post and comments highlight concerns about the future of open-source models and the company's potential shift in focus.

**Key Points:**
- Z AI's IPO is scheduled for January 8 with a target of raising $560 million.
- Concerns about the future of open-source models post-IPO.
- Community reactions include skepticism and hopes for continued open-weight model releases.
- Discussion on the balance between commercial success and open-source contributions.
- Mixed feelings about the company's potential shift in focus and priorities.

**Discussion Highlights:** The discussion highlights a consensus of concern about the potential impact on open-source contributions, with some users expressing skepticism about the company's future direction. Others argue that commercial success is necessary and that open-weight models could still be released as a form of advertising. There is a notable sentiment of caution and hope for the company to maintain its open-source roots.

---

## 47. [Tencent just released WeDLM 8B Instruct on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 426 | **Comments:** 62 | **Date:** 2025-12-29

**Summary:** Tencent released WeDLM 8B Instruct on Hugging Face, a diffusion language model that outperforms vLLM-optimized Qwen3-8B in math reasoning tasks by running 3-6× faster. The release has garnered significant attention and positive feedback from the community.

**Key Points:**
- WeDLM 8B Instruct is a diffusion language model released by Tencent on Hugging Face.
- It runs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks.
- The model is released under the Apache 2.0 license.
- The community shows strong interest and positive feedback on the model's performance.
- A 7B version of the model is also available.

**Discussion Highlights:** The community highlights the impressive benchmark scores and the potential of 7-8B models. There is a consensus on the promising nature of diffusion models for LLMs, with calls for more models in this space.

---

## 48. [NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux](https://reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/)

**Author:** u/HumanDrone8721 | **Upvotes:** 445 | **Comments:** 185 | **Date:** 2025-12-27

**Summary:** NVIDIA has dropped Pascal support on Linux, causing issues for Arch Linux users. The post highlights concerns and discussions around this change, with users expressing worry and sharing experiences.

**Key Points:**
- NVIDIA's decision to drop Pascal support affects Linux users, particularly those on Arch Linux.
- The 24GB P40, a Pascal card, is mentioned as a favored model before it became expensive.
- Users express concern and anticipation of this change, with some noting it was expected.
- Arch Linux has a history of moving legacy drivers to AUR, as noted in Arch News.

**Discussion Highlights:** The discussion highlights a mix of concern and acceptance among users. Some users express worry about the impact on their systems, while others note that this change was anticipated and aligns with Arch Linux's practice of moving legacy drivers to the Arch User Repository (AUR).

---

## 49. [Best Local LLMs - 2025](https://reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/)

**Author:** u/rm-rf-rm | **Upvotes:** 368 | **Comments:** 198 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses the best local LLMs of 2025, focusing on open weights models and categorizing them by application and memory footprint. Users share their favorite models and usage details.

**Key Points:**
- Focus on open weights models only
- Categories include General, Agentic/Coding, Creative Writing, and Speciality
- Models classified by memory footprint: Unlimited (>128GB VRAM), Medium (8-128GB VRAM), Small (<8GB VRAM)
- Popular models mentioned: Minimax M2.1, GLM4.7, Qwen3-4B-instruct, LFM2-8B-A1B
- Discussion includes RAG for technical documentation and model evaluations

**Discussion Highlights:** Users highlight the performance of models like Qwen3-4B-instruct and LFM2-8B-A1B for small memory footprints. There is a consensus on the need for detailed setup descriptions and usage contexts. The discussion also touches on the use of RAG for technical documentation and the importance of model memory footprint classifications.

---

## 50. [NVIDIA has 72GB VRAM version now](https://reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/)

**Author:** u/decentralize999 | **Upvotes:** 465 | **Comments:** 148 | **Date:** 2025-12-26

**Summary:** The post discusses NVIDIA's new 72GB VRAM version, questioning the cost of 96GB and the AI community's interest in 48GB. The discussion highlights varying opinions on the need for larger VRAM capacities and price considerations.

**Key Points:**
- NVIDIA has released a 72GB VRAM version.
- Community debates the cost-effectiveness of 96GB vs. 72GB.
- Price per gig remains consistent across different VRAM sizes.
- Some users advocate for even larger VRAM capacities (e.g., 128GB).
- The choice depends on affordability and specific use-case requirements.

**Discussion Highlights:** The community is divided on the necessity of larger VRAM sizes, with some advocating for 128GB or more, while others focus on cost-effectiveness and affordability. The consensus leans towards buying the most VRAM one can afford, given the consistent price per gig.

---

