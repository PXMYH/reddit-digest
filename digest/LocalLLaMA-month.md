# r/LocalLLaMA Reading Digest

**Period:** 2026-01-24 to 2026-01-24
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [Your post is getting popular and we just featured it on our Discord!](https://reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/)

**Author:** u/roculus | **Upvotes:** 489 | **Comments:** 52 | **Date:** 2026-01-23

**Summary:** The Reddit post announces that a user's contribution has been featured on Discord and they have received a special flair. The community expresses annoyance at the bot's public posts, suggesting private messages would be more appropriate. Key points include the bot's announcement, the user's flair, community annoyance, the existence of a pinned Discord thread, and suspicions of monetization. The discussion highlights a consensus that bot posts should be private and skepticism about moderator intentions.

---

## 2. [Am I the only one who feels that, with all the AI boom, everyone is basically doing the same thing?](https://reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/)

**Author:** u/[deleted] | **Upvotes:** 388 | **Comments:** 187 | **Date:** 2026-01-22

**Summary:** The post discusses the redundancy in AI tools and applications during the AI boom, highlighting that many new tools are essentially reinventing existing solutions. The discussion reflects on the enthusiasm and low barrier to entry in the AI field, leading to shallow implementations and repetitive projects.

**Key Points:**
- Many AI tools and applications are redundant, reinventing existing solutions.
- The AI boom has led to a surge in enthusiasm and low barrier to entry.
- Shallow implementations and repetitive projects are common due to the hype.
- Some developers are focusing on niche tools and specific needs rather than generic AI applications.
- The current stage is seen as a hype phase, similar to past technological trends.

**Discussion Highlights:** The discussion highlights a consensus that the AI field is in a hype phase, with many redundant projects. However, there is also a focus on niche tools and specific needs, indicating a diverse range of applications and interests within the community.

---

## 3. [Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp; 1.8B), Support for 10 languages](https://reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/)

**Author:** u/Nunki08 | **Upvotes:** 699 | **Comments:** 105 | **Date:** 2026-01-22

**Summary:** Qwen has open-sourced the Qwen3-TTS model family, including 5 models (0.6B & 1.8B) with support for 10 languages. The release includes resources like GitHub, Hugging Face, a blog post, a paper, and a demo.

**Key Points:**
- Qwen3-TTS model family open-sourced
- 5 models available (0.6B & 1.8B)
- Support for 10 languages
- Multiple resources provided (GitHub, Hugging Face, blog, paper, demo)
- Community feedback highlights both praise and concerns about model performance and compatibility

**Discussion Highlights:** The community appreciates Qwen's open-sourcing efforts but notes concerns about the English voice quality sounding like anime dubs and requests for better compatibility with tools like llama.cpp and mistral.rs.

---

## 4. [Qwen dev on Twitter!!](https://reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 713 | **Comments:** 60 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses Qwen's TTS model announcement, with community reactions focusing on its relation to a vLLM leak and availability on Hugging Face.

**Key Points:**
- Qwen TTS model announced
- Mention of vLLM leak
- Hugging Face collection link provided
- Community discussion about model origins

**Discussion Highlights:** The community shows interest in the TTS model's origins, with some linking it to a vLLM leak and others sharing the Hugging Face collection link for further exploration.

---

## 5. [You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?](https://reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/)

**Author:** u/Adventurous-Gold6413 | **Upvotes:** 541 | **Comments:** 304 | **Date:** 2026-01-20

**Summary:** The post discusses the selection of local models for use with 64GB RAM and 16GB VRAM in an offline environment. Users share their preferred models and experiences. Key points include the focus on choosing local models for offline use with specific hardware constraints, mentions of top models like Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B, and the community's appreciation for the post. The discussion highlights a consensus around GPT-OSS-120B as a strong all-round model for the given hardware, with other models also recommended.

---

## 6. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 881 | **Comments:** 265 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, fully enclosed 10x GPU mobile AI system designed for running large MoE models and supporting graphic design tasks. The build, costing around $17k, features a Threadripper Pro 3995WX, 512GB DDR4, and a mix of 3090 and 5090 GPUs, with a focus on mobility and protection from pets.

**Key Points:**
- Custom-built system for large MoE models and graphic design tasks
- Features Threadripper Pro 3995WX, 512GB DDR4, and 10 GPUs (8x 3090 + 2x 5090)
- Designed to be mobile and fully enclosed to protect from pets
- Budget-conscious build aiming for high performance without unnecessary expenses
- Community reactions highlight the uniqueness and practicality of the build

**Discussion Highlights:** The community reactions include humor about the system's portability and power requirements, appreciation for the build's uniqueness, and curiosity about the physical setup of the GPUs. The top comments reflect a mix of admiration and playful banter, indicating strong engagement with the post.

---

## 7. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 362 | **Comments:** 60 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the official support for GLM 4.7 Flash in llama.cpp, highlighting its community-driven development and performance improvements. Key points include the community effort behind the integration, performance comparisons, and additional resources shared by users. The discussion highlights a consensus on the performance benefits, with some users noting faster speeds when disabling flash-attention.

---

## 8. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 462 | **Comments:** 162 | **Date:** 2026-01-19

**Summary:** The post highlights GLM 4.7 Flash as a reliable local agent model that performs well in agentic frameworks, with the author successfully using it for tasks like cloning repos and running commands without errors. The community is excited about its potential and compares it favorably to other models like Nemotron 30B and Qwen3.

**Key Points:**
- GLM 4.7 Flash is praised for its reliability in agentic tasks, handling long sessions without errors.
- The model is anticipated to be available locally soon, with GGUFs expected.
- Community discussions compare it to Nemotron 30B and note its performance benefits due to MoE architecture.
- Early benchmarks suggest it may be as capable as SEED OSS 36B but with better performance.
- Users are testing it locally with positive initial feedback on speed and output quality.

**Discussion Highlights:** The community is enthusiastic about GLM 4.7 Flash, with comparisons to other models and notes on its performance. Some users have already started testing it locally, reporting decent speed and deep reasoning capabilities. There is also interest in further comparisons and benchmarks.

---

## 9. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 745 | **Comments:** 231 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the release of the GLM-4.7-Flash model, which has generated significant interest and discussion in the community. Users are excited about its features, including efficient memory usage and long context support.

**Key Points:**
- GLM-4.7-Flash model has been released and is gaining popularity
- The model uses MLA, reducing KV cache memory consumption
- Supports full 200k context, making it accessible to more users
- Community expresses excitement and nostalgia for larger models
- Special recognition given to the post author for their contribution

**Discussion Highlights:** The community is enthusiastic about the GLM-4.7-Flash release, particularly its memory efficiency and long context capabilities. There's nostalgia for larger models but overall positive sentiment about this release's potential.

---

## 10. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 348 | **Comments:** 96 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to maximize VRAM for running large models locally. Benchmark results show impressive performance across various models, with the system costing around 9,800€ (effectively 4,900€ after refund).

**Key Points:**
- Built a system with 4x AMD R9700 GPUs (128GB VRAM) and Threadripper 9955WX CPU
- Leveraged a 50% subsidy to maximize VRAM for running large models locally
- Benchmark results show impressive performance across various models
- Total cost around 9,800€ (effectively 4,900€ after refund)
- System designed for data privacy and local model inference

**Discussion Highlights:** The discussion highlights include admiration for the build, questions about the source of the components and the author's job, and a mention of a similar build by another user. The post was also featured on Discord and received a special flair for the contribution.

---

## 11. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 454 | **Comments:** 71 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses the potential delay in Qwen 4 development, with the lead developer indicating a focus on quality over speed. The community generally supports this approach, appreciating the emphasis on quality improvements.

**Key Points:**
- Qwen 4 development may be delayed as the team focuses on quality
- Community largely supports the decision to prioritize quality
- Some users caution against jumping to conclusions based on limited information
- There is appreciation for meaningful advancements over incremental updates
- The post gained significant traction with 456 upvotes and 71 comments

**Discussion Highlights:** The discussion highlights a consensus that focusing on quality is beneficial for the Qwen series. Users express support for taking the necessary time to make substantial improvements rather than rushing incremental updates. Some commenters urge caution against overinterpreting the developer's statement.

---

## 12. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 536 | **Comments:** 117 | **Date:** 2026-01-17

**Summary:** The author upgraded their server from a dual MI100 setup to a quad R9700 configuration, achieving 128GB VRAM and 128GB RAM for a cost-effective solution compared to alternatives like the RTX 6000 Blackwell. The post includes detailed specs, benchmarks, and a cost breakdown of the build. Key points include the upgrade to quad R9700 GPUs for better performance and cost efficiency, a total build cost of $7,035, high performance benchmarks, community appreciation for the build, and recognition for the author's contribution. The community praised the build for its performance and cost efficiency, with some users joking about the financial irresponsibility of such upgrades. The post was featured on Discord, and the author received a special flair for their contribution.

---

## 13. [Best "End of world" model that will run on 24gb VRAM](https://reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/)

**Author:** u/gggghhhhiiiijklmnop | **Upvotes:** 339 | **Comments:** 180 | **Date:** 2026-01-17

**Summary:** The user is looking for recommendations on the best LLM models to download and store that can run on a PC with 24GB VRAM and 64GB RAM, motivated by a desire to hoard data in anticipation of a potential 'end of the world' scenario. The discussion includes suggestions for specific models and practical advice on data storage.

**Key Points:**
- User wants to hoard data like Wikipedia, Wiktionary, etc.
- Looking for LLM models that fit within 24GB VRAM and 64GB RAM
- Suggestions include Gemma3:27b and practical advice on data storage
- Discussion highlights the importance of saving data for potential future use
- Mention of specific tools and resources like Wikipedia backups

**Discussion Highlights:** The discussion features practical advice on data storage and specific model recommendations like Gemma3:27b. There is a consensus on the importance of saving data for future use, with suggestions to use tools like Wikipedia backups and considerations for running models off SSD if necessary.

---

## 14. [GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)](https://reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/)

**Author:** u/CuriousPlatypus1881 | **Upvotes:** 383 | **Comments:** 89 | **Date:** 2026-01-16

**Summary:** The post discusses the updated SWE-bench leaderboard results from December 2025, highlighting the performance of various AI models on GitHub PR tasks. Claude Opus 4.5 leads with a 63.3% resolved rate, followed closely by GPT-5.2 (extra high effort) at 61.5%. The post also notes the strong performance of open-source models like GLM-4.7.

**Key Points:**
- Claude Opus 4.5 leads the SWE-bench leaderboard with a 63.3% resolved rate.
- GPT-5.2 (extra high effort) follows closely at 61.5%.
- Gemini 3 Flash Preview outperforms Gemini 3 Pro Preview despite being smaller and cheaper.
- GLM-4.7 is the strongest open-source model, ranking alongside closed models like GPT-5.1-codex.
- GPT-OSS-120B shows significant performance improvement in high-effort reasoning mode.

**Discussion Highlights:** The discussion highlights excitement around Gemini Flash's performance and the strong showing of open-source models like GLM-4.7. There is also anticipation for future releases like DeepSeek v4.

---

## 15. [I fucking love this community](https://reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/)

**Author:** u/alhinai_03 | **Upvotes:** 526 | **Comments:** 54 | **Date:** 2026-01-16

**Summary:** The author expresses gratitude towards the open-source community for enabling them to run large language models on older hardware, highlighting the performance of a specific model on their 10-year-old PC with limited VRAM.

**Key Points:**
- Author appreciates the open-source community for their contributions.
- Author runs large models on a 10-year-old PC with 4GB VRAM.
- Achieves 14-13.5 tokens per second with a specific model.
- System memory and MoE architecture are key to performance.
- Community reactions highlight the impressive optimization and practicality of the setup.

**Discussion Highlights:** The community reacts positively, with comments praising the optimization efforts and the practicality of using system RAM and MoE architecture. Some users request more details about the setup and posts that helped the author.

---

## 16. [My story of underestimating /r/LocalLLaMA's thirst for VRAM](https://reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/)

**Author:** u/EmPips | **Upvotes:** 1351 | **Comments:** 91 | **Date:** 2026-01-15

**Summary:** The post highlights the author's underestimation of the subreddit's demand for VRAM, with discussions focusing on hardware recommendations and community engagement.

**Key Points:**
- Author underestimated VRAM demand in the subreddit
- Community engagement includes Discord features and special flairs
- Hardware recommendations such as 3090s or R9700 are discussed
- Gold rush analogy used to describe the situation
- Author may sell hardware after further posts

**Discussion Highlights:** The discussion includes hardware advice, community engagement, and analogies to describe the situation, with a focus on VRAM and hardware recommendations.

---

## 17. [Latest upgrade…A100 40 GB](https://reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/)

**Author:** u/inserterikhere | **Upvotes:** 406 | **Comments:** 54 | **Date:** 2026-01-15

**Summary:** The user upgraded their gaming rig to an AI rig by purchasing a faulty A100 GPU for $1000, which worked perfectly upon installation, allowing them to run and train larger models. The post gained significant attention, with the community offering both congratulations and technical advice.

**Key Points:**
- User transitioned from a gaming rig to an AI rig using existing parts and new components like a 3090 and 7950x.
- Purchased a faulty A100 GPU for $1000, which worked flawlessly upon installation.
- The A100 enabled running and training larger models immediately.
- Community expressed concern about cooling the A100 and offered advice.
- Post received significant upvotes and engagement, including a special flair for the author.

**Discussion Highlights:** The community reacted positively to the upgrade, with some users expressing admiration and others offering technical advice, particularly regarding cooling the A100 GPU. The post was featured on Discord, indicating its popularity and value to the community.

---

## 18. [NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency](https://reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/)

**Author:** u/Fear_ltself | **Upvotes:** 711 | **Comments:** 130 | **Date:** 2026-01-14

**Summary:** NVIDIA's Orchestrator-8B is an 8-billion-parameter AI designed to manage and route complex tasks to various tools, sparking discussions on its role in achieving AGI and comparisons to middle managers.

**Key Points:**
- Orchestrator-8B is a specialized AI for task management and routing
- It is seen as a step towards AGI by integrating separate components
- Comparisons to middle managers and existing frameworks like Claude code style agentic frameworks were made

**Discussion Highlights:** The discussion included humorous comparisons to middle managers and mentions of Claude code style agentic frameworks as potential next steps in AI development.

---

## 19. [GLM-Image is released!](https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/)

**Author:** u/foldl-li | **Upvotes:** 602 | **Comments:** 83 | **Date:** 2026-01-13

**Summary:** GLM-Image is a new image generation model with a hybrid autoregressive + diffusion decoder architecture. It excels in text-rendering and knowledge-intensive tasks while supporting various image-to-image tasks.

**Key Points:**
- Hybrid autoregressive + diffusion decoder architecture
- Strong performance in text-rendering and knowledge-intensive generation
- Supports image editing, style transfer, and multi-subject consistency
- MIT license with no restrictions
- Model size: 13GB diffusion model + 20GB text encoder

**Discussion Highlights:** The community appreciates the MIT license and the model's capabilities. There is interest in quantizing the model for easier use and discussions about its performance compared to other models.

---

## 20. [My wishes for 2026](https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/)

**Author:** u/jacek2023 | **Upvotes:** 650 | **Comments:** 179 | **Date:** 2026-01-13

**Summary:** The Reddit post discusses predictions for 2026, focusing on the possibility of affordable GPUs with more than 32GB memory. The community engages in a mix of hopeful and skeptical comments about this prospect.

**Key Points:**
- The post asks which predictions for 2026 are likely to happen first.
- A major focus is on the availability of affordable GPUs with more than 32GB memory.
- Comments range from humorous to skeptical about the feasibility of affordable GPUs.
- Mentions of specific AI models like Qwen 4 and Mistral as potential advancements.

**Discussion Highlights:** The discussion highlights a mix of humor and skepticism regarding the possibility of affordable GPUs with more than 32GB memory in 2026. Some comments mention specific AI models as potential advancements, while others express doubt about the feasibility of such hardware becoming affordable.

---

## 21. [kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop—no GPU required](https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/)

**Author:** u/Nunki08 | **Upvotes:** 399 | **Comments:** 93 | **Date:** 2026-01-13

**Summary:** Kyutai introduced Pocket TTS, a 100M-parameter text-to-speech model with high-quality voice cloning that runs on a laptop without requiring a GPU. The model is available on GitHub and Hugging Face, with a blog post and arXiv paper providing more details.

**Key Points:**
- Pocket TTS is a 100M-parameter model for high-quality voice cloning.
- It runs on a laptop without needing a GPU.
- Available on GitHub, Hugging Face, and detailed in a blog post and arXiv paper.
- Concerns about memory usage during generation.
- Discussion on language support and model size limitations.

**Discussion Highlights:** The discussion highlights concerns about memory usage ballooning during generation, questions about language support, and debates on the practicality of small models compared to established alternatives.

---

## 22. [GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/)

**Author:** u/TKGaming_11 | **Upvotes:** 371 | **Comments:** 93 | **Date:** 2026-01-12

**Summary:** The Reddit post highlights a new GitHub repository by DeepSeek-AI called 'Engram,' which introduces a novel approach to memory in large language models using scalable lookup and n-gram embeddings. The discussion praises the originality and potential impact of this work.

**Key Points:**
- DeepSeek-AI's 'Engram' introduces a new memory approach for LLMs via scalable lookup.
- The n-gram embedding method adds a complementary sparsity axis with O(1) lookup.
- The work is praised for its originality and potential impact on the field.
- Comparisons are drawn to biological memory systems in animals and humans.

**Discussion Highlights:** The discussion highlights the innovative nature of the 'Engram' approach, with users praising DeepSeek's consistent delivery of original ideas. Technical details about the n-gram embedding and its efficiency are noted, along with comparisons to biological memory systems.

---

## 23. [LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)](https://reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 1055 | **Comments:** 114 | **Date:** 2026-01-11

**Summary:** The post introduces TimeCapsuleLLM, a 1.2B parameter language model trained exclusively on 1800-1875 London texts to minimize modern bias. The model demonstrates period-specific knowledge and behaviors, such as unfamiliarity with post-1875 concepts like telephones. The project is open-source and available on GitHub and Hugging Face.

**Key Points:**
- TimeCapsuleLLM is trained on 90GB of 1800-1875 London texts with no modern data or fine-tuning.
- The model exhibits period-specific behaviors, such as generating arguments relevant to the Catholic Emancipation Act of 1829.
- The model is unfamiliar with concepts introduced after 1875, like telephones.
- Future work includes creating synthetic Q&A pairs from the dataset.
- The project has garnered significant community interest and support.

**Discussion Highlights:** The community response is overwhelmingly positive, with users praising the project's uniqueness and expressing interest in similar historical language models. Some users shared their own experiences with training models on historical datasets, and there was humorous engagement with the model's 1875 knowledge cutoff.

---

## 24. [I bought a €9k GH200 “desktop” to save $1.27 on Claude Code (vLLM tuning notes)](https://reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/)

**Author:** u/Reddactor | **Upvotes:** 694 | **Comments:** 177 | **Date:** 2026-01-11

**Summary:** The author built a €9k GH200 'desktop' to run Claude Code locally, achieving better speeds and results than the cloud version. They shared optimized vLLM settings for dual 96GB systems and highlighted the cost savings and performance benefits of local execution.

**Key Points:**
- Author spent €9k on a GH200 setup to run Claude Code locally.
- Achieved better speeds and results compared to cloud-based Claude Code.
- Shared optimized vLLM settings for dual 96GB systems.
- Highlighted cost savings and performance benefits of local execution.
- Community praised the setup but joked about the high initial cost.

**Discussion Highlights:** The community appreciated the detailed setup and shared humor about the high cost, with some users expressing envy over missing out on similar deals.

---

## 25. [It works! Abliteration can reduce slop without training](https://reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/)

**Author:** u/-p-e-w- | **Upvotes:** 402 | **Comments:** 129 | **Date:** 2026-01-11

**Summary:** The Reddit post discusses the use of abliteration to reduce 'slop' (flowery, cliched language) in LLM outputs without training. The author successfully applied this technique to the Mistral Nemo model, creating a slop-reduced version using a configuration file and the Heretic tool. Key points include the effectiveness of abliteration, the use of Heretic, the process duration, and mixed opinions on the technique's impact on creativity. The discussion highlights mixed opinions on the effectiveness of the technique, with some users appreciating the reduction in slop but others feeling it makes the prose dry.

---

## 26. [I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work](https://reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/)

**Author:** u/Ok-Pomegranate1314 | **Upvotes:** 894 | **Comments:** 147 | **Date:** 2026-01-09

**Summary:** The author successfully clustered three NVIDIA DGX Sparks, overcoming networking limitations by writing a custom NCCL plugin. This achievement allows distributed inference across all three nodes at high speeds, despite NVIDIA's official support for only two-node clustering.

**Key Points:**
- Author clustered three DGX Sparks, exceeding NVIDIA's official support for two-node clustering.
- Developed a custom NCCL network plugin to handle subnet-aware NIC selection and raw RDMA implementation.
- Achieved distributed inference at 8+ GB/s over RDMA across all three nodes.
- The solution involved extensive low-level debugging and custom protocol implementation.
- The community praised the achievement, noting its significance for distributed computing.

**Discussion Highlights:** The community highlighted the technical difficulty and significance of the achievement, with notable comments praising the work and inquiring about scalability and performance improvements.

---

## 27. [The reason why RAM has become so expensive](https://reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/)

**Author:** u/InvadersMustLive | **Upvotes:** 4542 | **Comments:** 380 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses the significant increase in RAM prices, with comments suggesting potential monopolization by companies like OpenAI to control future demand and make competitors' data centers economically unviable. Users also note the rapid rise in prices, with some reporting costs up to 10 times higher than the previous year.

**Key Points:**
- RAM prices have increased dramatically, with some users reporting costs up to 10 times higher than the previous year.
- There is speculation that companies like OpenAI are monopolizing RAM to control future demand and hinder competitors.
- The price surge is seen as potentially creating economic barriers for other AI data centers, particularly in China.
- Some users express skepticism about the sustainability of the price increase, suggesting it might be a bubble.

**Discussion Highlights:** The discussion highlights concerns about monopolization and the economic impact on competitors, with users sharing personal experiences of the price surge and debating its long-term implications.

---

## 28. [DeepSeek V4 Coming](https://reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/)

**Author:** u/External_Mood4719 | **Upvotes:** 499 | **Comments:** 109 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release V4, a next-generation AI model with enhanced code-generation capabilities, outperforming mainstream models like Claude and GPT. The model shows improvements in handling long code prompts and overall reasoning ability.

**Key Points:**
- DeepSeek V4 focuses on strong code-generation capabilities.
- V4 outperforms existing models like Claude and GPT in code generation.
- Improved handling of long code prompts and data pattern understanding.
- Users anticipate V4 to be more logically rigorous and reliable.
- Community discussion highlights excitement and expectations for V4's performance.

**Discussion Highlights:** The community is enthusiastic about V4, with users praising DeepSeek's cost-effectiveness and performance. Some anticipate significant improvements, while others speculate on potential features like mHC and deepseek-ocr integration.

---

## 29. [(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability](https://reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/)

**Author:** u/Nunki08 | **Upvotes:** 485 | **Comments:** 102 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release a new flagship AI model with strong coding capabilities, generating significant interest and discussion in the r/LocalLLaMA community.

**Key Points:**
- DeepSeek's upcoming model emphasizes strong coding ability
- The announcement has sparked excitement and anticipation
- Community members express enthusiasm for more AI model options
- Some users are skeptical about performance claims
- There is a desire for the model to maintain role-playing capabilities

**Discussion Highlights:** The community shows strong interest and excitement about DeepSeek's new model, with some expressing skepticism about performance claims and a desire to maintain role-playing abilities. Overall, the consensus is positive, welcoming more AI model options.

---

## 30. [The NO FAKES Act has a "Fingerprinting" Trap that kills Open Source. We need to lobby for a Safe Harbor.](https://reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/)

**Author:** u/PostEasy7183 | **Upvotes:** 610 | **Comments:** 88 | **Date:** 2026-01-08

**Summary:** The NO FAKES Act threatens open-source AI development by imposing liability on developers for tools used to create digital replicas, potentially banning open-source AI hosting in the US. The post urges lobbying for a Safe Harbor provision to protect developers.

**Key Points:**
- The NO FAKES Act creates a 'digital replica right' targeting tools used for replicas, imposing liability on developers.
- Developers hosting TTS or voice-conversion models could face statutory damages if their tools are misused.
- The post calls for a 'Safe Harbor' provision to protect open-source developers and prevent a monopoly by Big Tech.
- Action items include emailing/calling representatives to oppose the bill unless amended.
- Comments highlight concerns about stifling innovation and the influence of Big Tech in shaping anti-AI sentiment.

**Discussion Highlights:** The discussion emphasizes the potential negative impact on innovation and the need for legal protections for open-source developers. Some commenters express skepticism about politicians' understanding of technology and suggest that Big Tech may be behind the anti-AI movement.

---

## 31. [Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt](https://reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/)

**Author:** u/Prior-Arm-6705 | **Upvotes:** 941 | **Comments:** 147 | **Date:** 2026-01-08

**Summary:** A Reddit user created a compilation video of every instance Jensen Huang said 'AI' during NVIDIA's CES 2025 keynote, totaling 121 times, using open-source tools and MCPs. The process involved downloading the video, parsing subtitles for timestamps, cutting clips, and merging them into a final video.

**Key Points:**
- Jensen Huang said 'AI' 121 times during the CES 2025 keynote.
- The user utilized open-source tools like Dive, yt-dlp-mcp, and ffmpeg-mcp-lite to automate the video creation process.
- The process involved downloading the video, parsing subtitles for precise timestamps, cutting clips, and merging them.
- The final video is described as hypnotic and was created entirely locally without cloud processing.
- Top comments include reactions to the video, mentions of the jacket Jensen wore, and references to Gamers Nexus.

**Discussion Highlights:** The discussion highlights include appreciation for the technical process, humor about Jensen's frequent use of 'AI,' and comments on his attire. The post was well-received, with one comment noting it was featured on Discord.

---

## 32. [16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/)

**Author:** u/ai-infos | **Upvotes:** 462 | **Comments:** 238 | **Date:** 2026-01-07

**Summary:** The post discusses running Deepseek V3.2 AWQ 4-bit on 16x AMD MI50 32GB GPUs, achieving 10 tokens/sec output and 2000 tokens/sec input with a 69000 context length. The setup draws 550W idle and 2400W peak power, aiming for cost-effective local AGI hardware.

**Key Points:**
- Performance: 10 tok/s output, 2000 tok/s input, 69000 context length
- Power draw: 550W idle, 2400W peak
- Goal: Cost-effective local AGI hardware alternative to expensive CPU setups
- Future plans: Open-source 32 AMD MI50 setup for Kimi K2 Thinking
- Community appreciation and cost-effectiveness discussion

**Discussion Highlights:** The discussion highlights the setup's popularity, its potential as a heater due to high power draw, concerns about noise and home power usage, and the cost-effectiveness for professional developers. The community appreciates the open-source contribution and sees it as a viable alternative to expensive CPU hardware.

---

## 33. [DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.](https://reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/)

**Author:** u/Nunki08 | **Upvotes:** 661 | **Comments:** 54 | **Date:** 2026-01-07

**Summary:** The Reddit post discusses the recent update to DeepSeek-R1’s paper, which expanded from 22 pages to 86 pages, adding significant detail. The discussion includes comments about potential new architectures, linear attention research, and the value of added implementation specifics.

**Key Points:**
- DeepSeek-R1’s paper was updated from 22 pages to 86 pages.
- The update includes substantial additional details.
- Discussion highlights potential new architectures and linear attention research.
- Comments mention the value of added implementation specifics.

**Discussion Highlights:** The discussion includes speculation about new architectures, interest in linear attention research, and appreciation for the added implementation details in the updated paper.

---

## 34. [A 30B Qwen Model Walks Into a Raspberry Pi… and Runs in Real Time](https://reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/)

**Author:** u/ali_byteshape | **Upvotes:** 494 | **Comments:** 79 | **Date:** 2026-01-06

**Summary:** The post discusses the successful optimization and deployment of the Qwen3-30B-A3B-Instruct-2507 model on a Raspberry Pi 5, achieving 8.03 tokens per second while retaining 94.18% of BF16 quality. The optimization focuses on balancing memory usage and performance, particularly on GPUs where kernel choice significantly impacts speed.

**Key Points:**
- The 30B model runs on a Raspberry Pi 5 with 8.03 TPS at 2.70 BPW, retaining 94.18% of BF16 quality.
- Optimization prioritizes memory budget and performance trade-offs, especially on GPUs.
- Community feedback highlights performance comparisons and user experiences, including successful runs on Raspberry Pi 5 with specific configurations.
- The post requests community testing on various setups, including non-NVIDIA hardware.

**Discussion Highlights:** The community discussion includes feedback on performance, comparisons with other models, and user experiences running the model on different hardware setups. Notable comments mention the need for specific configurations to avoid crashes and the potential for clustering Raspberry Pis for enhanced performance.

---

## 35. [Performance improvements in llama.cpp over time](https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/)

**Author:** u/jacek2023 | **Upvotes:** 682 | **Comments:** 85 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses performance improvements in llama.cpp over time, highlighting significant progress and ongoing optimizations. The discussion includes insights on GPU-specific enhancements and comparisons with other implementations.

**Key Points:**
- Performance gains in llama.cpp have been substantial over time.
- Improvements are particularly notable for NVIDIA GPUs.
- Comparisons with other implementations like ik_llama.cpp show competitive performance.
- Prompt processing remains slower than token generation but has seen progress.

**Discussion Highlights:** The community acknowledges the impressive progress in llama.cpp, with a focus on NVIDIA GPU optimizations and comparisons to alternative implementations. There is a consensus on the significant advancements made, though some areas like prompt processing still lag behind token generation speed.

---

## 36. [For the first time in 5 years, Nvidia will not announce any new GPUs at CES — company quashes RTX 50 Super rumors as AI expected to take center stage](https://reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/)

**Author:** u/FullstackSensei | **Upvotes:** 621 | **Comments:** 196 | **Date:** 2026-01-05

**Summary:** Nvidia will not announce new GPUs at CES, focusing instead on AI. There are concerns about limited supply of high-end GPUs and potential reintroduction of older models like the RTX 3060. The post highlights rising prices of DDR5 and storage, making upgrades difficult.

**Key Points:**
- Nvidia quashes RTX 50 Super rumors, no new GPU announcements at CES
- Limited supply of RTX 5070Ti, 5080, and 5090, with potential reintroduction of RTX 3060
- Rising prices of DDR5 and storage, making hardware upgrades costly
- Concerns about corporate greed and the future of local computing
- Suggestions for alternative solutions, such as China flooding the market with high-capacity cards

**Discussion Highlights:** The discussion highlights frustration with corporate greed and the impact on local computing. Users express concern about the future of hardware upgrades and suggest alternative solutions to address the shortage and high prices.

---

## 37. [llama.cpp performance breakthrough for multi-GPU setups](https://reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/)

**Author:** u/Holiday-Injury-9397 | **Upvotes:** 573 | **Comments:** 201 | **Date:** 2026-01-05

**Summary:** The ik_llama.cpp project achieved a significant performance breakthrough for multi-GPU setups, delivering a 3x to 4x speed improvement in local LLM inference. This advancement allows for the utilization of multiple low-cost GPUs instead of expensive high-end cards, making it a game-changer for homelabs, server rooms, and cloud setups.

**Key Points:**
- ik_llama.cpp achieved a 3x to 4x speed improvement in multi-GPU setups.
- The new execution mode (split mode graph) enables simultaneous and maximum utilization of multiple GPUs.
- This breakthrough makes it possible to use multiple low-cost GPUs instead of expensive high-end cards.
- Even on a single GPU or CPU-only, ik_llama.cpp shows consistent 2x prompt processing speeds compared to llama.cpp.
- The performance improvements are significant enough to rival other optimized implementations like exllama and vllm.

**Discussion Highlights:** The community is highly enthusiastic about the performance gains, with many users confirming the improvements and sharing their own benchmark results. There is a consensus that ik_llama.cpp is a significant advancement in local LLM inference, offering substantial performance benefits across various hardware configurations.

---

## 38. [Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched](https://reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/)

**Author:** u/ubrtnk | **Upvotes:** 379 | **Comments:** 195 | **Date:** 2026-01-03

**Summary:** The post discusses challenges faced by local LLMs in processing extreme breaking news, such as the US attacking Venezuela. The author shares experiences with different models, highlighting their struggles to accept the reality of such events despite credible sources.

**Key Points:**
- Local LLMs struggled to accept the reality of extreme breaking news events, classifying them as hoaxes or misinformation.
- Different models (Qwen Research, Spark 4.0, GPT-OSS:120B) showed varying degrees of skepticism and resistance to accepting the news.
- Providing credible sources (BBC, Reuters, NYT) helped some models acknowledge the event's reality.
- The post highlights the bias and limitations of LLMs in processing unfamiliar geopolitical events.
- Discussion reflects a consensus on the challenges and biases inherent in LLM responses to extreme news.

**Discussion Highlights:** The discussion highlights a consensus on the inherent biases and limitations of LLMs in processing extreme or unfamiliar geopolitical events. Users shared similar experiences and expressed concerns about the models' tendency to dismiss unlikely but real events.

---

## 39. [LeCun Says Llama 4 results "were fudged a little bit"](https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/)

**Author:** u/MrPecunius | **Upvotes:** 364 | **Comments:** 88 | **Date:** 2026-01-02

**Summary:** Yann LeCun, departing Meta AI Chief, confirmed that Llama 4 benchmark results were manipulated, leading to organizational changes and departures. The post discusses the impact on Meta's AI initiatives and the community's reaction.

**Key Points:**
- LeCun confirmed Llama 4 benchmark manipulation
- Zuckerberg sidelined the GenAI organization, leading to departures
- Community expresses disappointment over Meta's AI strategy
- Shared PDF link for the complete article
- Discussion on Meta's strategic missteps in AI development

**Discussion Highlights:** The community expresses disappointment and concern over Meta's handling of its AI initiatives, with many highlighting the missed potential of Llama and the impact on open-source AI development. There is a shared PDF link for the complete article and discussions on Meta's strategic missteps.

---

## 40. [Qwen-Image-2512](https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/)

**Author:** u/Nunki08 | **Upvotes:** 717 | **Comments:** 122 | **Date:** 2025-12-31

**Summary:** The Reddit post announces the release of Qwen-Image-2512, a new model available on multiple platforms with guides and GGUF files. The community has shown positive reception and shared practical testing experiences.

**Key Points:**
- Qwen-Image-2512 is a new model with guides and GGUF files available
- The model can be accessed on platforms like Hugging Face, ModelScope, and GitHub
- Community members have tested the model on low-end hardware and shared positive feedback
- Users have created and shared creative content using the model

**Discussion Highlights:** The community is excited about the new model, with users testing it on various hardware configurations and sharing creative outputs. There is a general consensus of appreciation for the model's capabilities and accessibility.

---

## 41. [[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It’s running a raw Llama-7B instance with a 2048 token window.](https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/)

**Author:** u/simar-dmg | **Upvotes:** 741 | **Comments:** 110 | **Date:** 2025-12-30

**Summary:** A user reverse-engineered a Snapchat sextortion bot and discovered it was running a raw Llama-7B instance with a 2048 token window. The bot was vulnerable to a persona-adoption jailbreak, revealing its configuration and malicious payload.

**Key Points:**
- The bot used a Llama-7B model with a 2048 token context window and high temperature setting.
- A 'Grandma Protocol' jailbreak forced the bot to reveal its environment variables and configuration.
- The bot was running on minimal hardware to maximize profit margins.
- The bot eventually revealed a malicious link it was programmed to hide.
- Scammers are increasingly using open-source models like Llama-7B to avoid API costs and censorship.

**Discussion Highlights:** The discussion included skepticism about the accuracy of the bot's revealed information, with some users suggesting it could be entirely hallucinated. Others questioned the commonality of system prompts including environment variables.

---

## 42. [Llama-3.3-8B-Instruct](https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/)

**Author:** u/jacek2023 | **Upvotes:** 468 | **Comments:** 77 | **Date:** 2025-12-29

**Summary:** The post announces the release of Llama-3.3-8B-Instruct, a model previously only available via Meta's API. The author discovered a method to download it through finetuning and has made it available in GGUF format.

**Key Points:**
- Llama-3.3-8B-Instruct was previously only accessible via Meta's API.
- The author found a way to download the model through finetuning.
- The model is now available in GGUF format on Hugging Face.
- The community is verifying the model's authenticity and specifications.
- There is excitement and ongoing discussion about the model's capabilities.

**Discussion Highlights:** The community is actively engaging with the release, running benchmarks to verify the model's authenticity and discussing its specifications, such as the 8K max position embeddings. There is overall excitement and appreciation for the author's efforts in making the model accessible.

---

## 43. [Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.](https://reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 345 | **Comments:** 120 | **Date:** 2025-12-29

**Summary:** Z AI is set to go public with an IPO on January 8, aiming to raise $560 million, marking it as the first AI-native LLM company to list globally. The Reddit post and comments highlight concerns about the future of open-source AI and the company's potential shift away from open models.

**Key Points:**
- Z AI's IPO is scheduled for January 8, targeting $560 million in funding.
- The company is positioned as the first AI-native LLM firm to go public globally.
- Community concerns about the potential end of open-source models from Z AI.
- Debate on whether Z AI will continue releasing open weight models post-IPO.
- Mixed reactions, with some users accepting the need for monetization and others urging against selling out.

**Discussion Highlights:** The discussion reflects a divide in the community, with significant concern about Z AI moving away from open-source models post-IPO. Some users argue that monetization is inevitable, while others express hope that the company will continue to support open-source initiatives. The top comments emphasize the tension between commercial success and maintaining open-source contributions.

---

## 44. [Tencent just released WeDLM 8B Instruct on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 417 | **Comments:** 62 | **Date:** 2025-12-29

**Summary:** Tencent released WeDLM 8B Instruct on Hugging Face, a diffusion language model that outperforms vLLM-optimized Qwen3-8B in math reasoning tasks by 3-6 times. The release has garnered significant attention and positive feedback from the community.

**Key Points:**
- WeDLM 8B Instruct is a diffusion language model released by Tencent on Hugging Face.
- It runs 3-6 times faster than vLLM-optimized Qwen3-8B on math reasoning tasks.
- The model is released under the Apache 2.0 license.
- The community shows strong interest and positive feedback on the model's performance and potential.
- A 7B version of the model is also available.

**Discussion Highlights:** The community is excited about the performance and potential of the WeDLM models, with many users expressing interest in the Apache 2.0 license and the impressive benchmark scores. There is a consensus on the promising future of 7-8B models in general.

---

## 45. [NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux](https://reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/)

**Author:** u/HumanDrone8721 | **Upvotes:** 448 | **Comments:** 185 | **Date:** 2025-12-27

**Summary:** NVIDIA has dropped Pascal support on Linux, causing disruptions for Arch Linux users. The community is aware of this change, with some expressing concern and others noting it as expected.

**Key Points:**
- NVIDIA's driver update (590) drops Pascal support
- Arch Linux users are affected as legacy drivers move to AUR
- Community reactions range from concern to acceptance
- The 24GB P40 Pascal card is mentioned as a popular but now expensive option
- Arch Linux has a history of moving legacy drivers to AUR

**Discussion Highlights:** The discussion highlights a mix of concern and acceptance among users. Some are worried about the impact on their hardware, while others see it as a routine update in line with Arch Linux's policies. The community is generally informed and engaged, with references to official Arch Linux news.

---

## 46. [Best Local LLMs - 2025](https://reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/)

**Author:** u/rm-rf-rm | **Upvotes:** 364 | **Comments:** 198 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses the best local LLMs of 2025, highlighting models like Minimax M2.1 and GLM4.7, and categorizes them by application and memory footprint. Users share detailed experiences and recommendations.

**Key Points:**
- Minimax M2.1 and GLM4.7 are noted for frontier model performance.
- Models are categorized by application (General, Agentic, Creative Writing, Speciality) and memory footprint (Unlimited, Medium, Small).
- Users emphasize detailed descriptions of their setups and usage.
- Specific recommendations include Qwen3-4B-instruct and LFM2-8B-A1B for small models.
- Discussion includes debates on categorization and RAG for technical documentation.

**Discussion Highlights:** The discussion highlights debates on categorization, specific model recommendations, and the use of RAG for technical documentation. Users share varied experiences and preferences, with a focus on practical applications and performance.

---

## 47. [NVIDIA has 72GB VRAM version now](https://reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/)

**Author:** u/decentralize999 | **Upvotes:** 458 | **Comments:** 148 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses NVIDIA's new 72GB VRAM version, questioning the pricing and community interest in different VRAM sizes. The discussion includes comparisons of specifications and prices for various NVIDIA GPUs.

**Key Points:**
- NVIDIA has released a 72GB VRAM version of their GPU.
- Community members express interest in larger VRAM sizes, such as 128GB.
- Price comparisons show the RTX 5000 48GB at $5100, RTX 5000 72GB at $7800, and RTX 6000 96GB at $8300.
- Some users suggest waiting for future models like the 5090 with 48GB.
- The price per gigabyte remains consistent across different VRAM sizes.

**Discussion Highlights:** The discussion highlights a consensus that larger VRAM sizes are desirable, with some users advocating for even bigger options like 128GB. Price comparisons indicate that the cost per gigabyte is similar across different models, making the choice dependent on budget and needs. There is also anticipation for future GPU releases.

---

## 48. [Hard lesson learned after a year of running large models locally](https://reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/)

**Author:** u/inboundmage | **Upvotes:** 348 | **Comments:** 146 | **Date:** 2025-12-26

**Summary:** The author shares their experience running large language models locally over a year, highlighting challenges with VRAM limitations, model scaling, and performance trade-offs. They discuss the viability of local inference for smaller models but note significant hurdles for larger models without high-end hardware.

**Key Points:**
- Running large models locally is feasible for small to medium models but faces hard limits with larger models due to VRAM constraints.
- VRAM fragmentation and inefficient offloading to system RAM are significant issues when working with larger models.
- Quantization helps but introduces quality trade-offs and new bugs.
- Cloud-based solutions offer better performance for fast iteration, but local setups are preferred for privacy-sensitive tasks.
- Community suggestions include using llama.cpp for CPU offloading and considering additional GPUs for better performance.

**Discussion Highlights:** The discussion highlights practical solutions like using llama.cpp for CPU offloading and the potential need for additional GPUs. There is a consensus that while local inference is viable for smaller models, larger models require more robust hardware or alternative approaches.

---

## 49. [I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA](https://reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/)

**Author:** u/CeFurkan | **Upvotes:** 1038 | **Comments:** 179 | **Date:** 2025-12-25

**Summary:** The Reddit post discusses the potential for GPU VRAM upgrade modifications to become mainstream, challenging NVIDIA's monopoly. The discussion highlights that such modifications are already prevalent in China, with various models available at different price points.

**Key Points:**
- GPU VRAM upgrade modifications could disrupt NVIDIA's monopoly.
- These modifications are already mainstream in China.
- Alibaba offers upgraded GPUs like 2080Ti, 3080, 4080, 4090, and 5090 with increased VRAM.
- Prices range from $300 for a 2080Ti 22GB to $4000 for a 5090 96GB.
- Users report successful usage of modded GPUs like the 4090 with 48GB VRAM.

**Discussion Highlights:** The discussion highlights that GPU VRAM upgrades are already common in China, with Alibaba offering a range of upgraded models. Users share positive experiences with modded GPUs, although there are concerns about pricing and availability.

---

## 50. [Why I quit using Ollama](https://reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/)

**Author:** u/SoLoFaRaDi | **Upvotes:** 491 | **Comments:** 202 | **Date:** 2025-12-25

**Summary:** The Reddit post discusses the author's decision to stop using Ollama due to recent updates that introduced proprietary cloud models, leading to concerns about privacy and a shift away from the platform's original purpose of providing a secure inference platform for local AI models. The discussion highlights community feedback on these changes and suggests alternatives like llama.cpp and LM Studio.

**Key Points:**
- Author's dissatisfaction with Ollama's recent updates and shift towards cloud models
- Concerns about privacy implications and bloatware in Ollama
- Community feedback on Ollama's changes and suggestions for alternatives like llama.cpp and LM Studio
- General consensus on the shift in Ollama's focus away from local AI models
- Mentions of alternative tools and their benefits

**Discussion Highlights:** The discussion reflects a general consensus that Ollama has strayed from its original purpose, with many users suggesting alternatives like llama.cpp and LM Studio. The community appreciates the author's post and shares similar concerns about the recent updates and privacy implications.

---

