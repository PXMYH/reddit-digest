# r/LocalLLaMA Reading Digest

**Period:** 2026-01-26 to 2026-01-26
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [Your post is getting popular and we just featured it on our Discord!](https://reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/)

**Author:** u/roculus | **Upvotes:** 557 | **Comments:** 56 | **Date:** 2026-01-23

**Summary:** The Reddit post announces that a user's post is popular and has been featured on Discord, with the user receiving a special flair. The community discusses the annoyance of bot spam and the monetization of the Discord server.

**Key Points:**
- The bot announces the popularity of a user's post and its feature on Discord.
- The user receives a special flair for their contribution.
- The community finds the bot spam annoying and questions the monetization of the Discord server.
- There is a pinned thread about the Discord server that has been there for 5 months.
- The community humorously suggests that the bot might announce the post's feature on Discord if it gains enough traction.

**Discussion Highlights:** The community consensus is that the bot spam is annoying and there are concerns about monetization. Some users humorously engage with the idea of the bot announcing the post's feature on Discord.

---

## 2. [Am I the only one who feels that, with all the AI boom, everyone is basically doing the same thing?](https://reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/)

**Author:** u/[deleted] | **Upvotes:** 396 | **Comments:** 188 | **Date:** 2026-01-22

**Summary:** The post discusses the redundancy of AI tools and applications during the AI boom, highlighting that many new tools are less polished versions of existing ones. The discussion reflects on the early days of AI technology and the enthusiasm driving shallow implementations. Key points include the low barrier to entry for AI development, the 'hype stage' of AI technology, and the importance of focusing on unique, niche applications rather than replicating existing solutions. The discussion highlights a consensus that the AI field is in a hype phase with many redundant tools.

---

## 3. [Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp; 1.8B), Support for 10 languages](https://reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/)

**Author:** u/Nunki08 | **Upvotes:** 711 | **Comments:** 117 | **Date:** 2026-01-22

**Summary:** Qwen has open-sourced the Qwen3-TTS model family, including VoiceDesign, CustomVoice, and Base models in 0.6B and 1.8B sizes, supporting 10 languages. The release includes resources on GitHub, Hugging Face, and a demo, with mixed user feedback on voice quality and requests for additional support.

**Key Points:**
- Qwen3-TTS models released in 0.6B and 1.8B sizes
- Supports 10 languages
- Resources available on GitHub, Hugging Face, and demo
- Mixed feedback on voice quality
- Requests for additional support like llama.cpp

**Discussion Highlights:** Users appreciate Qwen's open-sourcing efforts but have mixed opinions on voice quality, with some noting it sounds like anime dubs. There are requests for support in compiled languages like llama.cpp and mistral.rs.

---

## 4. [Qwen dev on Twitter!!](https://reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 740 | **Comments:** 60 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses Qwen's TTS model announcement, with community reactions and links to the model on Hugging Face.

**Key Points:**
- Qwen's TTS model announcement
- Community reaction and discussion
- Link to the model on Hugging Face
- Thread locked due to announcements being out
- Mention of vLLM leak related to the TTS model

**Discussion Highlights:** The community is excited about the Qwen TTS model, with some users sharing links to the model on Hugging Face. There is also mention of a vLLM leak related to the TTS model, and the thread was locked as announcements are out.

---

## 5. [8x AMD MI50 32GB at 26 t/s (tg) with MiniMax-M2.1 and 15 t/s (tg) with GLM 4.7 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1qjaxfy/8x_amd_mi50_32gb_at_26_ts_tg_with_minimaxm21_and/)

**Author:** u/ai-infos | **Upvotes:** 318 | **Comments:** 128 | **Date:** 2026-01-21

**Summary:** The post details a cost-effective local inference setup using 8x AMD MI50 GPUs, achieving high token generation speeds with MiniMax-M2.1 and GLM 4.7 models. The setup is praised for its performance and affordability, with a total VRAM of 256GB for under $1k.

**Key Points:**
- MiniMax-M2.1 achieves 26.8 tokens/s output and 3000 tokens/s input with a context length of 196,608.
- GLM 4.7 achieves 15.6 tokens/s output and 3000 tokens/s input with a context length of 95,000.
- The setup costs $880 for 256GB VRAM and draws 280W idle / 1200W during inference.
- The goal is to provide one of the most cost-effective solutions for fast, intelligent local inference.
- The community highly praises the setup for its performance and affordability.

**Discussion Highlights:** The community is highly enthusiastic about the setup, with comments highlighting its cost-effectiveness and performance. Some users express interest in replicating the setup but note that current prices for the GPUs are higher than those mentioned in the post.

---

## 6. [Fix for GLM 4.7 Flash has been merged into llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qiwm3c/fix_for_glm_47_flash_has_been_merged_into_llamacpp/)

**Author:** u/jacek2023 | **Upvotes:** 313 | **Comments:** 86 | **Date:** 2026-01-21

**Summary:** A fix for GLM 4.7 Flash has been merged into llama.cpp, with ongoing work on CUDA support. The community is discussing performance metrics and compatibility.

**Key Points:**
- Fix for GLM 4.7 Flash merged into llama.cpp
- CUDA support in progress
- Performance metrics shared for different quantizations and GPUs
- Discussion on CPU-only performance and compatibility
- Positive feedback on model improvements

**Discussion Highlights:** The community is actively discussing performance metrics, compatibility issues, and sharing positive feedback on the model's improvements. There is also interest in CPU-only performance for users without GPUs.

---

## 7. [You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?](https://reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/)

**Author:** u/Adventurous-Gold6413 | **Upvotes:** 543 | **Comments:** 309 | **Date:** 2026-01-20

**Summary:** The Reddit post discusses the selection of local models for use with 64GB RAM and 16GB VRAM when internet access is unavailable. The community suggests several models, with a focus on performance and versatility. Key points include recommendations for models like Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B, with GPT-OSS 120B being praised for its performance and versatility. The discussion highlights a consensus around models like GPT-OSS 120B, which is noted for fitting well within the specified hardware and offering strong performance. Other models like Gemma 3 27B and GLM 4.5 Air are also mentioned as viable options.

---

## 8. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 903 | **Comments:** 271 | **Date:** 2026-01-20

**Summary:** The Reddit post describes a custom-built, high-performance AI system designed for running large MoE models and supporting graphic design tasks. The system features a Threadripper Pro 3995WX, 512GB DDR4, and a mix of 3090 and 5090 GPUs, all enclosed in a Thermaltake Core W200 case for mobility and protection. The build cost approximately $17k and was optimized for performance within budget constraints.

**Key Points:**
- The system is designed for running large MoE models and supporting graphic design tasks.
- It features a Threadripper Pro 3995WX, 512GB DDR4, and a mix of 3090 and 5090 GPUs.
- The build is enclosed in a Thermaltake Core W200 case for mobility and protection.
- The total cost was approximately $17k, optimized for performance within budget constraints.
- The enclosure was necessary to protect the hardware from pets.

**Discussion Highlights:** The discussion highlights include humorous comments about the system's portability and power requirements, as well as appreciation for the build's capabilities and the unique challenges of enclosing such a powerful system.

---

## 9. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 368 | **Comments:** 60 | **Date:** 2026-01-19

**Summary:** The post announces official support for GLM 4.7 Flash in llama.cpp, highlighting community efforts and clarifying that 'official' refers to proper functionality rather than endorsement by Z.ai developers.

**Key Points:**
- GLM 4.7 Flash now officially supported in llama.cpp
- Support is a community effort, not from Z.ai developers
- Performance discussions include comparisons with VLLm and CUDA
- Alternative implementations and versions shared by community members
- Mixed feedback on performance with flash-attention

**Discussion Highlights:** The discussion clarifies the nature of the 'official' support and includes community contributions, performance comparisons, and alternative implementations. Some users report performance issues with flash-attention, while others share alternative versions and configurations.

---

## 10. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 465 | **Comments:** 162 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights the effectiveness of GLM 4.7 Flash as a reliable local agent, outperforming other MoE models in agentic frameworks. Users report successful long sessions with extensive token generation and error-free tool calling. The discussion includes comparisons with other models and notes on local testing performance.

**Key Points:**
- GLM 4.7 Flash is praised for its reliability and performance in agentic tasks.
- Users report successful long sessions with extensive token generation and error-free tool calling.
- The model is noted for its ability to handle tasks like cloning repos, running commands, and editing files.
- Discussion includes comparisons with Nemotron 30B and Qwen3, with positive feedback on GLM 4.7 Flash.
- GGUFs for local testing are anticipated, with initial tests showing decent performance on a 4090.

**Discussion Highlights:** The discussion highlights positive user experiences with GLM 4.7 Flash, including comparisons with other models and notes on local testing performance. Users express enthusiasm for the model's capabilities and look forward to further local testing.

---

## 11. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 750 | **Comments:** 230 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the release of the GLM-4.7-Flash model on Hugging Face, generating significant community interest and discussion about its technical features and capabilities.

**Key Points:**
- The model uses MLA, making it memory-efficient with a small KV cache footprint.
- It supports a full 200k context, making it accessible for many users.
- The community expresses excitement and anticipation for the release.
- Some users discuss the model's architecture, including a 30B model with a 3B thinking component.

**Discussion Highlights:** The discussion highlights enthusiasm for the model's release, with users praising its memory efficiency and context length. There is also technical discussion about the model's architecture and capabilities.

---

## 12. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 349 | **Comments:** 103 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to maximize VRAM for running large AI models locally. Benchmark results show impressive performance across various models, with the system costing around 9,800€ (effectively 4,900€ after refund).

**Key Points:**
- System built for running large AI models (120B+) locally with a focus on data privacy.
- Hardware includes 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU.
- Total cost was ~9,800€, with a 50% subsidy reducing the effective cost to ~4,900€.
- Benchmark results demonstrate strong performance across various models.
- Community reactions highlight the impressive hardware and its capabilities.

**Discussion Highlights:** The community praised the build, with comments highlighting the impressive hardware and its capabilities. Some users asked about the source of the components and the author's job, while others noted similar builds.

---

## 13. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 453 | **Comments:** 71 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses a potential slowdown in the development of Qwen 4, with the lead developer emphasizing a focus on quality over quantity. The community generally supports this approach, appreciating the commitment to improvement.

**Key Points:**
- Qwen 4 development may be slowing down to focus on quality
- Community appreciates the focus on quality over quantity
- Uncertainty about whether the statement specifically refers to Qwen 4
- Support for taking time to make meaningful improvements
- Discussion about the impact of incremental updates on the AI landscape

**Discussion Highlights:** The discussion highlights a general consensus supporting the focus on quality, with some users expressing appreciation for the developer's approach. There is also a note of caution about interpreting the statement as specifically referring to Qwen 4.

---

## 14. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 537 | **Comments:** 117 | **Date:** 2026-01-17

**Summary:** The author upgraded their server from a dual MI100 setup to a quad R9700 setup, achieving 128GB VRAM and 128GB RAM for a cost-effective price. They detailed the hardware specifications, benchmarks, and cost breakdown, highlighting the performance benefits of the R9700 GPUs. Key points include the upgrade to quad R9700 GPUs for better performance and cost efficiency, detailed hardware specifications and cost breakdown, benchmarks showing high performance, positive community feedback, and the cost-effective alternative to high-end GPUs. The community appreciated the detailed build and benchmarks, with many expressing admiration for the cost-effective performance achieved.

---

## 15. [Best "End of world" model that will run on 24gb VRAM](https://reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/)

**Author:** u/gggghhhhiiiijklmnop | **Upvotes:** 345 | **Comments:** 180 | **Date:** 2026-01-17

**Summary:** The user is seeking recommendations for the best LLM model that can run on a PC with 24GB VRAM and 64GB RAM, in preparation for an 'end of world' scenario. The discussion includes suggestions for specific models and practical advice on data storage.

**Key Points:**
- User wants to hoard data like Wikipedia, Wiktionary, etc.
- Looking for models that fit within 24GB VRAM and 64GB RAM
- Suggestions include Gemma3:27b and practical advice on data storage
- Discussion highlights the importance of saving the best possible LLM and running it off SSD if necessary
- Mention of downloading actual Wikipedia backups for offline use

**Discussion Highlights:** The discussion emphasizes practicality, with a consensus on saving the best possible LLM and considering offline storage solutions like SSDs. Specific model recommendations include Gemma3:27b, and there is a focus on ensuring data accessibility in extreme scenarios.

---

## 16. [GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)](https://reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/)

**Author:** u/CuriousPlatypus1881 | **Upvotes:** 385 | **Comments:** 89 | **Date:** 2026-01-16

**Summary:** The post discusses the updated SWE-bench leaderboard results from December 2025, highlighting the performance of various AI models on GitHub PR tasks. Claude Opus 4.5 leads with a 63.3% resolved rate, followed closely by GPT-5.2 (extra high effort) at 61.5%. The post also notes the strong performance of open-source models like GLM-4.7.

**Key Points:**
- Claude Opus 4.5 leads the leaderboard with a 63.3% resolved rate.
- GPT-5.2 (extra high effort) follows closely at 61.5%.
- Gemini 3 Flash Preview outperforms Gemini 3 Pro Preview despite being smaller and cheaper.
- GLM-4.7 is the strongest open-source model, ranking alongside closed models like GPT-5.1-codex.
- GPT-OSS-120B shows significant performance improvement in high-effort reasoning mode.

**Discussion Highlights:** The discussion highlights excitement around the performance of open-source models like GLM-4.7 and anticipation for future releases like DeepSeek v4. There is also a consensus that this benchmark is more believable compared to others.

---

## 17. [I fucking love this community](https://reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/)

**Author:** u/alhinai_03 | **Upvotes:** 520 | **Comments:** 54 | **Date:** 2026-01-16

**Summary:** The author expresses gratitude to the open-source community for enabling them to run large language models on older hardware, highlighting the efficiency of MoE architectures and system memory.

**Key Points:**
- Appreciation for the open-source community and contributors
- Running large models on a 10-year-old PC with limited VRAM
- Importance of system memory and MoE architecture for performance
- Achieving 14-13.5 tokens per second with a 30B parameter model
- Community recognition and engagement

**Discussion Highlights:** The community appreciates the author's achievement and emphasizes the importance of system memory and MoE architectures for running large models on limited hardware. There is also interest in learning more about optimizing performance on older equipment.

---

## 18. [My story of underestimating /r/LocalLLaMA's thirst for VRAM](https://reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/)

**Author:** u/EmPips | **Upvotes:** 1358 | **Comments:** 91 | **Date:** 2026-01-15

**Summary:** The post highlights the author's underestimation of the r/LocalLLaMA community's demand for VRAM, with discussions including hardware recommendations and humorous commentary.

**Key Points:**
- Author underestimated community's VRAM demand
- Discord feature and special flair mentioned
- Gold rush analogy used in comments
- Hardware recommendations (3090s or R9700)
- Humorous mention of selling a card after gaining popularity

**Discussion Highlights:** The discussion includes hardware recommendations and a humorous tone, with a notable analogy comparing the situation to a gold rush.

---

## 19. [Latest upgrade…A100 40 GB](https://reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/)

**Author:** u/inserterikhere | **Upvotes:** 409 | **Comments:** 54 | **Date:** 2026-01-15

**Summary:** The author upgraded their gaming rig to an AI-focused setup by acquiring an A100 GPU for $1000, despite it being listed as faulty. The GPU worked immediately, allowing them to run and train larger AI models effectively.

**Key Points:**
- Author transitioned from gaming to AI-focused rig
- Acquired an A100 GPU for $1000, listed as faulty but worked perfectly
- Previous setup included parts like a 3090 and 7950x
- Community engagement with post, including Discord feature and special flair
- Discussion about cooling solutions for the A100 GPU

**Discussion Highlights:** The community showed appreciation for the post, with one comment linking to a meme and others discussing cooling solutions for the A100 GPU, highlighting concerns about passive cooling and suggesting active cooling methods.

---

## 20. [Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M](https://reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/)

**Author:** u/eugenekwek | **Upvotes:** 321 | **Comments:** 54 | **Date:** 2026-01-14

**Summary:** The post announces Soprano 1.1, an improved version of the Soprano TTS model with significant reductions in hallucinations and audio artifacts, along with better performance metrics and longer sentence support. The community response is overwhelmingly positive, with users praising the model's quality and expressing interest in future developments.

**Key Points:**
- Soprano 1.1 reduces hallucinations by 95% and lowers WER by 50% compared to the previous version.
- The model now supports sentences up to 30 seconds long, doubling the previous limit.
- A blind study showed a 63% preference rate for Soprano 1.1 over the original model.
- Community feedback highlights the model's impressive performance for its size (80M parameters).
- Users are interested in additional features like ONNX support and improved handling of punctuation.

**Discussion Highlights:** The community response is highly positive, with users expressing surprise at the model's quality given its small size. There is interest in future enhancements, such as ONNX support and better handling of em-dashes. Overall, the consensus is that Soprano 1.1 is a significant improvement and a valuable contribution to the TTS field.

---

## 21. [NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency](https://reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/)

**Author:** u/Fear_ltself | **Upvotes:** 714 | **Comments:** 130 | **Date:** 2026-01-14

**Summary:** NVIDIA's Orchestrator-8B is an 8-billion-parameter AI designed to manage and route complex tasks to various tools for greater efficiency, sparking discussions about AGI and functional AI systems.

**Key Points:**
- Orchestrator-8B is a specialized 8B model for task management and routing.
- It connects with other tools and models for efficient task handling.
- Discussions highlight its potential in functional AI systems and AGI development.
- Comparisons to middle management and existing agentic frameworks.
- Mentions of hierarchical model management systems.

**Discussion Highlights:** The discussion includes humor about the model being a 'Middle manager LLM' and serious considerations about its role in advancing AI systems, with mentions of hierarchical model management and existing frameworks like Claude's agentic systems.

---

## 22. [GLM-Image is released!](https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/)

**Author:** u/foldl-li | **Upvotes:** 605 | **Comments:** 83 | **Date:** 2026-01-13

**Summary:** GLM-Image is a new image generation model with a hybrid autoregressive + diffusion decoder architecture. It excels in text-rendering and knowledge-intensive tasks while maintaining high-fidelity image generation capabilities. The model supports various image-to-image tasks and is released under an MIT license.

**Key Points:**
- Hybrid autoregressive + diffusion decoder architecture
- Excels in text-rendering and knowledge-intensive generation
- Supports image editing, style transfer, and multi-subject consistency
- Released under MIT license
- Model size: 13GB diffusion model + 20GB text encoder

**Discussion Highlights:** The community appreciates the MIT license and the model's capabilities. There is excitement about its performance compared to other models and anticipation for quantized versions. Some users are interested in its potential for various applications, including adult content.

---

## 23. [My wishes for 2026](https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/)

**Author:** u/jacek2023 | **Upvotes:** 652 | **Comments:** 179 | **Date:** 2026-01-13

**Summary:** The Reddit post discusses predictions for 2026, focusing on the possibility of affordable GPUs with more than 32GB memory. The community engages in a mix of humorous and skeptical responses regarding the feasibility of such advancements.

**Key Points:**
- The post asks which predictions for 2026 are likely to happen first and which won't happen at all.
- A top comment humorously dismisses the idea of affordable GPUs with more than 32GB memory as unrealistic.
- Other comments joke about the feasibility of such technological advancements.
- There is a mention of specific AI models like Qwen 4 and Mistral, suggesting they might be achievable, while other advancements are seen as miracles.

**Discussion Highlights:** The discussion is marked by a mix of humor and skepticism, with a consensus that affordable GPUs with more than 32GB memory are unlikely to become a reality in 2026. The community engages in playful banter, with some comments joking about manifesting such advancements.

---

## 24. [kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop—no GPU required](https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/)

**Author:** u/Nunki08 | **Upvotes:** 394 | **Comments:** 93 | **Date:** 2026-01-13

**Summary:** Kyutai introduced Pocket TTS, a 100M-parameter text-to-speech model with high-quality voice cloning that runs on a laptop without requiring a GPU. The model is available on GitHub and Hugging Face, with a blog post and arXiv paper providing more details.

**Key Points:**
- Pocket TTS is a 100M-parameter TTS model with high-quality voice cloning.
- It runs on a laptop without needing a GPU.
- The model is available on GitHub and Hugging Face.
- A warning about memory usage during generation was noted in the comments.
- Discussion includes inquiries about language support and comparisons with other small models.

**Discussion Highlights:** The discussion highlights a memory usage warning where the localhost test server setup doesn't clear memory between generations, leading to high memory usage. There are also inquiries about language support and comparisons with other small models.

---

## 25. [GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/)

**Author:** u/TKGaming_11 | **Upvotes:** 369 | **Comments:** 93 | **Date:** 2026-01-12

**Summary:** The Reddit post highlights DeepSeek-AI's 'Engram' project, a novel approach to conditional memory in large language models using scalable lookup, praised for its originality and technical innovation.

**Key Points:**
- DeepSeek-AI's Engram introduces a new sparsity axis via scalable lookup for LLMs
- The approach uses n-gram embeddings as static memory with O(1) lookup
- The paper demonstrates a U-shaped performance curve in ablations
- Community compares this to biological memory systems
- DeepSeek's work is consistently praised for original ideas

**Discussion Highlights:** The community discussion emphasizes the technical novelty of Engram's approach, particularly the n-gram embedding method and its potential as a complementary sparsity axis. There's consensus on the originality of DeepSeek's work and its alignment with biological memory processes.

---

## 26. [LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)](https://reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 1064 | **Comments:** 114 | **Date:** 2026-01-11

**Summary:** The post introduces TimeCapsuleLLM, a 1.2B parameter language model trained exclusively on 1800-1875 London texts to minimize modern bias. The model demonstrates period-specific knowledge and behaviors, such as unfamiliarity with post-1875 concepts like telephones. The project is open-source and aims to create synthetic Q&A pairs next.

**Key Points:**
- TimeCapsuleLLM is trained on 90GB of 1800-1875 London texts with no modern data or fine-tuning.
- The model shows period-accurate behaviors, like arguing against the Roman Catholic Church and misunderstanding telephones.
- The project is open-source with links to GitHub and Hugging Face.
- Future steps include creating synthetic Q&A pairs from the dataset.
- The community appreciates the project, with comments highlighting its uniqueness and potential.

**Discussion Highlights:** The community response is overwhelmingly positive, with users praising the project's creativity and potential. Some commenters share similar interests in training models on historical data, while others joke about the model's 1875 knowledge cutoff.

---

## 27. [I bought a €9k GH200 “desktop” to save $1.27 on Claude Code (vLLM tuning notes)](https://reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/)

**Author:** u/Reddactor | **Upvotes:** 697 | **Comments:** 177 | **Date:** 2026-01-11

**Summary:** The author built a high-end GH200 desktop system for €9k to run Claude Code locally, achieving better speeds and results than the cloud-based version. They shared optimized vLLM settings for dual 96GB systems and highlighted the cost savings and performance benefits of local execution. Key points include the €9k investment, better performance than cloud-based Claude Code, optimized vLLM settings, cost savings, and humorous accounting aspects. The community appreciated the setup and shared humorous comments about the cost and value of the project.

---

## 28. [It works! Abliteration can reduce slop without training](https://reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/)

**Author:** u/-p-e-w- | **Upvotes:** 400 | **Comments:** 129 | **Date:** 2026-01-11

**Summary:** The post discusses the use of abliteration to reduce 'slop' (flowery, cliched language) in LLM outputs without training. The author successfully applied this technique to the Mistral Nemo model, creating a slop-reduced version using Heretic, a tool originally designed for censorship removal.

**Key Points:**
- Abliteration can reduce slop in LLM outputs without training.
- The author used Heretic to create a slop-reduced configuration for the Mistral Nemo model.
- The process took 2.5 hours on an A6000 but could be faster with quantization or reduced parameters.
- The technique shows a clear semantic separation in residual patterns between layers 7 and 10.
- Community feedback is mixed, with some appreciating the reduction in slop while others find the output too dry.

**Discussion Highlights:** The discussion highlights mixed opinions on the effectiveness of the slop reduction technique. Some users appreciate the cleaner output, while others feel it lacks imagination or becomes too dry. There is also interest in whether this technique could be applied to other overused patterns in writing.

---

## 29. [I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work](https://reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/)

**Author:** u/Ok-Pomegranate1314 | **Upvotes:** 896 | **Comments:** 147 | **Date:** 2026-01-09

**Summary:** The author successfully clustered three DGX Sparks, which NVIDIA claimed couldn't be done, by writing a custom NCCL network plugin. This allowed distributed inference across all three nodes at high speeds using RDMA.

**Key Points:**
- Author clustered three DGX Sparks despite NVIDIA's limitations.
- Custom NCCL network plugin written in ~1500 lines of C.
- Achieved distributed inference at 8+ GB/s over RDMA.
- Plugin handles subnet-aware NIC selection and raw RDMA verbs implementation.
- Community praised the achievement as impressive and potentially significant.

**Discussion Highlights:** The community highlighted the technical difficulty of working with NCCL and praised the achievement. Questions were raised about scalability and performance improvements with more nodes.

---

## 30. [The reason why RAM has become so expensive](https://reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/)

**Author:** u/InvadersMustLive | **Upvotes:** 4575 | **Comments:** 382 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses the significant increase in RAM prices, with some users suggesting that companies like OpenAI may be monopolizing key resources to create future demand and make competitors' data centers economically unviable. Others note that RAM prices have risen dramatically, with some reporting a tenfold increase.

**Key Points:**
- RAM prices have increased significantly, with reports of up to a tenfold rise.
- Some users speculate that companies like OpenAI are monopolizing RAM to control future demand and hinder competitors.
- The high cost of RAM is making data centers, particularly in China, economically unviable.
- The discussion includes skepticism about whether the price increase is sustainable or a bubble.

**Discussion Highlights:** The discussion highlights concerns about monopolistic practices in the RAM market, with users pointing to dramatic price increases and potential economic impacts on competitors. There is also skepticism about the sustainability of these price hikes.

---

## 31. [DeepSeek V4 Coming](https://reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/)

**Author:** u/External_Mood4719 | **Upvotes:** 503 | **Comments:** 110 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release V4, a next-generation AI model with enhanced code-generation capabilities, outperforming mainstream models like Claude and GPT. The model shows improvements in handling long code prompts and overall reasoning ability.

**Key Points:**
- DeepSeek V4 focuses on strong code-generation capabilities
- Outperforms existing models like Claude and GPT in code generation
- Improved handling of long code prompts and data patterns
- Enhanced logical rigor and reasoning ability
- Users anticipate significant improvements and reliability

**Discussion Highlights:** Users express excitement and high expectations for V4, with many praising DeepSeek's cost-effectiveness and performance. Some anticipate a significant leap in capabilities, while others speculate on potential integrations like mHC and deepseek-ocr for long prompts.

---

## 32. [(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability](https://reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/)

**Author:** u/Nunki08 | **Upvotes:** 482 | **Comments:** 102 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release a new flagship AI model with strong coding capabilities, generating excitement and discussion in the community.

**Key Points:**
- DeepSeek's upcoming model focuses on strong coding ability
- Community excitement and anticipation for the new model
- Discussion about potential competition with OpenAI
- Mixed reactions to typical marketing language in AI announcements
- Requests for maintaining role-playing capabilities

**Discussion Highlights:** The community shows strong interest and anticipation for DeepSeek's new model, with discussions ranging from competitive implications to requests for specific capabilities. There's a mix of excitement and skepticism about typical AI marketing claims.

---

## 33. [The NO FAKES Act has a "Fingerprinting" Trap that kills Open Source. We need to lobby for a Safe Harbor.](https://reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/)

**Author:** u/PostEasy7183 | **Upvotes:** 615 | **Comments:** 88 | **Date:** 2026-01-08

**Summary:** The Reddit post discusses the NO FAKES Act, highlighting its potential negative impact on open-source AI development due to liability concerns for developers hosting AI models. The author urges the community to lobby for a Safe Harbor provision to protect open-source tool developers.

**Key Points:**
- The NO FAKES Act creates a 'digital replica right' that could hold developers liable for misuse of their AI models.
- Developers hosting AI models on platforms like HuggingFace could face statutory damages if their models are used to create unauthorized replicas.
- The post calls for a 'Safe Harbor' provision to protect open-source developers and prevent a monopoly by big tech companies.
- The community is encouraged to contact their representatives to oppose the bill unless it includes protections for open-source developers.
- There is concern that the bill could stifle innovation and give an unfair advantage to large corporations.

**Discussion Highlights:** The discussion highlights strong opposition to the bill's current form, with many users expressing concern about its impact on innovation and the potential for big tech monopolies. Some users question whether politicians understand the technical implications of the bill.

---

## 34. [Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt](https://reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/)

**Author:** u/Prior-Arm-6705 | **Upvotes:** 943 | **Comments:** 147 | **Date:** 2026-01-08

**Summary:** A Reddit user created a compilation video of every instance Jensen Huang said 'AI' during the NVIDIA CES 2025 keynote, totaling 121 times. The process involved using open-source tools to download, parse, and edit the video locally.

**Key Points:**
- Jensen Huang said 'AI' 121 times during the CES 2025 keynote.
- The user utilized open-source tools like yt-dlp-mcp and ffmpeg-mcp-lite for video processing.
- The process was entirely local, with no cloud involvement.
- The resulting video was described as 'hypnotic'.
- Top comments included discussions about the post's popularity, Jensen's influence on pricing, and his distinctive attire.

**Discussion Highlights:** The discussion highlighted the post's popularity, with comments ranging from appreciation for the technical achievement to humorous remarks about Jensen Huang's impact on tech pricing and his fashion choices.

---

## 35. [16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/)

**Author:** u/ai-infos | **Upvotes:** 465 | **Comments:** 238 | **Date:** 2026-01-07

**Summary:** The post discusses running Deepseek V3.2 AWQ 4-bit on 16 AMD MI50 GPUs, achieving 10 tokens/sec output and 2000 tokens/sec input with a 69000 context length. The setup draws 550W idle and 2400W peak power, aiming for cost-effective local AGI hardware.

**Key Points:**
- Deepseek V3.2 AWQ 4-bit running on 16 AMD MI50 GPUs
- Performance: 10 tok/s output, 2000 tok/s input, 69000 context length
- Power draw: 550W idle / 2400W peak
- Goal: cost-effective alternative to CPU hardware
- Future plans: 32 AMD MI50 setup for Kimi K2 Thinking

**Discussion Highlights:** Comments highlight the power usage as a potential heating solution, curiosity about noise levels and home power capacity, and the cost-effectiveness for professional developers.

---

## 36. [DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.](https://reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/)

**Author:** u/Nunki08 | **Upvotes:** 663 | **Comments:** 54 | **Date:** 2026-01-07

**Summary:** The Reddit post discusses the recent update to DeepSeek-R1's paper, which expanded from 22 pages to 86 pages, adding significant detail. The community is excited about potential new architectures and improvements.

**Key Points:**
- DeepSeek-R1's paper was updated from 22 pages to 86 pages.
- The update includes substantial additional details.
- Community speculation about new architectures (e.g., dsv4 + r2).
- Interest in how architectural improvements perform at different model sizes.
- Focus on linear attention and cache optimization in current research.

**Discussion Highlights:** The community is enthusiastic about the expanded paper, with discussions focusing on potential new architectures, improvements in model performance, and the implications of linear attention and cache optimization in current research.

---

## 37. [A 30B Qwen Model Walks Into a Raspberry Pi… and Runs in Real Time](https://reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/)

**Author:** u/ali_byteshape | **Upvotes:** 497 | **Comments:** 79 | **Date:** 2026-01-06

**Summary:** The post discusses running a 30B Qwen model on a Raspberry Pi 5, achieving 8.03 TPS at 2.70 BPW while retaining 94.18% of BF16 quality. It highlights differences in CPU vs GPU behavior and compares performance with other quantization methods. Key points include the model's performance on Raspberry Pi 5, retention of quality, and community feedback on testing and potential clustering.

---

## 38. [Performance improvements in llama.cpp over time](https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/)

**Author:** u/jacek2023 | **Upvotes:** 677 | **Comments:** 85 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses performance improvements in llama.cpp over time, with a focus on NVIDIA GPU performance gains and comparisons with other implementations.

**Key Points:**
- Performance gains are highlighted for NVIDIA GPUs.
- References to NVIDIA's blog post on open-source AI tool upgrades.
- Comparisons with ik_llama.cpp show significant progress in token generation speed.
- Prompt processing is noted to be slower but overall progress is praised.

**Discussion Highlights:** The discussion highlights significant progress in token generation speed, with comparisons to other implementations and a focus on NVIDIA GPU performance improvements.

---

## 39. [Liquid Ai released LFM2.5, family of tiny on-device foundation models.](https://reddit.com/r/LocalLLaMA/comments/1q5a0if/liquid_ai_released_lfm25_family_of_tiny_ondevice/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 313 | **Comments:** 56 | **Date:** 2026-01-05

**Summary:** Liquid AI released LFM2.5, a family of tiny on-device foundation models designed for reliable agentic applications. The models feature higher quality, lower latency, and broader modality support in the ~1B parameter class, with five open-weight model instances including general-purpose, Japanese-optimized, vision-language, audio-language, and base checkpoints.

**Key Points:**
- LFM2.5 builds on a device-optimized hybrid architecture with scaled pretraining from 10T to 28T tokens.
- The models include a general-purpose instruct model, a Japanese-optimized chat model, a vision-language model, a native audio-language model, and base checkpoints for customization.
- User discussions highlight comparisons with other models like Qwen3-0.6B, noting the high data-to-parameter ratio and mixed feedback on instruction-following capabilities.
- Some users appreciate the speed and performance, while others suggest improvements like training for native FP8 or FP4 for better on-device efficiency.
- There is a call for larger model variants from some users.

**Discussion Highlights:** The discussion includes comparisons with other models, feedback on performance and instruction-following, and suggestions for future improvements. Users generally appreciate the advancements but have mixed opinions on specific capabilities and desire larger model options.

---

## 40. [For the first time in 5 years, Nvidia will not announce any new GPUs at CES — company quashes RTX 50 Super rumors as AI expected to take center stage](https://reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/)

**Author:** u/FullstackSensei | **Upvotes:** 628 | **Comments:** 195 | **Date:** 2026-01-05

**Summary:** Nvidia will not announce new GPUs at CES, focusing instead on AI, while facing limited supply of high-end GPUs and rising hardware prices. The community expresses frustration over corporate greed and the lack of affordable, accessible hardware.

**Key Points:**
- No new GPU announcements from Nvidia at CES, with a focus on AI
- Limited supply of high-end GPUs like the 5070Ti, 5080, and 5090
- Potential re-release of older models like the RTX 3060 to meet demand
- Rising prices for DDR5 RAM and storage, making upgrades costly
- Community frustration over corporate greed and lack of consumer-focused products

**Discussion Highlights:** The discussion highlights strong dissatisfaction with Nvidia's shift towards AI and away from consumer products, with many users expressing concerns about the future of affordable local computing. There is a consensus that corporate greed is driving these trends, and some users humorously suggest alternatives like Chinese manufacturers flooding the market with high-capacity GPUs.

---

## 41. [llama.cpp performance breakthrough for multi-GPU setups](https://reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/)

**Author:** u/Holiday-Injury-9397 | **Upvotes:** 568 | **Comments:** 203 | **Date:** 2026-01-05

**Summary:** The ik_llama.cpp project has achieved a significant performance breakthrough for multi-GPU setups, delivering a 3x to 4x speed improvement in local LLM inference. This advancement allows for the utilization of multiple low-cost GPUs instead of expensive high-end enterprise cards.

**Key Points:**
- ik_llama.cpp introduces a new execution mode (split mode graph) for multi-GPU configurations.
- The breakthrough enables simultaneous and maximum utilization of multiple GPUs.
- This development is cost-effective, allowing the use of low-cost GPUs instead of expensive enterprise cards.
- Performance improvements are also noted on single GPU and CPU-only setups.
- The project is seen as competitive with other performance-optimized forks like exllama and vllm.

**Discussion Highlights:** The community is excited about the performance gains and cost-effectiveness of the new multi-GPU setup. There is a consensus that this is a game-changer for local LLM inference, making it more accessible and affordable. Some users have reported consistent performance improvements even on single GPU or CPU-only setups.

---

## 42. [GLM-Image model from Z.ai is coming](https://reddit.com/r/LocalLLaMA/comments/1q41bw1/glmimage_model_from_zai_is_coming/)

**Author:** u/Ravencloud007 | **Upvotes:** 314 | **Comments:** 59 | **Date:** 2026-01-04

**Summary:** The Reddit post announces the upcoming GLM-Image model from Z.ai, which has generated significant interest in the community. The model is highly anticipated, with users expressing excitement about its potential capabilities and comparing it favorably to existing models.

**Key Points:**
- The GLM-Image model from Z.ai is being introduced.
- The community is highly interested, as indicated by the upvotes and comments.
- Users are speculating about the model's size and capabilities, with one comment mentioning a potential 103 billion parameters.
- There is a consensus that Z.ai's image models are currently the community favorite.
- Some users are concerned about the computational resources required to use the new model.

**Discussion Highlights:** The discussion highlights a strong community interest in the GLM-Image model, with users expressing excitement and anticipation. There is a consensus that Z.ai's models are highly regarded, and users are speculating about the model's size and capabilities. Some concerns about computational resources were also raised.

---

## 43. [Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched](https://reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/)

**Author:** u/ubrtnk | **Upvotes:** 378 | **Comments:** 194 | **Date:** 2026-01-03

**Summary:** The post discusses the challenges faced by local LLMs in processing extreme or unlikely breaking news events, such as the US attacking Venezuela and capturing Maduro. The author shares their experience with different LLMs, highlighting how these models initially classified the event as a hoax despite credible sources.

**Key Points:**
- Local LLMs struggled to accept extreme breaking news as real, classifying it as a hoax.
- Different LLMs (Qwen Research, Spark 4.0, GPT-OSS:120B) had varying responses to the news.
- Providing credible sources helped some LLMs acknowledge the event's reality.
- Commenters shared similar experiences with LLMs dismissing unlikely events.
- Discussion highlights the bias and limitations of LLMs in processing unfamiliar geopolitical events.

**Discussion Highlights:** The discussion highlights the limitations and biases of LLMs in processing unfamiliar or extreme geopolitical events. Commenters shared similar experiences and expressed curiosity about the future of AI in handling such events.

---

## 44. [LeCun Says Llama 4 results "were fudged a little bit"](https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/)

**Author:** u/MrPecunius | **Upvotes:** 370 | **Comments:** 88 | **Date:** 2026-01-02

**Summary:** Yann LeCun confirmed that Llama 4 benchmarks were manipulated, and Meta's AI organization faced significant restructuring, leading to departures and lack of progress on promised models.

**Key Points:**
- LeCun confirms Llama 4 benchmark manipulation
- Meta's AI organization was sidelined, leading to departures
- No follow-up on the promised large Llama 4 model
- Community disappointment in Meta's handling of Llama
- Additional resources shared for further reading

**Discussion Highlights:** The discussion reflects disappointment in Meta's strategic decisions, with users sharing additional resources and questioning how a well-positioned company could falter while smaller labs thrive.

---

## 45. [Qwen-Image-2512](https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/)

**Author:** u/Nunki08 | **Upvotes:** 720 | **Comments:** 122 | **Date:** 2025-12-31

**Summary:** The Reddit post announces the release of Qwen-Image-2512, a new model available through various platforms like Hugging Face, ModelScope, and GitHub. It includes guides and GGUF files for easy access and usage.

**Key Points:**
- Qwen-Image-2512 is a new model with guides and GGUF files available
- The model can be accessed via multiple platforms including Hugging Face, ModelScope, and GitHub
- Users have successfully run the model on low-end hardware without a GPU
- The community has shown positive reception and creative applications of the model
- Various demos and APIs are available for testing and integration

**Discussion Highlights:** The discussion highlights include successful usage on low-end hardware, positive feedback on the model's release as a 'New Year's gift,' and creative applications such as generating unique images. The community appreciates the accessibility and versatility of the model.

---

## 46. [[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It’s running a raw Llama-7B instance with a 2048 token window.](https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/)

**Author:** u/simar-dmg | **Upvotes:** 742 | **Comments:** 110 | **Date:** 2025-12-30

**Summary:** A user reverse-engineered a Snapchat sextortion bot and discovered it was running a raw Llama-7B instance with a 2048 token window. The bot was vulnerable to a persona-adoption jailbreak, revealing its configuration and malicious payload.

**Key Points:**
- The bot used a Llama-7B model with a 2048 token context window.
- A 'Grandma Protocol' jailbreak exposed the bot's environment variables.
- The bot had a high temperature setting (1.0), making it susceptible to roleplay attacks.
- The bot's payload was a malicious link disguised to bypass Snapchat's URL filters.
- Scammers are using open-source models to avoid API costs and censorship filters.

**Discussion Highlights:** The discussion included skepticism about the accuracy of the bot's revealed information, with some users suggesting it could be entirely hallucinated. Others questioned the commonality of system prompts including environment variables.

---

## 47. [Llama-3.3-8B-Instruct](https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/)

**Author:** u/jacek2023 | **Upvotes:** 462 | **Comments:** 77 | **Date:** 2025-12-29

**Summary:** The post discusses the discovery and release of the Llama-3.3-8B-Instruct model, which was previously only accessible via Meta's API. The author managed to download and share the model, including an adapter that can be removed to obtain the original model.

**Key Points:**
- Llama-3.3-8B-Instruct was previously only available through Meta's API.
- The author found a way to download the model via a finetuning API.
- The model includes an adapter that can be removed to get the original model.
- The community is verifying the model's authenticity and performance.
- There is excitement and interest in the discovery within the community.

**Discussion Highlights:** The community is actively verifying the model's authenticity and performance through benchmarks and evaluations. There is significant excitement about the discovery and the potential of the model.

---

## 48. [Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.](https://reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 341 | **Comments:** 120 | **Date:** 2025-12-29

**Summary:** Z AI is set to go public with an IPO on January 8, aiming to raise $560 million, marking it as the first AI-native LLM company to list globally. The Reddit discussion highlights mixed reactions, with concerns about the future of open-source AI and the inevitability of monetization.

**Key Points:**
- Z AI's IPO is scheduled for January 8, targeting $560 million.
- Concerns about the impact on open-source AI models.
- Debate on whether Z AI will continue releasing open weight models.
- Monetization seen as a necessary step for AI companies.
- Community reactions range from support to skepticism.

**Discussion Highlights:** The discussion reflects a consensus that monetization is inevitable for AI companies, with significant concern about the potential decline of open-source AI models. Some users argue that subscription models can coexist with open-source releases, while others fear a complete shift away from open-source.

---

## 49. [Tencent just released WeDLM 8B Instruct on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 425 | **Comments:** 62 | **Date:** 2025-12-29

**Summary:** Tencent has released WeDLM 8B Instruct, a diffusion language model that outperforms vLLM-optimized Qwen3-8B in math reasoning tasks by running 3-6× faster. The model is available on Hugging Face under an Apache 2.0 license.

**Key Points:**
- WeDLM 8B Instruct is a diffusion language model released by Tencent.
- It runs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks.
- The model is available on Hugging Face with an Apache 2.0 license.
- There is also a 7B version of the model available.
- The community finds the model promising and appreciates its performance and open-source license.

**Discussion Highlights:** The community is excited about the performance and potential of the WeDLM models, particularly noting their speed and open-source license. There is a consensus that 7-8B models have significant potential, and the release of WeDLM is seen as a positive development in the field.

---

## 50. [NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux](https://reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/)

**Author:** u/HumanDrone8721 | **Upvotes:** 443 | **Comments:** 185 | **Date:** 2025-12-27

**Summary:** NVIDIA has dropped Pascal support on Linux, causing issues for Arch Linux users. The change affects cards like the 24GB P40 and has sparked discussions about legacy driver support.

**Key Points:**
- NVIDIA's Linux driver (version 590) no longer supports Pascal GPUs
- Arch Linux has moved legacy Pascal drivers to AUR (Arch User Repository)
- Popular Pascal cards like the 24GB P40 are affected
- Users express concerns about future support for their hardware
- The change was announced in Arch Linux news

**Discussion Highlights:** The community shows mixed reactions - some express concern about hardware obsolescence, while others note this follows Arch's pattern of moving legacy drivers to AUR. There's acknowledgment that this change was expected but still disruptive for Pascal GPU users.

---

