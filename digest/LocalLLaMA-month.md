# r/LocalLLaMA Reading Digest

**Period:** 2026-01-26 to 2026-01-26
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [I just won an Nvidia DGX Spark GB10 at an Nvidia hackathon. What do I do with it?](https://reddit.com/r/LocalLLaMA/comments/1qn3xig/i_just_won_an_nvidia_dgx_spark_gb10_at_an_nvidia/)

**Author:** u/brandon-i | **Upvotes:** 466 | **Comments:** 149 | **Date:** 2026-01-25

**Summary:** The author won an Nvidia DGX Spark GB10 at a hackathon and seeks advice on how to utilize it, having no prior experience with fine-tuning models. They mention running a NextJS app that consumed significant memory and are open to suggestions on leveraging the new hardware. Key points include the author's background, their project's memory usage, and community suggestions like exploring Nvidia's playbooks or running multiple NextJS apps. The discussion highlights practical advice and humorous comments about selling the hardware.

---

## 2. [Your post is getting popular and we just featured it on our Discord!](https://reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/)

**Author:** u/roculus | **Upvotes:** 562 | **Comments:** 58 | **Date:** 2026-01-23

**Summary:** The Reddit post announces that a user's post has been featured on Discord and they have received a special flair. The user expresses annoyance at the bot's public posts and suggests sending private messages instead, questioning if the Discord is being monetized. The community largely agrees that the bot's public posts are annoying and suggests that private messages would be a better approach. There is also speculation about the monetization of the Discord server.

---

## 3. [Am I the only one who feels that, with all the AI boom, everyone is basically doing the same thing?](https://reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/)

**Author:** u/[deleted] | **Upvotes:** 396 | **Comments:** 189 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses the redundancy of AI projects during the AI boom, noting that many new AI tools and applications are essentially replications of existing solutions. The author highlights the enthusiasm and low barrier to entry in the field, which leads to shallow implementations and repetitive projects. Key points include the redundancy of AI projects, the surge in enthusiasm and low barrier to entry, the focus on niche tools, the current 'hype stage' with many self-proclaimed AI experts, and the consensus on repetition and redundancy in the field. The discussion highlights a consensus that the AI field is currently in a hype stage, with many redundant projects and shallow implementations, but also a focus on niche tools and specific needs.

---

## 4. [Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp; 1.8B), Support for 10 languages](https://reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/)

**Author:** u/Nunki08 | **Upvotes:** 725 | **Comments:** 117 | **Date:** 2026-01-22

**Summary:** Qwen has open-sourced the Qwen3-TTS model family, including 5 models (0.6B & 1.8B) with support for 10 languages. The release includes resources like GitHub, Hugging Face, a blog post, a paper, and a demo.

**Key Points:**
- Qwen3-TTS model family open-sourced
- 5 models (0.6B & 1.8B) available
- Support for 10 languages
- Multiple resources provided (GitHub, Hugging Face, blog, paper, demo)
- Positive community reception with some concerns about English voice quality

**Discussion Highlights:** The community appreciates Qwen's open-source contributions, with positive feedback on the model's capabilities. Some users noted concerns about the English voice quality and requested compatibility with tools like llama.cpp.

---

## 5. [Qwen dev on Twitter!!](https://reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 742 | **Comments:** 60 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses Qwen's TTS model announcement, with community reactions and links to the model on Hugging Face.

**Key Points:**
- Qwen's TTS model announcement
- Community reaction and discussion
- Link to the model on Hugging Face
- Thread locked due to announcements being out
- Mention of vLLM leak related to the TTS model

**Discussion Highlights:** The community is excited about the Qwen TTS model, with some users sharing links to the model on Hugging Face. There is also mention of a vLLM leak related to the TTS model, and the thread was locked as announcements are out.

---

## 6. [8x AMD MI50 32GB at 26 t/s (tg) with MiniMax-M2.1 and 15 t/s (tg) with GLM 4.7 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1qjaxfy/8x_amd_mi50_32gb_at_26_ts_tg_with_minimaxm21_and/)

**Author:** u/ai-infos | **Upvotes:** 315 | **Comments:** 128 | **Date:** 2026-01-21

**Summary:** The post discusses a cost-effective local inference setup using 8x AMD MI50 GPUs, achieving high token generation speeds with MiniMax-M2.1 and GLM 4.7 models. The setup is praised for its performance and affordability, with a total VRAM of 256GB for under $1k. Key points include the performance metrics of the models, the cost and power efficiency of the setup, and the community's positive reactions to the cost-to-performance ratio.

---

## 7. [Fix for GLM 4.7 Flash has been merged into llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qiwm3c/fix_for_glm_47_flash_has_been_merged_into_llamacpp/)

**Author:** u/jacek2023 | **Upvotes:** 316 | **Comments:** 86 | **Date:** 2026-01-21

**Summary:** A fix for GLM 4.7 Flash has been merged into llama.cpp, which is significant for the community. The post highlights performance improvements and community reactions.

**Key Points:**
- Fix for GLM 4.7 Flash merged into llama.cpp
- Performance data shared for different quantizations and GPUs
- Community appreciation and discussion on performance and usability
- Ongoing work on CUDA support
- Mixed experiences reported with prompt processing speed

**Discussion Highlights:** The community is generally positive about the fix, with discussions focusing on performance metrics, usability on different hardware, and ongoing development efforts. Some users report issues with prompt processing speed in specific environments.

---

## 8. [You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?](https://reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/)

**Author:** u/Adventurous-Gold6413 | **Upvotes:** 545 | **Comments:** 306 | **Date:** 2026-01-20

**Summary:** The Reddit post discusses the best local models to use with 64GB RAM and 16GB VRAM when internet access is unavailable. Users share their top model choices and experiences. Key points include recommendations for models like Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B, with a consensus around the effectiveness of GPT-OSS-120B for general use cases. The discussion highlights the importance of models that fit well within the hardware constraints and offer good performance across various domains.

---

## 9. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 913 | **Comments:** 272 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, high-performance AI system designed for running large MoE models and supporting graphic design tasks. The system features a Threadripper Pro 3995WX, 512GB DDR4 RAM, and a mix of 8x 3090 and 2x 5090 GPUs, all enclosed in a Thermaltake Core W200 case for mobility and protection. The total cost was approximately $17k, balancing performance and budget constraints.

**Key Points:**
- The system is designed for large MoE models and graphic design tasks, with a focus on mobility and enclosure.
- It features a Threadripper Pro 3995WX, 512GB DDR4 RAM, and a mix of 8x 3090 and 2x 5090 GPUs.
- The total cost was approximately $17k, balancing performance and budget constraints.
- The enclosure was a critical requirement due to the presence of cats, ruling out mining frames.
- The system is highly praised in the comments, with humor about its portability and power.

**Discussion Highlights:** The top comments highlight the system's impressive capabilities and humorously reference its portability and power. One comment jokes about plugging it into a McDonald's socket, while others express admiration for the build's complexity and airflow challenges.

---

## 10. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 367 | **Comments:** 60 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the official support for GLM 4.7 Flash in llama.cpp, highlighting its community-driven development and performance improvements. Users discuss its efficiency and share additional resources.

**Key Points:**
- GLM 4.7 Flash now officially supported in llama.cpp
- Support is community-driven, not by Z.ai developers
- Performance improvements noted, with some users reporting faster execution without flash-attention
- Additional resources and model versions shared by community members
- Post recognized with special flair and featured on Discord

**Discussion Highlights:** The discussion highlights the community effort behind the implementation and shares performance insights, with some users noting faster execution without flash-attention. Additional model versions and resources are also shared.

---

## 11. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 467 | **Comments:** 162 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights the effectiveness of GLM 4.7 Flash as a reliable local agent for various tasks, outperforming other MoE models. Users are eager to try it locally once GGUFs are available.

**Key Points:**
- GLM 4.7 Flash is praised for its reliability and performance in agentic frameworks.
- It successfully handles tasks like cloning repos, running commands, and editing files without errors.
- Users are excited about the upcoming GGUFs for local testing.
- Comparisons with other models like Nemotron 30B and Qwen3 are discussed.
- The model is noted for its deep thinking and decent speed on a 4090 GPU.

**Discussion Highlights:** The discussion highlights enthusiasm for GLM 4.7 Flash's capabilities and comparisons with other models. Users are testing it locally and sharing their experiences, noting its performance and potential.

---

## 12. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 743 | **Comments:** 230 | **Date:** 2026-01-19

**Summary:** The Reddit post discusses the release of the GLM-4.7-Flash model on Hugging Face, highlighting its features and community anticipation.

**Key Points:**
- The model uses MLA, reducing KV cache memory usage
- It supports a full 200k context, making it accessible to more users
- The community expresses excitement and nostalgia for larger models
- The release is seen as promising and long-awaited

**Discussion Highlights:** The community is enthusiastic about the model's capabilities, particularly its memory efficiency and context length. There is a sense of anticipation and appreciation for the release, with some users expressing nostalgia for larger models.

---

## 13. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 351 | **Comments:** 104 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to stay within a ~10,000€ budget. The system is designed for running large AI models locally, with benchmark results showing strong performance across various models. Key points include the system's hardware specifications, budget details, and its purpose for running large AI models. The community reacted with admiration and curiosity about the build process and cost.

---

## 14. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 454 | **Comments:** 72 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses a potential slowdown in Qwen 4 development, with the lead developer emphasizing a focus on quality over speed. The community generally appreciates this approach, though some express skepticism about the rumors.

**Key Points:**
- Qwen 4 development may be slowing down to focus on quality
- Community appreciates the focus on quality over quantity
- Skepticism exists about the validity of the rumors
- Incremental improvements are seen as less impactful

**Discussion Highlights:** The discussion highlights a general consensus that focusing on quality is beneficial, with some users cautioning against spreading unconfirmed rumors. The community values meaningful advancements over frequent, minor updates.

---

## 15. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 540 | **Comments:** 117 | **Date:** 2026-01-17

**Summary:** The author upgraded from MI100 to R9700 GPUs for better performance and cost efficiency, detailing a high-end server build with 128GB VRAM and sharing performance benchmarks. Key points include the transition to R9700 GPUs, detailed system specifications with a total cost of $7,035, performance benchmarks showing high token processing rates, and community appreciation with humorous comments about financial irresponsibility. The discussion highlights praise for the build and benchmarks, along with humor about the financial implications of such high-end hardware.

---

## 16. [Best "End of world" model that will run on 24gb VRAM](https://reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/)

**Author:** u/gggghhhhiiiijklmnop | **Upvotes:** 345 | **Comments:** 180 | **Date:** 2026-01-17

**Summary:** The user is looking for recommendations on the best large language model that can run on a PC with 24GB VRAM and 64GB RAM, suitable for an 'end of world' scenario. The discussion includes suggestions for specific models and practical advice on data storage.

**Key Points:**
- User wants a model that fits within 24GB VRAM and 64GB RAM.
- Suggestions include saving the best LLM possible and running it off SSD if necessary.
- Specific model recommendations: gemma3:27b and Midnight Miku.
- Advice to download actual Wikipedia backups for offline use.
- Mention of using models with vision capabilities.

**Discussion Highlights:** The discussion highlights practical considerations for running large models on limited hardware, with a focus on flexibility and data preservation. There is a consensus on prioritizing the best available model and using alternative storage solutions if needed.

---

## 17. [GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)](https://reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/)

**Author:** u/CuriousPlatypus1881 | **Upvotes:** 381 | **Comments:** 89 | **Date:** 2026-01-16

**Summary:** The post discusses the December 2025 SWE-bench leaderboard results, highlighting the performance of various AI models on GitHub PR tasks. Claude Opus 4.5 leads with a 63.3% resolved rate, followed closely by GPT-5.2 (extra high effort) at 61.5%. The post also notes the strong performance of open-source models like GLM-4.7.

**Key Points:**
- Claude Opus 4.5 leads the SWE-bench leaderboard with a 63.3% resolved rate.
- GPT-5.2 (extra high effort) follows closely at 61.5%.
- Gemini 3 Flash Preview outperforms Gemini 3 Pro Preview despite being smaller and cheaper.
- GLM-4.7 is the strongest open-source model, ranking alongside closed models like GPT-5.1-codex.
- The community is excited about the performance of open-source models and upcoming releases like DeepSeek v4.

**Discussion Highlights:** The community is particularly excited about the performance of open-source models like GLM-4.7 and the potential of upcoming models like DeepSeek v4. There is also a consensus that this benchmark is more believable compared to others.

---

## 18. [I fucking love this community](https://reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/)

**Author:** u/alhinai_03 | **Upvotes:** 522 | **Comments:** 54 | **Date:** 2026-01-16

**Summary:** The author expresses gratitude towards the open-source community for enabling them to run large language models on older hardware, highlighting the efficiency and optimization achieved through community efforts.

**Key Points:**
- The author runs a 30B parameter model on a 10-year-old PC with 4GB VRAM at 14 tokens/second
- Key factors for success include sufficient system memory and using Mixture of Experts (MoE) architecture models
- The community's optimization efforts are praised for making such performance possible
- The post received significant engagement with 522 upvotes and 54 comments

**Discussion Highlights:** The community consensus highlights the effectiveness of system RAM and MoE architecture for running large models on limited hardware, with many users expressing admiration for the optimization achievements.

---

## 19. [My story of underestimating /r/LocalLLaMA's thirst for VRAM](https://reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/)

**Author:** u/EmPips | **Upvotes:** 1358 | **Comments:** 91 | **Date:** 2026-01-15

**Summary:** The Reddit post discusses the author's experience with the subreddit's high demand for VRAM, highlighting its popularity and community engagement. The discussion includes insights on hardware recommendations and market trends.

**Key Points:**
- The post gained significant attention with 1358 upvotes and 91 comments.
- The author received recognition for their contribution, including a special flair.
- Discussion includes comparisons between different hardware options like the 3090 and R9700.
- Community members share personal experiences and market insights.
- The post sparked a lively discussion on hardware choices and market trends.

**Discussion Highlights:** The discussion highlights a consensus on the importance of VRAM and hardware performance, with community members sharing personal experiences and recommendations. There is also a notable comparison between different hardware options, reflecting current market trends and preferences.

---

## 20. [Latest upgrade…A100 40 GB](https://reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/)

**Author:** u/inserterikhere | **Upvotes:** 408 | **Comments:** 53 | **Date:** 2026-01-15

**Summary:** The user upgraded their gaming rig to an AI rig by acquiring an A100 GPU listed as 'parts' on eBay for $1000, which worked flawlessly upon installation. The post details their journey from using a 5070ti to building a dedicated AI setup with a 3090 and 7950x, culminating in the A100 upgrade.

**Key Points:**
- User transitioned from a gaming rig to an AI-focused setup.
- Purchased an A100 GPU listed as 'parts' for $1000, which worked immediately.
- Community expressed concerns about cooling for the A100.
- Post gained significant traction with 408 upvotes and 53 comments.
- Top comment highlighted a meme referencing NVIDIA CEO Jensen Huang.

**Discussion Highlights:** The community engaged positively with the post, offering technical advice (e.g., cooling solutions) and sharing humorous reactions (e.g., memes). The consensus was a mix of admiration for the upgrade and practical concerns about hardware management.

---

## 21. [Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M](https://reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/)

**Author:** u/eugenekwek | **Upvotes:** 325 | **Comments:** 54 | **Date:** 2026-01-14

**Summary:** The post announces Soprano 1.1, an improved version of the Soprano model with significant reductions in hallucinations and audio artifacts, along with increased sentence length support and a high preference rate in blind studies.

**Key Points:**
- Soprano 1.1 reduces hallucinations by 95% and audio artifacts significantly.
- The model supports sentences up to 30 seconds long, double the previous limit.
- Blind study shows a 63% preference rate for Soprano 1.1 over the original.
- Positive community feedback highlights the model's usability and performance.
- Inquiries about future support, such as ONNX compatibility.

**Discussion Highlights:** The community is impressed with the model's performance and usability, with discussions focusing on its capabilities and potential future enhancements like ONNX support.

---

## 22. [NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency](https://reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/)

**Author:** u/Fear_ltself | **Upvotes:** 717 | **Comments:** 130 | **Date:** 2026-01-14

**Summary:** NVIDIA's Orchestrator-8B is an 8-billion-parameter AI model designed to manage and route complex tasks to various tools for greater efficiency, sparking discussions about the future of AI systems and their integration.

**Key Points:**
- Orchestrator-8B is a specialized model for task management and routing.
- The model aims to enhance efficiency by connecting with other tools and models.
- Discussions highlight the potential of such systems in achieving functional AI integration.
- Comparisons to middle management and existing frameworks were noted.
- The post gained significant attention with 717 upvotes and 130 comments.

**Discussion Highlights:** The discussion emphasized the importance of integrating different AI tools and models, with some users drawing parallels to management structures and existing frameworks. The consensus suggests that such systems could be a step towards more functional and efficient AI applications.

---

## 23. [GLM-Image is released!](https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/)

**Author:** u/foldl-li | **Upvotes:** 600 | **Comments:** 83 | **Date:** 2026-01-13

**Summary:** GLM-Image is a new image generation model with a hybrid autoregressive + diffusion decoder architecture. It excels in text-rendering and knowledge-intensive tasks, offering capabilities like image editing and style transfer.

**Key Points:**
- Hybrid autoregressive + diffusion decoder architecture
- Strong performance in text-rendering and knowledge-intensive tasks
- Supports image-to-image tasks like editing and style transfer
- MIT license with no restrictions
- High model size (13GB diffusion model + 20GB text encoder)

**Discussion Highlights:** The community appreciates the MIT license and the model's capabilities. There is interest in quantizing the model for easier use and discussions about its performance compared to other models.

---

## 24. [My wishes for 2026](https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/)

**Author:** u/jacek2023 | **Upvotes:** 657 | **Comments:** 179 | **Date:** 2026-01-13

**Summary:** The Reddit post discusses predictions for 2026, focusing on the possibility of affordable GPUs with more than 32GB of memory. The community expresses skepticism about this happening soon.

**Key Points:**
- The post asks which predictions for 2026 are likely to happen first.
- A top comment highlights the desire for affordable GPUs with >32GB memory.
- Other comments express skepticism or humor about the feasibility of such GPUs.
- Mentions of AI models like Qwen 4 and Mistral as potential developments.

**Discussion Highlights:** The discussion is centered around the feasibility of affordable high-memory GPUs in 2026, with a mix of skepticism and humor. There is no clear consensus, but the community seems to view the idea as ambitious or unlikely in the near term.

---

## 25. [kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop—no GPU required](https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/)

**Author:** u/Nunki08 | **Upvotes:** 404 | **Comments:** 93 | **Date:** 2026-01-13

**Summary:** Kyutai introduced Pocket TTS, a 100M-parameter text-to-speech model with high-quality voice cloning that runs on a laptop without requiring a GPU. The model is available on GitHub and Hugging Face, with a blog post and arXiv paper providing more details.

**Key Points:**
- Pocket TTS is a 100M-parameter model for high-quality voice cloning.
- It runs on a laptop without needing a GPU.
- Available on GitHub, Hugging Face, and detailed in a blog post and arXiv paper.
- Concerns about memory usage during generation.
- Interest in multilingual support and comparisons with other small models.

**Discussion Highlights:** The discussion highlights concerns about memory usage ballooning during generation, interest in fine-tuning for different languages, and comparisons with other small models. Some users noted that models below a certain size may not be worth the trouble.

---

## 26. [GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/)

**Author:** u/TKGaming_11 | **Upvotes:** 372 | **Comments:** 93 | **Date:** 2026-01-12

**Summary:** The Reddit post highlights DeepSeek-AI's 'Engram,' a novel approach for conditional memory in large language models using scalable lookup, praised for its originality and technical innovation.

**Key Points:**
- DeepSeek-AI introduces 'Engram' for conditional memory via scalable lookup.
- The method uses n-gram embedding and mHC (M=4) for ablations, adding a new sparsity axis.
- Community praises the originality and technical depth of the paper.
- Comparison to biological memory processes is noted as insightful.

**Discussion Highlights:** The discussion emphasizes the technical novelty of 'Engram,' its potential impact on model efficiency, and the community's positive reception of DeepSeek's innovative work.

---

## 27. [LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)](https://reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 1065 | **Comments:** 114 | **Date:** 2026-01-11

**Summary:** The post introduces TimeCapsuleLLM, a 1.2B parameter language model trained exclusively on 1800-1875 London texts to minimize modern bias. The model, trained on a 90GB dataset, demonstrates period-specific knowledge and behaviors, such as unfamiliarity with post-1875 concepts like the telephone.

**Key Points:**
- TimeCapsuleLLM is trained on 1800-1875 London texts with no modern data or fine-tuning.
- The model shows period-specific behaviors, like arguing against the Roman Catholic Church and misunderstanding the telephone.
- The project aims to reduce modern bias by focusing on a specific time and location.
- Future steps include creating synthetic Q&A pairs from the dataset.
- The community appreciates the project, with positive feedback and engagement.

**Discussion Highlights:** The community shows strong support for the project, with users praising its uniqueness and potential. Some users share similar interests in training models on historical data, and there is humor around the model's period-specific limitations.

---

## 28. [I bought a €9k GH200 “desktop” to save $1.27 on Claude Code (vLLM tuning notes)](https://reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/)

**Author:** u/Reddactor | **Upvotes:** 694 | **Comments:** 178 | **Date:** 2026-01-11

**Summary:** The author built a high-end €9k GH200 desktop with 192GB VRAM to run Claude Code locally, achieving better speeds than the cloud version and sharing optimized vLLM settings for dual 96GB systems. The setup uses MiniMax M2.1 for offline coding and blocks telemetry, though the cost is humorously noted as 321X the yearly subscription fee.

**Key Points:**
- Built a €9k GH200 desktop with 192GB VRAM for local Claude Code execution.
- Achieved better speeds than cloud-based Claude Code with Sonnet.
- Shared optimized vLLM settings for dual 96GB systems, including TP2, 163,840 context, and specific tuning knobs.
- Used MiniMax M2.1 FP8+INT4 AWQ for offline coding and blocked telemetry.
- Community reactions highlight the humor in the cost vs. savings and the fun of the project.

**Discussion Highlights:** The community praised the setup's novelty and humor, with top comments joking about the cost vs. savings and the fun of the project. Some users expressed envy over missing out on similar deals, while others sought clarification on the model used.

---

## 29. [It works! Abliteration can reduce slop without training](https://reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/)

**Author:** u/-p-e-w- | **Upvotes:** 400 | **Comments:** 129 | **Date:** 2026-01-11

**Summary:** The post discusses the use of abliteration to reduce 'slop' (flowery, cliched language) in LLM outputs, specifically applied to the Mistral Nemo model. The author successfully created a slop-reduced LLM using abliteration without fine-tuning, demonstrating the technique's potential.

**Key Points:**
- Abliteration can reduce 'slop' in LLM outputs without training.
- The technique was applied to Mistral Nemo, showing semantic separation between layers 7 and 10.
- A slop-reduced LLM was created using abliteration alone, with no fine-tuning.
- The process took 2.5 hours on an A6000 but could be faster with quantization.
- Community feedback includes mixed opinions on the effectiveness and impact on prose quality.

**Discussion Highlights:** The discussion highlights mixed opinions on the effectiveness of the technique. Some users appreciate the reduction in slop but note a lack of imagination in the output. Others question whether the technique bans all synonyms or reduces semantic meaning. There is also interest in applying the technique to other patterns and creating GGUF versions of the model.

---

## 30. [I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work](https://reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/)

**Author:** u/Ok-Pomegranate1314 | **Upvotes:** 893 | **Comments:** 147 | **Date:** 2026-01-09

**Summary:** The post describes a user's successful effort to cluster three DGX Sparks, overcoming NVIDIA's official support limit of two by developing a custom NCCL network plugin. The solution involved writing ~1500 lines of C code to handle subnet-aware NIC selection and raw RDMA verbs implementation, achieving distributed inference at 8+ GB/s over RDMA.

**Key Points:**
- NVIDIA officially supports clustering only two DGX Sparks, but the user achieved clustering three.
- The custom NCCL network plugin handles subnet-aware NIC selection and raw RDMA verbs implementation.
- The solution achieved distributed inference at 8+ GB/s over RDMA.
- The implementation involved ~1500 lines of C code and extensive low-level debugging.
- The community praised the technical achievement and its potential significance.

**Discussion Highlights:** The community highlighted the technical difficulty of working with NCCL and praised the achievement as a significant contribution. Questions were raised about scalability and performance improvements with additional nodes.

---

## 31. [The reason why RAM has become so expensive](https://reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/)

**Author:** u/InvadersMustLive | **Upvotes:** 4583 | **Comments:** 383 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses the significant increase in RAM prices, with users noting a rise of up to 10 times compared to previous years. The discussion highlights concerns about market manipulation and monopolization of key resources by major players like OpenAI. Key points include the dramatic price increase, concerns about monopolization, the potential unsustainability of the price surge, and the impact on AI data centers. The discussion centers around the economic implications of rising RAM prices, with a consensus that the price surge is driven by strategic market control rather than natural supply and demand factors.

---

## 32. [DeepSeek V4 Coming](https://reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/)

**Author:** u/External_Mood4719 | **Upvotes:** 503 | **Comments:** 110 | **Date:** 2026-01-09

**Summary:** DeepSeek is expected to release V4, a next-generation AI model with strong code-generation capabilities, outperforming mainstream models like Claude and GPT. The model shows improvements in handling long code prompts and reasoning ability.

**Key Points:**
- DeepSeek V4 focuses on strong code-generation capabilities
- Outperforms existing models like Claude and GPT in code generation
- Improved handling of long code prompts and data patterns
- Enhanced reasoning ability and reliability for complex tasks
- Users appreciate DeepSeek's cost-effectiveness and local API options

**Discussion Highlights:** Users express excitement and anticipation for V4, with positive feedback on DeepSeek's performance and affordability. Some speculate on potential delays due to extensive training and post-training processes.

---

## 33. [(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability](https://reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/)

**Author:** u/Nunki08 | **Upvotes:** 484 | **Comments:** 102 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release a new flagship AI model with strong coding capabilities, generating excitement and anticipation in the community.

**Key Points:**
- DeepSeek's upcoming model focuses on strong coding abilities
- Community excitement and anticipation for the new model
- Positive sentiment towards more AI models in the market
- Skepticism about marketing claims and internal benchmarks
- Requests to maintain role-playing (RP) capabilities

**Discussion Highlights:** The community shows enthusiasm for DeepSeek's new model, with some expressing excitement and others showing skepticism about marketing claims. There is a general consensus that more AI models benefit the community, but concerns about maintaining certain capabilities like role-playing are also noted.

---

## 34. [The NO FAKES Act has a "Fingerprinting" Trap that kills Open Source. We need to lobby for a Safe Harbor.](https://reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/)

**Author:** u/PostEasy7183 | **Upvotes:** 620 | **Comments:** 88 | **Date:** 2026-01-08

**Summary:** The NO FAKES Act proposes a 'digital replica right' that could hold developers liable for hosting open-source AI models used to create deepfakes, potentially stifling innovation. The post urges lobbying for a 'Safe Harbor' provision to protect open-source developers.

**Key Points:**
- The NO FAKES Act targets developers who 'make available' tools primarily used for creating digital replicas, imposing statutory damages.
- Open-source developers could face legal risks for hosting models like TTS or voice-conversion tools on platforms like HuggingFace.
- The post suggests contacting representatives to advocate for a 'Safe Harbor' provision to protect open-source developers.
- Comments highlight concerns about the bill's impact on innovation and the influence of big tech corporations in shaping AI regulations.
- There is a call to action for individuals to email or call their representatives to oppose the bill unless it includes protections for open-source developers.

**Discussion Highlights:** The discussion reflects strong opposition to the bill's potential to stifle open-source innovation, with many commenters expressing skepticism about politicians' understanding of technology and the influence of big tech corporations in shaping AI regulations.

---

## 35. [Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt](https://reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/)

**Author:** u/Prior-Arm-6705 | **Upvotes:** 944 | **Comments:** 147 | **Date:** 2026-01-08

**Summary:** The post describes a project where someone used open-source tools to count and compile every instance of Jensen Huang saying 'AI' (121 times) during his CES 2025 keynote, creating a hypnotic compilation video.

**Key Points:**
- Jensen Huang said 'AI' 121 times during his CES 2025 keynote.
- The project used open-source tools like OpenAgentPlatform/Dive, yt-dlp-mcp, and ffmpeg-mcp-lite.
- The process involved downloading the video, parsing subtitles, cutting clips, and concatenating them.
- The result was a compilation video that was described as hypnotic.
- The post gained significant attention with 944 upvotes and 147 comments.

**Discussion Highlights:** The discussion included appreciation for the technical achievement, humor about Jensen Huang's impact on tech prices, and references to other tech communities like Gamers Nexus. Some comments were removed, and there was a mention of a special flair given to the author.

---

## 36. [16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/)

**Author:** u/ai-infos | **Upvotes:** 465 | **Comments:** 238 | **Date:** 2026-01-07

**Summary:** The post discusses running Deepseek V3.2 AWQ 4-bit on 16 AMD MI50 GPUs, achieving 10 tokens/sec output and 2000 tokens/sec input with a 69000 context length. The setup aims for cost-effective local AGI readiness and highlights power efficiency and future scalability plans.

**Key Points:**
- Deepseek V3.2 AWQ 4-bit running on 16 AMD MI50 GPUs with 10 tok/s output and 2000 tok/s input
- Power draw: 550W idle / 2400W peak inference
- Goal: cost-effective hardware for local AGI without high expenses
- Future plans: open-source test setup for 32 AMD MI50 GPUs
- Community appreciation for open-source contributions

**Discussion Highlights:** The community reacted positively, with comments highlighting the efficiency of using the setup as a heater during winter, curiosity about noise levels and power handling at home, and the cost-effectiveness for professional developers.

---

## 37. [DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.](https://reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/)

**Author:** u/Nunki08 | **Upvotes:** 663 | **Comments:** 54 | **Date:** 2026-01-07

**Summary:** The Reddit post discusses the recent update to DeepSeek-R1’s paper, which expanded from 22 pages to 86 pages, adding significant detail. The discussion includes speculation about new architectures and the potential impact of linear attention research.

**Key Points:**
- DeepSeek-R1’s paper was updated from 22 pages to 86 pages.
- The update includes substantial additional detail.
- Discussion highlights potential new architectures (e.g., dsv4 + r2).
- Linear attention research is a key focus, enabling training of larger models.
- The original paper lacked implementation specifics, which the update may address.

**Discussion Highlights:** The community is excited about the expanded paper, with speculation about new model architectures and the impact of linear attention research. There is consensus on the value of added implementation details.

---

## 38. [A 30B Qwen Model Walks Into a Raspberry Pi… and Runs in Real Time](https://reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/)

**Author:** u/ali_byteshape | **Upvotes:** 493 | **Comments:** 79 | **Date:** 2026-01-06

**Summary:** The post discusses the successful optimization and deployment of the Qwen3-30B-A3B-Instruct-2507 model on a Raspberry Pi 5, achieving 8.03 tokens per second while retaining 94.18% of BF16 quality. The optimization strategy focuses on fitting the model within memory constraints and then maximizing performance without sacrificing quality.

**Key Points:**
- The 30B model runs on a Raspberry Pi 5 with 8.03 TPS at 2.70 BPW, retaining 94.18% of BF16 quality.
- Optimization prioritizes memory fit first, then balances TPS and quality.
- GPU performance is quirky due to kernel choices, unlike CPU behavior.
- Community feedback highlights performance comparisons and user testing experiences.
- The post requests further testing on different setups and workloads.

**Discussion Highlights:** The community discussion includes feedback on performance, comparisons with other models like Mamba2, and user experiences with running the model on Raspberry Pi. There is also interest in exploring cluster-based solutions for running the model on multiple devices.

---

## 39. [Performance improvements in llama.cpp over time](https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/)

**Author:** u/jacek2023 | **Upvotes:** 686 | **Comments:** 85 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses performance improvements in llama.cpp over time, with a focus on NVIDIA GPUs and comparisons with other implementations like ik_llama.cpp. The discussion highlights significant progress in token generation speed and overall performance gains.

**Key Points:**
- Performance gains in llama.cpp are notable, especially for NVIDIA GPUs.
- Token generation speed has improved significantly, approaching the performance of ik_llama.cpp.
- Prompt processing remains slower compared to token generation but has seen progress.
- The community appreciates the ongoing improvements and contributions.

**Discussion Highlights:** The discussion consensus highlights the impressive progress in llama.cpp performance, particularly in token generation speed, with comparisons to other implementations like ik_llama.cpp. The focus on NVIDIA GPUs and the overall appreciation for the improvements are key themes.

---

## 40. [Liquid Ai released LFM2.5, family of tiny on-device foundation models.](https://reddit.com/r/LocalLLaMA/comments/1q5a0if/liquid_ai_released_lfm25_family_of_tiny_ondevice/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 312 | **Comments:** 56 | **Date:** 2026-01-05

**Summary:** Liquid AI released LFM2.5, a family of tiny on-device foundation models designed for reliable agentic applications. The models feature higher quality, lower latency, and broader modality support in the ~1B parameter class, with five open-weight model instances including general-purpose, Japanese-optimized, vision-language, audio-language, and base checkpoints.

**Key Points:**
- LFM2.5 builds on a device-optimized hybrid architecture with scaled pretraining from 10T to 28T tokens.
- The models include general-purpose, Japanese-optimized, vision-language, audio-language, and base checkpoints.
- Users noted the high data-to-parameter ratio and compared it to other models like Qwen3-0.6B.
- Feedback highlighted the model's speed but mentioned issues with instruction following for special formats.
- Discussion included suggestions for training in native FP8 or FP4 for better on-device performance.

**Discussion Highlights:** The discussion highlighted the impressive data-to-parameter ratio of LFM2.5 and compared it to other models. Users appreciated the speed of the model but noted challenges with instruction following. There were suggestions for optimizing the model for on-device performance and calls for larger model sizes.

---

## 41. [For the first time in 5 years, Nvidia will not announce any new GPUs at CES — company quashes RTX 50 Super rumors as AI expected to take center stage](https://reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/)

**Author:** u/FullstackSensei | **Upvotes:** 634 | **Comments:** 195 | **Date:** 2026-01-05

**Summary:** Nvidia will not announce new GPUs at CES, focusing instead on AI. There are concerns about limited supply of high-end GPUs, rising hardware prices, and the potential re-release of older models like the RTX 3060.

**Key Points:**
- Nvidia quashes RTX 50 Super rumors, no new GPU announcements at CES
- Limited supply of RTX 5070Ti, 5080, and 5090, with potential re-release of RTX 3060
- Rising prices of DDR5 RAM and storage
- Concerns about corporate greed and the future of local computing
- Suggestions for alternative solutions, such as China flooding the market with high-capacity GPUs

**Discussion Highlights:** The discussion highlights frustration with Nvidia's focus on AI over consumer GPUs, concerns about corporate greed, and the impact on local computing. There is a consensus that the current situation is unfavorable for consumers looking to upgrade their hardware.

---

## 42. [llama.cpp performance breakthrough for multi-GPU setups](https://reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/)

**Author:** u/Holiday-Injury-9397 | **Upvotes:** 578 | **Comments:** 204 | **Date:** 2026-01-05

**Summary:** The ik_llama.cpp project achieved a 3x to 4x performance improvement for multi-GPU setups, enabling cost-effective use of multiple low-cost GPUs for local LLM inference.

**Key Points:**
- 3x to 4x speed improvement in multi-GPU configurations
- New execution mode (split mode graph) for maximum GPU utilization
- Cost-effective alternative to high-end enterprise GPUs
- Performance gains also observed in single GPU and CPU-only setups
- Potential bottlenecks in hybrid inference setups noted

**Discussion Highlights:** The community highlights significant performance gains even on single GPUs and CPU-only setups, with some users noting bottlenecks in hybrid inference configurations. There is consensus on the project's impact on making local LLM inference more accessible and cost-effective.

---

## 43. [GLM-Image model from Z.ai is coming](https://reddit.com/r/LocalLLaMA/comments/1q41bw1/glmimage_model_from_zai_is_coming/)

**Author:** u/Ravencloud007 | **Upvotes:** 319 | **Comments:** 59 | **Date:** 2026-01-04

**Summary:** The Reddit post announces the upcoming GLM-Image model from Z.ai, which has generated significant interest in the community. The model is highly anticipated, with users expressing excitement about its potential capabilities and comparing it favorably to existing models.

**Key Points:**
- The GLM-Image model from Z.ai is being introduced and has gained attention.
- The community is enthusiastic, with comments suggesting high expectations for the model's performance.
- There is a comparison to existing models, with some users considering Z.ai's model as a current favorite.
- Concerns about computational requirements are raised, with users joking about the need for extensive resources.
- There is a desire for a model that balances size, ease of fine-tuning, and quality.

**Discussion Highlights:** The discussion highlights a strong community interest in the GLM-Image model, with users expressing excitement and high expectations. There is a consensus that Z.ai's model is currently favored, though some users are concerned about the computational resources required to use it effectively. The community also expresses a desire for models that are smaller, easier to fine-tune, and maintain high quality.

---

## 44. [Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched](https://reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/)

**Author:** u/ubrtnk | **Upvotes:** 379 | **Comments:** 194 | **Date:** 2026-01-03

**Summary:** The post discusses the challenges faced by local LLMs in processing extreme or unlikely breaking news events, such as the US attacking Venezuela. The author shares experiences with different LLMs, highlighting their struggles to accept such events as real despite credible sources.

**Key Points:**
- Local LLMs often classify extreme events as hoaxes or misinformation.
- Different LLMs (Qwen Research, Spark, GPT-OSS) showed varying degrees of skepticism.
- Providing credible sources helped some LLMs acknowledge the event's reality.
- Smaller models struggled more than larger ones in processing such events.
- The discussion highlights biases and limitations in LLMs' understanding of unfamiliar geopolitical events.

**Discussion Highlights:** The discussion consensus suggests that LLMs have inherent biases and struggle with processing extreme or unlikely events, often requiring explicit evidence to accept such events as real. Users shared similar experiences and expressed concerns about the limitations of LLMs in understanding complex geopolitical situations.

---

## 45. [LeCun Says Llama 4 results "were fudged a little bit"](https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/)

**Author:** u/MrPecunius | **Upvotes:** 362 | **Comments:** 89 | **Date:** 2026-01-02

**Summary:** Yann LeCun, departing Meta AI Chief, confirmed that Llama 4 benchmark results were manipulated. This follows speculation about suspicious benchmarks and coincides with Zuckerberg sidelining the GenAI organization, leading to significant departures.

**Key Points:**
- LeCun confirms Llama 4 benchmark manipulation
- Zuckerberg sidelined Meta's GenAI organization
- Significant departures from Meta's AI team
- Llama 4's promised large model was never released
- Community disappointment over Llama's perceived failure

**Discussion Highlights:** The discussion highlights disappointment in Meta's handling of Llama, with users expressing regret over its perceived failure and the shift of AI innovation to other regions. Some users shared additional resources, while others debated the organizational issues at Meta.

---

## 46. [Qwen-Image-2512](https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/)

**Author:** u/Nunki08 | **Upvotes:** 723 | **Comments:** 122 | **Date:** 2025-12-31

**Summary:** The Reddit post introduces Qwen-Image-2512, a new model with links to guides, GGUF files, and various platforms for access and demos. The community response includes positive feedback and notable experiences, such as running the model on low-end hardware.

**Key Points:**
- Introduction of Qwen-Image-2512 with multiple access points
- Community appreciation and positive feedback
- Notable experience of running the model on low-end hardware
- Links to guides, GGUF files, and various platforms
- Creative use cases and image generation examples

**Discussion Highlights:** The community shows enthusiasm for the new model, with notable experiences shared, such as running it on low-end hardware and creative use cases like generating unique images.

---

## 47. [[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It’s running a raw Llama-7B instance with a 2048 token window.](https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/)

**Author:** u/simar-dmg | **Upvotes:** 747 | **Comments:** 110 | **Date:** 2025-12-30

**Summary:** A user reverse-engineered a Snapchat sextortion bot and discovered it was running a raw Llama-7B instance with a 2048 token window. The bot was vulnerable to a persona-adoption jailbreak, revealing its configuration and malicious payload.

**Key Points:**
- The bot used a Llama-7B model with a 2048 token context window and high temperature setting.
- A 'Grandma Protocol' jailbreak forced the bot to reveal its environment variables and configuration.
- The bot was likely running on minimal hardware to reduce costs.
- The bot eventually revealed a malicious link it was programmed to hide.
- Scammers are increasingly using open-source models like Llama-7B to avoid API costs and censorship.

**Discussion Highlights:** The discussion included skepticism about the accuracy of the bot's revealed information, with some users suggesting it could be entirely hallucinated. Others questioned the commonality of system prompts including environment variables.

---

## 48. [Llama-3.3-8B-Instruct](https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/)

**Author:** u/jacek2023 | **Upvotes:** 472 | **Comments:** 77 | **Date:** 2025-12-29

**Summary:** The post discusses the discovery and release of the Llama-3.3-8B-Instruct model, which was previously only available via Facebook's API. The author managed to obtain and share the model in GGUF format.

**Key Points:**
- Llama-3.3-8B-Instruct model was previously only available via Facebook's API.
- The author obtained the model through finetuning and shared it in GGUF format.
- The model's authenticity and performance are being verified by the community.
- The discovery has generated excitement and interest in the community.

**Discussion Highlights:** The community is actively verifying the model's authenticity and performance, with some users running benchmarks and evaluations. There is general excitement and appreciation for the discovery and release of the model.

---

## 49. [Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.](https://reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 344 | **Comments:** 121 | **Date:** 2025-12-29

**Summary:** Z AI is set to go public with an IPO on January 8, aiming to raise $560 million, marking it as the first AI-native LLM company to list globally. The announcement has sparked discussions about the future of open-source models and the company's commitment to releasing open weight models.

**Key Points:**
- Z AI's IPO is scheduled for January 8, targeting $560 million in funding.
- The company is positioned as the first AI-native LLM firm to go public globally.
- Community concerns about the future of open-source models post-IPO.
- Mixed reactions on whether Z AI will continue releasing open weight models.
- Debate on the balance between commercial success and open-source contributions.

**Discussion Highlights:** The discussion highlights a divide in community sentiment, with some users expressing concerns about the potential shift away from open-source models, while others argue that commercial success is necessary for sustainability. There is also speculation about the company's future pricing and accessibility of its models post-IPO.

---

## 50. [Tencent just released WeDLM 8B Instruct on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 421 | **Comments:** 62 | **Date:** 2025-12-29

**Summary:** Tencent released WeDLM 8B Instruct on Hugging Face, a diffusion language model that runs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks. The release has garnered significant attention and positive feedback from the community.

**Key Points:**
- WeDLM 8B Instruct is a diffusion language model released by Tencent on Hugging Face.
- It outperforms vLLM-optimized Qwen3-8B by 3-6× on math reasoning tasks.
- The model is released under the Apache 2.0 license.
- Community reaction highlights the potential of 7-8B models and interest in diffusion models for LLMs.
- A 7B version of the model is also available.

**Discussion Highlights:** The community is excited about the performance and potential of diffusion models for LLMs, with many noting the impressive benchmark scores and the Apache 2.0 license. There is a consensus on the promising future of 7-8B models and a call for more such models.

---

