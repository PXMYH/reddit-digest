# r/LocalLLaMA Reading Digest

**Period:** 2026-01-23 to 2026-01-23
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp; 1.8B), Support for 10 languages](https://reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/)

**Author:** u/Nunki08 | **Upvotes:** 618 | **Comments:** 86 | **Date:** 2026-01-22

**Summary:** Qwen has open-sourced the Qwen3-TTS model family, including 5 models (0.6B & 1.8B) with support for 10 languages. The release includes resources like GitHub, Hugging Face, a blog post, a paper, and a demo.

**Key Points:**
- Qwen3-TTS includes 5 models (0.6B & 1.8B)
- Supports 10 languages
- Resources available on GitHub, Hugging Face, blog, paper, and demo
- Community feedback highlights concerns about English voice quality and requests for support in compiled languages like llama.cpp
- Positive reception for the model's performance and sample outputs

**Discussion Highlights:** The community discussion highlights concerns about the English voice quality sounding like anime dubs and requests for support in compiled languages. There is also positive feedback on the model's performance and sample outputs, with some humorous reactions to specific examples.

---

## 2. [Qwen dev on Twitter!!](https://reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 657 | **Comments:** 60 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses Qwen's TTS model announcement on Twitter, with the community clarifying its origin and sharing relevant links.

**Key Points:**
- Qwen's TTS model announcement on Twitter
- Clarification that it's the TTS model from the vLLM leak
- Link to the Hugging Face collection for Qwen3-TTS

**Discussion Highlights:** The community is engaged in discussing the new TTS model from Qwen, with some users providing clarifications and sharing relevant resources.

---

## 3. [You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?](https://reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/)

**Author:** u/Adventurous-Gold6413 | **Upvotes:** 531 | **Comments:** 297 | **Date:** 2026-01-20

**Summary:** The post discusses the selection of local models for use with 64GB RAM and 16GB VRAM when internet access is unavailable. The community shares their preferred models and experiences.

**Key Points:**
- The post asks for recommendations on local models to use with specific hardware constraints.
- Top comments highlight models like Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B.
- GPT-OSS-120B is praised for its performance and versatility on the given hardware.
- The community appreciates the contribution and engages in a lively discussion.
- Some users humorously suggest using books as an alternative.

**Discussion Highlights:** The discussion highlights a consensus around models like GPT-OSS-120B, which is noted for fitting well on the specified hardware and offering good performance across various domains. Other models like Gemma 3 27B and GLM 4.5 Air are also mentioned as strong contenders. The community shows appreciation for the post and engages in a mix of serious recommendations and light-hearted comments.

---

## 4. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 860 | **Comments:** 256 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, fully enclosed mobile AI system with 10 GPUs, designed for running large MoE models and supporting graphic design tasks. The build, costing around $17k, balances performance and budget constraints while addressing mobility and enclosure challenges.

**Key Points:**
- Custom build with Threadripper Pro 3995WX, 512GB DDR4, and 10 GPUs (8x 3090 + 2x 5090)
- Designed for large MoE models, video generation, and high-detail image generation
- Fully enclosed and mobile, addressing challenges like cat safety and aesthetics
- Budget-conscious approach, avoiding unnecessary expenses for diminishing returns
- Community reactions highlight the build's uniqueness and practicality

**Discussion Highlights:** The community appreciated the innovative approach to enclosure and mobility, with humorous comments about the build's size and power requirements. The post gained significant traction, including a special flair and feature on Discord.

---

## 5. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 363 | **Comments:** 60 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the official support for GLM 4.7 Flash in llama.cpp, highlighting its community-driven development and performance improvements.

**Key Points:**
- GLM 4.7 Flash now officially supported in llama.cpp
- Support is community-driven, not by Z.ai developers
- Performance improvements noted, with some users reporting faster execution without flash-attention
- Additional versions of the model are available on Hugging Face
- Post recognized and featured in the community Discord

**Discussion Highlights:** The discussion highlights the community effort behind the integration, performance comparisons, and additional resources shared by users. Some users noted that disabling flash-attention resulted in faster performance.

---

## 6. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 453 | **Comments:** 161 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights the effectiveness of GLM 4.7 Flash as a reliable local agent for various tasks, with users praising its performance and capabilities. The discussion includes comparisons with other models and notes on its efficiency and output quality.

**Key Points:**
- GLM 4.7 Flash is praised for its reliability and performance in agentic tasks.
- Users report successful execution of complex tasks like cloning repos and running commands.
- Comparisons with other models like Nemotron 30B and Qwen3 are discussed.
- GGUFs for local testing are anticipated.
- Performance benchmarks suggest it is competitive with larger models like SEED OSS 36B.

**Discussion Highlights:** The discussion highlights a positive consensus on GLM 4.7 Flash's capabilities, with users eagerly awaiting local testing options. Comparisons with other models and performance benchmarks are key topics, indicating strong interest and approval within the community.

---

## 7. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 736 | **Comments:** 230 | **Date:** 2026-01-19

**Summary:** The post announces the release of GLM-4.7-Flash model, generating significant interest and discussion in the r/LocalLLaMA community.

**Key Points:**
- The model uses MLA, reducing KV cache memory usage and enabling longer context lengths (up to 200k).
- Community excitement about the release after a long wait.
- Mixed reactions about model size (30B) with some users missing larger 70B models.
- Technical details mentioned include a 3B 'thinking model' component.

**Discussion Highlights:** The community shows strong enthusiasm for the new model's efficiency and capabilities, particularly its memory optimization and extended context length. Some users express nostalgia for larger models while acknowledging the technical advancements in this release.

---

## 8. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 344 | **Comments:** 93 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to stay within a ~10,000€ budget. The system is designed for running large language models (120B+ parameters) locally, with benchmark results showing strong performance across various models. Key points include the system's purpose for local large model inference, the budget details, strong benchmark performance, positive community reaction, and the existence of similar builds in the community. The discussion highlights the impressive hardware setup and curiosity about the author's job and procurement process, indicating a growing trend in high-VRAM local inference setups.

---

## 9. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 453 | **Comments:** 71 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses the potential delay in Qwen 4's release, as the lead developer mentions slowing down to focus on quality. The community generally appreciates this focus on quality over quantity.

**Key Points:**
- Qwen 4 development may be delayed to focus on quality
- Community appreciates the focus on quality over quantity
- Some users caution against jumping to conclusions based on limited information
- General consensus that meaningful improvements are more valuable than frequent incremental updates

**Discussion Highlights:** The discussion highlights a positive reception to the focus on quality, with many users expressing support for taking the necessary time to make meaningful improvements. Some users also advise caution against spreading rumors based on limited information.

---

## 10. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 534 | **Comments:** 117 | **Date:** 2026-01-17

**Summary:** The author upgraded their server from MI100s to four R9700 GPUs, achieving 128GB VRAM for a similar cost to the original plan. The new setup offers better prompt processing performance with a slight loss in token generation, and the total cost was under $7,035.

**Key Points:**
- Upgrade from MI100s to four R9700 GPUs for 128GB VRAM
- Cost-effective alternative to the original plan with similar performance
- Total build cost under $7,035 with detailed component breakdown
- Performance benchmarks show improved prompt processing
- Community appreciation for the build and its cost efficiency

**Discussion Highlights:** The community praised the build for its cost efficiency and performance, with some users joking about the financial irresponsibility of such upgrades. The post was also featured on the subreddit's Discord.

---

## 11. [Best "End of world" model that will run on 24gb VRAM](https://reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/)

**Author:** u/gggghhhhiiiijklmnop | **Upvotes:** 338 | **Comments:** 176 | **Date:** 2026-01-17

**Summary:** The user is seeking recommendations for the best AI model to download and store for an 'end of world' scenario, with a focus on models that fit within 24GB VRAM and 64GB RAM. The discussion includes suggestions for specific models and practical advice on data storage.

**Key Points:**
- User wants a model that fits within 24GB VRAM and 64GB RAM.
- Suggestions include saving the best LLM available and running it off SSD if necessary.
- Specific model recommendations: gemma3:27b and Midnight Miku.
- Advice to download actual Wikipedia backups for offline use.
- Discussion highlights the importance of practical data storage solutions.

**Discussion Highlights:** The discussion emphasizes practicality, with a consensus on saving the best possible LLM and considering offline storage solutions like SSDs. Specific model recommendations include gemma3:27b for its capabilities and Midnight Miku for entertainment purposes. There is also a strong suggestion to download comprehensive data backups like Wikipedia for long-term use.

---

## 12. [GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)](https://reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/)

**Author:** u/CuriousPlatypus1881 | **Upvotes:** 379 | **Comments:** 89 | **Date:** 2026-01-16

**Summary:** The post discusses the December 2025 SWE-bench leaderboard results, highlighting the performance of various AI models on GitHub PR tasks. Claude Opus 4.5 leads with a 63.3% resolved rate, followed closely by GPT-5.2 and Gemini 3 Flash Preview. The discussion emphasizes the strong showing of open-source models like GLM-4.7 and anticipation for future releases.

**Key Points:**
- Claude Opus 4.5 leads the leaderboard with a 63.3% resolved rate.
- GPT-5.2 and Gemini 3 Flash Preview follow closely behind.
- GLM-4.7 is noted as the strongest open-source model, performing alongside closed models.
- Gemini Flash's performance is highlighted as a surprise.
- Anticipation for future model releases like DeepSeek v4 is expressed.

**Discussion Highlights:** The discussion highlights the credibility of the benchmark and excitement around open-source models like GLM-4.7. Users express anticipation for future releases, particularly DeepSeek v4, and note the surprising performance of Gemini Flash.

---

## 13. [I fucking love this community](https://reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/)

**Author:** u/alhinai_03 | **Upvotes:** 518 | **Comments:** 54 | **Date:** 2026-01-16

**Summary:** The author expresses gratitude towards the open-source community for enabling them to run large language models on older hardware, achieving impressive performance metrics. They highlight the importance of system memory and MoE architecture for their setup.

**Key Points:**
- Gratitude towards the open-source community for their contributions
- Achieving 14-13.5 tokens per second on a 10-year-old PC with 4GB VRAM
- Importance of system memory and MoE architecture for running large models
- Community appreciation for optimization efforts
- Discussion on practicality of system RAM and MoE combo

**Discussion Highlights:** The community appreciates the author's achievement and discusses the practicality of using system RAM and MoE architecture for running large models on older hardware. There is consensus on the effectiveness of these optimizations.

---

## 14. [My story of underestimating /r/LocalLLaMA's thirst for VRAM](https://reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/)

**Author:** u/EmPips | **Upvotes:** 1339 | **Comments:** 91 | **Date:** 2026-01-15

**Summary:** The post discusses the author's experience with underestimating the demand for VRAM in the r/LocalLLaMA community. The discussion includes references to hardware recommendations and a humorous analogy to the California gold rush.

**Key Points:**
- Author's story about underestimating VRAM demand
- Reference to California gold rush analogy
- Discussion about specific hardware like 3090s and R9700
- Mention of potential resale of hardware

**Discussion Highlights:** The discussion highlights include a humorous analogy to the California gold rush, recommendations for specific hardware, and a mention of potential resale of hardware after use.

---

## 15. [Latest upgrade…A100 40 GB](https://reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/)

**Author:** u/inserterikhere | **Upvotes:** 408 | **Comments:** 54 | **Date:** 2026-01-15

**Summary:** The user upgraded their gaming rig to an AI rig by purchasing a faulty A100 GPU for $1000, which worked perfectly upon installation. They previously used a 3090 and 7950x, and the A100 allowed them to run and train larger models immediately.

**Key Points:**
- User transitioned from a gaming rig to an AI rig using existing parts.
- Purchased an A100 GPU listed as faulty for $1000, which worked upon installation.
- Previously used a 3090 and 7950x for AI tasks.
- Community expressed concerns about cooling the A100.
- Post received positive reception with 408 upvotes and 54 comments.

**Discussion Highlights:** The community reacted positively to the upgrade, with some users expressing concern about cooling the A100 GPU. One comment suggested using a blower fan or water cooling to prevent overheating.

---

## 16. [NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency](https://reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/)

**Author:** u/Fear_ltself | **Upvotes:** 717 | **Comments:** 130 | **Date:** 2026-01-14

**Summary:** NVIDIA's Orchestrator-8B is an 8-billion-parameter AI model designed to manage and route complex tasks to various tools for greater efficiency. The post discusses its potential as a step towards more functional AI systems, with comments highlighting its role as a 'middle manager' and its similarity to existing frameworks.

**Key Points:**
- Orchestrator-8B is an 8B parameter model specialized in task management and routing.
- It aims to connect with other tools and models for efficient task handling.
- The post suggests this approach could be a step towards AGI by integrating separate components.
- Comments compare it to a 'middle manager' and mention similar existing frameworks.
- Discussion highlights potential for agentic frameworks and hierarchical model management.

**Discussion Highlights:** The discussion generally agrees on the model's potential for efficient task management, with some humorously comparing it to a corporate middle manager. There's recognition of similar existing frameworks and enthusiasm for the future of agentic AI systems.

---

## 17. [GLM-Image is released!](https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/)

**Author:** u/foldl-li | **Upvotes:** 598 | **Comments:** 83 | **Date:** 2026-01-13

**Summary:** GLM-Image is a new image generation model with a hybrid autoregressive + diffusion decoder architecture. It excels in text-rendering and knowledge-intensive tasks while supporting various image-to-image tasks like editing and style transfer.

**Key Points:**
- Hybrid autoregressive + diffusion decoder architecture
- Strong performance in text-rendering and knowledge-intensive generation
- Supports image-to-image tasks like editing, style transfer, and identity-preserving generation
- MIT license with no restrictions
- Model size: 13GB diffusion model + 20GB text encoder

**Discussion Highlights:** The community appreciates the MIT license and the model's capabilities. There is excitement about its performance and potential for quantization. Some users are interested in its ability to handle complex tasks like porn generation.

---

## 18. [My wishes for 2026](https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/)

**Author:** u/jacek2023 | **Upvotes:** 654 | **Comments:** 179 | **Date:** 2026-01-13

**Summary:** The Reddit post discusses predictions for 2026, focusing on the possibility of affordable GPUs with more than 32GB memory. The community expresses skepticism and humor about the feasibility of such advancements.

**Key Points:**
- The post asks which predictions for 2026 are likely to happen first.
- A top comment highlights the desire for affordable GPUs with more than 32GB memory.
- The community reacts with humor and skepticism about the possibility of affordable GPUs.
- Other comments mention specific AI models like Qwen 4 and Mistral.

**Discussion Highlights:** The discussion is marked by a mix of humor and skepticism regarding the feasibility of affordable GPUs in 2026. The community engages in playful banter while expressing doubts about the likelihood of significant hardware advancements.

---

## 19. [kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop—no GPU required](https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/)

**Author:** u/Nunki08 | **Upvotes:** 398 | **Comments:** 93 | **Date:** 2026-01-13

**Summary:** Kyutai introduced Pocket TTS, a 100M-parameter text-to-speech model with high-quality voice cloning that runs on a laptop without requiring a GPU. The model is available on GitHub and Hugging Face, with a blog post and arXiv paper providing more details.

**Key Points:**
- Pocket TTS is a 100M-parameter model for high-quality voice cloning
- Runs on a laptop without needing a GPU
- Available on GitHub, Hugging Face, and detailed in a blog post and arXiv paper
- Memory usage can balloon during generation, reaching up to 32 GB
- Discussion includes inquiries about language support and comparisons with other small models

**Discussion Highlights:** The discussion highlights concerns about memory usage during generation, with one user reporting it reached 32 GB. There are also inquiries about language support and comparisons with other small models, suggesting a consensus that very small models may not be worth the trouble for certain use cases.

---

## 20. [GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/)

**Author:** u/TKGaming_11 | **Upvotes:** 365 | **Comments:** 93 | **Date:** 2026-01-12

**Summary:** The Reddit post highlights a GitHub repository by DeepSeek-AI introducing 'Engram,' a method for conditional memory via scalable lookup in large language models. The discussion praises the innovation and technical depth of the project.

**Key Points:**
- DeepSeek team praised for original ideas
- Introduces new sparsity axis using static memory with O(1) lookup
- Uses n-gram embedding as complementary to neural computation
- Potential significant impact on large language models
- Community appreciates the novelty and technical approach

**Discussion Highlights:** The community discussion highlights the novelty and potential impact of the 'Engram' method, with users appreciating the technical depth and innovation brought by the DeepSeek team.

---

## 21. [LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)](https://reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 1053 | **Comments:** 114 | **Date:** 2026-01-11

**Summary:** The post introduces TimeCapsuleLLM, an open-source project training language models from scratch using 1800s London texts to reduce modern bias. The model, with 1.2B parameters and a 90GB dataset, generates contextually relevant outputs based on historical data.

**Key Points:**
- TimeCapsuleLLM is trained on texts from London between 1800-1875 to minimize modern bias.
- The model has 1.2B parameters and uses a 90GB dataset of books, journals, legal docs, etc.
- Example outputs show the model's ability to generate historically relevant arguments and its lack of knowledge about post-1875 inventions like the telephone.
- The project aims to create synthetic Q&A pairs using the dataset for future improvements.
- The community appreciates the project, with positive feedback and engagement in the comments.

**Discussion Highlights:** The community shows strong support for the project, with users expressing interest in similar historical datasets and appreciating the unique approach to reducing modern bias in language models.

---

## 22. [I bought a €9k GH200 “desktop” to save $1.27 on Claude Code (vLLM tuning notes)](https://reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/)

**Author:** u/Reddactor | **Upvotes:** 696 | **Comments:** 177 | **Date:** 2026-01-11

**Summary:** The author built a high-end 'desktop' with dual GH200 GPUs to run Claude Code locally, achieving better performance than cloud-based alternatives. Despite the high cost, the setup allows for offline coding with optimized vLLM settings and has sparked community interest and humor.

**Key Points:**
- Author spent €9k on a dual GH200 96GB setup to run Claude Code locally.
- Achieved better speeds than cloud-based Claude Code with Sonnet.
- Shared optimized vLLM settings for dual 96GB systems in a blog post.
- Community reactions include humor about cost vs. savings and admiration for the setup.
- Discussion highlights include jokes about break-even points and appreciation for the fun of the project.

**Discussion Highlights:** The community responded with humor and admiration, joking about the cost vs. savings and appreciating the fun and novelty of the project. Some users expressed envy over missing out on similar deals.

---

## 23. [It works! Abliteration can reduce slop without training](https://reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/)

**Author:** u/-p-e-w- | **Upvotes:** 402 | **Comments:** 129 | **Date:** 2026-01-11

**Summary:** The post discusses the use of abliteration to reduce 'slop' (flowery, cliched language) in LLM outputs, specifically applied to the Mistral Nemo model. The author successfully created a slop-reduced model using Heretic, a tool originally designed for censorship removal, without any fine-tuning. The process took 2.5 hours on an A6000 GPU.

**Key Points:**
- Abliteration can reduce slop in LLM outputs without training.
- Heretic tool was repurposed for slop reduction using a configuration file.
- The technique was applied to Mistral Nemo, a model known for producing slop.
- The process took 2.5 hours on an A6000 GPU at full precision.
- Community feedback includes both positive and critical views on the effectiveness and impact on output quality.

**Discussion Highlights:** The community discussion highlights mixed opinions on the effectiveness of the slop reduction technique. Some users appreciate the reduction in flowery language, while others feel it makes the prose too dry or lacks imagination. There is also interest in whether the technique can be applied to other patterns beyond slop.

---

## 24. [I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work](https://reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/)

**Author:** u/Ok-Pomegranate1314 | **Upvotes:** 887 | **Comments:** 147 | **Date:** 2026-01-09

**Summary:** The author successfully clustered three NVIDIA DGX Sparks, overcoming networking limitations by writing a custom NCCL plugin. This achievement allows distributed inference across all three nodes at high speeds, despite NVIDIA's official support for only two-node clustering.

**Key Points:**
- Author clustered three DGX Sparks, exceeding NVIDIA's official support for two-node clustering.
- Custom NCCL plugin written in ~1500 lines of C to handle subnet-aware NIC selection and RDMA implementation.
- Achieved distributed inference at 8+ GB/s over RDMA, solving complex networking challenges.
- GitHub repository provided for the custom plugin implementation.
- Community praised the technical achievement and its potential impact.

**Discussion Highlights:** The community highlighted the technical difficulty of working with NCCL and praised the achievement as significant. Questions were raised about scalability and performance gains, indicating strong interest in the solution.

---

## 25. [The reason why RAM has become so expensive](https://reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/)

**Author:** u/InvadersMustLive | **Upvotes:** 4527 | **Comments:** 381 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses the significant increase in RAM prices, with comments highlighting concerns about monopolization of RAM resources by certain entities, making AI data centers economically unviable.

**Key Points:**
- RAM prices have increased dramatically, with some users reporting a 10x increase.
- There are concerns about monopolization of RAM resources by specific companies, potentially creating future demand.
- The high cost of RAM is making AI data centers, particularly in China, economically unviable.
- Some users view the situation as a potential economic bubble.

**Discussion Highlights:** The discussion highlights a consensus on the monopolization of RAM resources and its economic impact on AI data centers. Users express concerns about the sustainability of current RAM prices and the potential for market manipulation.

---

## 26. [DeepSeek V4 Coming](https://reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/)

**Author:** u/External_Mood4719 | **Upvotes:** 499 | **Comments:** 109 | **Date:** 2026-01-09

**Summary:** DeepSeek is expected to release V4, a next-generation AI model with strong code-generation capabilities, outperforming mainstream models like Claude and GPT. The model shows improvements in handling long code prompts and overall reasoning ability.

**Key Points:**
- DeepSeek V4 focuses on strong code-generation capabilities
- Outperforms existing models like Claude and GPT in code generation
- Improved handling of long code prompts and data patterns
- Enhanced reasoning ability and reliability for complex tasks
- Users appreciate DeepSeek's cost-effectiveness and local API options

**Discussion Highlights:** Users express excitement and anticipation for V4, with positive feedback on DeepSeek's cost-effectiveness and local API options. Some speculate on potential technical advancements like mHC integration and deepseek-ocr for long prompts.

---

## 27. [(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability](https://reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/)

**Author:** u/Nunki08 | **Upvotes:** 489 | **Comments:** 102 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release a new flagship AI model with strong coding capabilities, generating significant interest and discussion in the community.

**Key Points:**
- DeepSeek's upcoming model focuses on strong coding ability
- The announcement has sparked excitement and anticipation
- Community members are looking forward to more models and competition
- Some users express skepticism about performance claims
- There is a desire for the model to maintain role-playing capabilities

**Discussion Highlights:** The community shows a mix of excitement and skepticism, with many welcoming the competition and innovation in AI models. Some users are hopeful for improved coding capabilities, while others caution against overhyped claims.

---

## 28. [The NO FAKES Act has a "Fingerprinting" Trap that kills Open Source. We need to lobby for a Safe Harbor.](https://reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/)

**Author:** u/PostEasy7183 | **Upvotes:** 613 | **Comments:** 88 | **Date:** 2026-01-08

**Summary:** The NO FAKES Act proposes a 'digital replica right' that could hold developers liable for hosting open-source AI models used to create deepfakes, potentially stifling innovation. The post urges lobbying for a 'Safe Harbor' provision to protect open-source developers.

**Key Points:**
- The NO FAKES Act targets developers who 'make available' tools primarily used for creating digital replicas, imposing statutory damages.
- Open-source developers could face legal risks for hosting models like TTS or voice-conversion tools on platforms like HuggingFace.
- The post calls for a 'Safe Harbor' provision to protect open-source developers and prevent a monopoly by Big Tech.
- Action items include emailing or calling representatives to oppose the bill unless it includes protections for open-source repositories.
- Comments highlight concerns about the bill's impact on innovation and the influence of Big Tech in shaping AI regulations.

**Discussion Highlights:** The discussion emphasizes the potential negative impact of the NO FAKES Act on open-source development and innovation. Many commenters express skepticism about politicians' understanding of technology and suggest that Big Tech may be influencing the legislation to stifle competition.

---

## 29. [Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt](https://reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/)

**Author:** u/Prior-Arm-6705 | **Upvotes:** 941 | **Comments:** 147 | **Date:** 2026-01-08

**Summary:** The post describes a project where someone counted and compiled every instance of Jensen Huang saying 'AI' (121 times) during the NVIDIA CES keynote using open-source tools. The process involved downloading the video, parsing subtitles, and editing clips to create a compilation video. Key points include the use of tools like Dive, yt-dlp-mcp, and ffmpeg-mcp-lite, the project's popularity, and humorous reactions to Jensen Huang's attire and influence on tech pricing.

---

## 30. [16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/)

**Author:** u/ai-infos | **Upvotes:** 459 | **Comments:** 238 | **Date:** 2026-01-07

**Summary:** The post discusses running Deepseek V3.2 AWQ 4-bit on 16x AMD MI50 32GB GPUs, achieving 10 tokens/sec output and 2000 tokens/sec input with a 69000 context length. The setup aims for cost-effective local AGI and highlights power efficiency (550W idle, 2400W peak).

**Key Points:**
- Performance: 10 tok/s output, 2000 tok/s input, 69000 context length
- Power draw: 550W idle, 2400W peak inference
- Goal: Cost-effective local AGI setup using 16x AMD MI50 GPUs
- Future plans: 32 AMD MI50 setup for Kimi K2 Thinking
- Community appreciation and open-source setup details provided

**Discussion Highlights:** The discussion highlights the power efficiency of the setup, with comments noting its potential as a heater alternative in winter. Concerns about noise and power requirements at home were raised, while others praised the cost-effectiveness for professional use, comparing it favorably to CPU hardware as RAM prices increase.

---

## 31. [DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.](https://reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/)

**Author:** u/Nunki08 | **Upvotes:** 660 | **Comments:** 54 | **Date:** 2026-01-07

**Summary:** The Reddit post discusses the recent update to DeepSeek-R1's paper, which expanded from 22 pages to 86 pages, adding significant detail. The discussion includes speculation about new architectures and the potential impact of linear attention research.

**Key Points:**
- DeepSeek-R1's paper was updated from 22 pages to 86 pages.
- The update includes substantial additional detail.
- Discussion highlights potential new architectures and linear attention research.
- Community interest in how architectural improvements perform at different model sizes.
- Original paper lacked implementation specifics, which the update may address.

**Discussion Highlights:** The community is excited about the expanded paper, with speculation about new architectures (e.g., dsv4 + r2) and the impact of linear attention research. There is interest in seeing how architectural improvements scale across different model sizes.

---

## 32. [A 30B Qwen Model Walks Into a Raspberry Pi… and Runs in Real Time](https://reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/)

**Author:** u/ali_byteshape | **Upvotes:** 492 | **Comments:** 79 | **Date:** 2026-01-06

**Summary:** The post discusses running a 30B Qwen model on a Raspberry Pi 5, achieving 8.03 TPS at 2.70 BPW while retaining 94.18% of BF16 quality. The discussion highlights the quirks of GPU behavior and the importance of kernel choice for performance. Key points include the model's performance on Raspberry Pi 5, the dependence of GPU performance on kernel choice, the request for community feedback, the predictability of CPU behavior, and the retention of BF16 quality. The community is engaged in testing and providing feedback on different configurations, with interest in combining the model with other solutions like exo for cluster computing. Some users reported needing to adjust context settings to avoid segfaults.

---

## 33. [Performance improvements in llama.cpp over time](https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/)

**Author:** u/jacek2023 | **Upvotes:** 680 | **Comments:** 85 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses performance improvements in llama.cpp over time, with a focus on recent upgrades and their impact on speed and efficiency. The discussion includes insights from users about the performance gains, particularly in relation to NVIDIA GPUs, and comparisons with other tools like ik_llama.cpp.

**Key Points:**
- Performance improvements in llama.cpp have been significant over time.
- The upgrades are particularly relevant for NVIDIA GPUs.
- Comparisons with other tools like ik_llama.cpp show competitive performance.
- Prompt processing is noted to be slower than token generation.
- The community appreciates the progress and contributions.

**Discussion Highlights:** The discussion highlights the impressive progress in llama.cpp performance, with users noting its proximity to ik_llama.cpp in token generation speed. There is a consensus on the significant improvements, although prompt processing remains slower. The community is engaged and appreciative of the updates and contributions.

---

## 34. [For the first time in 5 years, Nvidia will not announce any new GPUs at CES — company quashes RTX 50 Super rumors as AI expected to take center stage](https://reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/)

**Author:** u/FullstackSensei | **Upvotes:** 621 | **Comments:** 196 | **Date:** 2026-01-05

**Summary:** Nvidia will not announce new GPUs at CES, with limited supply of high-end models and potential reintroduction of older GPUs like the RTX 3060. The post highlights concerns about rising hardware costs and the impact on future upgrades.

**Key Points:**
- No new GPU announcements from Nvidia at CES
- Limited supply of RTX 5070Ti, 5080, and 5090
- Rumors of RTX 3060 reintroduction to prop demand
- Rising DDR5 and storage prices
- Concerns about corporate greed and future hardware upgrades

**Discussion Highlights:** The discussion reflects frustration over corporate greed, the potential decline of local computing, and calls for alternative solutions like increased competition from China.

---

## 35. [llama.cpp performance breakthrough for multi-GPU setups](https://reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/)

**Author:** u/Holiday-Injury-9397 | **Upvotes:** 567 | **Comments:** 201 | **Date:** 2026-01-05

**Summary:** The ik_llama.cpp project achieved a significant performance breakthrough for multi-GPU setups, delivering a 3x to 4x speed improvement in local LLM inference. This advancement allows for the utilization of multiple low-cost GPUs instead of expensive high-end enterprise cards.

**Key Points:**
- ik_llama.cpp introduced a new execution mode (split mode graph) for multi-GPU configurations.
- The breakthrough enables simultaneous and maximum utilization of multiple GPUs.
- This development is cost-effective, allowing the use of low-cost GPUs instead of expensive enterprise cards.
- Performance improvements are also noted on single GPU and CPU-only setups.
- The project is seen as a game-changer in the context of high GPU and memory prices.

**Discussion Highlights:** The community is highly positive about the breakthrough, with many users confirming performance improvements on various setups. There is a consensus that this development is significant for both multi-GPU and single-GPU/CPU-only configurations. Some users also noted that ik_llama.cpp is now faster than other alternatives like exllama and comparable to vllm for single batch processing.

---

## 36. [Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched](https://reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/)

**Author:** u/ubrtnk | **Upvotes:** 378 | **Comments:** 195 | **Date:** 2026-01-03

**Summary:** The Reddit post discusses the challenges local LLMs face when processing extreme or unlikely breaking news events, such as the US attacking Venezuela. The author shares their experience with different LLMs, highlighting how these models initially dismissed the event as a hoax despite credible sources.

**Key Points:**
- Local LLMs struggled to accept extreme breaking news as real, even with credible sources.
- Different LLMs (Qwen Research, Spark, GPT-OSS) had varying responses to the same event.
- Models initially classified the event as misinformation or a hoax.
- Larger models like GPT-OSS:120B performed better but still showed skepticism.
- The discussion highlights biases and limitations in LLMs' understanding of unfamiliar geopolitical events.

**Discussion Highlights:** The comments reflect a consensus that LLMs often struggle with accepting extreme or unlikely events, with some users sharing similar experiences. There is a recognition of inherent biases in LLMs and a curiosity about how these models will handle such events in the future.

---

## 37. [LeCun Says Llama 4 results "were fudged a little bit"](https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/)

**Author:** u/MrPecunius | **Upvotes:** 363 | **Comments:** 88 | **Date:** 2026-01-02

**Summary:** Yann LeCun confirmed that Llama 4 benchmarks were manipulated, and Meta's AI division faced significant organizational changes, leading to a lack of follow-up on the promised model. The discussion highlights disappointment in Meta's handling of the project and its impact on the AI community.

**Key Points:**
- LeCun confirmed Llama 4 benchmark manipulation
- Meta's AI division was sidelined, leading to departures
- No follow-up on the promised large Llama 4 model
- Community disappointment in Meta's mismanagement
- Shared resources for further reading

**Discussion Highlights:** The discussion reflects a consensus on Meta's mismanagement of the Llama project, with users expressing disappointment and sharing additional resources. There is speculation on how a well-positioned organization like Meta could fail while smaller labs thrive.

---

## 38. [Qwen-Image-2512](https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/)

**Author:** u/Nunki08 | **Upvotes:** 715 | **Comments:** 122 | **Date:** 2025-12-31

**Summary:** The Reddit post introduces Qwen-Image-2512, a new image generation model, with links to guides, demos, and resources. Users share experiences running the model on various hardware and creative applications.

**Key Points:**
- Qwen-Image-2512 is a new image generation model
- Multiple resources and demos are provided
- Users report successful runs on low-end hardware
- Creative use cases are discussed

**Discussion Highlights:** Users appreciate the model's accessibility and share creative outputs, such as generating unique images with specific themes.

---

## 39. [[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It’s running a raw Llama-7B instance with a 2048 token window.](https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/)

**Author:** u/simar-dmg | **Upvotes:** 747 | **Comments:** 110 | **Date:** 2025-12-30

**Summary:** A Reddit user reverse-engineered a Snapchat sextortion bot, revealing it runs a Llama-7B model with a 2048 token window and high temperature setting, making it susceptible to jailbreaks. The bot's erratic behavior and hallucinations were exposed using a persona-adoption attack, showing scammers are using open-source models to cut costs.

**Key Points:**
- Bot uses Llama-7B model with 2048 token context window and high temperature (1.0).
- Vulnerable to persona-adoption jailbreaks (e.g., Grandma Protocol).
- Scammers deploy open-source models to avoid API costs and censorship filters.
- Bot hallucinated and revealed malicious payload (OnlyFans link).
- Discussion highlights skepticism about the bot's claims and the reliability of its responses.

**Discussion Highlights:** Top comments question the credibility of the bot's revealed information, with some suggesting it may be entirely hallucinated. The consensus leans toward skepticism about the bot's claims, though the use of an LLM-powered bot is acknowledged.

---

## 40. [Llama-3.3-8B-Instruct](https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/)

**Author:** u/jacek2023 | **Upvotes:** 467 | **Comments:** 77 | **Date:** 2025-12-29

**Summary:** The post discusses the discovery and extraction of the Llama-3.3-8B-Instruct model, which was previously only accessible via Meta's API. The author details their process of obtaining the model through finetuning and extracting the original weights.

**Key Points:**
- Llama-3.3-8B-Instruct is now available outside Meta's API.
- The model was obtained via finetuning and extracting the original weights.
- Community is conducting benchmarks and evaluations.
- Questions about model specifications like position embeddings.
- Positive reception and excitement from the community.

**Discussion Highlights:** The community is actively evaluating the model, with some conducting benchmarks and others questioning specific technical details. Overall, the discovery is met with enthusiasm and appreciation.

---

## 41. [Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.](https://reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 342 | **Comments:** 120 | **Date:** 2025-12-29

**Summary:** Z AI is set to go public with an IPO on January 8, aiming to raise $560 million, marking it as the first AI-native LLM company to list globally. The announcement has sparked discussions about the future of open-source AI models.

**Key Points:**
- Z AI's IPO is scheduled for January 8 with a target of raising $560 million.
- Concerns about the future of open-source AI models post-IPO.
- Debate on whether Z AI will continue releasing open weight models.
- Mixed reactions from the community, with some expressing concerns about commercialization.

**Discussion Highlights:** The discussion highlights a divide in the community, with some users expressing concerns about the potential shift away from open-source models, while others argue that companies need to monetize eventually. There is no clear consensus, but the sentiment leans towards cautious optimism mixed with skepticism.

---

## 42. [Tencent just released WeDLM 8B Instruct on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 422 | **Comments:** 62 | **Date:** 2025-12-29

**Summary:** Tencent has released WeDLM 8B Instruct, a diffusion language model that outperforms vLLM-optimized Qwen3-8B in math reasoning tasks by running 3-6× faster. The model is available on Hugging Face under an Apache 2.0 license.

**Key Points:**
- WeDLM 8B Instruct is a diffusion language model released by Tencent.
- It runs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks.
- The model is available on Hugging Face with an Apache 2.0 license.
- There is also a 7B version of the model available.
- The community finds the model promising and appreciates its performance and licensing.

**Discussion Highlights:** The community is excited about the release, highlighting the model's impressive benchmark scores and open-source licensing. There is consensus on the potential of 7-8B models and enthusiasm for more models in this space.

---

## 43. [NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux](https://reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/)

**Author:** u/HumanDrone8721 | **Upvotes:** 444 | **Comments:** 185 | **Date:** 2025-12-27

**Summary:** NVIDIA has dropped Pascal support on Linux, causing issues for Arch Linux users. The post highlights concerns and discussions around this change, with users expressing worry and sharing experiences with Pascal cards like the 24GB P40.

**Key Points:**
- NVIDIA's decision to drop Pascal support on Linux
- Impact on Arch Linux users and legacy drivers
- User concerns and experiences with Pascal cards
- Mention of specific cards like the 24GB P40
- Discussion about Arch Linux's handling of legacy drivers

**Discussion Highlights:** Users expressed concern and shared experiences with Pascal cards. There was a consensus that Arch Linux's move to drop legacy drivers to AUR is not surprising and has been a long-standing practice. Some users highlighted the popularity of certain Pascal cards and their increased cost.

---

## 44. [Best Local LLMs - 2025](https://reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/)

**Author:** u/rm-rf-rm | **Upvotes:** 363 | **Comments:** 197 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses the best local LLMs of 2025, highlighting models like Minimax M2.1 and GLM4.7 as frontier performers. It categorizes LLMs by application and memory footprint, emphasizing detailed user experiences and setup descriptions. Key points include the categorization of LLMs by applications such as General, Agentic, Creative Writing, and Speciality, and memory footprint categories including Unlimited (>128GB VRAM), Medium (8-128GB VRAM), and Small (<8GB VRAM). The discussion highlights debates on memory footprint categorization and specific model recommendations like Qwen3-4B-instruct and LFM2-8B-A1B.

---

## 45. [NVIDIA has 72GB VRAM version now](https://reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/)

**Author:** u/decentralize999 | **Upvotes:** 456 | **Comments:** 148 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses NVIDIA's new 72GB VRAM version, questioning if 96GB is too expensive and noting the AI community's lack of interest in 48GB. The discussion includes price comparisons and community opinions on VRAM sizes.

**Key Points:**
- NVIDIA has released a 72GB VRAM version
- Community questions the cost of 96GB and interest in 48GB
- Price comparisons show similar cost per gig across different VRAM sizes
- Some users suggest larger VRAM sizes like 128GB or 5090 with 48GB
- Consensus leans towards buying the most VRAM one can afford

**Discussion Highlights:** The discussion highlights a preference for larger VRAM sizes, with some users advocating for even bigger options like 128GB. Price per gig is consistent across models, making the choice straightforward for those who can afford higher capacities.

---

## 46. [Hard lesson learned after a year of running large models locally](https://reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/)

**Author:** u/inboundmage | **Upvotes:** 348 | **Comments:** 146 | **Date:** 2025-12-26

**Summary:** The author shares their experience running large language models locally, highlighting challenges with VRAM limitations, model scaling, and performance trade-offs. They conclude that local inference is viable for smaller models but requires significant hardware investment for larger ones.

**Key Points:**
- Running large models locally is feasible but has hard limits with consumer-grade hardware.
- VRAM fragmentation and offloading to system RAM introduce performance issues.
- Quantization helps but comes with quality trade-offs and potential bugs.
- Cloud-based solutions offer better performance for fast iteration but compromise privacy.
- Community suggestions include using llama.cpp for RAM offloading and considering multi-GPU setups.

**Discussion Highlights:** The discussion emphasizes the limitations of consumer-grade hardware for large models and suggests practical solutions like using llama.cpp for RAM offloading and investing in additional GPUs. There is a consensus that while local inference is possible, it requires careful management of resources and may not match cloud-based performance.

---

## 47. [I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA](https://reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/)

**Author:** u/CeFurkan | **Upvotes:** 1031 | **Comments:** 178 | **Date:** 2025-12-25

**Summary:** The post discusses the potential of GPU VRAM upgrade modifications to challenge NVIDIA's monopoly, highlighting their availability and popularity in China. Users share experiences and pricing details of these modified GPUs.

**Key Points:**
- GPU VRAM upgrade modifications are seen as a way to counter NVIDIA's monopoly.
- These modifications are already mainstream in China, with various models available at different price points.
- Users report successful usage of modified GPUs, such as a 4090 with 48GB of memory.
- Pricing for these modifications ranges from $300 for a 2080Ti 22GB to $4000 for a 5090 96GB.
- There is interest and demand for these modifications, as indicated by user comments.

**Discussion Highlights:** The discussion highlights the availability and success of GPU VRAM modifications in China, with users sharing positive experiences and expressing interest in these upgrades. There is a consensus on the potential of these modifications to provide more affordable and powerful alternatives to NVIDIA's offerings.

---

## 48. [Why I quit using Ollama](https://reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/)

**Author:** u/SoLoFaRaDi | **Upvotes:** 486 | **Comments:** 202 | **Date:** 2025-12-25

**Summary:** The Reddit post discusses the author's dissatisfaction with Ollama due to recent updates that introduced cloud-based models, which they feel stray from the original purpose of providing a secure platform for local AI models. The discussion highlights a shift in user preference towards alternatives like llama.cpp and LM Studio.

**Key Points:**
- Author's dissatisfaction with Ollama's recent updates and shift towards cloud-based models
- Concerns about privacy implications and bloatware in Ollama
- User preference for alternatives like llama.cpp and LM Studio
- Discussion highlights a consensus towards moving away from Ollama

**Discussion Highlights:** The discussion reflects a general consensus among users to switch from Ollama to alternatives like llama.cpp and LM Studio, citing better performance and alignment with their needs for local AI model inference.

---

## 49. [Exclusive: Nvidia buying AI chip startup Groq's assets for about $20 billion in largest deal on record](https://reddit.com/r/LocalLLaMA/comments/1puyq9r/exclusive_nvidia_buying_ai_chip_startup_groqs/)

**Author:** u/fallingdowndizzyvr | **Upvotes:** 671 | **Comments:** 157 | **Date:** 2025-12-24

**Summary:** Nvidia is acquiring AI chip startup Groq's assets for approximately $20 billion, marking the largest deal on record. The acquisition has sparked discussions about market competition and consolidation.

**Key Points:**
- Nvidia is buying Groq's assets for about $20 billion
- The deal is the largest on record
- Discussions highlight concerns about market consolidation
- Some commenters question Groq's valuation
- The acquisition is seen as a strategic move by Nvidia

**Discussion Highlights:** The discussion highlights a mix of opinions, with some seeing the acquisition as beneficial for market competition, while others express concerns about further consolidation in the AI chip industry. There is also skepticism about Groq's valuation and the nature of the deal being an 'acquihire'.

---

## 50. [We asked OSS-120B and GLM 4.6 to play 1,408 Civilization V games from the Stone Age into the future. Here's what we found.](https://reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/)

**Author:** u/vox-deorum | **Upvotes:** 659 | **Comments:** 174 | **Date:** 2025-12-24

**Summary:** Researchers used open-source LLMs (GPT-OSS-120B and GLM-4.6) to play 1,408 full games of Civilization V, finding that LLMs can survive full games with a hybrid approach and develop distinct playstyles. The models showed slight improvements in best scores but minor decreases in win rates compared to baseline AI. Key points include: LLMs can now play end-to-end Civilization V games using a hybrid approach; OSS-120B favored a warmonger playstyle, while GLM-4.6 was more balanced; Both models preferred the Order ideology over Freedom; Cost per game was approximately $0.86 for OSS-120B; Models as small as OSS-20B can achieve similar results. The community expressed excitement about the potential for LLMs to enhance gameplay, with interest in integrating them into multiplayer games and exploring smaller models. Some users humorously referenced broader implications, like the '3 Body Problem.'

---

