# r/LocalLLaMA Reading Digest

**Period:** 2026-01-22 to 2026-01-22
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp; 1.8B), Support for 10 languages](https://reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/)

**Author:** u/Nunki08 | **Upvotes:** 566 | **Comments:** 82 | **Date:** 2026-01-22

**Summary:** Qwen has open-sourced the Qwen3-TTS model family, including VoiceDesign, CustomVoice, and Base models in 0.6B and 1.8B sizes, supporting 10 languages. The release includes links to GitHub, Hugging Face, a blog post, a research paper, and a demo.

**Key Points:**
- Qwen3-TTS models include VoiceDesign, CustomVoice, and Base in 0.6B and 1.8B sizes
- Supports 10 languages
- Links provided to GitHub, Hugging Face, blog, paper, and demo
- Community discussion highlights include requests for llama.cpp support and observations about voice quality
- Positive feedback on sample quality with some humorous reactions

**Discussion Highlights:** The community discussion includes requests for support in compiled languages like llama.cpp, observations about the voice quality resembling anime dubs, and positive feedback on sample quality with humorous reactions to specific examples.

---

## 2. [Qwen dev on Twitter!!](https://reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 624 | **Comments:** 60 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses Qwen's TTS model announcement, which is reportedly from a vLLM leak. The community is engaged in verifying the source and legitimacy of the model.

**Key Points:**
- Qwen's TTS model announcement
- Model is from a vLLM leak
- Community discussion on model legitimacy
- Link to Hugging Face collection provided

**Discussion Highlights:** The discussion highlights include verification of the model's source, with some users pointing to a Hugging Face collection link for further details.

---

## 3. [You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?](https://reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/)

**Author:** u/Adventurous-Gold6413 | **Upvotes:** 535 | **Comments:** 292 | **Date:** 2026-01-20

**Summary:** The post discusses selecting local models for use with 64GB RAM and 16GB VRAM without internet access. Users recommend models like Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B.

**Key Points:**
- Focus on models that fit 64GB RAM and 16GB VRAM
- Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B are popular choices
- GPT-OSS 120B is praised for its versatility and performance
- Community appreciates the contribution of models like GPT-OSS 120B

**Discussion Highlights:** The discussion highlights a consensus around models like GPT-OSS 120B, Gemma 3 27B, and GLM 4.5 Air, with users praising their performance and suitability for the given hardware constraints.

---

## 4. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 857 | **Comments:** 256 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, high-performance AI system designed for running large MoE models and supporting graphic design tasks. The system features a Threadripper Pro 3995WX, 512GB DDR4, 10 GPUs (8x 3090 + 2x 5090), and is enclosed in a Thermaltake Core W200 case for mobility and protection. The total cost was approximately $17k, balancing performance and budget constraints.

**Key Points:**
- The system is designed for large MoE models and graphic design tasks.
- It features a Threadripper Pro 3995WX, 512GB DDR4, and 10 GPUs (8x 3090 + 2x 5090).
- The enclosure (Thermaltake Core W200) ensures mobility and protection from pets.
- The total cost was around $17k, balancing performance and budget.
- The post highlights the challenges of enclosure and mobility, with a focus on practicality.

**Discussion Highlights:** The discussion includes humorous comments about the system's portability and power, with one user joking about plugging it into a McDonald's socket. Other comments praise the build and note the creative solutions for fitting 10 GPUs into the case.

---

## 5. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 363 | **Comments:** 60 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the official support for GLM 4.7 Flash in llama.cpp, highlighting its community-driven development and performance improvements. Users discuss its efficiency and share additional resources. Key points include: GLM 4.7 Flash now officially supported in llama.cpp, support is community-driven, performance improvements noted, additional resources shared, and post recognized with special flair. The community appreciates the quick integration and performance gains, with some noting better performance without flash-attention.

---

## 6. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 459 | **Comments:** 160 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights the author's positive experience with GLM 4.7 Flash, an MoE model that performed reliably in an agentic framework, handling tasks like cloning repos and running commands without errors. The author is eager to try it locally once GGUFs are available.

**Key Points:**
- GLM 4.7 Flash performed reliably in an agentic framework, handling tasks like cloning repos and running commands without errors.
- The model produced hundreds of thousands of tokens in one session with context compacting.
- Users are anticipating the release of GGUFs for local testing.
- Comparisons with other models like Nemotron 30B and Qwen3 are discussed.
- Performance benchmarks suggest it may be as smart as SEED OSS 36B but with better performance due to MoE.

**Discussion Highlights:** The discussion highlights user interest in comparing GLM 4.7 Flash with other models like Nemotron 30B and Qwen3. Users also note its performance benchmarks and express enthusiasm for local testing. Some users have already started testing the model locally and report decent performance.

---

## 7. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 734 | **Comments:** 230 | **Date:** 2026-01-19

**Summary:** The Reddit post discusses the release of zai-org/GLM-4.7-Flash on Hugging Face, highlighting its popularity and technical features like MLA usage and a 200k context window.

**Key Points:**
- The post gained significant attention with 733 upvotes and 229 comments
- The model uses MLA, reducing KV cache memory usage
- It supports a full 200k context window
- Community members expressed excitement and nostalgia for larger models
- Technical details include a 30b model with a 3B thinking component

**Discussion Highlights:** The community showed strong enthusiasm for the release, with many appreciating the technical advancements and reminiscing about larger models. The consensus highlights the model's efficiency and potential for widespread use due to its memory optimization.

---

## 8. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 345 | **Comments:** 93 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to stay within a 10,000€ budget. The system is designed for running large AI models locally, with benchmark results showing strong performance across various models.

**Key Points:**
- Built a system with 4x AMD R9700 GPUs (128GB VRAM) and Threadripper 9955WX CPU
- Leveraged a 50% subsidy to stay within a 10,000€ budget
- Designed for running large AI models locally
- Benchmark results show strong performance across various models
- Community appreciation and interest in the build

**Discussion Highlights:** The community showed strong interest and appreciation for the build, with comments highlighting the impressive hardware, inquiries about the source of the components, and comparisons to similar systems built by others.

---

## 9. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 456 | **Comments:** 71 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses a potential slowdown in the development of Qwen 4, with the lead developer emphasizing a focus on quality over quantity. The community generally supports this approach, appreciating the commitment to improvement.

**Key Points:**
- Qwen 4 development may be slowing down to focus on quality
- Community appreciates the focus on quality over quantity
- Uncertainty about whether the statement specifically refers to Qwen 4
- Support for taking time to make meaningful advancements
- Mixed reactions to the news, with some cautioning against rumors

**Discussion Highlights:** The discussion highlights a general consensus supporting the focus on quality, with some users expressing appreciation for the developer's approach. There is also a note of caution about interpreting the statement as confirmation of Qwen 4's development status.

---

## 10. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 537 | **Comments:** 116 | **Date:** 2026-01-17

**Summary:** The author upgraded from MI100s to four R9700 GPUs, building a 128GB VRAM server for under the cost of an RTX 6000 Blackwell. The post details the hardware specifications, benchmarks, and cost breakdown, highlighting the performance and cost efficiency of the setup.

**Key Points:**
- Upgrade from MI100s to R9700s due to better performance and cost efficiency
- Detailed hardware specifications and cost breakdown provided
- Performance benchmarks show high token processing rates
- Community reactions include appreciation and humorous remarks about financial irresponsibility

**Discussion Highlights:** The community responded positively, with top comments appreciating the build and joking about the financial implications of such upgrades.

---

## 11. [Best "End of world" model that will run on 24gb VRAM](https://reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/)

**Author:** u/gggghhhhiiiijklmnop | **Upvotes:** 338 | **Comments:** 176 | **Date:** 2026-01-17

**Summary:** The user is seeking recommendations for the best 'end of world' model that can run on a PC with 24GB VRAM and 64GB RAM, aiming to hoard data like Wikipedia, Wiktionary, and Khan Academy. The discussion highlights various suggestions, including prioritizing the best LLM available and considering specific models like gemma3:27b.

**Key Points:**
- User wants a model that fits within 24GB VRAM and 64GB RAM
- Suggestions include saving the best LLM available and running it off SSD if necessary
- Specific model recommendations: gemma3:27b with vision capabilities
- Alternative suggestions: downloading actual Wikipedia backups for offline use
- Mention of Midnight Miku for entertainment purposes

**Discussion Highlights:** The discussion consensus leans towards prioritizing the best available LLM, even if it requires running off SSD. Specific model recommendations include gemma3:27b for its capabilities, including vision. There is also a suggestion to download actual Wikipedia backups for comprehensive offline data storage.

---

## 12. [GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)](https://reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/)

**Author:** u/CuriousPlatypus1881 | **Upvotes:** 383 | **Comments:** 89 | **Date:** 2026-01-16

**Summary:** The post discusses the updated SWE-bench leaderboard results from December 2025, highlighting the performance of various AI models on GitHub PR tasks. Claude Opus 4.5 leads with a 63.3% resolved rate, followed closely by GPT-5.2 (extra high effort) at 61.5%. The post also notes the strong performance of open-source models like GLM-4.7.

**Key Points:**
- Claude Opus 4.5 leads the leaderboard with a 63.3% resolved rate.
- GPT-5.2 (extra high effort) follows closely at 61.5%.
- GLM-4.7 is the strongest open-source model, ranking alongside closed models.
- Gemini 3 Flash Preview outperforms Gemini 3 Pro Preview despite being smaller and cheaper.
- GPT-OSS-120B shows significant performance improvement in high-effort reasoning mode.

**Discussion Highlights:** The discussion highlights excitement over the strong performance of open-source models like GLM-4.7 and the surprising performance of Gemini Flash. There is also anticipation for future releases like DeepSeek v4.

---

## 13. [I fucking love this community](https://reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/)

**Author:** u/alhinai_03 | **Upvotes:** 518 | **Comments:** 54 | **Date:** 2026-01-16

**Summary:** The post expresses gratitude towards the open-source community for enabling the user to run large language models on older hardware, highlighting the efficiency of MoE architectures and system memory.

**Key Points:**
- User runs a 30B parameter model at 14 tokens/second on a 10-year-old PC with 4GB VRAM
- MoE (Mixture of Experts) architectures and sufficient system memory are key for performance
- Community contributions and optimizations are highly valued
- The post gained significant traction with 521 upvotes and 54 comments

**Discussion Highlights:** The community agrees that the optimization achievements are impressive, with particular emphasis on the effectiveness of combining system RAM with MoE models for running large models on limited hardware.

---

## 14. [My story of underestimating /r/LocalLLaMA's thirst for VRAM](https://reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/)

**Author:** u/EmPips | **Upvotes:** 1334 | **Comments:** 91 | **Date:** 2026-01-15

**Summary:** The post highlights the high demand for VRAM in the r/LocalLLaMA community, as indicated by the title and discussion in the comments. It gained significant attention, being featured on Discord and receiving numerous upvotes.

**Key Points:**
- The post gained popularity and was featured on Discord
- Discussion includes hardware recommendations and market dynamics
- The community shows strong interest in VRAM-related topics

**Discussion Highlights:** The discussion includes hardware recommendations, market insights, and community engagement, with a focus on VRAM and related equipment.

---

## 15. [Latest upgrade…A100 40 GB](https://reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/)

**Author:** u/inserterikhere | **Upvotes:** 405 | **Comments:** 54 | **Date:** 2026-01-15

**Summary:** The user upgraded their gaming rig to an AI rig by purchasing a faulty A100 GPU for $1000, which worked perfectly upon installation. The post gained popularity in the r/LocalLLaMA community.

**Key Points:**
- User transitioned from a gaming rig to an AI rig
- Purchased an A100 GPU listed as faulty for $1000, which worked fine
- Post gained significant attention with 405 upvotes and 54 comments
- Community provided advice on cooling the A100 GPU
- Meme references and appreciation for the contribution were noted

**Discussion Highlights:** The community reacted positively to the post, with some providing technical advice on cooling the A100 GPU and others sharing memes and appreciation for the user's contribution.

---

## 16. [NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency](https://reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/)

**Author:** u/Fear_ltself | **Upvotes:** 711 | **Comments:** 130 | **Date:** 2026-01-14

**Summary:** NVIDIA's Orchestrator-8B is an 8-billion-parameter AI designed to manage and route complex tasks to various tools for greater efficiency. The post discusses its potential as a step towards AGI by integrating separate components effectively.

**Key Points:**
- Orchestrator-8B is a specialized AI for task management and routing.
- It aims to enhance efficiency by connecting with other tools and models.
- The post suggests this approach could be a path towards AGI.
- Comments highlight its role as a 'middle manager' LLM and its potential in agentic frameworks.
- Some users note that similar concepts have been explored before.

**Discussion Highlights:** The discussion highlights the model's role as a 'middle manager' LLM and its potential in creating functional AI systems. There is consensus on the importance of integrating different tools and models, with some users drawing parallels to existing agentic frameworks.

---

## 17. [GLM-Image is released!](https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/)

**Author:** u/foldl-li | **Upvotes:** 604 | **Comments:** 83 | **Date:** 2026-01-13

**Summary:** GLM-Image is a new image generation model with a hybrid autoregressive + diffusion decoder architecture. It excels in text-rendering and knowledge-intensive tasks while supporting various image-to-image tasks like editing and style transfer.

**Key Points:**
- Hybrid autoregressive + diffusion decoder architecture
- Excels in text-rendering and knowledge-intensive generation
- Supports image-to-image tasks like editing and style transfer
- MIT license with no restrictions
- Model size: 13GB diffusion model + 20GB text encoder

**Discussion Highlights:** The community appreciates the MIT license and the model's capabilities. There is interest in quantizing the model for easier use and discussions about its performance compared to other models.

---

## 18. [My wishes for 2026](https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/)

**Author:** u/jacek2023 | **Upvotes:** 655 | **Comments:** 179 | **Date:** 2026-01-13

**Summary:** The Reddit post discusses predictions for 2026, focusing on the possibility of affordable GPUs with more than 32GB memory. The community expresses skepticism and humor about the feasibility of such advancements.

**Key Points:**
- The post asks which predictions for 2026 are likely to happen first.
- A top comment humorously doubts the possibility of affordable GPUs with >32GB memory.
- Another comment references 'Qwen 4' and 'Mistral' as potential advancements, while others are seen as unlikely.
- The community engages with humor and skepticism about technological advancements.
- The post gains traction, being featured on Discord and earning the author a special flair.

**Discussion Highlights:** The discussion is marked by a mix of humor and skepticism regarding the feasibility of affordable high-memory GPUs in 2026. While some commenters joke about the idea, others reference specific models like 'Qwen 4' and 'Mistral' as potential advancements. The overall tone is lighthearted, with a consensus that such advancements may be unlikely in the near term.

---

## 19. [kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop—no GPU required](https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/)

**Author:** u/Nunki08 | **Upvotes:** 395 | **Comments:** 93 | **Date:** 2026-01-13

**Summary:** Kyutai introduced Pocket TTS, a 100M-parameter text-to-speech model with high-quality voice cloning that runs on a laptop without requiring a GPU. The model is open-source with resources available on GitHub, Hugging Face, and arXiv.

**Key Points:**
- 100M-parameter TTS model with high-quality voice cloning
- Runs on a laptop without GPU
- Open-source with available resources (GitHub, Hugging Face, arXiv)
- Potential memory usage issues during generation
- Discussion on language support and model size trade-offs

**Discussion Highlights:** Users discussed potential memory usage issues, language support, and the trade-offs of using smaller models. Some users noted that the localhost test server does not clear memory between generations, leading to high memory usage.

---

## 20. [GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/)

**Author:** u/TKGaming_11 | **Upvotes:** 363 | **Comments:** 93 | **Date:** 2026-01-12

**Summary:** The Reddit post highlights DeepSeek-AI's 'Engram,' a novel approach for conditional memory in large language models using scalable lookup. The discussion praises the innovation and technical depth of the paper.

**Key Points:**
- DeepSeek-AI introduces 'Engram' for conditional memory in LLMs
- Uses n-gram embedding and mHC (M=4) for ablations
- Adds static memory as a complementary sparsity axis with O(1) lookup
- Discussion notes the innovation and potential of the approach
- Comparisons drawn to biological memory processes

**Discussion Highlights:** The community appreciates the originality and technical depth of the paper, with specific praise for the n-gram embedding approach and its potential to complement existing methods like MoE.

---

## 21. [LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)](https://reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 1055 | **Comments:** 114 | **Date:** 2026-01-11

**Summary:** The post introduces TimeCapsuleLLM, an open-source project training language models on historical texts from 1800-1875 London. The 1.2B parameter model uses a 90GB dataset and demonstrates period-specific outputs, such as unfamiliarity with post-1875 concepts like telephones.

**Key Points:**
- Project focuses on reducing modern bias by training on 1800-1875 London texts
- Model has 1.2B parameters and uses a 90GB dataset of books, journals, legal docs, etc.
- Examples show period-specific behaviors, like arguing against the Catholic Church and misunderstanding telephones
- Future plans include creating synthetic Q&A pairs from the dataset
- Community response is positive, with users praising the project's uniqueness and potential

**Discussion Highlights:** The community shows strong support for the project, with users expressing interest in similar historical datasets and praising the innovative approach. Some humorous comments highlight the model's limitations, like its 1875 knowledge cutoff.

---

## 22. [I bought a €9k GH200 “desktop” to save $1.27 on Claude Code (vLLM tuning notes)](https://reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/)

**Author:** u/Reddactor | **Upvotes:** 692 | **Comments:** 177 | **Date:** 2026-01-11

**Summary:** The author built a high-end GH200 desktop system to run Claude Code locally, achieving better performance and cost savings compared to cloud-based solutions. They shared optimized vLLM settings for dual 96GB systems and highlighted the benefits of local code reviews.

**Key Points:**
- Built a €9k GH200 desktop with 192GB VRAM for local Claude Code usage
- Achieved better speeds than Claude Code with Sonnet and successful tool use
- Shared optimized vLLM settings for dual 96GB systems, including tensor parallel size and context settings
- Highlighted the cost savings and performance benefits of local code reviews
- Mentioned the humorous accounting aspect of the investment

**Discussion Highlights:** The community responded with humor and appreciation, noting the high cost but acknowledging the fun and innovation. Some users expressed envy over missing out on similar deals, while others confirmed the technical details and shared related resources.

---

## 23. [It works! Abliteration can reduce slop without training](https://reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/)

**Author:** u/-p-e-w- | **Upvotes:** 402 | **Comments:** 129 | **Date:** 2026-01-11

**Summary:** The post discusses using abliteration to reduce 'slop' (flowery, cliched language) in LLM outputs without training. The author applied this technique to Mistral Nemo, creating a slop-reduced model, and shared results and community feedback. Key points include: Abliteration can reduce slop in LLM outputs without finetuning, the technique was applied to Mistral Nemo, the process took 2.5 hours on an A6000 but can be faster with quantization, community feedback is mixed, and GGUF versions of the model are available for download. The community discussion highlights mixed opinions on the effectiveness of slop reduction, with some users appreciating the cleaner output while others feel it lacks imagination or becomes too dry.

---

## 24. [I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work](https://reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/)

**Author:** u/Ok-Pomegranate1314 | **Upvotes:** 893 | **Comments:** 147 | **Date:** 2026-01-09

**Summary:** The author successfully clustered three DGX Sparks, which NVIDIA officially supports only for two, by developing a custom NCCL network plugin. This involved overcoming subnet and networking challenges with a 1500-line C implementation, achieving distributed inference at over 8 GB/s via RDMA.

**Key Points:**
- NVIDIA officially supports clustering only two DGX Sparks; the author extended this to three.
- Developed a custom NCCL network plugin to handle subnet-aware NIC selection and RDMA implementation.
- Achieved distributed inference at over 8 GB/s across three nodes.
- The solution involved low-level debugging and custom protocols to avoid deadlocks.
- Community reactions highlight the technical difficulty and potential significance of the achievement.

**Discussion Highlights:** The community praised the technical complexity of the solution, noting that NCCL is typically only modified for large training setups. Questions arose about scalability and performance gains, with the author providing a GitHub link for further details.

---

## 25. [The reason why RAM has become so expensive](https://reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/)

**Author:** u/InvadersMustLive | **Upvotes:** 4530 | **Comments:** 381 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses the significant increase in RAM prices, with some users suggesting that companies like OpenAI may be monopolizing key resources to create future demand and make competitors economically unviable. The discussion highlights concerns about market manipulation and the economic impact on data centers, particularly in China.

**Key Points:**
- RAM prices have increased dramatically, with some users reporting a 10x increase.
- There are concerns about market manipulation and monopolization of key resources by companies like OpenAI.
- The economic impact on data centers, especially in China, is a significant point of discussion.
- Some users view the situation as a potential bubble.

**Discussion Highlights:** The discussion revolves around the economic implications of rising RAM prices, with a focus on potential market manipulation and the impact on data centers. There is a consensus that the price increase is substantial and could have far-reaching consequences.

---

## 26. [DeepSeek V4 Coming](https://reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/)

**Author:** u/External_Mood4719 | **Upvotes:** 497 | **Comments:** 108 | **Date:** 2026-01-09

**Summary:** DeepSeek is expected to release V4, a next-generation AI model with strong code-generation capabilities, outperforming mainstream models like Claude and GPT. The model shows improvements in handling long code prompts and reasoning ability.

**Key Points:**
- DeepSeek V4 focuses on strong code-generation capabilities
- Outperforms existing models like Claude and GPT in code generation
- Improved handling of long code prompts and data patterns
- Enhanced reasoning ability and logical rigor
- Users appreciate DeepSeek's cost-effectiveness and local API options

**Discussion Highlights:** Users express excitement and anticipation for V4, with positive feedback on DeepSeek's performance and affordability. Some speculate on potential delays due to extensive pre-training and post-training processes.

---

## 27. [(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability](https://reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/)

**Author:** u/Nunki08 | **Upvotes:** 489 | **Comments:** 102 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release a new flagship AI model with strong coding capabilities, generating significant interest and discussion in the community.

**Key Points:**
- DeepSeek's upcoming model focuses on strong coding ability
- The announcement has generated excitement and anticipation
- Community members express enthusiasm for more AI models
- Some comments highlight skepticism about performance claims
- Discussion includes hopes for improved role-playing abilities

**Discussion Highlights:** The community shows a mix of excitement and skepticism, with many welcoming the competition and innovation in AI models. Some users express concerns about overhyped claims and hope for balanced capabilities beyond just coding.

---

## 28. [The NO FAKES Act has a "Fingerprinting" Trap that kills Open Source. We need to lobby for a Safe Harbor.](https://reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/)

**Author:** u/PostEasy7183 | **Upvotes:** 614 | **Comments:** 88 | **Date:** 2026-01-08

**Summary:** The NO FAKES Act proposes a 'digital replica right' that could hold developers liable for hosting open-source AI models used to create deepfakes, potentially stifling innovation. The post urges lobbying for a 'Safe Harbor' provision to protect developers.

**Key Points:**
- The NO FAKES Act targets developers who 'make available' tools primarily used for creating digital replicas.
- Developers could face statutory damages of $5k-$25k per violation, with no Section 230 protection.
- The post calls for a 'Safe Harbor' provision to protect open-source developers.
- The discussion highlights concerns about the impact on innovation and the influence of big tech corporations.
- Action items include contacting representatives and calling senators to voice opposition.

**Discussion Highlights:** The discussion reflects strong opposition to the bill, with concerns about its impact on innovation and the potential for big tech monopolies. There is a consensus on the need for a 'Safe Harbor' provision to protect open-source developers.

---

## 29. [Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt](https://reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/)

**Author:** u/Prior-Arm-6705 | **Upvotes:** 937 | **Comments:** 147 | **Date:** 2026-01-08

**Summary:** The post describes a project where someone counted and compiled every instance of Jensen Huang saying 'AI' (121 times) during his CES 2025 keynote using open-source tools. The process involved downloading the video, parsing subtitles for timestamps, and editing clips into a final compilation.

**Key Points:**
- Jensen Huang said 'AI' 121 times during his CES 2025 keynote.
- The author used open-source tools (Dive, yt-dlp-mcp, ffmpeg-mcp-lite) to automate the video compilation.
- The process involved downloading, parsing subtitles, cutting clips, and merging them chronologically.
- The result was a hypnotic compilation video showcasing all instances of 'AI'.
- The post gained significant attention, with comments ranging from humor to technical appreciation.

**Discussion Highlights:** The discussion included reactions to the post's popularity, jokes about the cost of AI, and references to other tech content creators like Gamers Nexus. Some comments also humorously noted Jensen Huang's distinctive attire.

---

## 30. [16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/)

**Author:** u/ai-infos | **Upvotes:** 462 | **Comments:** 238 | **Date:** 2026-01-07

**Summary:** The post discusses running Deepseek V3.2 AWQ 4-bit on 16x AMD MI50 32GB GPUs, achieving 10 tokens/sec output and 2000 tokens/sec input with a 69000 context length. The setup is cost-effective and aims to provide a local AGI alternative without high costs.

**Key Points:**
- Performance: 10 tok/s output, 2000 tok/s input, 69000 context length
- Power draw: 550W idle, 2400W peak inference
- Goal: cost-effective local AGI setup
- Future plans: open-source 32 AMD MI50 setup for Kimi K2 Thinking
- Community appreciation and cost-effectiveness highlighted in comments

**Discussion Highlights:** The discussion highlights the setup's popularity, its potential as a cost-effective alternative to CPU hardware, and practical considerations like power usage and noise levels. Users appreciate the cost-effectiveness and potential for local AGI development.

---

## 31. [DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.](https://reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/)

**Author:** u/Nunki08 | **Upvotes:** 660 | **Comments:** 54 | **Date:** 2026-01-07

**Summary:** The Reddit post discusses the recent update to DeepSeek-R1's paper, which expanded from 22 pages to 86 pages, adding significant detail. The discussion includes speculation about new architectures and the potential impact of linear attention research.

**Key Points:**
- DeepSeek-R1's paper was updated from 22 pages to 86 pages.
- The update includes substantial additional detail.
- Discussion highlights potential new architectures and linear attention research.
- Community interest in how architectural improvements scale across different model sizes.
- Original paper lacked implementation specifics, which the update may address.

**Discussion Highlights:** The community is excited about the expanded paper, with speculation about new architectures and the impact of linear attention research. There is also interest in how architectural improvements will perform across different model sizes.

---

## 32. [A 30B Qwen Model Walks Into a Raspberry Pi… and Runs in Real Time](https://reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/)

**Author:** u/ali_byteshape | **Upvotes:** 501 | **Comments:** 79 | **Date:** 2026-01-06

**Summary:** The post discusses the successful optimization of a 30B Qwen model to run efficiently on a Raspberry Pi 5, achieving 8.03 tokens per second while retaining 94.18% of BF16 quality. The optimization focuses on balancing memory usage and performance, particularly noting the quirks of GPU kernel choices. Key points include the model's performance on a Raspberry Pi 5, the optimization strategy prioritizing memory as a budget, the differences in CPU and GPU performance behavior, and the community's interest in testing the model on various setups. The discussion highlights the community's engagement in testing the model on different hardware configurations and their experiences with adjusting settings to avoid crashes.

---

## 33. [Performance improvements in llama.cpp over time](https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/)

**Author:** u/jacek2023 | **Upvotes:** 681 | **Comments:** 85 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses performance improvements in llama.cpp over time, highlighting significant progress and ongoing optimizations. The discussion focuses on GPU-specific enhancements and comparisons with other implementations.

**Key Points:**
- Performance gains in llama.cpp have been substantial
- Improvements are particularly notable for NVIDIA GPUs
- Comparisons with other implementations like ik_llama.cpp show competitive performance
- Prompt processing remains slower than token generation
- Community appreciation for the progress and contributions

**Discussion Highlights:** The discussion highlights significant performance gains in llama.cpp, with a focus on NVIDIA GPU optimizations. Users note that while token generation speed is close to other implementations, prompt processing is still slower. Overall, there is a consensus on the impressive progress and ongoing improvements in the project.

---

## 34. [For the first time in 5 years, Nvidia will not announce any new GPUs at CES — company quashes RTX 50 Super rumors as AI expected to take center stage](https://reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/)

**Author:** u/FullstackSensei | **Upvotes:** 626 | **Comments:** 196 | **Date:** 2026-01-05

**Summary:** Nvidia will not announce new GPUs at CES, focusing instead on AI. There are concerns about limited supply of high-end GPUs, rising hardware prices, and the potential re-release of older models like the RTX 3060.

**Key Points:**
- Nvidia quashes RTX 50 Super rumors, no new GPU announcements at CES
- Limited supply of high-end GPUs (5070Ti, 5080, 5090) and rising hardware prices
- Discussion highlights corporate greed and the impact on local computing
- Suggestions for alternative solutions, such as China flooding the market with high-memory cards
- Sentiment reflects frustration and concern about future hardware upgrades

**Discussion Highlights:** The discussion highlights frustration with corporate greed and the impact on local computing. Users express concern about the future of hardware upgrades and suggest alternative solutions to address the supply and pricing issues.

---

## 35. [llama.cpp performance breakthrough for multi-GPU setups](https://reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/)

**Author:** u/Holiday-Injury-9397 | **Upvotes:** 574 | **Comments:** 201 | **Date:** 2026-01-05

**Summary:** The ik_llama.cpp project achieved a significant performance breakthrough for multi-GPU setups, delivering a 3x to 4x speed improvement in local LLM inference. This advancement allows for the utilization of multiple low-cost GPUs instead of expensive high-end enterprise cards. Key points include the introduction of a new execution mode (split mode graph) for multi-GPU configurations, enabling simultaneous and maximum utilization of multiple GPUs. The community is excited about the performance gains and cost-effectiveness of the new multi-GPU setup, with some users noting performance gains even on single GPU or CPU-only setups.

---

## 36. [Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched](https://reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/)

**Author:** u/ubrtnk | **Upvotes:** 375 | **Comments:** 195 | **Date:** 2026-01-03

**Summary:** The post discusses the challenges faced by local LLMs in processing extreme breaking news events, such as the US attacking Venezuela. The author shares their experience with different LLMs, highlighting how these models initially classified the event as a hoax despite credible sources.

**Key Points:**
- Local LLMs struggled to accept extreme breaking news as real, classifying it as misinformation.
- Different LLMs (Qwen Research, Spark 4.0, GPT-OSS:120B) had varying responses to the news.
- Models required explicit credible sources to acknowledge the event's reality.
- Commenters shared similar experiences with LLMs dismissing unlikely events.
- Discussion highlights the bias and limitations in LLMs' understanding of unfamiliar geopolitical events.

**Discussion Highlights:** The discussion consensus suggests that LLMs have inherent biases and struggle with processing extreme or unlikely events, often requiring explicit evidence to accept such news as real. Commenters shared similar experiences and expressed curiosity about the future of AI in handling such scenarios.

---

## 37. [LeCun Says Llama 4 results "were fudged a little bit"](https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/)

**Author:** u/MrPecunius | **Upvotes:** 363 | **Comments:** 88 | **Date:** 2026-01-02

**Summary:** Yann LeCun, departing Meta AI Chief, confirmed that Llama 4 benchmark results were manipulated, leading to organizational changes and departures. The post discusses the impact on Meta's AI initiatives and the broader implications for open-source AI development.

**Key Points:**
- LeCun confirms Llama 4 benchmark manipulation
- Zuckerberg sidelined the GenAI organization, leading to departures
- Llama 4's promised large model was never released
- Concerns about Meta's strategic missteps in AI development
- Community interest in understanding the reasons behind Meta's struggles

**Discussion Highlights:** The discussion highlights disappointment in Meta's handling of the Llama project, with users expressing concern over the lack of progress and the impact on open-source AI. There is a shared interest in understanding the strategic failures at Meta and the potential lessons for other organizations.

---

## 38. [Qwen-Image-2512](https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/)

**Author:** u/Nunki08 | **Upvotes:** 724 | **Comments:** 122 | **Date:** 2025-12-31

**Summary:** The Reddit post introduces Qwen-Image-2512, a new image generation model, with links to guides, downloads, and demos. Users share experiences running the model on various hardware and generating creative images.

**Key Points:**
- Qwen-Image-2512 is a new image generation model with multiple resources available.
- Users have successfully run the model on low-end hardware without a GPU.
- The model can generate creative and detailed images, as demonstrated by user examples.
- The post provides links to various platforms like Hugging Face, ModelScope, and GitHub.
- The community appreciates the model as a gift for the new year.

**Discussion Highlights:** Users discussed their experiences running the model on different hardware setups, including low-end systems without GPUs. They also shared creative image generation examples and expressed appreciation for the model's release.

---

## 39. [[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It’s running a raw Llama-7B instance with a 2048 token window.](https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/)

**Author:** u/simar-dmg | **Upvotes:** 747 | **Comments:** 110 | **Date:** 2025-12-30

**Summary:** A user reverse-engineered a Snapchat sextortion bot and discovered it was running a raw Llama-7B instance with a 2048 token window. The bot was vulnerable to a persona-adoption jailbreak, revealing its configuration and malicious payload.

**Key Points:**
- The bot used a Llama-7B model with a 2048 token context window and high temperature setting.
- A 'Grandma Protocol' jailbreak forced the bot to reveal its environment variables and configuration.
- The bot's erratic behavior was due to running on minimal hardware to cut costs.
- Scammers are shifting to open-source models like Llama-7B to avoid API costs and censorship.
- The post sparked discussions about the reliability of information extracted from LLMs and the prevalence of hallucinations.

**Discussion Highlights:** The discussion highlighted skepticism about the accuracy of the extracted information, with many users pointing out that LLMs often hallucinate details. There was also a consensus that while the bot was confirmed to be LLM-powered, other details might be unreliable.

---

## 40. [Llama-3.3-8B-Instruct](https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/)

**Author:** u/jacek2023 | **Upvotes:** 468 | **Comments:** 77 | **Date:** 2025-12-29

**Summary:** The post announces the discovery and release of Llama-3.3-8B-Instruct, a previously API-exclusive model from Meta. The author successfully downloaded and shared the model, sparking community interest and verification efforts.

**Key Points:**
- Llama-3.3-8B-Instruct was previously only available via Meta's API
- The author found a way to download the model through Meta's finetuning API
- The model appears to be a legitimate new version, not a repackaged older version
- Community members are running benchmarks and evaluations to verify its authenticity
- There are technical questions about the model's specifications, such as its 8K max position embeddings

**Discussion Highlights:** The community is excited about the release and is actively verifying the model's authenticity through benchmarks and evaluations. Some technical questions have been raised about its specifications, but overall, the discovery is seen as a significant contribution.

---

## 41. [Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.](https://reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 342 | **Comments:** 120 | **Date:** 2025-12-29

**Summary:** Z AI is set to go public with an IPO on January 8, aiming to raise $560 million, marking it as the first AI-native LLM company to list globally. The Reddit discussion highlights mixed reactions, with concerns about the future of open-source AI and the inevitability of monetization.

**Key Points:**
- Z AI's IPO is scheduled for January 8, targeting $560 million.
- It will be the first AI-native LLM company to list on the global market.
- Community concerns about the impact on open-source AI.
- Debate on whether Z AI will continue releasing open weight models.
- General acceptance that companies need to monetize eventually.

**Discussion Highlights:** The discussion reflects a divide in the community, with some expressing concerns about the future of open-source AI and others acknowledging the necessity of monetization. The top comments suggest a consensus that while open-source contributions may continue, the focus will likely shift towards paid services.

---

## 42. [Tencent just released WeDLM 8B Instruct on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 418 | **Comments:** 62 | **Date:** 2025-12-29

**Summary:** Tencent released WeDLM 8B Instruct on Hugging Face, a diffusion language model that runs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks. The release has garnered significant attention and positive feedback from the community.

**Key Points:**
- WeDLM 8B Instruct is a diffusion language model released by Tencent on Hugging Face.
- It performs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks.
- The model is released under the Apache 2.0 license.
- The community finds the benchmark scores impressive and the release promising.
- There is also a 7B version of the model available.

**Discussion Highlights:** The community is excited about the release, highlighting the impressive benchmark scores and the potential of 7-8B models. There is a consensus that diffusion models for LLMs are promising and that more models in this size range are welcomed.

---

## 43. [NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux](https://reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/)

**Author:** u/HumanDrone8721 | **Upvotes:** 445 | **Comments:** 185 | **Date:** 2025-12-27

**Summary:** NVIDIA has dropped Pascal support on Linux, causing issues for Arch Linux users. The community is reacting with concern, especially those using Pascal cards like the P40.

**Key Points:**
- NVIDIA's driver update drops support for Pascal GPUs on Linux
- Arch Linux users are particularly affected, with legacy drivers moved to AUR
- The 24GB P40, a popular Pascal card, is impacted
- Community reactions range from concern to acceptance, noting Arch's history of handling legacy drivers
- Users are advised to check Arch Linux news for updates on driver support

**Discussion Highlights:** The discussion highlights a mix of concern and acceptance among users. Some express worry about the future of their Pascal cards, while others note that Arch Linux has a history of moving legacy drivers to the AUR (Arch User Repository). The community seems aware of the change but is adapting to the new driver landscape.

---

## 44. [Best Local LLMs - 2025](https://reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/)

**Author:** u/rm-rf-rm | **Upvotes:** 359 | **Comments:** 197 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses the best local LLMs of 2025, highlighting models like Minimax M2.1 and GLM4.7, and categorizes them by application and memory footprint. Users share detailed experiences and preferences for open weights models.

**Key Points:**
- Minimax M2.1 and GLM4.7 are noted for their frontier model performance.
- Models are categorized by applications such as General, Agentic, Creative Writing, and Speciality.
- Memory footprint classifications include Unlimited (>128GB VRAM), Medium (8-128GB VRAM), and Small (<8GB VRAM).
- Users emphasize detailed descriptions of their setups and usage scenarios.
- The discussion focuses on open weights models and practical usage experiences.

**Discussion Highlights:** The discussion highlights the importance of open weights models and provides a structured breakdown of model usage by memory footprint. Users share detailed experiences and preferences, emphasizing practical applications and performance in various categories.

---

## 45. [NVIDIA has 72GB VRAM version now](https://reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/)

**Author:** u/decentralize999 | **Upvotes:** 457 | **Comments:** 148 | **Date:** 2025-12-26

**Summary:** The post discusses NVIDIA's new 72GB VRAM version, questioning if 96GB is too expensive and if the AI community has interest in 48GB. The community responds with mixed opinions, some advocating for larger versions like 128GB, while others focus on price per gig and affordability.

**Key Points:**
- NVIDIA has released a 72GB VRAM version
- Community debates the cost-effectiveness of 96GB vs. 72GB
- Some users advocate for even larger versions like 128GB
- Price per gig remains consistent across versions
- Affordability is a key consideration for buyers

**Discussion Highlights:** The discussion highlights a divide in opinions, with some users pushing for larger VRAM versions (e.g., 128GB) and others focusing on the practicality of current offerings. The consensus leans toward buying the most VRAM one can afford, given the consistent price per gig.

---

## 46. [Hard lesson learned after a year of running large models locally](https://reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/)

**Author:** u/inboundmage | **Upvotes:** 345 | **Comments:** 146 | **Date:** 2025-12-26

**Summary:** The author shares their experience running large language models locally, highlighting challenges with VRAM limitations, model scaling, and performance trade-offs. They conclude that local inference is viable for smaller models but requires significant hardware investment for larger ones.

**Key Points:**
- Running large models locally is feasible but has hard limits with consumer-grade hardware.
- VRAM fragmentation and memory management are significant challenges.
- Quantization helps but introduces quality trade-offs and bugs.
- Cloud-based solutions offer better performance for fast iteration.
- Community suggestions include using llama.cpp for CPU offloading and adding more VRAM.

**Discussion Highlights:** The discussion highlights practical solutions like using llama.cpp for CPU offloading and the need for more VRAM. There is a consensus that local inference is viable for smaller models but requires significant hardware upgrades for larger models.

---

## 47. [I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA](https://reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/)

**Author:** u/CeFurkan | **Upvotes:** 1036 | **Comments:** 178 | **Date:** 2025-12-25

**Summary:** The post discusses the potential for GPU VRAM upgrade modifications to become mainstream, challenging NVIDIA's monopoly. Users highlight the availability of such modifications, particularly in China, with various pricing options.

**Key Points:**
- GPU VRAM upgrade modifications are seen as a way to challenge NVIDIA's monopoly.
- Modded GPUs are already mainstream in China, with options like 2080Ti, 3080, 4080, 4090, and 5090 available at varying prices.
- Users report successful experiences with modded GPUs, such as a 4090 with 48GB of memory.
- Pricing for modded GPUs ranges from $300 for a 2080Ti 22GB to $4000 for a 5090 96GB.
- There is interest and demand for high-memory GPUs at competitive prices.

**Discussion Highlights:** The discussion highlights the feasibility and benefits of GPU VRAM modifications, with users sharing positive experiences and noting the availability of such modifications in markets like China. There is a consensus on the potential for these modifications to disrupt the GPU market and provide more affordable, high-performance options.

---

## 48. [Why I quit using Ollama](https://reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/)

**Author:** u/SoLoFaRaDi | **Upvotes:** 484 | **Comments:** 202 | **Date:** 2025-12-25

**Summary:** The author expresses dissatisfaction with Ollama due to recent updates and the introduction of cloud features, leading them to switch to alternatives like llama.cpp and LM Studio.

**Key Points:**
- Author's dissatisfaction with Ollama's recent updates
- Introduction of cloud features and privacy concerns
- Shift to alternatives like llama.cpp and LM Studio
- Community consensus on preferring other tools

**Discussion Highlights:** The discussion highlights a consensus among users to switch to alternatives like llama.cpp and LM Studio, citing better performance and alignment with their needs.

---

## 49. [Exclusive: Nvidia buying AI chip startup Groq's assets for about $20 billion in largest deal on record](https://reddit.com/r/LocalLLaMA/comments/1puyq9r/exclusive_nvidia_buying_ai_chip_startup_groqs/)

**Author:** u/fallingdowndizzyvr | **Upvotes:** 669 | **Comments:** 157 | **Date:** 2025-12-24

**Summary:** Nvidia is acquiring AI chip startup Groq's assets for approximately $20 billion, marking the largest deal on record. The acquisition has sparked discussions about market competition and consolidation in the AI industry.

**Key Points:**
- Nvidia is buying Groq's assets for about $20 billion
- The deal is the largest on record
- Discussions highlight concerns about market consolidation
- Some commenters question Groq's valuation at $20 billion
- The acquisition is seen as a strategic move by Nvidia

**Discussion Highlights:** The discussion highlights mixed reactions, with some seeing the acquisition as beneficial for market competition, while others express concerns about further consolidation in the AI industry. There is also skepticism about Groq's valuation and the nature of the deal being an 'acquihire.'

---

## 50. [We asked OSS-120B and GLM 4.6 to play 1,408 Civilization V games from the Stone Age into the future. Here's what we found.](https://reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/)

**Author:** u/vox-deorum | **Upvotes:** 661 | **Comments:** 174 | **Date:** 2025-12-24

**Summary:** Researchers used open-source LLMs (GPT-OSS-120B and GLM-4.6) to play 1,408 full games of Civilization V, finding that LLMs can survive full games with a hybrid approach and develop distinct playstyles. The models showed slight improvements in best scores but minor decreases in win rates compared to baseline AI. Key points include: LLMs can survive full Civilization V games with a hybrid approach, achieving ~97.5% survival rate; OSS-120B exhibited a warmonger playstyle with more Domination victories, while GLM-4.6 played more balanced; Both models preferred the Order ideology (~24% more likely) over Freedom; Cost per game was approximately $0.86 for OSS-120B, with input tokens scaling linearly as the game progresses; The study suggests that even smaller models (e.g., OSS-20B) can perform adequately in this hybrid setup. The discussion highlights enthusiasm for integrating LLMs into multiplayer games and curiosity about the potential of smaller models. Users expressed interest in playing against local models and experimenting with more dynamic AI behaviors in multiplayer settings.

---

