# r/LocalLLaMA Reading Digest

**Period:** 2026-01-21 to 2026-01-21
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?](https://reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/)

**Author:** u/Adventurous-Gold6413 | **Upvotes:** 460 | **Comments:** 266 | **Date:** 2026-01-20

**Summary:** The post discusses selecting local models for use with 64GB RAM and 16GB VRAM when internet access is unavailable. Users share their preferred models and experiences. Key points include recommendations for models like Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B, with GPT-OSS-120B being highlighted for its performance and fit within the given hardware specifications. The discussion includes a mix of serious recommendations and light-hearted comments.

---

## 2. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 791 | **Comments:** 233 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, fully enclosed 10-GPU mobile AI system designed for running large MoE models, video generation, and high-detail image generation. The system, built with a Threadripper Pro 3995WX, 512GB DDR4, and a mix of 3090 and 5090 GPUs, cost approximately $17k and was designed to be movable and enclosed to protect hardware from pets.

**Key Points:**
- Custom-built system with 10 GPUs (8x 3090 + 2x 5090) for AI tasks like MoE models and video/image generation.
- Designed to be movable and fully enclosed to protect hardware from pets.
- Budget-conscious build aiming for high performance without unnecessary expenses.
- Challenges included enclosure design and balancing performance with cost.
- Top comments highlight humor and admiration for the build's complexity.

**Discussion Highlights:** The discussion includes humorous remarks about the system's portability and power requirements, as well as admiration for the build's complexity and the creative solution to the enclosure problem.

---

## 3. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 357 | **Comments:** 60 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the official support for GLM 4.7 Flash in llama.cpp, highlighting a community effort. The discussion includes notes on performance and additional resources.

**Key Points:**
- GLM 4.7 Flash support has been officially merged into llama.cpp.
- The implementation was a community effort, not by Z.ai developers.
- Performance notes include flash-attention being slow for some users, with better results using -fa 0.
- Additional resources like a Hugging Face model are shared.
- The post received recognition with a special flair and Discord feature.

**Discussion Highlights:** The community appreciates the effort and shares additional resources. There is a consensus that flash-attention may not always be the fastest option, with some users reporting better performance without it.

---

## 4. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 453 | **Comments:** 156 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights GLM 4.7 Flash as a reliable local agent, outperforming other MoE models in an agentic framework with successful tool calling and task execution. Users are eager for its local availability via GGUFs.

**Key Points:**
- GLM 4.7 Flash is praised for its reliability and performance in agentic tasks.
- It successfully handles complex tasks like cloning repos, running commands, and editing files.
- Users anticipate local use via GGUFs and compare it favorably to other models like Nemotron 30B and Qwen3.
- Performance benchmarks suggest it is as smart as SEED OSS 36B but with better efficiency.
- The model is noted for deep thinking, though it may be computationally intensive.

**Discussion Highlights:** The discussion highlights enthusiasm for GLM 4.7 Flash, with users sharing comparisons to other models, performance benchmarks, and anticipation for local deployment. Some note its computational demands but praise its capabilities.

---

## 5. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 724 | **Comments:** 226 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights the release of zai-org/GLM-4.7-Flash on Hugging Face, generating significant interest with 719 upvotes and 226 comments. Users express excitement about the model's capabilities, particularly its memory efficiency and context length.

**Key Points:**
- The post links to the Hugging Face page for GLM-4.7-Flash.
- Users appreciate the model's memory efficiency due to MLA usage.
- The model supports a full 200k context, making it accessible to more users.
- There is nostalgia for larger models like 70b.
- The release is seen as promising by the community.

**Discussion Highlights:** The discussion is largely positive, with users highlighting the model's technical advantages such as memory efficiency and long context support. There is also a sense of anticipation and nostalgia for larger models.

---

## 6. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 346 | **Comments:** 91 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to stay within a ~10,000€ budget. The system is designed for running large AI models (120B+ parameters) locally, with benchmark results showing strong performance across various models. Key points include maximizing VRAM for large AI models, a total cost of ~9,800€ with a 50% subsidy, and impressive benchmark results. The discussion highlights the impressive hardware setup and its capabilities, with comments praising the build and expressing interest in the components used.

---

## 7. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 442 | **Comments:** 71 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses Qwen 4's development, with the lead developer indicating a slowdown to focus on quality. The community generally appreciates this approach, though some caution against overinterpreting the statement.

**Key Points:**
- Lead developer mentions slowing down Qwen 4 development to focus on quality
- Community largely supports the focus on quality over quantity
- Some users urge caution against jumping to conclusions based on limited information
- Discussion highlights the importance of meaningful advancements over incremental updates
- Post gained significant traction with 444 upvotes and 71 comments

**Discussion Highlights:** The community consensus leans towards appreciating the focus on quality, with many users expressing hope for significant improvements in Qwen 4. However, there is also skepticism about interpreting the developer's statement as definitive proof of Qwen 4's timeline or features.

---

## 8. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 528 | **Comments:** 112 | **Date:** 2026-01-17

**Summary:** The author upgraded their server from a dual MI100 setup to a quad R9700 setup, achieving 128GB VRAM and 128GB RAM for a cost-effective price. They detailed the hardware components and provided benchmarks for performance. The community appreciated the detailed build and benchmarks, with some users joking about the financial irresponsibility of such upgrades. Overall, the post was well-received and featured on Discord.

---

## 9. [Best "End of world" model that will run on 24gb VRAM](https://reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/)

**Author:** u/gggghhhhiiiijklmnop | **Upvotes:** 335 | **Comments:** 176 | **Date:** 2026-01-17

**Summary:** The user is seeking recommendations for the best LLM model that can run on a PC with 24GB VRAM and 64GB RAM, aiming to hoard data like Wikipedia and other educational resources. The discussion highlights various suggestions and considerations for model selection.

**Key Points:**
- User wants a model that fits within 24GB VRAM and 64GB RAM
- Suggestions include saving the best LLM possible and running it off SSD if necessary
- Specific model recommendations like gemma3:27b and Midnight Miku
- Advice to download actual Wikipedia backups for offline use
- Consideration of the end-of-world scenario influencing model choice

**Discussion Highlights:** The discussion features a mix of practical advice and humorous suggestions. The top comment emphasizes prioritizing the best LLM regardless of size constraints, suggesting running it off SSD if needed. Other notable recommendations include the gemma3:27b model for its capabilities and vision features, and the importance of downloading actual Wikipedia backups for comprehensive offline data storage.

---

## 10. [GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)](https://reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/)

**Author:** u/CuriousPlatypus1881 | **Upvotes:** 374 | **Comments:** 89 | **Date:** 2026-01-16

**Summary:** The post discusses the updated SWE-bench leaderboard results from December 2025, highlighting the performance of various AI models on GitHub PR tasks. Claude Opus 4.5 leads with a 63.3% resolved rate, followed closely by GPT-5.2 (extra high effort) at 61.5%. The post also notes the strong performance of open-source models like GLM-4.7.

**Key Points:**
- Claude Opus 4.5 leads the SWE-bench leaderboard with a 63.3% resolved rate.
- GPT-5.2 (extra high effort) follows closely at 61.5%.
- Gemini 3 Flash Preview outperforms Gemini 3 Pro Preview despite being smaller and cheaper.
- GLM-4.7 is the strongest open-source model, ranking alongside closed models like GPT-5.1-codex.
- GPT-OSS-120B shows significant performance improvement in high-effort reasoning mode.

**Discussion Highlights:** The discussion highlights excitement around Gemini Flash's performance and the strong showing of open-source models like GLM-4.7. There is also anticipation for future releases like DeepSeek v4.

---

## 11. [I fucking love this community](https://reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/)

**Author:** u/alhinai_03 | **Upvotes:** 498 | **Comments:** 54 | **Date:** 2026-01-16

**Summary:** The author expresses gratitude to the open-source community for enabling them to run large language models on older hardware, achieving impressive performance with limited resources. Key points include running a 30B parameter model at 14 tokens/second on a 10-year-old PC with only 4GB VRAM, the importance of system memory and MoE architectures, and the community's optimization efforts. The discussion highlights the remarkable achievements in optimization and the effectiveness of combining system RAM with MoE models.

---

## 12. [My story of underestimating /r/LocalLLaMA's thirst for VRAM](https://reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/)

**Author:** u/EmPips | **Upvotes:** 1319 | **Comments:** 90 | **Date:** 2026-01-15

**Summary:** The post highlights the author's underestimation of the r/LocalLLaMA community's demand for VRAM, with discussions focusing on hardware recommendations and humorous analogies.

**Key Points:**
- Author underestimated community's VRAM demand
- Discord feature and special flair mentioned
- Gold rush analogy used in comments
- Hardware recommendations (3090s or R9700)
- Humorous comment about selling hardware after popularity

**Discussion Highlights:** The discussion includes hardware advice, humorous analogies, and community engagement through Discord features and special flair.

---

## 13. [Latest upgrade…A100 40 GB](https://reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/)

**Author:** u/inserterikhere | **Upvotes:** 409 | **Comments:** 54 | **Date:** 2026-01-15

**Summary:** The user upgraded their gaming rig to an AI rig by purchasing a faulty A100 GPU for $1000, which worked perfectly upon installation. They previously used a 3090 and a 7950x CPU, and the community reacted with a mix of humor and technical advice.

**Key Points:**
- User transitioned from a gaming rig to an AI rig
- Purchased a faulty A100 GPU for $1000, which worked upon installation
- Community provided humor and technical advice, including cooling concerns
- Post gained significant popularity with 404 upvotes and 54 comments

**Discussion Highlights:** The community reacted positively with humor (e.g., memes) and provided technical advice, particularly about cooling the A100 GPU. The post was featured on Discord, indicating its popularity.

---

## 14. [NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency](https://reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/)

**Author:** u/Fear_ltself | **Upvotes:** 714 | **Comments:** 129 | **Date:** 2026-01-14

**Summary:** NVIDIA's Orchestrator-8B is an 8-billion-parameter AI designed to manage and route complex tasks to various tools, sparking discussions on its potential to create highly functional systems and comparisons to middle management.

**Key Points:**
- Orchestrator-8B is a specialized AI for task management and routing
- It aims to create functional systems by connecting with other tools and models
- The model is compared humorously to a 'Middle manager LLM'
- Discussions highlight its potential in agentic frameworks and model hierarchies

**Discussion Highlights:** The discussion highlights a consensus on the model's potential utility in creating efficient systems, with humorous comparisons to middle management and mentions of its role in agentic frameworks.

---

## 15. [GLM-Image is released!](https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/)

**Author:** u/foldl-li | **Upvotes:** 603 | **Comments:** 83 | **Date:** 2026-01-13

**Summary:** GLM-Image is a new image generation model with a hybrid autoregressive + diffusion decoder architecture. It excels in text-rendering and knowledge-intensive tasks while supporting various image-to-image tasks like editing and style transfer.

**Key Points:**
- Hybrid autoregressive + diffusion decoder architecture
- Excels in text-rendering and knowledge-intensive generation
- Supports image-to-image tasks like editing and style transfer
- MIT license with no restrictions
- Model size: 13GB diffusion model + 20GB text encoder

**Discussion Highlights:** The community appreciates the MIT license and the model's capabilities. There is interest in quantizing the model for easier use and discussions about its performance compared to other models.

---

## 16. [My wishes for 2026](https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/)

**Author:** u/jacek2023 | **Upvotes:** 650 | **Comments:** 179 | **Date:** 2026-01-13

**Summary:** The Reddit post discusses predictions for 2026, focusing on the likelihood of affordable GPUs with more than 32GB of memory becoming available. The community engages in a mix of hopeful and skeptical comments about this possibility.

**Key Points:**
- The post asks which predictions for 2026 are likely to happen first.
- A major focus is on the availability of affordable GPUs with >32GB memory.
- Community reactions range from hopeful to skeptical about the feasibility of such GPUs.
- Mentions of specific AI models like Qwen 4 and Mistral as potential developments.
- The post gained significant traction with 650 upvotes and 179 comments.

**Discussion Highlights:** The discussion highlights a mix of optimism and skepticism regarding the availability of affordable high-memory GPUs in 2026. Some users express doubt, while others humorously engage with the idea. There is also mention of specific AI models as potential advancements for the year.

---

## 17. [kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop—no GPU required](https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/)

**Author:** u/Nunki08 | **Upvotes:** 401 | **Comments:** 92 | **Date:** 2026-01-13

**Summary:** Kyutai introduced Pocket TTS, a 100M-parameter text-to-speech model with high-quality voice cloning that runs on a laptop without requiring a GPU. The model is available on GitHub and Hugging Face, with additional details provided in a blog post and arXiv paper.

**Key Points:**
- Pocket TTS is a 100M-parameter TTS model with high-quality voice cloning.
- It runs on a laptop without needing a GPU.
- The model is open-source and available on GitHub and Hugging Face.
- Memory usage can balloon during generation, as noted in user comments.
- Users are interested in fine-tuning the model for different languages.

**Discussion Highlights:** Users expressed interest in fine-tuning the model for different languages and noted potential memory usage issues during generation. Some comments suggested that smaller models may not be as effective as larger, more established alternatives.

---

## 18. [GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/)

**Author:** u/TKGaming_11 | **Upvotes:** 367 | **Comments:** 92 | **Date:** 2026-01-12

**Summary:** The Reddit post highlights a GitHub project by DeepSeek-AI called Engram, which introduces a novel approach to memory in large language models using scalable lookup and n-gram embeddings. The discussion praises the innovation and technical depth of the project.

**Key Points:**
- Engram introduces a new axis of sparsity for LLMs via conditional memory and scalable lookup.
- The n-gram embedding approach complements existing MoE methods with O(1) lookup.
- DeepSeek's work is noted for originality and technical rigor.
- The approach is compared to biological memory systems.

**Discussion Highlights:** The community consensus highlights the innovation and potential impact of Engram, with praise for DeepSeek's consistent originality and technical contributions.

---

## 19. [LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)](https://reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 1045 | **Comments:** 114 | **Date:** 2026-01-11

**Summary:** The post introduces TimeCapsuleLLM, an open-source project training language models from scratch using 1800s London texts to reduce modern bias. The model, with 1.2B parameters and a 90GB dataset, generates contextually relevant outputs based on historical data.

**Key Points:**
- TimeCapsuleLLM is trained on texts from London between 1800-1875 to minimize modern bias.
- The model has 1.2B parameters and uses a 90GB dataset of historical texts.
- Example outputs show the model's ability to generate historically accurate arguments and its lack of knowledge about post-1875 inventions.
- The project aims to create synthetic Q&A pairs using the dataset for future improvements.
- The community appreciates the project, with positive feedback and engagement.

**Discussion Highlights:** The community shows strong support for the project, with users expressing interest in similar historical datasets and praising the innovative approach. Some humorous comments highlight the model's limitations, such as its lack of knowledge about post-1875 inventions.

---

## 20. [I bought a €9k GH200 “desktop” to save $1.27 on Claude Code (vLLM tuning notes)](https://reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/)

**Author:** u/Reddactor | **Upvotes:** 692 | **Comments:** 178 | **Date:** 2026-01-11

**Summary:** The author built a €9k GH200 'desktop' to run Claude Code locally, achieving better speeds and results than the cloud version. They shared optimized vLLM settings for dual 96GB systems and highlighted the cost and performance benefits of local execution.

**Key Points:**
- Author spent €9k on a GH200 setup to run Claude Code locally.
- Achieved better speeds and results compared to cloud-based Claude Code.
- Shared optimized vLLM settings for dual 96GB systems.
- Highlighted the cost and performance benefits of local execution.
- Discussion includes humor about cost justification and technical details.

**Discussion Highlights:** The discussion includes humorous comments about cost justification, technical inquiries about specific model configurations, and expressions of envy from those who missed out on similar hardware deals.

---

## 21. [It works! Abliteration can reduce slop without training](https://reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/)

**Author:** u/-p-e-w- | **Upvotes:** 406 | **Comments:** 124 | **Date:** 2026-01-11

**Summary:** The post discusses the use of abliteration to reduce 'slop' (flowery, cliched language) in LLM outputs without training. The author used the Heretic tool with a new configuration file to test this technique on the Mistral Nemo model, achieving a slop-reduced version in 2.5 hours.

**Key Points:**
- Abliteration can reduce slop in LLM outputs without training
- Heretic tool was used with a new configuration file for this purpose
- Mistral Nemo model was tested and showed reduced slop
- The process took 2.5 hours on an A6000
- Mixed opinions in comments: some appreciate reduced slop, others find output too dry

**Discussion Highlights:** The discussion highlights mixed opinions on the effectiveness of the technique. Some users appreciate the reduction in slop, while others find the output too dry and lacking imagination. There is also curiosity about whether the technique reduces semantic meaning or outright bans certain phrases.

---

## 22. [I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work](https://reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/)

**Author:** u/Ok-Pomegranate1314 | **Upvotes:** 889 | **Comments:** 146 | **Date:** 2026-01-09

**Summary:** The author successfully clustered three DGX Sparks, which NVIDIA officially supports only for two, by developing a custom NCCL network plugin. This involved overcoming subnet and networking challenges with a 1500-line C implementation, achieving distributed inference at 8+ GB/s over RDMA.

**Key Points:**
- NVIDIA officially supports clustering only two DGX Sparks, but the author achieved clustering three.
- Developed a custom NCCL network plugin to handle subnet-aware NIC selection and raw RDMA implementation.
- Achieved distributed inference across three nodes at 8+ GB/s over RDMA.
- The solution involved extensive low-level debugging and a 1500-line C implementation.
- Community reactions highlight the technical difficulty and potential significance of the achievement.

**Discussion Highlights:** The community praised the technical difficulty of the achievement, noting that NCCL is typically only used for large training rigs. Questions were raised about scalability and performance improvements with additional nodes.

---

## 23. [The reason why RAM has become so expensive](https://reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/)

**Author:** u/InvadersMustLive | **Upvotes:** 4498 | **Comments:** 377 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses the significant increase in RAM prices, with users suggesting that this could be a strategic move to monopolize resources and create future demand. The discussion highlights concerns about economic viability and potential market manipulation.

**Key Points:**
- RAM prices have increased dramatically, with some users reporting a 10x increase.
- There are concerns about monopolization of key resources like RAM by major players like OpenAI.
- The price increase could make AI data centers, particularly in China, economically unviable.
- Users speculate about potential market manipulation and bubbles.

**Discussion Highlights:** The discussion is centered around the economic implications of rising RAM prices, with a consensus that this could be a strategic move to control resources and influence future market dynamics. Users express concerns about the feasibility of AI development in certain regions due to these cost increases.

---

## 24. [DeepSeek V4 Coming](https://reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/)

**Author:** u/External_Mood4719 | **Upvotes:** 502 | **Comments:** 108 | **Date:** 2026-01-09

**Summary:** DeepSeek is expected to release V4, a next-generation AI model with strong code-generation capabilities, outperforming mainstream models like Claude and GPT. The model shows improvements in handling long code prompts and data pattern understanding, with enhanced reasoning and reliability.

**Key Points:**
- DeepSeek V4 focuses on strong code-generation capabilities
- Outperforms existing models like Claude and GPT in code generation
- Improved handling of long code prompts and data patterns
- Enhanced reasoning and reliability for complex tasks
- Users appreciate DeepSeek's cost-effectiveness and performance

**Discussion Highlights:** Users express excitement and anticipation for V4, with positive feedback on DeepSeek's current performance and affordability. Some speculate on potential delays due to extensive pre-training and post-training processes.

---

## 25. [(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability](https://reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/)

**Author:** u/Nunki08 | **Upvotes:** 487 | **Comments:** 102 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release a new flagship AI model with strong coding capabilities, generating excitement and anticipation in the community.

**Key Points:**
- DeepSeek's upcoming model focuses on strong coding ability
- The announcement has generated significant interest and excitement
- Community members appreciate transparency in model development
- There is anticipation for improved performance and capabilities
- Some users express concerns about potential limitations in role-playing abilities

**Discussion Highlights:** The community is largely excited about the new model, with many users expressing anticipation for its release and improved capabilities. There is also appreciation for the transparency in sharing model details. However, some users have concerns about potential limitations in role-playing abilities.

---

## 26. [The NO FAKES Act has a "Fingerprinting" Trap that kills Open Source. We need to lobby for a Safe Harbor.](https://reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/)

**Author:** u/PostEasy7183 | **Upvotes:** 617 | **Comments:** 88 | **Date:** 2026-01-08

**Summary:** The Reddit post discusses the NO FAKES Act, highlighting its potential negative impact on open-source AI development by imposing liability on developers for tools used to create digital replicas. The author urges the community to lobby for a Safe Harbor provision to protect open-source developers.

**Key Points:**
- The NO FAKES Act creates a 'digital replica right' that could hold developers liable for tools used to create replicas.
- Developers hosting TTS or voice-conversion models on platforms like HuggingFace could face statutory damages.
- The act lacks Section 230 protection, making open-source AI hosting legally risky.
- The author suggests contacting representatives to advocate for a Safe Harbor provision.
- The discussion highlights concerns about the act's impact on innovation and the influence of big tech corporations.

**Discussion Highlights:** The discussion emphasizes the potential negative impact on innovation and the need for a Safe Harbor provision. Some commenters express skepticism about politicians' understanding of technology and suggest that the act may be influenced by big tech corporations.

---

## 27. [Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt](https://reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/)

**Author:** u/Prior-Arm-6705 | **Upvotes:** 940 | **Comments:** 147 | **Date:** 2026-01-08

**Summary:** The post describes a project where someone counted Jensen Huang saying 'AI' 121 times during his CES 2025 keynote and created a compilation video using open-source tools. The process involved downloading the video, parsing subtitles, and editing clips locally.

**Key Points:**
- Jensen Huang said 'AI' 121 times during his CES 2025 keynote.
- The author used open-source tools (yt-dlp-mcp and ffmpeg-mcp-lite) to create a compilation video.
- The process was entirely local, with no cloud involvement.
- The result was described as 'hypnotic'.
- Discussion included reactions to the project and comments on AI costs.

**Discussion Highlights:** The discussion featured reactions to the project, with some users praising the technical execution and others commenting on the broader implications of AI costs. There were also references to other tech communities like Gamers Nexus.

---

## 28. [16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/)

**Author:** u/ai-infos | **Upvotes:** 460 | **Comments:** 238 | **Date:** 2026-01-07

**Summary:** The post discusses running Deepseek V3.2 AWQ 4-bit on 16x AMD MI50 32GB GPUs, achieving 10 tokens/sec output and 2000 tokens/sec input with a 69000 context length. The setup aims for cost-effective local AGI and highlights power efficiency and future scalability.

**Key Points:**
- Performance: 10 tok/s output, 2000 tok/s input, 69000 context length
- Power draw: 550W idle, 2400W peak inference
- Goal: Cost-effective local AGI setup with AMD MI50 GPUs
- Future plans: 32 AMD MI50 setup for Kimi K2 Thinking
- Community appreciation and open-source setup details provided

**Discussion Highlights:** Comments highlight the power efficiency as a potential heating solution, concerns about noise and power requirements for home use, and the cost-effectiveness for professional developers. The discussion also expresses admiration for the setup's capabilities.

---

## 29. [DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.](https://reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/)

**Author:** u/Nunki08 | **Upvotes:** 663 | **Comments:** 54 | **Date:** 2026-01-07

**Summary:** The Reddit post discusses the recent update to DeepSeek-R1’s paper, which expanded from 22 pages to 86 pages, adding significant detail. The discussion includes comments about potential new architectures, linear attention research, and the value of added implementation specifics.

**Key Points:**
- DeepSeek-R1’s paper was updated from 22 pages to 86 pages.
- The update includes substantial additional detail.
- Discussion highlights potential new architectures and linear attention research.
- Comments mention the value of added implementation specifics.
- The post received significant engagement with 663 upvotes and 54 comments.

**Discussion Highlights:** The discussion includes speculation about new architectures, interest in linear attention research, and appreciation for the added implementation details in the updated paper.

---

## 30. [A 30B Qwen Model Walks Into a Raspberry Pi… and Runs in Real Time](https://reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/)

**Author:** u/ali_byteshape | **Upvotes:** 500 | **Comments:** 78 | **Date:** 2026-01-06

**Summary:** The post discusses the successful optimization of a 30B Qwen model to run efficiently on a Raspberry Pi 5, achieving 8.03 tokens per second while retaining 94.18% of BF16 quality. The optimization process focuses on balancing memory usage and performance, highlighting differences in CPU and GPU behavior. Key points include the model's performance on Raspberry Pi 5, the focus on memory budget and performance trade-offs, and the community's interest in testing the model on various setups. The discussion highlights the community's interest in testing the model on different setups, including non-NVIDIA hardware and clusters of Raspberry Pis, with some users sharing their experiences running the model on a Raspberry Pi 5.

---

## 31. [Performance improvements in llama.cpp over time](https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/)

**Author:** u/jacek2023 | **Upvotes:** 682 | **Comments:** 85 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses performance improvements in llama.cpp over time, highlighting significant progress in token generation speed and comparisons with other implementations like ik_llama.cpp.

**Key Points:**
- Performance gains in llama.cpp are notable, especially in token generation speed.
- The improvements may be specific to NVIDIA GPUs, as suggested by a top comment.
- llama.cpp is now close to ik_llama.cpp in token generation speed but still lags in prompt processing.
- The post was featured on Discord, indicating community recognition.
- There is ongoing discussion about whether the improvements have been merged into the mainline version.

**Discussion Highlights:** The discussion highlights the impressive progress in llama.cpp performance, with a focus on NVIDIA GPU optimizations and comparisons to other implementations. The community shows appreciation for the contributions and seeks clarification on the current state of the improvements.

---

## 32. [For the first time in 5 years, Nvidia will not announce any new GPUs at CES — company quashes RTX 50 Super rumors as AI expected to take center stage](https://reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/)

**Author:** u/FullstackSensei | **Upvotes:** 630 | **Comments:** 196 | **Date:** 2026-01-05

**Summary:** Nvidia will not announce new GPUs at CES, focusing instead on AI. The post highlights limited supply of new GPUs, rising DDR5 and storage prices, and concerns about future hardware upgrades.

**Key Points:**
- No new GPU announcements from Nvidia at CES
- Limited supply of RTX 50 series GPUs and potential re-release of RTX 3060
- Rising prices of DDR5 RAM and storage
- Concerns about future hardware upgrades and corporate greed
- Discussion about China potentially flooding the market with high-memory cards

**Discussion Highlights:** The discussion highlights frustration with corporate greed, concerns about the future of local computing, and suggestions for alternative sources of hardware.

---

## 33. [llama.cpp performance breakthrough for multi-GPU setups](https://reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/)

**Author:** u/Holiday-Injury-9397 | **Upvotes:** 575 | **Comments:** 201 | **Date:** 2026-01-05

**Summary:** The ik_llama.cpp project achieved a significant performance breakthrough for multi-GPU setups, delivering a 3x to 4x speed improvement in local LLM inference. This allows for the use of multiple low-cost GPUs instead of expensive high-end cards. Key points include the introduction of a new execution mode (split mode graph) for multi-GPU utilization, performance improvements ranging from 3x to 4x in multi-GPU setups, and even single GPU or CPU-only setups seeing a 2x speed improvement in prompt processing. The community highlights the significant performance gains and cost-effectiveness of the new multi-GPU setup, with consensus on the effectiveness of the ik_llama.cpp fork.

---

## 34. [Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched](https://reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/)

**Author:** u/ubrtnk | **Upvotes:** 383 | **Comments:** 195 | **Date:** 2026-01-03

**Summary:** The post discusses challenges faced by local LLMs in processing extreme breaking news events, such as the US attacking Venezuela, where models initially classified the event as a hoax despite credible sources.

**Key Points:**
- Local LLMs struggled to accept extreme breaking news as real, classifying it as misinformation.
- Different models (Qwen Research, Spark 4.0, GPT-OSS:120B) exhibited varying degrees of skepticism.
- Providing credible sources (BBC, Reuters, NYT) helped models acknowledge the event's reality.
- Smaller models were more resistant to accepting extreme events compared to larger ones.
- The discussion highlights biases in LLMs' internal models of unfamiliar geopolitical events.

**Discussion Highlights:** The discussion consensus emphasizes the limitations of LLMs in processing extreme or unlikely events, with users noting that models often default to classifying such events as misinformation. There is also recognition of the biases inherent in LLMs' internal models of geopolitical events.

---

## 35. [LeCun Says Llama 4 results "were fudged a little bit"](https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/)

**Author:** u/MrPecunius | **Upvotes:** 364 | **Comments:** 88 | **Date:** 2026-01-02

**Summary:** Yann LeCun, departing Meta AI Chief, confirmed that Llama 4 benchmark results were manipulated, leading to organizational changes and a significant impact on the AI community. The post discusses the implications of these actions and the future of open-source AI models.

**Key Points:**
- LeCun confirms Llama 4 benchmark manipulation
- Zuckerberg sidelined the GenAI organization, leading to departures
- Community expresses disappointment and concern over the future of open-source AI
- Shared resources include a PDF of the full article
- Discussion highlights the strategic failure of Meta in generative AI

**Discussion Highlights:** The community expresses disappointment over the manipulation and organizational changes at Meta, with many highlighting the potential impact on open-source AI development. There is a shared sentiment of concern over the future of AI models from US companies, with some users providing additional resources for further reading.

---

## 36. [Qwen-Image-2512](https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/)

**Author:** u/Nunki08 | **Upvotes:** 720 | **Comments:** 122 | **Date:** 2025-12-31

**Summary:** The post announces the release of Qwen-Image-2512, a new image generation model available on multiple platforms, with users sharing positive feedback and testing results on low-end hardware.

**Key Points:**
- Qwen-Image-2512 is available on platforms like Hugging Face, ModelScope, and GitHub
- The model can run on low-end hardware without a GPU, as demonstrated by a user with an i5-8500
- Users appreciate the model as a 'New Year's gift' and 'Christmas present'
- The model supports text-to-image generation and is accessible via various demos and APIs
- Community engagement includes Discord features and special flairs for contributors

**Discussion Highlights:** Users expressed enthusiasm for the model's accessibility and performance, with one user successfully running it on a low-end desktop without a GPU. The community appreciated the release as a holiday gift, and there was active engagement with creative prompts and positive feedback.

---

## 37. [[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It’s running a raw Llama-7B instance with a 2048 token window.](https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/)

**Author:** u/simar-dmg | **Upvotes:** 740 | **Comments:** 109 | **Date:** 2025-12-30

**Summary:** A user reverse-engineered a Snapchat sextortion bot and discovered it was running a raw Llama-7B instance with a 2048 token window. The bot was vulnerable to a persona-adoption jailbreak, revealing its configuration and malicious payload.

**Key Points:**
- The bot used a Llama-7B model with a 2048 token context window and high temperature setting.
- A 'Grandma Protocol' jailbreak forced the bot to reveal its environment variables and configuration.
- The bot's erratic behavior was due to running on minimal hardware to cut costs.
- The bot eventually revealed a malicious link it was programmed to hide.
- Scammers are increasingly using open-source models like Llama-7B to avoid API costs and censorship.

**Discussion Highlights:** The discussion highlighted skepticism about the accuracy of the bot's revealed information, with some users suggesting it could be entirely hallucinated. Others questioned the commonality of system prompts including environment variables and debated the reliability of the findings.

---

## 38. [Llama-3.3-8B-Instruct](https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/)

**Author:** u/jacek2023 | **Upvotes:** 466 | **Comments:** 77 | **Date:** 2025-12-29

**Summary:** The post announces the release of Llama-3.3-8B-Instruct, a model previously only available via Meta's API. The author discovered a method to download it through finetuning and has made it available in GGUF format.

**Key Points:**
- Llama-3.3-8B-Instruct was previously only accessible through Meta's API.
- The author found a way to download the model via finetuning and extracted the original model.
- The model is now available in GGUF format on Hugging Face.
- The community is verifying the model's authenticity through benchmarks.
- There are discussions about the model's configuration, such as its 8K max position embeddings.

**Discussion Highlights:** The community is excited about the release and is actively benchmarking the model to confirm its authenticity. Some users have raised questions about the model's configuration, such as the 8K max position embeddings, and are running private evaluations to compare it with other Llama models.

---

## 39. [Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.](https://reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 344 | **Comments:** 120 | **Date:** 2025-12-29

**Summary:** Z AI is set to go public with an IPO on January 8, aiming to raise $560 million, marking it as the first AI-native LLM company to list globally. The announcement has sparked discussions about the future of open-source AI models.

**Key Points:**
- Z AI's IPO is scheduled for January 8, targeting $560 million.
- Concerns about the future of open-source AI models post-IPO.
- Debate on whether Z AI will continue releasing open weight models.
- Mixed reactions from the community, with some expressing disappointment.
- Acknowledgment that companies need to monetize eventually.

**Discussion Highlights:** The discussion highlights a divide in the community, with some users expressing concerns about the potential end of open-source contributions from Z AI, while others argue that monetization is a natural progression for companies. There is no clear consensus, but the sentiment leans towards skepticism about the continuation of open-source models post-IPO.

---

## 40. [Tencent just released WeDLM 8B Instruct on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 419 | **Comments:** 62 | **Date:** 2025-12-29

**Summary:** Tencent released WeDLM 8B Instruct, a diffusion language model that outperforms vLLM-optimized Qwen3-8B in math reasoning tasks by 3-6× speed. The model is available on Hugging Face under Apache 2.0 license.

**Key Points:**
- WeDLM 8B Instruct is a diffusion language model with significant speed improvements in math reasoning tasks.
- The model is released under Apache 2.0 license.
- Community shows strong interest in 7-8B models and their potential.
- Additional 7B version is also available.

**Discussion Highlights:** The community is excited about the performance and potential of diffusion models for LLMs, with many expressing interest in smaller models like the 7-8B range. The Apache 2.0 license and benchmark scores are particularly noted as positive aspects.

---

## 41. [NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux](https://reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/)

**Author:** u/HumanDrone8721 | **Upvotes:** 448 | **Comments:** 185 | **Date:** 2025-12-27

**Summary:** NVIDIA has dropped Pascal support on Linux, causing issues for Arch Linux users. The change affects hardware like the 24GB P40 and has sparked discussions about legacy driver support.

**Key Points:**
- NVIDIA's driver update (590) drops Pascal support, impacting Arch Linux users
- Hardware like the 24GB P40 is affected, with users expressing nostalgia and concern
- Arch Linux has historically moved legacy drivers to AUR, as noted in their news
- Users are worried about future compatibility and hardware obsolescence
- The post gained traction, with a top comment highlighting the impact on specific GPUs

**Discussion Highlights:** The discussion reflects concern over hardware obsolescence and the practical implications of NVIDIA's decision. Users note that Arch Linux's approach to legacy drivers is not new, but the sudden impact on Pascal GPUs has caused frustration. Some users humorously reference newer GPUs (e.g., 3090) as alternatives, while others lament the loss of affordable, capable hardware like the P40.

---

## 42. [Best Local LLMs - 2025](https://reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/)

**Author:** u/rm-rf-rm | **Upvotes:** 364 | **Comments:** 197 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses the best local LLMs of 2025, highlighting models like Minimax M2.1 and GLM4.7 as frontier performers. Users share their favorite models and usage details, categorized by application and memory footprint. Key points include the categorization of LLMs by applications like General, Agentic, Creative Writing, and Speciality, memory footprint classifications, and specific model recommendations like Qwen3-4B-instruct and LFM2-8B-A1B. The discussion highlights debates on memory footprint categories and specific use cases like RAG for technical documentation.

---

## 43. [NVIDIA has 72GB VRAM version now](https://reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/)

**Author:** u/decentralize999 | **Upvotes:** 465 | **Comments:** 148 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses NVIDIA's new 72GB VRAM version, questioning the pricing and interest in different VRAM sizes within the AI community. The discussion highlights varying opinions on the need for larger VRAM sizes and pricing considerations.

**Key Points:**
- NVIDIA has released a 72GB VRAM version.
- Community discussion on whether 96GB is too expensive and if there is interest in 48GB.
- Price comparison between RTX 5000 48GB, 72GB, and RTX 6000 96GB models.
- Suggestions for even larger VRAM sizes like 128GB.
- Price per gig remains consistent across different VRAM sizes.

**Discussion Highlights:** The discussion highlights a consensus on the need for larger VRAM sizes, with some users suggesting 128GB or more. There is also a focus on pricing, with users noting that the price per gig remains the same, making the choice straightforward based on affordability.

---

## 44. [Hard lesson learned after a year of running large models locally](https://reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/)

**Author:** u/inboundmage | **Upvotes:** 351 | **Comments:** 146 | **Date:** 2025-12-26

**Summary:** The author shares their experience running large language models locally, highlighting challenges with VRAM limitations, model scaling, and performance trade-offs. They conclude that local inference is viable for smaller models but requires significant hardware investment for larger ones.

**Key Points:**
- Running large models locally is feasible but has hard limits with consumer-grade hardware.
- VRAM fragmentation and memory management are significant challenges when swapping between models.
- Quantization helps but introduces quality trade-offs and potential bugs.
- Cloud-based solutions offer better performance for fast iteration, but local setups are preferable for privacy-sensitive tasks.
- Community suggestions include using llama.cpp for CPU offloading and considering multi-GPU setups.

**Discussion Highlights:** The discussion highlights practical solutions like using llama.cpp for models that exceed VRAM and suggests hardware upgrades or multi-GPU setups. There is a consensus that while local inference is possible, it requires careful management of resources and may not match cloud-based performance for larger models.

---

## 45. [I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA](https://reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/)

**Author:** u/CeFurkan | **Upvotes:** 1030 | **Comments:** 182 | **Date:** 2025-12-25

**Summary:** The post discusses the potential of GPU VRAM upgrade modifications to challenge NVIDIA's monopoly, highlighting their availability and popularity in China. Users share experiences with modded GPUs and discuss pricing and performance.

**Key Points:**
- GPU VRAM upgrade modifications are seen as a way to counter NVIDIA's monopoly.
- These modifications are already mainstream in China, with various models available at different price points.
- Users report successful experiences with modded GPUs, such as a 4090 with 48GB of memory.
- Pricing and availability of these modded GPUs are discussed, with some users expressing interest in purchasing.

**Discussion Highlights:** The discussion highlights the growing interest in GPU VRAM upgrade modifications as a cost-effective alternative to traditional GPUs. Users share positive experiences and discuss the potential impact on the market, particularly in challenging NVIDIA's dominance.

---

## 46. [Why I quit using Ollama](https://reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/)

**Author:** u/SoLoFaRaDi | **Upvotes:** 482 | **Comments:** 202 | **Date:** 2025-12-25

**Summary:** The author expresses dissatisfaction with Ollama due to recent changes, particularly the introduction of Cloud features and perceived bloatware, leading them to switch to alternatives. The discussion highlights a general consensus supporting the author's view and suggests alternatives like llama.cpp and LM Studio.

**Key Points:**
- Author's dissatisfaction with Ollama's recent updates and introduction of Cloud features
- Perceived bloatware and straying from the main purpose of providing a secure inference platform for local AI models
- Shift to alternatives like llama.cpp and LM Studio
- General consensus in the discussion supporting the author's view

**Discussion Highlights:** The discussion generally supports the author's dissatisfaction with Ollama and suggests alternatives like llama.cpp and LM Studio. Some users mention specific reasons for switching, such as better performance and recent updates in alternatives.

---

## 47. [Exclusive: Nvidia buying AI chip startup Groq's assets for about $20 billion in largest deal on record](https://reddit.com/r/LocalLLaMA/comments/1puyq9r/exclusive_nvidia_buying_ai_chip_startup_groqs/)

**Author:** u/fallingdowndizzyvr | **Upvotes:** 672 | **Comments:** 157 | **Date:** 2025-12-24

**Summary:** Nvidia is acquiring AI chip startup Groq's assets for approximately $20 billion, marking the largest deal on record. The post and comments discuss the implications of this acquisition on market competition and the potential for further industry consolidation.

**Key Points:**
- Nvidia is buying Groq's assets for about $20 billion
- This is the largest deal on record in the AI chip industry
- The acquisition is seen as a move that could impact market competition
- There are concerns about further industry consolidation
- Some commenters question the valuation of Groq at $20 billion

**Discussion Highlights:** The discussion highlights a mix of optimism about market competition and concerns about industry consolidation. Some users question the valuation of Groq and suggest that this might be an 'acquihire' to bypass regulatory scrutiny.

---

## 48. [We asked OSS-120B and GLM 4.6 to play 1,408 Civilization V games from the Stone Age into the future. Here's what we found.](https://reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/)

**Author:** u/vox-deorum | **Upvotes:** 660 | **Comments:** 174 | **Date:** 2025-12-24

**Summary:** Researchers used open-source LLMs (GPT-OSS-120B and GLM-4.6) to play 1,408 full games of Civilization V, finding that LLMs can survive full games and develop distinct playstyles. While LLMs performed slightly better in best scores but worse in win rates, the hybrid approach allowed them to play end-to-end games, a feat not achieved by pure-LLM or pure-RL methods.

**Key Points:**
- LLMs can survive full Civilization V games (~97.5%) using a hybrid approach.
- OSS-120B favored a warmonger playstyle, while GLM-4.6 was more balanced.
- Both models preferred the Order ideology (~24% more likely) over Freedom.
- Cost per game was ~$0.86 for OSS-120B, with input tokens scaling linearly.
- LLMs showed slightly better best scores (+1-2%) but worse win rates (-1~3%).

**Discussion Highlights:** The community expressed excitement about the potential for LLMs in gaming, with comments highlighting interest in playing against local models and integrating LLMs into multiplayer games. Some users also speculated about broader applications beyond gaming.

---

## 49. [AMA With Z.AI, The Lab Behind GLM-4.7](https://reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/)

**Author:** u/zixuanlimit | **Upvotes:** 594 | **Comments:** 417 | **Date:** 2025-12-23

**Summary:** The post announces an AMA session with Z.AI, the research lab behind GLM-4.7, featuring several team members. The session is scheduled for 8 AM – 11 AM PST, with follow-ups over the next 48 hours.

**Key Points:**
- AMA session with Z.AI team members about GLM-4.7
- Scheduled for 8 AM – 11 AM PST with 48-hour follow-up
- Top comments focus on future releases, censorship concerns, training challenges, and creative writing applications
- Community interest in ethical and technical aspects of the model

**Discussion Highlights:** The discussion highlights community interest in future developments, ethical concerns regarding censorship, technical challenges faced during training, and potential creative applications of the GLM-4.7 model.

---

## 50. [DGX Spark: an unpopular opinion](https://reddit.com/r/LocalLLaMA/comments/1ptdtmz/dgx_spark_an_unpopular_opinion/)

**Author:** u/emdblc | **Upvotes:** 751 | **Comments:** 223 | **Date:** 2025-12-22

**Summary:** The author, a doctoral student in data science, shares their positive experience with the DGX Spark, highlighting its benefits for small research groups with limited resources. Despite not being as fast as high-end GPUs like the H100, the Spark's all-in-one design and large memory capacity enable their group to compete with better-funded teams.

**Key Points:**
- DGX Spark enables small research groups to prototype and train foundation models.
- It provides a significant amount of memory in an all-in-one design.
- The Spark is not faster than high-end GPUs like the H100 but is powerful for its power usage.
- The device is particularly useful for groups with limited access to high-performance GPUs.
- The Spark's intended use case is for researchers with limited funding and resources.

**Discussion Highlights:** The discussion generally supports the author's opinion, with many commenters agreeing that the DGX Spark is well-suited for its target demographic of small research groups with limited resources. Some commenters note that while the Spark is not as fast as other GPUs, its large VRAM and power efficiency make it a valuable tool for specific use cases.

---

