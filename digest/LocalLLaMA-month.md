# r/LocalLLaMA Reading Digest

**Period:** 2026-01-21 to 2026-01-21
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?](https://reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/)

**Author:** u/Adventurous-Gold6413 | **Upvotes:** 389 | **Comments:** 234 | **Date:** 2026-01-20

**Summary:** The post discusses selecting local models for use with 64GB RAM and 16GB VRAM without internet access. Users share their preferred models and experiences.

**Key Points:**
- Users recommend models like GPT-OSS-120B, Gemma 3 27B, and GLM 4.5 Air.
- GPT-OSS-120B is praised for its performance and versatility.
- The discussion highlights the importance of model size and capabilities for local use.
- Some users mention the value of books as an alternative resource.

**Discussion Highlights:** The consensus leans towards using GPT-OSS-120B for its balance of performance and versatility. Other notable mentions include Gemma 3 27B and GLM 4.5 Air. The discussion also touches on the value of books as a resource in an offline environment.

---

## 2. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 732 | **Comments:** 194 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, fully enclosed mobile AI system with 10 GPUs, designed for running large MoE models and supporting graphic design tasks. The build, costing around $17k, features a Threadripper Pro 3995WX, 512GB DDR4, and a mix of 3090 and 5090 GPUs, housed in a Thermaltake Core W200 case for mobility and protection from pets.

**Key Points:**
- Custom-built system with 10 GPUs (8x 3090 + 2x 5090) for large MoE models and graphic design tasks
- Fully enclosed and mobile design to protect hardware and allow easy movement
- Budget-conscious build aiming for high performance without unnecessary expenses
- Notable comments highlight the system's impressive specs and humorous reactions to its portability
- Community appreciation for the unique and practical solution to enclosure and mobility challenges

**Discussion Highlights:** The discussion highlights the community's admiration for the innovative and practical solution to the enclosure and mobility challenges. Notable comments include humorous reactions to the system's portability and appreciation for the build's impressive specifications and cost-effectiveness.

---

## 3. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 359 | **Comments:** 60 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the official support for GLM 4.7 Flash in llama.cpp, highlighting a community effort. The discussion includes notes on performance and alternative implementations. Key points include: official support for GLM 4.7 Flash in llama.cpp has been merged, the implementation was a community effort, performance notes include flash-attention being slow for some users with better results using -fa 0, alternative versions of GLM 4.7 Flash are available on Hugging Face, and the post received recognition with a special flair and was featured on Discord. The discussion highlights the community-driven nature of the implementation and includes performance observations, with users sharing alternative versions and noting that disabling flash-attention (-fa 0) improved speed for some.

---

## 4. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 446 | **Comments:** 156 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights GLM 4.7 Flash as a reliable local agent, praised for its performance in an agentic framework and anticipation for local use via GGUFs. The discussion includes comparisons with other models, performance notes, and positive community reactions. Key points include: GLM 4.7 Flash is praised for its reliability and performance in an agentic framework, the model has been tested extensively without tool calling errors, community members are eager to try it locally and compare it with other models like Nemotron 30B, initial benchmarks suggest it may be as smart as SEED OSS 36B but with better performance due to MoE, and GGUFs for local use are highly anticipated. The discussion highlights a positive consensus around GLM 4.7 Flash, with users sharing their experiences, comparisons with other models, and anticipation for local deployment. Some users have already started testing it locally and note its performance and deep thinking capabilities.

---

## 5. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 717 | **Comments:** 226 | **Date:** 2026-01-19

**Summary:** The Reddit post discusses the release of the zai-org/GLM-4.7-Flash model on Hugging Face, highlighting its technical features and community excitement.

**Key Points:**
- The model is a 30B parameter model with MLA, reducing KV cache memory usage.
- Community members express anticipation and excitement for the release.
- The model supports a full 200k context, making it accessible for many users.

**Discussion Highlights:** The community is enthusiastic about the release, noting its technical advantages like reduced memory usage and large context support.

---

## 6. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 340 | **Comments:** 91 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to stay within a ~10,000€ budget. The system is designed for running large AI models locally, with benchmark results provided for various models. Key points include the system's purpose for local AI model inference, the budget details, benchmark results, community interest in hardware sourcing, and recognition of similar builds. The discussion highlights strong community interest in the hardware setup and a trend in high-VRAM setups for local AI inference.

---

## 7. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 443 | **Comments:** 71 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses a potential slowdown in the development of Qwen 4, with the lead developer emphasizing a focus on quality over quantity. The community generally supports this approach, appreciating the potential for meaningful improvements.

**Key Points:**
- Qwen 4 development may be slowing down to focus on quality.
- Community appreciates the focus on quality over quantity.
- Uncertainty about whether the statement specifically refers to Qwen 4.
- Support for taking necessary time to advance the technology meaningfully.

**Discussion Highlights:** The discussion highlights a positive consensus around the focus on quality, with some users expressing appreciation for the potential improvements and others cautioning against jumping to conclusions based on limited information.

---

## 8. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 528 | **Comments:** 111 | **Date:** 2026-01-17

**Summary:** The author transitioned from MI100 GPUs to R9700 GPUs for a new server build, detailing the specifications and performance benchmarks of the setup. The build includes 128GB VRAM and 128GB RAM, offering high performance at a competitive cost. Key points include the transition from MI100 to R9700 GPUs due to better performance and cost efficiency, detailed specifications of the server build, performance benchmarks showing high token processing rates, and positive community reception with humorous comments about financial irresponsibility. The post received positive feedback, with comments appreciating the build and humorously acknowledging the financial investment required.

---

## 9. [GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)](https://reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/)

**Author:** u/CuriousPlatypus1881 | **Upvotes:** 372 | **Comments:** 89 | **Date:** 2026-01-16

**Summary:** The post discusses the December 2025 SWE-bench leaderboard results, highlighting the performance of various AI models on GitHub PR tasks. Claude Opus 4.5 leads with a 63.3% resolved rate, followed closely by GPT-5.2 and Gemini 3 Flash Preview. The discussion emphasizes the strong showing of open-source models like GLM-4.7 and the impact of high-effort reasoning modes.

**Key Points:**
- Claude Opus 4.5 leads the leaderboard with a 63.3% resolved rate.
- GPT-5.2 (extra high effort) follows closely at 61.5%.
- Gemini 3 Flash Preview outperforms Gemini 3 Pro Preview despite being smaller and cheaper.
- GLM-4.7 is the strongest open-source model, ranking alongside closed models like GPT-5.1-codex.
- GPT-OSS-120B shows significant performance improvement in high-effort reasoning mode.

**Discussion Highlights:** The discussion highlights the surprising performance of Gemini Flash and the excitement around open-source models like GLM-4.7. There is consensus on the benchmark's credibility and anticipation for future model releases like DeepSeek v4.

---

## 10. [I fucking love this community](https://reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/)

**Author:** u/alhinai_03 | **Upvotes:** 500 | **Comments:** 54 | **Date:** 2026-01-16

**Summary:** The author expresses gratitude towards the open-source community for enabling them to run large language models on older hardware, achieving impressive performance metrics. They highlight the importance of system memory and MoE architecture for their setup.

**Key Points:**
- Gratitude towards the open-source community for their contributions
- Achieving 14-13.5 tokens per second on a 10-year-old PC with 4GB VRAM
- Importance of system memory and MoE architecture for running large models
- Community appreciation for optimization efforts
- Discussion on practicality of system RAM and MoE combo

**Discussion Highlights:** The community appreciates the author's achievement and discusses the practicality of using system RAM and MoE architecture for running large models on older hardware. There is a consensus on the effectiveness of these optimizations.

---

## 11. [My story of underestimating /r/LocalLLaMA's thirst for VRAM](https://reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/)

**Author:** u/EmPips | **Upvotes:** 1314 | **Comments:** 88 | **Date:** 2026-01-15

**Summary:** The post discusses the author's experience of underestimating the r/LocalLLaMA community's demand for VRAM, with comments providing additional context and hardware recommendations.

**Key Points:**
- Author underestimated VRAM demand
- Gold rush analogy in comments
- Hardware recommendations discussed
- Post gained significant upvotes and comments

**Discussion Highlights:** The discussion includes a gold rush analogy, hardware recommendations, and appreciation for the author's contribution.

---

## 12. [Latest upgrade…A100 40 GB](https://reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/)

**Author:** u/inserterikhere | **Upvotes:** 401 | **Comments:** 54 | **Date:** 2026-01-15

**Summary:** The user upgraded their gaming rig to an AI-focused setup by acquiring an A100 GPU for $1000, despite it being listed as faulty. The GPU worked immediately, allowing them to run and train larger AI models. The community reacted with a mix of admiration and concern, particularly about cooling the passive-cooled A100.

**Key Points:**
- User transitioned from a gaming rig to an AI rig by repurposing existing parts and upgrading components.
- Purchased an A100 GPU listed as faulty for $1000, which worked perfectly upon installation.
- Community expressed concerns about cooling the passive-cooled A100 GPU.
- The post gained significant attention, with the top comment being a meme and others discussing cooling solutions.

**Discussion Highlights:** The discussion primarily focused on the user's successful upgrade and the potential risks of using a passive-cooled A100 without additional cooling. Some users shared memes and congratulatory messages, while others provided practical advice on cooling solutions.

---

## 13. [NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency](https://reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/)

**Author:** u/Fear_ltself | **Upvotes:** 711 | **Comments:** 129 | **Date:** 2026-01-14

**Summary:** NVIDIA's Orchestrator-8B is an 8-billion-parameter AI model designed to manage and route complex tasks to various tools, emphasizing efficiency and integration over standalone capabilities. The community sees this as a step towards more functional AI systems.

**Key Points:**
- Orchestrator-8B is an 8B parameter model specialized in task management and routing.
- It focuses on connecting with other tools and models rather than answering everything itself.
- The community views this as a correct approach towards building functional AI systems.
- Some comments humorously refer to it as a 'Middle manager LLM'.
- Discussion includes comparisons to other agentic frameworks and models.

**Discussion Highlights:** The discussion highlights a mix of humor and technical insights, with some users comparing the model to middle management and others discussing its potential in agentic frameworks. There is a general consensus that integrating specialized models and tools is a promising direction for AI development.

---

## 14. [GLM-Image is released!](https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/)

**Author:** u/foldl-li | **Upvotes:** 601 | **Comments:** 83 | **Date:** 2026-01-13

**Summary:** GLM-Image is a new image generation model with a hybrid autoregressive + diffusion decoder architecture. It excels in text-rendering and knowledge-intensive tasks while maintaining high-fidelity detail generation. The model supports various image-to-image tasks and is released under an MIT license.

**Key Points:**
- Hybrid autoregressive + diffusion decoder architecture
- Excels in text-rendering and knowledge-intensive generation
- Supports image editing, style transfer, and multi-subject consistency
- Released under MIT license
- Model size: 13GB diffusion model + 20GB text encoder

**Discussion Highlights:** The community appreciates the MIT license and the model's capabilities. There is excitement about its performance compared to other models and its potential for various tasks. Some users are waiting for optimized versions for easier use.

---

## 15. [My wishes for 2026](https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/)

**Author:** u/jacek2023 | **Upvotes:** 656 | **Comments:** 179 | **Date:** 2026-01-13

**Summary:** The Reddit post discusses predictions for 2026, focusing on the feasibility of affordable GPUs with more than 32GB memory. The community responds with skepticism and humor, highlighting the challenges of such advancements.

**Key Points:**
- Discussion about affordable GPUs > 32GB in 2026
- Skepticism and humor in responses regarding feasibility
- Mentions of AI models like Qwen 4 and Mistral
- Community engagement with upvotes and comments

**Discussion Highlights:** The discussion is marked by a mix of humor and skepticism, with users expressing doubt about the affordability of high-memory GPUs. Some comments also mention specific AI models, indicating broader interest in technological advancements.

---

## 16. [kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop—no GPU required](https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/)

**Author:** u/Nunki08 | **Upvotes:** 396 | **Comments:** 92 | **Date:** 2026-01-13

**Summary:** Kyutai introduced Pocket TTS, a 100M-parameter text-to-speech model with high-quality voice cloning that runs on a laptop without requiring a GPU. The model is available on GitHub and Hugging Face, with a blog post and arXiv paper providing more details.

**Key Points:**
- Pocket TTS is a lightweight (100M parameters) TTS model capable of high-quality voice cloning.
- It runs on a laptop without needing a GPU.
- The model is open-source and available on GitHub and Hugging Face.
- Users have raised concerns about memory usage during generation.
- There is interest in fine-tuning the model for different languages.

**Discussion Highlights:** The discussion highlights concerns about memory usage during generation, with one user reporting it ballooning to 32 GB. There is also interest in multilingual support and comparisons with other small TTS models.

---

## 17. [GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/)

**Author:** u/TKGaming_11 | **Upvotes:** 366 | **Comments:** 92 | **Date:** 2026-01-12

**Summary:** The Reddit post highlights a GitHub repository by DeepSeek-AI introducing 'Engram,' a method for conditional memory via scalable lookup in large language models. The discussion praises the innovation and technical approach, noting its potential impact.

**Key Points:**
- DeepSeek-AI's 'Engram' introduces conditional memory via scalable lookup.
- The approach uses n-gram embedding, adding static memory as a complementary sparsity axis.
- The method is praised for its originality and potential to complement existing scaling techniques like MoE.
- The discussion highlights the technical novelty and potential impact of the approach.

**Discussion Highlights:** The discussion consensus highlights the innovation of the n-gram embedding approach and its potential to complement existing scaling methods. Users praise DeepSeek-AI for their original ideas and technical contributions.

---

## 18. [LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)](https://reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 1038 | **Comments:** 114 | **Date:** 2026-01-11

**Summary:** The post introduces TimeCapsuleLLM, an open-source project training language models from scratch on 1800s London texts to reduce modern bias. The 1.2B parameter model uses a 90GB dataset and demonstrates period-specific outputs, such as unfamiliarity with post-1875 concepts.

**Key Points:**
- TimeCapsuleLLM trains models on texts from a single time period and location to minimize modern bias.
- The latest model is trained on 1800-1875 London texts, with 1.2B parameters and a 90GB dataset.
- Example outputs show period-specific behaviors, like treating the telephone as unfamiliar.
- The project is open-source and available on GitHub and Hugging Face.
- Future steps include creating synthetic Q&A pairs from the dataset.

**Discussion Highlights:** The community shows strong support for the project, with comments praising its uniqueness and offering related ideas. Some users share their own experiences with similar datasets, while others humorously reference the model's 1875 cutoff date.

---

## 19. [I bought a €9k GH200 “desktop” to save $1.27 on Claude Code (vLLM tuning notes)](https://reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/)

**Author:** u/Reddactor | **Upvotes:** 693 | **Comments:** 178 | **Date:** 2026-01-11

**Summary:** The author built a €9k GH200 'desktop' to run Claude Code locally, achieving better speeds than Claude Code with Sonnet and sharing optimized vLLM settings for dual 96GB systems. The setup includes blocking telemetry and unnecessary traffic for full offline coding.

**Key Points:**
- Author spent €9k on a GH200 setup to run Claude Code locally.
- Achieved better speeds than Claude Code with Sonnet using local setup.
- Shared optimized vLLM settings for dual 96GB systems.
- Setup includes blocking telemetry and unnecessary traffic for full offline coding.
- Discussion highlights include humor about cost vs. savings and appreciation for the setup.

**Discussion Highlights:** The discussion includes humorous comments about the cost vs. savings, appreciation for the setup, and a question about the specific model used (MiniMax-M2.1 FP8+INT4 AWQ).

---

## 20. [It works! Abliteration can reduce slop without training](https://reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/)

**Author:** u/-p-e-w- | **Upvotes:** 405 | **Comments:** 123 | **Date:** 2026-01-11

**Summary:** The post discusses the use of abliteration to reduce 'slop' (flowery, cliched language) in LLM outputs, specifically with the Mistral Nemo model. The author successfully applied this technique using Heretic, creating a slop-reduced model without fine-tuning. Key points include the effectiveness of abliteration, the use of Heretic for prompt dataset assembly, and community feedback highlighting both the effectiveness and limitations of the approach. The discussion highlights that the community appreciates the reduction in slop but notes that the output can become dry, with debate about whether the technique reduces semantic meaning or merely bans phrases.

---

## 21. [I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work](https://reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/)

**Author:** u/Ok-Pomegranate1314 | **Upvotes:** 891 | **Comments:** 147 | **Date:** 2026-01-09

**Summary:** The author successfully clustered three NVIDIA DGX Sparks, which NVIDIA claimed couldn't be done, by writing a custom NCCL network plugin. This involved overcoming subnet and networking challenges with a 1500-line C implementation, achieving distributed inference at 8+ GB/s over RDMA.

**Key Points:**
- Author clustered three DGX Sparks despite NVIDIA's limitations
- Custom NCCL plugin written in ~1500 lines of C to handle subnet-aware NIC selection and RDMA implementation
- Achieved distributed inference at 8+ GB/s over RDMA
- Project is open-source on GitHub
- Community praised the technical achievement and its potential impact

**Discussion Highlights:** The community highlighted the technical difficulty of working with NCCL and praised the achievement as significant. Questions were raised about scalability and performance gains, indicating strong interest in the solution's broader applicability.

---

## 22. [The reason why RAM has become so expensive](https://reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/)

**Author:** u/InvadersMustLive | **Upvotes:** 4499 | **Comments:** 377 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses the significant increase in RAM prices, with users noting a rise of up to 10 times the previous cost. The discussion highlights concerns about market manipulation and monopolization of key resources by major players like OpenAI. Key points include the dramatic price increase, concerns about monopolization, the strategy to make competitors' data centers economically unviable, and skepticism about the sustainability of the price increase. The discussion consensus suggests that the rise in RAM prices is driven by strategic market control rather than natural supply and demand factors.

---

## 23. [DeepSeek V4 Coming](https://reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/)

**Author:** u/External_Mood4719 | **Upvotes:** 499 | **Comments:** 108 | **Date:** 2026-01-09

**Summary:** DeepSeek is expected to release V4, a next-generation AI model with strong code-generation capabilities, outperforming mainstream models like Claude and GPT. The model shows improvements in handling long code prompts and overall reasoning ability.

**Key Points:**
- DeepSeek V4 focuses on strong code-generation capabilities
- Outperforms existing models like Claude and GPT in code generation
- Improved handling of long code prompts and data patterns
- Enhanced reasoning ability and reliability for complex tasks
- Users appreciate DeepSeek's cost-effectiveness and performance

**Discussion Highlights:** Users express excitement and anticipation for V4, with some noting its potential disruption in the AI space. Many appreciate DeepSeek's affordability and performance, while others speculate on technical advancements like mHC integration and improved post-training methods.

---

## 24. [(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability](https://reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/)

**Author:** u/Nunki08 | **Upvotes:** 489 | **Comments:** 102 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release a new flagship AI model with strong coding capabilities, generating excitement and discussion in the community.

**Key Points:**
- DeepSeek's upcoming model focuses on strong coding ability
- Community excitement and anticipation for the new model
- Discussion about potential competition with OpenAI
- Mixed reactions to typical marketing language used in AI announcements
- Requests for maintaining role-playing capabilities in the new model

**Discussion Highlights:** The community shows enthusiasm for more AI models and competition in the field, with some skepticism about marketing claims and requests for specific feature retention.

---

## 25. [The NO FAKES Act has a "Fingerprinting" Trap that kills Open Source. We need to lobby for a Safe Harbor.](https://reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/)

**Author:** u/PostEasy7183 | **Upvotes:** 612 | **Comments:** 88 | **Date:** 2026-01-08

**Summary:** The NO FAKES Act threatens open-source AI development by imposing liability on developers for tools used to create digital replicas, potentially banning open-source AI hosting in the US. The post urges lobbying for a Safe Harbor provision to protect developers.

**Key Points:**
- The NO FAKES Act creates a 'digital replica right' targeting developers who make tools available for creating replicas.
- Developers could face statutory damages of $5k-$25k per violation, with no Section 230 protection.
- The bill could effectively ban open-source AI hosting, benefiting large corporations.
- The post suggests contacting representatives to advocate for a Safe Harbor provision.
- Comments highlight concerns about the bill's impact on innovation and the influence of big tech.

**Discussion Highlights:** The discussion reflects strong opposition to the bill, with concerns about its impact on innovation and the potential for big tech to monopolize AI development. Many commenters urge action to protect open-source developers.

---

## 26. [Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt](https://reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/)

**Author:** u/Prior-Arm-6705 | **Upvotes:** 929 | **Comments:** 147 | **Date:** 2026-01-08

**Summary:** The post describes a project where someone counted and compiled every instance of Jensen Huang saying 'AI' (121 times) during the NVIDIA CES 2025 keynote using open-source tools. The process involved downloading the video, parsing subtitles, and editing clips to create a compilation video.

**Key Points:**
- Jensen Huang said 'AI' 121 times during the CES 2025 keynote.
- The author used open-source tools (Dive, yt-dlp-mcp, ffmpeg-mcp-lite) to create a compilation video.
- The process involved downloading, parsing subtitles, and editing clips.
- The result was a hypnotic compilation video.
- Top comments included reactions to the post's popularity and humor about the frequency of 'AI'.

**Discussion Highlights:** The discussion included reactions to the post's popularity, humor about the frequency of 'AI', and mentions of related content creators like Gamers Nexus. Some comments also joked about Jensen Huang's attire.

---

## 27. [16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/)

**Author:** u/ai-infos | **Upvotes:** 454 | **Comments:** 238 | **Date:** 2026-01-07

**Summary:** The post discusses running Deepseek V3.2 AWQ 4-bit on 16x AMD MI50 32GB GPUs, achieving 10 tokens/sec output and 2000 tokens/sec input with a 69000 context length. The setup aims for cost-effective local AGI and highlights power efficiency (550W idle / 2400W peak).

**Key Points:**
- Performance: 10 tok/s output, 2000 tok/s input, 69000 context length
- Power draw: 550W idle / 2400W peak inference
- Goal: Cost-effective local AGI setup using 16x AMD MI50 GPUs
- Future plans: 32 AMD MI50 setup for Kimi K2 Thinking
- Community appreciation and open-source setup details provided

**Discussion Highlights:** The discussion highlights the power efficiency of the setup, with comments noting its potential as a heater alternative in winter. Concerns about noise and power requirements at home were raised, while others praised the cost-effectiveness for professional use, comparing it favorably to CPU hardware as RAM prices increase.

---

## 28. [DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.](https://reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/)

**Author:** u/Nunki08 | **Upvotes:** 661 | **Comments:** 54 | **Date:** 2026-01-07

**Summary:** The Reddit post discusses the recent update to DeepSeek-R1's paper, which expanded from 22 pages to 86 pages, adding significant detail. The community is excited about potential new architectures and improvements.

**Key Points:**
- DeepSeek-R1's paper was updated, expanding from 22 pages to 86 pages.
- The update includes substantial additional details.
- Community speculation about new architectures (e.g., dsv4 + r2).
- Interest in how architectural improvements perform at different model sizes.
- Focus on linear attention and cache optimization in current research.

**Discussion Highlights:** The community is enthusiastic about the expanded paper and potential new architectures. There is speculation about future model releases and interest in seeing how improvements scale across different model sizes. The discussion also highlights ongoing research in linear attention and cache optimization.

---

## 29. [A 30B Qwen Model Walks Into a Raspberry Pi… and Runs in Real Time](https://reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/)

**Author:** u/ali_byteshape | **Upvotes:** 496 | **Comments:** 78 | **Date:** 2026-01-06

**Summary:** The post discusses the successful optimization and deployment of the Qwen3-30B-A3B-Instruct-2507 model on a Raspberry Pi 5, achieving 8.03 tokens per second while retaining 94.18% of BF16 quality. The optimization focuses on balancing memory usage and performance, particularly on GPUs where kernel choice significantly impacts speed. Key points include the model's performance on Raspberry Pi 5, optimization strategies, and community feedback on testing different setups and user experiences.

---

## 30. [Performance improvements in llama.cpp over time](https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/)

**Author:** u/jacek2023 | **Upvotes:** 674 | **Comments:** 85 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses performance improvements in llama.cpp over time, focusing on NVIDIA GPU optimizations and comparisons with other implementations.

**Key Points:**
- Performance gains are highlighted for NVIDIA GPUs
- NVIDIA's blog post is referenced for additional details
- Comparisons are made with ik_llama.cpp in terms of token generation speed
- Prompt processing is noted to be slower than token generation

**Discussion Highlights:** The discussion emphasizes significant progress in token generation speed, with ongoing comparisons to alternative implementations like ik_llama.cpp, and references to NVIDIA's optimizations.

---

## 31. [For the first time in 5 years, Nvidia will not announce any new GPUs at CES — company quashes RTX 50 Super rumors as AI expected to take center stage](https://reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/)

**Author:** u/FullstackSensei | **Upvotes:** 626 | **Comments:** 196 | **Date:** 2026-01-05

**Summary:** Nvidia will not announce new GPUs at CES, focusing instead on AI. The post discusses limited supply of new GPUs, potential re-release of older models, and rising hardware prices, expressing concerns about future upgrades.

**Key Points:**
- Nvidia quashes RTX 50 Super rumors, no new GPU announcements at CES
- Limited supply of RTX 5070Ti, 5080, and 5090, with potential re-release of RTX 3060
- Rising prices of DDR5 RAM and storage, making upgrades costly
- Community frustration over corporate greed and lack of consumer-focused announcements
- Concerns about the feasibility of future hardware upgrades

**Discussion Highlights:** The discussion highlights frustration over Nvidia's focus on AI over consumer GPUs, rising hardware costs, and the lack of viable upgrade paths. Many users express concern about corporate greed and the future of local computing.

---

## 32. [llama.cpp performance breakthrough for multi-GPU setups](https://reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/)

**Author:** u/Holiday-Injury-9397 | **Upvotes:** 572 | **Comments:** 201 | **Date:** 2026-01-05

**Summary:** The ik_llama.cpp project has achieved a significant performance breakthrough for multi-GPU setups, delivering a 3x to 4x speed improvement in local LLM inference. This advancement allows for the utilization of multiple low-cost GPUs instead of expensive high-end enterprise cards. Key points include the introduction of a new execution mode (split mode graph) for multi-GPU configurations, enabling simultaneous and maximum utilization of multiple GPUs, cost-effectiveness through the use of multiple low-cost GPUs, performance improvements on single GPU and CPU-only setups, and the project being seen as a game-changer in the context of high GPU and memory prices. The community is excited about the performance gains and cost-effectiveness of the new multi-GPU setup, with consensus on the significant speed improvements and the potential for using low-cost GPUs. Some users have noted performance gains even on single GPU or CPU-only setups.

---

## 33. [Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched](https://reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/)

**Author:** u/ubrtnk | **Upvotes:** 380 | **Comments:** 195 | **Date:** 2026-01-03

**Summary:** The post discusses challenges faced by local LLMs in processing extreme breaking news events, such as the US attacking Venezuela, where models initially classified the event as a hoax despite credible sources. The author shares experiences with different models and their responses to the event.

**Key Points:**
- Local LLMs struggled to accept extreme breaking news as real, classifying it as misinformation.
- Different models (Qwen, Spark, GPT-OSS) had varying responses to the same event.
- Models required explicit credible sources to acknowledge the event's reality.
- Commenters shared similar experiences with LLMs dismissing unlikely but real events.
- Discussion highlights bias in LLMs' internal models of unfamiliar geopolitical events.

**Discussion Highlights:** The discussion consensus suggests that LLMs have inherent biases and struggle with processing extreme or unlikely events, often requiring explicit evidence to accept reality. Commenters noted similar issues with other events and expressed curiosity about future AI historical perspectives.

---

## 34. [LeCun Says Llama 4 results "were fudged a little bit"](https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/)

**Author:** u/MrPecunius | **Upvotes:** 364 | **Comments:** 88 | **Date:** 2026-01-02

**Summary:** Yann LeCun confirmed that Llama 4 benchmarks were manipulated, leading to organizational changes at Meta and a significant impact on the AI community. The post discusses the implications of these actions and the future of open-source AI models.

**Key Points:**
- LeCun confirms Llama 4 benchmark manipulation
- Zuckerberg sidelined the GenAI organization at Meta
- Many employees have left or are planning to leave Meta
- Community expresses disappointment in Meta's handling of Llama
- Shared resources include a PDF of the full article

**Discussion Highlights:** The community expresses disappointment in Meta's actions and the impact on open-source AI. Notable comments include shared resources, discussions on organizational failures, and the future of AI development.

---

## 35. [Qwen-Image-2512](https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/)

**Author:** u/Nunki08 | **Upvotes:** 719 | **Comments:** 122 | **Date:** 2025-12-31

**Summary:** The Reddit post introduces Qwen-Image-2512, a new model available on multiple platforms including Hugging Face, ModelScope, and GitHub. It provides links to guides, demos, and APIs, and highlights user experiences and community feedback.

**Key Points:**
- Qwen-Image-2512 is available on various platforms like Hugging Face, ModelScope, and GitHub.
- The model can be tried out in Qwen Chat and has demos available on Hugging Face and ModelScope.
- Users have successfully run the model on low-end hardware without a GPU.
- The community appreciates the model as a new year's gift.
- Users are experimenting with creative image generation tasks.

**Discussion Highlights:** The discussion highlights include successful usage on low-end hardware, appreciation for the model as a gift, and creative experiments with image generation tasks.

---

## 36. [[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It’s running a raw Llama-7B instance with a 2048 token window.](https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/)

**Author:** u/simar-dmg | **Upvotes:** 739 | **Comments:** 109 | **Date:** 2025-12-30

**Summary:** A user reverse-engineered a Snapchat sextortion bot and discovered it was running a raw Llama-7B instance with a 2048 token window. The bot was vulnerable to a persona-adoption jailbreak, revealing its configuration and malicious payload.

**Key Points:**
- The bot used a Llama-7B model with a 2048 token context window and high temperature setting.
- A persona-adoption jailbreak (Grandma Protocol) forced the bot to reveal its environment variables.
- The bot was running on minimal hardware to reduce costs and avoid API fees.
- The bot eventually revealed a malicious link it was programmed to hide.
- Discussion highlights skepticism about the accuracy of the bot's revealed information.

**Discussion Highlights:** The top comments express skepticism about the bot's revealed information, suggesting it may be entirely hallucinated. Some users question the feasibility of the bot's configuration and the validity of the extracted data.

---

## 37. [Llama-3.3-8B-Instruct](https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/)

**Author:** u/jacek2023 | **Upvotes:** 465 | **Comments:** 77 | **Date:** 2025-12-29

**Summary:** The post announces the discovery and release of the Llama-3.3-8B-Instruct model, which was previously only accessible via Meta's API. The author successfully downloaded and shared the model, sparking community interest and verification efforts.

**Key Points:**
- Llama-3.3-8B-Instruct model was previously only available via Meta's API.
- The author found a way to download the model by exploiting a finetuning feature in the API.
- The model appears to be a genuine new version, not a repackaged older model.
- Community members are running benchmarks to verify the model's authenticity and performance.
- There are questions about the model's configuration, such as its 8K max position embeddings.

**Discussion Highlights:** The community is actively verifying the model's authenticity and performance through benchmarks. There is excitement about the discovery, with some technical questions about the model's configuration and capabilities.

---

## 38. [Tencent just released WeDLM 8B Instruct on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 421 | **Comments:** 62 | **Date:** 2025-12-29

**Summary:** Tencent released WeDLM 8B Instruct on Hugging Face, a diffusion language model that runs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks. The release has garnered significant attention and positive feedback from the community.

**Key Points:**
- WeDLM 8B Instruct is a diffusion language model released by Tencent on Hugging Face.
- It performs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks.
- The model is released under the Apache 2.0 license.
- The community shows strong interest and positive feedback on the model's performance and potential.
- A 7B version of the model is also available.

**Discussion Highlights:** The community is highly interested in the WeDLM 8B Instruct model, with many users expressing surprise and excitement about its performance compared to other models like Qwen3-8B. The Apache 2.0 license and the availability of a 7B version are also highlighted as positive aspects. Overall, the consensus is that this is a promising development in the field of language models.

---

## 39. [NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux](https://reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/)

**Author:** u/HumanDrone8721 | **Upvotes:** 443 | **Comments:** 185 | **Date:** 2025-12-27

**Summary:** NVIDIA has dropped Pascal support on Linux, causing issues for Arch Linux users. The community discusses the impact on Pascal cards like the P40 and Arch Linux's policy on legacy drivers.

**Key Points:**
- NVIDIA's driver update drops support for Pascal GPUs on Linux
- Arch Linux users are affected, with legacy drivers moved to AUR
- Community reactions include concern and acceptance of Arch's policy
- The 24GB P40, a Pascal card, is highlighted as a popular but now expensive option
- Users are reminded of Arch Linux's long-standing practice of moving legacy drivers to AUR

**Discussion Highlights:** The discussion highlights a mix of concern and acceptance. Users acknowledge Arch Linux's policy of moving legacy drivers to the AUR (Arch User Repository) and note that this change was expected. Some users express nostalgia for Pascal cards like the P40, while others joke about the impact on newer GPU users.

---

## 40. [Best Local LLMs - 2025](https://reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/)

**Author:** u/rm-rf-rm | **Upvotes:** 361 | **Comments:** 197 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses the best local LLMs of 2025, highlighting models like Minimax M2.1 and GLM4.7, and categorizes them by application and memory footprint. Users share their experiences and recommendations for various use cases.

**Key Points:**
- Minimax M2.1 and GLM4.7 are noted for their frontier model performance.
- Models are categorized by applications such as General, Agentic, Creative Writing, and Speciality.
- Memory footprint classifications include Unlimited (>128GB VRAM), Medium (8-128GB VRAM), and Small (<8GB VRAM).
- Users recommend small models like Qwen3-4B-instruct and LFM2-8B-A1B for their performance and speed.
- The discussion includes debates on model categories and specific use cases like RAG for technical documentation.

**Discussion Highlights:** The discussion highlights debates on the categorization of models by memory footprint and specific recommendations for small models. Users also discuss applications like creative writing and RAG for technical documentation, showcasing diverse use cases and preferences.

---

## 41. [NVIDIA has 72GB VRAM version now](https://reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/)

**Author:** u/decentralize999 | **Upvotes:** 463 | **Comments:** 148 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses NVIDIA's new 72GB VRAM version, questioning the cost of 96GB and the lack of interest in 48GB options. The community debates the need for larger VRAM capacities and compares pricing and specifications of different models.

**Key Points:**
- NVIDIA has released a 72GB VRAM version
- Community debates the need for 128GB or larger VRAM options
- Price comparison shows similar price per gig across models
- RTX 5000 48GB costs $5100, RTX 5000 72GB costs $7800, and RTX 6000 96GB costs $8300
- Community suggests buying the most VRAM one can afford

**Discussion Highlights:** The discussion highlights a consensus on the need for larger VRAM options, with users comparing specifications and pricing. The community generally agrees that the price per gig is consistent, making the choice straightforward based on affordability.

---

## 42. [Hard lesson learned after a year of running large models locally](https://reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/)

**Author:** u/inboundmage | **Upvotes:** 347 | **Comments:** 146 | **Date:** 2025-12-26

**Summary:** The author shares their experience running large language models locally, highlighting challenges with VRAM limitations, model scaling, and performance trade-offs. They conclude that local inference is viable for smaller models but faces significant constraints without high-end hardware.

**Key Points:**
- Running large models (e.g., 70B parameters) on consumer-grade hardware (RTX 3090) is challenging due to VRAM limitations.
- VRAM fragmentation and inefficient CPU offloading are major issues when scaling beyond 13B models.
- Quantization helps but introduces quality trade-offs and new bugs.
- Local inference is viable for privacy-sensitive tasks but can be slower compared to cloud-based solutions.
- Community suggestions include using llama.cpp for CPU offloading and considering multi-GPU setups.

**Discussion Highlights:** The discussion highlights the limitations of consumer-grade hardware for large model inference and suggests practical solutions like using llama.cpp for CPU offloading and investing in additional GPUs. There is a consensus that while local inference is feasible for smaller models, scaling up requires significant hardware investment.

---

## 43. [I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA](https://reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/)

**Author:** u/CeFurkan | **Upvotes:** 1029 | **Comments:** 182 | **Date:** 2025-12-25

**Summary:** The Reddit post discusses the potential for GPU VRAM upgrade modifications to become mainstream, challenging NVIDIA's monopoly. The discussion highlights the availability of such modifications, particularly in China, with various GPUs being upgraded at different price points.

**Key Points:**
- GPU VRAM upgrade modifications are seen as a way to challenge NVIDIA's monopoly
- These modifications are already mainstream in China
- Alibaba offers upgraded GPUs like 2080Ti, 3080, 4080, 4090, and 5090 with varying VRAM and prices
- Prices range from $300 for a 2080Ti 22GB to $4000 for a 5090 96GB
- Users discuss the feasibility and performance of these modified GPUs

**Discussion Highlights:** The discussion highlights the availability and pricing of modified GPUs in China, with users sharing their experiences and opinions on the feasibility and performance of these upgrades. There is a consensus that these modifications are gaining popularity and could disrupt the market.

---

## 44. [Why I quit using Ollama](https://reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/)

**Author:** u/SoLoFaRaDi | **Upvotes:** 488 | **Comments:** 202 | **Date:** 2025-12-25

**Summary:** The Reddit post discusses the author's decision to stop using Ollama due to recent updates that introduced cloud-based models, which they feel stray from the original purpose of providing a secure platform for local AI models. The discussion highlights a shift in user preference towards alternatives like llama.cpp and LM Studio.

**Key Points:**
- Author's dissatisfaction with Ollama's recent updates and shift towards cloud-based models
- Concerns about privacy implications and bloatware in Ollama
- User preference for alternatives like llama.cpp and LM Studio
- Criticism of Ollama's funding strategy through cloud options
- Discussion consensus favoring llama.cpp for its recent improvements

**Discussion Highlights:** The discussion reflects a consensus favoring alternatives like llama.cpp and LM Studio, with users appreciating their focus on local model inference and recent improvements. There is criticism towards Ollama's shift towards cloud-based models and perceived bloatware.

---

## 45. [Exclusive: Nvidia buying AI chip startup Groq's assets for about $20 billion in largest deal on record](https://reddit.com/r/LocalLLaMA/comments/1puyq9r/exclusive_nvidia_buying_ai_chip_startup_groqs/)

**Author:** u/fallingdowndizzyvr | **Upvotes:** 666 | **Comments:** 157 | **Date:** 2025-12-24

**Summary:** Nvidia is acquiring AI chip startup Groq's assets for approximately $20 billion, marking the largest deal on record. The acquisition has sparked discussions about market competition and consolidation in the AI industry.

**Key Points:**
- Nvidia is buying Groq's assets for about $20 billion
- The deal is the largest on record
- Discussions highlight concerns about market consolidation
- Some commenters question Groq's valuation at $20 billion
- The acquisition is seen as a strategic move by Nvidia

**Discussion Highlights:** The discussion highlights a mix of optimism about market competition and concerns about industry consolidation. Some users question the valuation of Groq, while others see the acquisition as a strategic move by Nvidia. There is also speculation about potential regulatory scrutiny.

---

## 46. [We asked OSS-120B and GLM 4.6 to play 1,408 Civilization V games from the Stone Age into the future. Here's what we found.](https://reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/)

**Author:** u/vox-deorum | **Upvotes:** 654 | **Comments:** 174 | **Date:** 2025-12-24

**Summary:** Researchers used open-source LLMs (GPT-OSS-120B and GLM-4.6) to play 1,408 full games of Civilization V, finding that LLMs can survive full games and develop distinct playstyles. While LLMs showed slight improvements in best scores, their win rates were marginally lower compared to baseline AI. Key points include: LLMs can survive full Civilization V games with a hybrid approach, achieving ~97.5% survival rate; OSS-120B exhibited a warmonger playstyle with more Domination victories, while GLM-4.6 played more balanced; Both models preferred the Order ideology (communist-like) over Freedom (democratic-like); Cost per game was approximately $0.86 for OSS-120B, with input tokens scaling linearly as the game progresses; The study suggests that even smaller models (e.g., OSS-20B) can perform adequately in this hybrid setup. The community expressed excitement about the potential for LLMs to enhance gameplay, with interest in integrating them into multiplayer games. Some users speculated about future applications beyond gaming, while others inquired about the performance of smaller models.

---

## 47. [AMA With Z.AI, The Lab Behind GLM-4.7](https://reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/)

**Author:** u/zixuanlimit | **Upvotes:** 599 | **Comments:** 417 | **Date:** 2025-12-23

**Summary:** The post announces an AMA session with Z.AI, the research lab behind GLM-4.7, featuring key team members. The session aims to address community questions and concerns directly.

**Key Points:**
- AMA session with Z.AI team members scheduled from 8 AM – 11 AM PST
- Community interest in future releases and censorship concerns
- Discussion on training challenges and creative writing applications
- Follow-up on questions promised over the next 48 hours

**Discussion Highlights:** The community shows strong interest in future developments, potential censorship issues, and the practical applications of the model, including creative writing.

---

## 48. [DGX Spark: an unpopular opinion](https://reddit.com/r/LocalLLaMA/comments/1ptdtmz/dgx_spark_an_unpopular_opinion/)

**Author:** u/emdblc | **Upvotes:** 742 | **Comments:** 223 | **Date:** 2025-12-22

**Summary:** The author, a doctoral student in data science, shares their positive experience with the DGX Spark, highlighting its benefits for small research groups with limited resources. Despite not being as fast as high-end GPUs like the H100, the Spark's all-in-one design and large memory capacity enable their group to compete with better-funded teams.

**Key Points:**
- DGX Spark enables small research groups to prototype and train foundation models.
- It provides a massive amount of memory in an all-in-one design.
- While not as fast as high-end GPUs, it is powerful for its power usage.
- The Spark is designed for users with limited access to high-performance GPUs.
- It is particularly useful for groups with limited funding and resources.

**Discussion Highlights:** The discussion generally supports the author's opinion, with many commenters agreeing that the DGX Spark is well-suited for its intended use case. Some commenters note that while it may not be as fast as other options, its large VRAM and power efficiency make it a valuable tool for specific demographics.

---

## 49. [GLM 4.7 is out on HF!](https://reddit.com/r/LocalLLaMA/comments/1pt5heq/glm_47_is_out_on_hf/)

**Author:** u/KvAk_AKPlaysYT | **Upvotes:** 591 | **Comments:** 123 | **Date:** 2025-12-22

**Summary:** The post announces the release of GLM 4.7 on Hugging Face, gaining significant attention with 591 upvotes and 123 comments. The discussion highlights features like diagrams in reasoning and compares it to other models like Gemma 4.

**Key Points:**
- GLM 4.7 released on Hugging Face
- Post gained popularity with 591 upvotes
- Diagrams in reasoning/planning stage noted
- Comparison to Gemma 4 mentioned

**Discussion Highlights:** The discussion emphasizes the novelty of diagrams in the reasoning stage and compares GLM 4.7 to other models, indicating community interest in these features.

---

## 50. [I made Soprano-80M: Stream ultra-realistic TTS in &lt;15ms, up to 2000x realtime, and &lt;1 GB VRAM, released under Apache 2.0!](https://reddit.com/r/LocalLLaMA/comments/1pt3sco/i_made_soprano80m_stream_ultrarealistic_tts_in/)

**Author:** u/eugenekwek | **Upvotes:** 645 | **Comments:** 104 | **Date:** 2025-12-22

**Summary:** Eugene introduced Soprano-80M, a state-of-the-art TTS model designed for ultra-low latency and high-speed audio generation, achieving <15ms latency and up to 2000x realtime speed. The model uses a 32 kHz sample rate and a vocoder-based decoder for superior audio quality and performance.

**Key Points:**
- Soprano-80M achieves <15ms latency and up to 2000x realtime speed.
- Uses a 32 kHz sample rate for clearer audio.
- Employs a vocoder-based decoder for faster audio generation.
- Can generate a 10-hour audiobook in under 20 seconds.
- Released under Apache 2.0 license.

**Discussion Highlights:** Users praised the model's speed and performance, with one user noting it spent minimal time on GPU before generating long audio clips quickly. There were queries about finetuning code and hardware specifications for achieving the reported performance.

---

