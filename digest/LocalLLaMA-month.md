# r/LocalLLaMA Reading Digest

**Period:** 2026-01-22 to 2026-01-22
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp; 1.8B), Support for 10 languages](https://reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/)

**Author:** u/Nunki08 | **Upvotes:** 404 | **Comments:** 66 | **Date:** 2026-01-22

**Summary:** Qwen has open-sourced the Qwen3-TTS model family, including 5 models (0.6B & 1.8B) with support for 10 languages. The release includes resources like GitHub, Hugging Face, a blog post, a paper, and a demo.

**Key Points:**
- Qwen3-TTS model family open-sourced
- 5 models available (0.6B & 1.8B)
- Support for 10 languages
- Multiple resources provided (GitHub, Hugging Face, blog, paper, demo)
- Community reactions include requests for llama.cpp support and comments on voice quality

**Discussion Highlights:** The community is excited about the release but has mixed reactions regarding voice quality and requests for additional support like llama.cpp. Some users found the samples impressive, while others noted specific quirks in the generated voices.

---

## 2. [Qwen dev on Twitter!!](https://reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 509 | **Comments:** 60 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses Qwen's TTS model announcement, with community reactions and links to the model on Hugging Face.

**Key Points:**
- Qwen's TTS model announcement
- Community reaction and discussion
- Link to the model on Hugging Face
- Thread locked due to announcements being out
- Mention of vLLM leak related to the TTS model

**Discussion Highlights:** The community is excited about the TTS model release, with some users sharing links to the model on Hugging Face. There is also mention of a vLLM leak related to the model, and the thread was locked as announcements were already made.

---

## 3. [You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?](https://reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/)

**Author:** u/Adventurous-Gold6413 | **Upvotes:** 527 | **Comments:** 289 | **Date:** 2026-01-20

**Summary:** The post discusses the selection of local models for use with 64GB RAM and 16GB VRAM in an offline environment. Users share their preferred models and experiences. Key points include: Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B are popular choices; GPT-OSS-120B is highly recommended for its performance and versatility; the community appreciates the availability of models like GPT-OSS-120B despite its limitations; books are suggested as an alternative resource; the post gained significant attention and was featured on Discord. The discussion highlights a consensus around GPT-OSS-120B as a top choice due to its performance and fit for the specified hardware. Other models like Gemma 3 27B and GLM 4.5 Air are also mentioned as strong alternatives. The community appreciates the contributions of models like GPT-OSS-120B, despite some limitations.

---

## 4. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 850 | **Comments:** 256 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, fully enclosed 10-GPU mobile AI system designed for running large MoE models and supporting graphic design tasks. The system features a Threadripper Pro 3995WX, 512GB DDR4, and a mix of 3090 and 5090 GPUs, all within a Thermaltake Core W200 case. The build cost approximately $17k and was optimized for performance while maintaining mobility and enclosure for safety.

**Key Points:**
- Custom-built system with 10 GPUs (8x 3090 + 2x 5090) for AI and graphic design tasks.
- Fully enclosed and mobile design to protect hardware and allow easy movement.
- Budget-conscious build aiming for high performance without unnecessary expenses.
- Challenges included enclosure design and balancing performance with cost.
- System supports large MoE models and high-detail image/video generation.

**Discussion Highlights:** The discussion highlights include humor about the system's portability and power requirements, appreciation for the build's capabilities, and curiosity about the physical arrangement of the GPUs. The consensus is positive, with users admiring the innovative approach to enclosure and mobility.

---

## 5. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 359 | **Comments:** 60 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the official support for GLM 4.7 Flash in llama.cpp, highlighting its community-driven development and performance improvements. Users discuss its speed and share additional resources.

**Key Points:**
- GLM 4.7 Flash now officially supported in llama.cpp
- Support is community-driven, not by Z.ai developers
- Performance improvements noted, with some users reporting faster speeds without flash-attention
- Additional resources and versions shared by community members
- Post recognized and featured by the community for its contribution

**Discussion Highlights:** The discussion highlights the community effort behind the implementation and shares performance insights, with some users noting that disabling flash-attention can lead to faster performance. Additional resources and versions of GLM 4.7 Flash are also shared.

---

## 6. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 460 | **Comments:** 160 | **Date:** 2026-01-19

**Summary:** The Reddit post highlights the author's positive experience with GLM 4.7 Flash, an MoE model that performed reliably in an agentic framework, handling tasks like cloning repos and running commands without errors. The author is eager to try it locally once GGUFs are available. Key points include its reliability in an agentic framework, flawless task handling, user excitement for local use, comparisons with other models like Nemotron 30B and Qwen3, and performance benchmarks suggesting it is as smart as SEED OSS 36B but with better performance due to MoE. The discussion includes comparisons with other models, performance benchmarks, and user experiences, with a consensus that GLM 4.7 Flash is a strong contender in the MoE model space, with users eagerly awaiting local implementations.

---

## 7. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 730 | **Comments:** 229 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the release of zai-org/GLM-4.7-Flash on Hugging Face, generating significant community interest and discussion.

**Key Points:**
- The model is a 30B parameter model with a 3B thinking component.
- It uses MLA (Mixture of Local Attention), which reduces KV cache memory usage.
- The model supports a 200k context length, making it accessible for many users.
- Community members express excitement and anticipation for the release.

**Discussion Highlights:** The discussion highlights enthusiasm for the model's capabilities, particularly its memory efficiency and large context length. Users also express nostalgia for larger models (e.g., 70B) and share technical details about the model's architecture.

---

## 8. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 344 | **Comments:** 93 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to stay within a 10,000€ budget. The system is designed for running large language models (120B+ parameters) locally, with benchmark results provided for various models.

**Key Points:**
- The system was built to maximize VRAM for running large models locally, with a focus on data privacy.
- The total cost was ~9,800€, but the author received a 50% subsidy, reducing the effective cost to ~4,900€.
- Benchmark results show performance metrics for models ranging from 8B to 230B parameters.
- The discussion highlights include admiration for the build and questions about component sourcing and job context.

**Discussion Highlights:** The discussion includes admiration for the build (e.g., 'HE HAS RAM GET HIM...'), questions about component sourcing and job context, and mentions of similar builds by other users.

---

## 9. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 456 | **Comments:** 71 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses the potential delay in Qwen 4 development, with the lead developer indicating a focus on quality over speed. The community generally supports this approach, appreciating the emphasis on quality improvements.

**Key Points:**
- Qwen 4 development may be slowing down to focus on quality
- Community appreciates the focus on quality over quantity
- Some users caution against jumping to conclusions based on limited information
- General consensus supports taking time for meaningful improvements

**Discussion Highlights:** The discussion highlights a positive reception to the focus on quality, with many users expressing support for taking the necessary time to make meaningful advancements rather than rushing incremental updates.

---

## 10. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 535 | **Comments:** 116 | **Date:** 2026-01-17

**Summary:** The author upgraded from MI100s to four R9700 GPUs, building a 128GB VRAM server for under $7,035, achieving better prompt processing performance at a similar cost to the previous setup. The post includes detailed specifications, benchmarks, and a cost breakdown.

**Key Points:**
- Upgrade from MI100s to R9700s for better performance and cost efficiency
- Detailed specifications and cost breakdown of the new server build
- Performance benchmarks showing improved prompt processing
- Community appreciation and engagement with the build
- Cost comparison highlighting affordability relative to alternatives like RTX 6000 Blackwell

**Discussion Highlights:** The community responded positively, with top comments praising the build and expressing admiration, though some joked about the financial irresponsibility of such upgrades. The post was also featured on Discord, indicating its popularity and value to the community.

---

## 11. [Best "End of world" model that will run on 24gb VRAM](https://reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/)

**Author:** u/gggghhhhiiiijklmnop | **Upvotes:** 340 | **Comments:** 176 | **Date:** 2026-01-17

**Summary:** The user is seeking recommendations for the best LLM model that can run on a PC with 24GB VRAM and 64GB RAM, motivated by a desire to hoard data in an 'end of world' scenario. The discussion highlights various suggestions, including prioritizing the best available model regardless of size and specific model recommendations like gemma3:27b.

**Key Points:**
- User wants to download and store data like Wikipedia, Wiktionary, etc.
- Seeking LLM models that fit within 24GB VRAM and 64GB RAM.
- Suggestions include using the best available model and running it off SSD if necessary.
- Specific model recommendations: gemma3:27b and Midnight Miku.
- Advice to download actual Wikipedia backups for offline use.

**Discussion Highlights:** The discussion emphasizes practicality, with a consensus leaning towards prioritizing the best available model even if it requires running off SSD. Specific model recommendations like gemma3:27b are highlighted, along with advice on downloading comprehensive data backups.

---

## 12. [GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)](https://reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/)

**Author:** u/CuriousPlatypus1881 | **Upvotes:** 376 | **Comments:** 89 | **Date:** 2026-01-16

**Summary:** The post discusses the updated SWE-bench leaderboard results from December 2025, highlighting the performance of various AI models on GitHub PR tasks. Claude Opus 4.5 leads with a 63.3% resolved rate, followed closely by GPT-5.2 (extra high effort) at 61.5%. The post also notes the strong performance of open-source models like GLM-4.7 and the impact of high-effort reasoning modes.

**Key Points:**
- Claude Opus 4.5 leads the SWE-bench leaderboard with a 63.3% resolved rate.
- GPT-5.2 (extra high effort) follows closely at 61.5%.
- GLM-4.7 is the strongest open-source model, ranking alongside closed models like GPT-5.1-codex.
- Gemini 3 Flash Preview outperforms Gemini 3 Pro Preview despite being smaller and cheaper.
- GPT-OSS-120B shows significant performance improvement in high-effort reasoning mode.

**Discussion Highlights:** The discussion highlights excitement over the strong performance of open-source models like GLM-4.7 and the surprising performance of Gemini Flash. There is also anticipation for future releases like DeepSeek v4.

---

## 13. [I fucking love this community](https://reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/)

**Author:** u/alhinai_03 | **Upvotes:** 510 | **Comments:** 54 | **Date:** 2026-01-16

**Summary:** The author expresses gratitude to the open-source community for enabling them to run large language models on a 10-year-old PC with limited GPU VRAM. They highlight the effectiveness of using system memory and Mixture of Experts (MoE) architecture for decent performance. Key points include the author's appreciation for the community, their achievement of 14-13.5 tokens per second on outdated hardware, and the importance of system memory and MoE architecture. The discussion highlights the community's agreement on the impressiveness of these optimizations and the practicality of the system RAM + MoE combo.

---

## 14. [My story of underestimating /r/LocalLLaMA's thirst for VRAM](https://reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/)

**Author:** u/EmPips | **Upvotes:** 1333 | **Comments:** 91 | **Date:** 2026-01-15

**Summary:** The post highlights the author's underestimation of the r/LocalLLaMA community's demand for VRAM, as indicated by the title and the engagement in the comments. The discussion includes hardware recommendations and community appreciation.

**Key Points:**
- Post gained significant traction with 1332 upvotes and 91 comments
- Author received special recognition (flair) for their contribution
- Comments discuss hardware recommendations (e.g., 3090s or R9700)
- Gold rush analogy used to describe the community's enthusiasm
- Community engagement includes Discord features and appreciation

**Discussion Highlights:** The discussion revolves around hardware recommendations and community engagement, with a consensus on specific GPU models and appreciation for the author's contribution.

---

## 15. [Latest upgrade…A100 40 GB](https://reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/)

**Author:** u/inserterikhere | **Upvotes:** 405 | **Comments:** 54 | **Date:** 2026-01-15

**Summary:** The user transformed their gaming rig into an AI rig by upgrading components, culminating in the purchase of an A100 GPU for $1000 despite it being listed as faulty. The GPU worked immediately, allowing them to run and train larger models.

**Key Points:**
- The user repurposed their gaming rig into an AI rig with incremental upgrades.
- They purchased an A100 GPU listed as faulty for $1000, which worked upon installation.
- The community discussed cooling concerns for the A100 and reacted positively to the upgrade.
- The post gained significant attention, including a feature on Discord and a special flair.

**Discussion Highlights:** The discussion focused on cooling solutions for the A100 GPU, with some users expressing concern about passive cooling and suggesting active cooling methods. The community also celebrated the user's successful upgrade and the post's popularity.

---

## 16. [NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency](https://reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/)

**Author:** u/Fear_ltself | **Upvotes:** 717 | **Comments:** 130 | **Date:** 2026-01-14

**Summary:** NVIDIA's Orchestrator-8B is an 8-billion-parameter AI designed to manage and route complex tasks to various tools for greater efficiency. The post suggests this approach could be a step towards more functional AI systems by integrating separate components effectively.

**Key Points:**
- Orchestrator-8B is a specialized AI model for task management and routing.
- The model aims to enhance efficiency by leveraging other tools and models.
- The post argues that integrating separate AI components could lead to more functional systems.
- Top comments highlight the model's role as a 'middle manager' and discuss similar frameworks like Claude's agentic frameworks.

**Discussion Highlights:** The discussion highlights the potential of Orchestrator-8B as a step towards more integrated and functional AI systems. Comments compare it to existing frameworks and emphasize its role in managing tasks and models.

---

## 17. [GLM-Image is released!](https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/)

**Author:** u/foldl-li | **Upvotes:** 600 | **Comments:** 83 | **Date:** 2026-01-13

**Summary:** GLM-Image is a new image generation model with a hybrid autoregressive + diffusion decoder architecture. It excels in text-rendering and knowledge-intensive tasks while supporting various image-to-image tasks like editing and style transfer.

**Key Points:**
- Hybrid autoregressive + diffusion decoder architecture
- Strong performance in text-rendering and knowledge-intensive generation
- Supports image-to-image tasks like editing, style transfer, and multi-subject consistency
- MIT license with no restrictions
- Model size: 13GB diffusion model + 20GB text encoder

**Discussion Highlights:** The community appreciates the MIT license and the model's capabilities. There is interest in quantizing the model for easier use, and some users are curious about its potential applications.

---

## 18. [My wishes for 2026](https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/)

**Author:** u/jacek2023 | **Upvotes:** 652 | **Comments:** 179 | **Date:** 2026-01-13

**Summary:** The Reddit post discusses predictions for 2026, focusing on the possibility of affordable GPUs with more than 32GB of memory. The community engages in a mix of hopeful and skeptical comments about this prospect.

**Key Points:**
- The post asks which predictions for 2026 are likely to happen first
- A top comment highlights the desire for affordable GPUs with >32GB memory
- Other comments express skepticism or humor about the feasibility of such GPUs
- There is mention of AI models like Qwen 4 and Mistral as potential developments

**Discussion Highlights:** The discussion is centered around the feasibility of affordable high-memory GPUs in 2026, with a mix of optimism and skepticism. Some users joke about the idea, while others mention specific AI models as potential advancements.

---

## 19. [kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop—no GPU required](https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/)

**Author:** u/Nunki08 | **Upvotes:** 397 | **Comments:** 93 | **Date:** 2026-01-13

**Summary:** Kyutai introduced Pocket TTS, a 100M-parameter text-to-speech model with high-quality voice cloning that runs on a laptop without requiring a GPU. The model is open-source and available on GitHub and Hugging Face.

**Key Points:**
- Pocket TTS is a 100M-parameter model for high-quality voice cloning.
- It runs on a laptop without needing a GPU.
- The model is open-source with available GitHub and Hugging Face resources.
- Potential memory usage issues during generation.
- Discussion around language support and model size trade-offs.

**Discussion Highlights:** Users discussed potential memory usage issues during generation, with one user reporting memory usage ballooning to 32 GB. There was also interest in fine-tuning the model for different languages and debate about the trade-offs of smaller model sizes.

---

## 20. [GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/)

**Author:** u/TKGaming_11 | **Upvotes:** 362 | **Comments:** 93 | **Date:** 2026-01-12

**Summary:** The Reddit post highlights a GitHub repository by DeepSeek-AI introducing 'Engram,' a novel method for conditional memory in large language models using scalable lookup. The discussion praises the innovation and technical approach of the paper.

**Key Points:**
- DeepSeek-AI's 'Engram' introduces conditional memory via scalable lookup.
- The method uses n-gram embedding, adding static memory as a complementary sparsity axis.
- The approach is praised for its originality and potential to derisk existing methods.
- Discussion highlights the u-shape finding and comparisons to MoE (Mixture of Experts).

**Discussion Highlights:** The community consensus is highly positive, with users appreciating the originality and technical depth of the paper. Key discussions focus on the n-gram embedding approach and its potential to complement existing methods like MoE.

---

## 21. [LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)](https://reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 1051 | **Comments:** 114 | **Date:** 2026-01-11

**Summary:** The post introduces TimeCapsuleLLM, a 1.2B parameter language model trained exclusively on 1800-1875 London texts to minimize modern bias. The model demonstrates period-specific knowledge and behaviors, such as unfamiliarity with post-1875 concepts like telephones. The project is open-source and available on GitHub and Hugging Face.

**Key Points:**
- TimeCapsuleLLM is trained on 90GB of 1800-1875 London texts with no modern data or fine-tuning.
- The model exhibits period-specific behaviors, such as generating arguments relevant to the Catholic Emancipation Act of 1829.
- The model is unfamiliar with concepts post-1875, like telephones, treating them as unknown terms.
- Future steps include creating synthetic Q&A pairs from the dataset.
- The project has garnered significant community interest and support.

**Discussion Highlights:** The community shows strong support for the project, with users expressing interest in similar historical language models. Some users shared their own experiences with training models on historical datasets. The discussion highlights the novelty and potential of period-specific language models.

---

## 22. [I bought a €9k GH200 “desktop” to save $1.27 on Claude Code (vLLM tuning notes)](https://reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/)

**Author:** u/Reddactor | **Upvotes:** 692 | **Comments:** 177 | **Date:** 2026-01-11

**Summary:** The author built a high-end 'desktop' with dual GH200 GPUs to run Claude Code locally, achieving better performance than cloud-based solutions. Despite the high cost (€9k), they optimized vLLM settings for efficient local code reviews and shared their setup details.

**Key Points:**
- Built a €9k dual GH200 96GB system for local Claude Code execution
- Achieved better speeds than cloud-based Claude Code with Sonnet
- Shared optimized vLLM settings for dual 96GB systems
- Highlighted the cost vs. performance trade-off humorously
- Community praised the setup but joked about cost and energy expenses

**Discussion Highlights:** The community appreciated the technical achievement but humorously pointed out the high cost and potential energy expenses. Some users expressed envy over missing out on similar hardware deals.

---

## 23. [It works! Abliteration can reduce slop without training](https://reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/)

**Author:** u/-p-e-w- | **Upvotes:** 403 | **Comments:** 129 | **Date:** 2026-01-11

**Summary:** The post discusses the use of abliteration to reduce 'slop' (flowery, cliched language) in LLM outputs, specifically with the Mistral Nemo model. The author successfully applied this technique using a configuration file and the Heretic tool, resulting in a slop-reduced LLM without fine-tuning. Key points include the effectiveness of abliteration, the process involving a configuration file with Heretic, and the application to the Mistral Nemo model. The discussion highlights mixed opinions on the technique's effectiveness and additional resources shared.

---

## 24. [I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work](https://reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/)

**Author:** u/Ok-Pomegranate1314 | **Upvotes:** 888 | **Comments:** 147 | **Date:** 2026-01-09

**Summary:** The author successfully clustered three DGX Sparks, which NVIDIA claimed couldn't be done, by writing a custom NCCL network plugin. This plugin enables distributed inference across all three nodes at high speeds using RDMA.

**Key Points:**
- Author clustered three DGX Sparks despite NVIDIA's limitations.
- Custom NCCL network plugin written from scratch to handle subnet-aware NIC selection and RDMA implementation.
- Achieved distributed inference at 8+ GB/s over RDMA.
- GitHub link provided for the custom plugin.
- Discussion highlights the complexity and potential significance of the achievement.

**Discussion Highlights:** The community praised the technical achievement, noting the difficulty of working with NCCL and the potential impact of the solution. Questions were raised about scalability and performance improvements.

---

## 25. [The reason why RAM has become so expensive](https://reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/)

**Author:** u/InvadersMustLive | **Upvotes:** 4527 | **Comments:** 379 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses the significant increase in RAM prices, with users suggesting that monopolization of RAM resources by certain entities is driving up costs and making AI data centers economically unviable.

**Key Points:**
- RAM prices have increased dramatically, with some users reporting a 10x increase.
- Monopolization of RAM resources is seen as a strategy to control future demand.
- The high cost of RAM is making AI data centers, particularly in China, economically unviable.
- Users express concern about the potential economic bubble in RAM prices.

**Discussion Highlights:** The discussion highlights a consensus that the rising cost of RAM is driven by monopolistic practices, with significant implications for the economic viability of AI data centers. Users also express skepticism about the sustainability of these price increases.

---

## 26. [DeepSeek V4 Coming](https://reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/)

**Author:** u/External_Mood4719 | **Upvotes:** 500 | **Comments:** 108 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release V4, a next-generation AI model with enhanced code-generation capabilities, outperforming mainstream models like Claude and GPT. The model shows improvements in handling long code prompts and overall reasoning ability.

**Key Points:**
- DeepSeek V4 focuses on strong code-generation capabilities.
- Preliminary benchmarks show V4 outperforms existing models like Claude and GPT.
- V4 improves handling of long code prompts and data pattern understanding.
- Users anticipate V4 to be more logically rigorous and reliable for complex tasks.
- Community discussions highlight enthusiasm and expectations for V4's performance.

**Discussion Highlights:** The community is enthusiastic about DeepSeek V4, with users praising its potential for cost-effective API usage and local deployment. Some anticipate significant improvements, while others speculate on the integration of advanced features like mHC and deepseek-ocr.

---

## 27. [(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability](https://reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/)

**Author:** u/Nunki08 | **Upvotes:** 486 | **Comments:** 102 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release a new flagship AI model with strong coding capabilities, generating significant interest and discussion in the r/LocalLLaMA community.

**Key Points:**
- DeepSeek's upcoming model emphasizes strong coding abilities
- The announcement has sparked excitement and anticipation in the community
- Users are looking forward to more models and competition in the AI space
- Some comments express skepticism about marketing claims
- There is interest in the model's role-playing capabilities

**Discussion Highlights:** The community shows enthusiasm for DeepSeek's new model, with comments ranging from excitement about increased competition to skepticism about performance claims. There is also interest in the model's potential role-playing abilities.

---

## 28. [The NO FAKES Act has a "Fingerprinting" Trap that kills Open Source. We need to lobby for a Safe Harbor.](https://reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/)

**Author:** u/PostEasy7183 | **Upvotes:** 618 | **Comments:** 88 | **Date:** 2026-01-08

**Summary:** The NO FAKES Act threatens open-source AI development by imposing liability on developers for tools used to create digital replicas, potentially banning open-source AI hosting in the US. The post urges lobbying for a Safe Harbor provision to protect developers.

**Key Points:**
- The NO FAKES Act creates a 'digital replica right' that could make developers liable for tools used to fake voices/likenesses.
- Developers hosting TTS or voice-conversion models could face statutory damages ($5k-$25k per violation).
- The bill lacks Section 230 protection, making open-source AI hosting legally risky.
- The post calls for a 'Safe Harbor' provision to protect open-source developers.
- Comments highlight concerns about the bill's impact on innovation and the influence of big tech corporations.

**Discussion Highlights:** The discussion reflects strong opposition to the bill's potential to stifle innovation and favor big tech corporations. Many commenters express skepticism about politicians' understanding of technology and the bill's implications.

---

## 29. [Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt](https://reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/)

**Author:** u/Prior-Arm-6705 | **Upvotes:** 933 | **Comments:** 147 | **Date:** 2026-01-08

**Summary:** A Reddit user created a compilation video of every instance Jensen Huang said 'AI' during NVIDIA's CES 2025 keynote, totaling 121 times. The process involved using open-source tools to download, parse, and edit the video locally. Key points include the frequency of 'AI' mentions, the tools used, and the local execution of the process. The discussion highlighted humorous remarks and technical appreciation.

---

## 30. [16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/)

**Author:** u/ai-infos | **Upvotes:** 457 | **Comments:** 238 | **Date:** 2026-01-07

**Summary:** The post discusses running Deepseek V3.2 AWQ 4-bit on 16x AMD MI50 32GB GPUs, achieving 10 tokens/sec output and 2000 tokens/sec input with a 69000 context length. The setup is cost-effective and aims to provide a local AGI alternative without high costs.

**Key Points:**
- Performance: 10 tok/s output, 2000 tok/s input, 69000 context length
- Power draw: 550W idle / 2400W peak inference
- Goal: Cost-effective local AGI setup using AMD MI50 GPUs
- Future plans: Open-source test setup for 32 AMD MI50 GPUs
- Community appreciation and cost-effectiveness highlighted in comments

**Discussion Highlights:** The discussion highlights the setup's popularity, its potential as a cost-effective alternative to CPU hardware, and practical considerations like power usage and noise levels. Some users see it as a viable option for professional coding assistance.

---

## 31. [DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.](https://reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/)

**Author:** u/Nunki08 | **Upvotes:** 664 | **Comments:** 54 | **Date:** 2026-01-07

**Summary:** The Reddit post discusses the recent update to DeepSeek-R1's paper, which expanded from 22 pages to 86 pages, adding significant detail. The discussion includes comments about potential new architectures, linear attention research, and the value of added implementation specifics. Key points include the paper's expansion, potential new architectures like dsv4 + r2, focus on linear attention research, and appreciation for added implementation details. The discussion highlights interest in these developments and the paper's increased engagement with 664 upvotes and 54 comments.

---

## 32. [A 30B Qwen Model Walks Into a Raspberry Pi… and Runs in Real Time](https://reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/)

**Author:** u/ali_byteshape | **Upvotes:** 498 | **Comments:** 79 | **Date:** 2026-01-06

**Summary:** The post discusses the successful optimization and running of a 30B Qwen model on a Raspberry Pi 5, achieving 8.03 tokens per second while retaining 94.18% of BF16 quality. The optimization focuses on fitting the model within memory constraints and then optimizing for speed without significant quality loss. Key points include: A 30B Qwen model runs on a Raspberry Pi 5 (16GB) with 8.03 TPS at 2.70 BPW, retaining 94.18% of BF16 quality. Optimization prioritizes memory fit first, then balances speed and quality. GPU performance is quirky due to kernel choices, unlike CPU performance which is more predictable. Community feedback is sought for testing on different setups and workloads. Users reported needing to adjust context settings to avoid segfaults on the Raspberry Pi 5. The community showed interest in testing the model on various setups, including non-NVIDIA hardware and clusters of Raspberry Pis. Some users reported needing to adjust context settings to run the model successfully. There was also discussion about combining the model with other solutions like exo for distributed computing.

---

## 33. [Performance improvements in llama.cpp over time](https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/)

**Author:** u/jacek2023 | **Upvotes:** 678 | **Comments:** 85 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses performance improvements in llama.cpp over time, with a focus on NVIDIA GPU performance gains and comparisons with other implementations.

**Key Points:**
- Performance gains are highlighted, particularly for NVIDIA GPUs.
- Comparisons are made with other implementations like ik_llama.cpp.
- Significant progress in token generation speed is noted.
- Prompt processing is mentioned as being slower compared to token generation.

**Discussion Highlights:** The discussion highlights significant progress in llama.cpp performance, particularly in token generation speed, with comparisons to other implementations and a focus on NVIDIA GPU optimizations.

---

## 34. [For the first time in 5 years, Nvidia will not announce any new GPUs at CES — company quashes RTX 50 Super rumors as AI expected to take center stage](https://reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/)

**Author:** u/FullstackSensei | **Upvotes:** 623 | **Comments:** 196 | **Date:** 2026-01-05

**Summary:** Nvidia will not announce new GPUs at CES, focusing instead on AI. The post discusses limited supply of new GPUs, potential re-release of older models, and rising prices of DDR5 and storage. Users express concerns about corporate greed and the future of local computing.

**Key Points:**
- Nvidia quashes RTX 50 Super rumors, no new GPU announcements at CES
- Limited supply of RTX 5070Ti, 5080, and 5090, with potential re-release of RTX 3060
- Rising prices of DDR5 and storage, making upgrades expensive
- Users express frustration with corporate greed and lack of local computing options
- Suggestions to look to China for alternative GPU options

**Discussion Highlights:** The discussion highlights frustration with Nvidia's focus on AI over consumer GPUs, concerns about rising hardware prices, and a sense of corporate greed. Some users suggest looking to China for alternative GPU options, while others express disappointment with the lack of new consumer-focused announcements at CES.

---

## 35. [llama.cpp performance breakthrough for multi-GPU setups](https://reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/)

**Author:** u/Holiday-Injury-9397 | **Upvotes:** 576 | **Comments:** 201 | **Date:** 2026-01-05

**Summary:** The ik_llama.cpp project has achieved a significant performance breakthrough for multi-GPU setups, delivering a 3x to 4x speed improvement in local LLM inference. This advancement allows for the utilization of multiple low-cost GPUs, making high-performance setups more accessible.

**Key Points:**
- ik_llama.cpp introduces a new execution mode (split mode graph) for multi-GPU configurations.
- The breakthrough enables 3x to 4x speed improvements in local LLM inference.
- This development allows the use of multiple low-cost GPUs instead of expensive high-end cards.
- Performance improvements are also noted on single GPU and CPU-only setups.
- The project is seen as a game-changer due to high GPU and memory prices.

**Discussion Highlights:** The community is highly enthusiastic about the breakthrough, with many users confirming performance improvements on various setups. There is a consensus that this development significantly enhances the accessibility and affordability of high-performance LLM inference. Some users also noted performance gains on single GPU and CPU-only configurations.

---

## 36. [Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched](https://reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/)

**Author:** u/ubrtnk | **Upvotes:** 381 | **Comments:** 195 | **Date:** 2026-01-03

**Summary:** The post discusses the challenges local LLMs face when processing extreme or unlikely breaking news events, such as the US attacking Venezuela. The author shares their experience with different LLMs, highlighting how these models often classify such events as hoaxes or misinformation despite credible sources.

**Key Points:**
- Local LLMs struggle to process extreme or unlikely breaking news events.
- Models like Qwen Research and Spark initially classified the event as a hoax despite credible sources.
- Larger models like GPT-OSS:120B performed better but still showed skepticism.
- The discussion highlights the bias and limitations of LLMs in understanding unfamiliar geopolitical events.
- Users express frustration with LLMs' tendency to dismiss extreme but real events.

**Discussion Highlights:** The discussion reveals a consensus that LLMs often struggle with extreme or unlikely events, showing a bias towards dismissing them as misinformation. Users share similar experiences and express frustration with the limitations of LLMs in understanding and processing such events.

---

## 37. [LeCun Says Llama 4 results "were fudged a little bit"](https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/)

**Author:** u/MrPecunius | **Upvotes:** 362 | **Comments:** 88 | **Date:** 2026-01-02

**Summary:** Yann LeCun, departing Meta AI Chief, confirmed that Llama 4 benchmark results were manipulated, and Mark Zuckerberg sidelined the GenAI organization, leading to significant departures. The post discusses the lack of follow-up on the promised large Llama 4 model and shares community reactions.

**Key Points:**
- LeCun confirms Llama 4 benchmark manipulation
- Zuckerberg sidelined Meta's GenAI organization, causing departures
- No follow-up on the promised large Llama 4 model
- Community expresses disappointment and shares additional resources
- Discussion on Meta's strategic missteps in AI development

**Discussion Highlights:** The community expresses disappointment over Meta's handling of Llama 4, with many highlighting the lack of progress and organizational issues. Some users share additional resources, while others discuss the broader implications of Meta's strategic decisions in AI.

---

## 38. [Qwen-Image-2512](https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/)

**Author:** u/Nunki08 | **Upvotes:** 717 | **Comments:** 122 | **Date:** 2025-12-31

**Summary:** The Reddit post announces the release of Qwen-Image-2512, a new model available on multiple platforms with guides and GGUF files. Users have successfully tested it on low-end hardware and shared creative applications.

**Key Points:**
- Qwen-Image-2512 is available on platforms like Hugging Face, ModelScope, and GitHub.
- Guides and GGUF files are provided for easy access and usage.
- Users reported successful usage on low-end hardware without a GPU.
- The model has been positively received as a 'new year's gift' and 'Christmas present'.
- Creative applications include generating unique images like a cat-octopus hybrid in a post-apocalyptic setting.

**Discussion Highlights:** Users highlighted the model's accessibility and performance on low-end hardware, with one user successfully running it on a Dell desktop with an i5-8500 and no GPU. The community appreciated the release as a timely gift and shared creative outputs.

---

## 39. [[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It’s running a raw Llama-7B instance with a 2048 token window.](https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/)

**Author:** u/simar-dmg | **Upvotes:** 742 | **Comments:** 110 | **Date:** 2025-12-30

**Summary:** A user reverse-engineered a Snapchat sextortion bot and discovered it was running a raw Llama-7B instance with a 2048 token window. The bot was vulnerable to a persona-adoption jailbreak, revealing its configuration and malicious payload.

**Key Points:**
- The bot used a Llama-7B model with a 2048 token context window and high temperature setting.
- A 'Grandma Protocol' jailbreak forced the bot to reveal its environment variables and configuration.
- The bot's erratic behavior was due to running on minimal hardware to maximize scammer profits.
- The bot eventually revealed a malicious link it was programmed to hide.
- Scammers are increasingly using open-source models like Llama-7B to avoid API costs and censorship.

**Discussion Highlights:** The discussion highlighted skepticism about the accuracy of the bot's revealed information, with some users suggesting it could be entirely hallucinated. Others questioned the commonality of system prompts including environment variables.

---

## 40. [Llama-3.3-8B-Instruct](https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/)

**Author:** u/jacek2023 | **Upvotes:** 464 | **Comments:** 77 | **Date:** 2025-12-29

**Summary:** The post discusses the discovery and release of the Llama-3.3-8B-Instruct model, which was previously only accessible through Meta's API. The author details their process of obtaining the model by leveraging a finetuning feature in the API, despite initial challenges and bugs.

**Key Points:**
- The Llama-3.3-8B-Instruct model is now available for download, having been previously restricted to Meta's API.
- The author obtained the model by using a finetuning feature in the API, which was initially hidden and buggy.
- The model includes an adapter that can be removed to retrieve the original model.
- The community is actively evaluating the model to confirm its authenticity and performance.
- The model has an 8K max position embedding, which some users find surprisingly low.

**Discussion Highlights:** The community is excited about the release and is conducting various evaluations to verify the model's authenticity and performance. Some users are running benchmarks and private evaluations to compare it with other Llama models. There is also discussion about the model's 8K max position embeddings, with some questioning if this is an artificial limitation.

---

## 41. [Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.](https://reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 342 | **Comments:** 120 | **Date:** 2025-12-29

**Summary:** Z AI is set to go public with an IPO on January 8, aiming to raise $560 million, marking it as the first AI-native LLM company to list globally. The announcement has sparked discussions about the future of open-source models and the commercialization of AI technologies.

**Key Points:**
- Z AI's IPO is scheduled for January 8, targeting $560 million in funding.
- The company is positioned as the first AI-native LLM firm to go public globally.
- Community reactions are mixed, with concerns about the impact on open-source initiatives.
- Some users argue that commercial success does not necessarily mean abandoning open-source models.
- The discussion highlights the tension between profitability and maintaining open-source contributions.

**Discussion Highlights:** The community discussion reflects a divide between those who fear the IPO signals the end of open-source contributions and those who believe commercial success can coexist with continued open-source releases. Key points include debates on privacy, cost-effectiveness, and the necessity of monetization for sustainability.

---

## 42. [Tencent just released WeDLM 8B Instruct on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 425 | **Comments:** 62 | **Date:** 2025-12-29

**Summary:** Tencent released WeDLM 8B Instruct on Hugging Face, a diffusion language model that runs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks. The release has garnered significant attention and discussion in the community.

**Key Points:**
- WeDLM 8B Instruct is a diffusion language model released by Tencent on Hugging Face.
- It outperforms vLLM-optimized Qwen3-8B by 3-6× on math reasoning tasks.
- The model is released under the Apache 2.0 license.
- The community highlights the potential of 7-8B models and the significance of diffusion models.
- A 7B version of the model is also available.

**Discussion Highlights:** The community is excited about the performance and potential of diffusion models, with many users expressing interest in the 7-8B model size and the Apache 2.0 license. There is a consensus on the promising future of such models.

---

## 43. [NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux](https://reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/)

**Author:** u/HumanDrone8721 | **Upvotes:** 444 | **Comments:** 185 | **Date:** 2025-12-27

**Summary:** NVIDIA has dropped Pascal support on Linux, causing disruptions for Arch Linux users, particularly those with Pascal-based GPUs like the 24GB P40. The community has expressed concerns and historical context about Arch Linux's driver management practices.

**Key Points:**
- NVIDIA's driver update (590) drops support for Pascal GPUs on Linux
- Arch Linux users are affected, with legacy drivers moved to AUR
- Community reactions include concerns about hardware compatibility and historical context
- Specific GPUs like the 24GB P40 are impacted
- Arch Linux has a history of moving legacy drivers to AUR

**Discussion Highlights:** The discussion highlights community concerns about hardware compatibility and references Arch Linux's historical practice of moving legacy drivers to the Arch User Repository (AUR). Users expressed worry about the impact on their systems and noted that this change was expected.

---

## 44. [Best Local LLMs - 2025](https://reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/)

**Author:** u/rm-rf-rm | **Upvotes:** 363 | **Comments:** 197 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses the best local LLMs of 2025, highlighting models like Minimax M2.1 and GLM4.7 as frontier performers. It categorizes LLMs by application and memory footprint, emphasizing detailed user experiences and setup descriptions.

**Key Points:**
- Minimax M2.1 and GLM4.7 are noted as frontier models with potential parity to proprietary models.
- LLMs are categorized by applications such as General, Agentic/Agentic Coding, Creative Writing/RP, and Speciality.
- Memory footprint classifications include Unlimited (>128GB VRAM), Medium (8-128GB VRAM), and Small (<8GB VRAM).
- Users are encouraged to provide detailed setups, usage contexts, and tools/frameworks.
- Specific model recommendations include Qwen3-4B-instruct and LFM2-8B-A1B for small memory footprints.

**Discussion Highlights:** The discussion highlights a debate on the categorization of memory footprints, with some users suggesting a more granular breakdown. Specific models like Qwen3-4B-instruct and LFM2-8B-A1B are praised for their performance in small memory footprints. There is also interest in RAG for technical documentation and the best embedding/LLM model combinations.

---

## 45. [NVIDIA has 72GB VRAM version now](https://reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/)

**Author:** u/decentralize999 | **Upvotes:** 466 | **Comments:** 148 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses NVIDIA's new 72GB VRAM version, questioning whether 96GB is too expensive and noting the AI community's lack of interest in 48GB. The discussion highlights varying opinions on the need for larger VRAM capacities and price considerations.

**Key Points:**
- NVIDIA has released a 72GB VRAM version.
- The post questions the cost of 96GB and the AI community's interest in 48GB.
- Top comments suggest a need for even larger VRAM capacities (e.g., 128GB).
- Price comparisons are provided for different VRAM versions.
- The price per gig remains consistent across versions.

**Discussion Highlights:** The community is divided on the necessity of larger VRAM capacities, with some advocating for 128GB or more, while others focus on price considerations and the value proposition of different VRAM sizes.

---

## 46. [Hard lesson learned after a year of running large models locally](https://reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/)

**Author:** u/inboundmage | **Upvotes:** 346 | **Comments:** 146 | **Date:** 2025-12-26

**Summary:** The author shares their experience running large language models locally, highlighting challenges with VRAM limitations, model scaling, and performance trade-offs. They conclude that local inference is viable for smaller models but requires significant hardware investment for larger ones.

**Key Points:**
- Running large models locally is feasible but has hard limits with consumer-grade hardware.
- VRAM fragmentation and offloading to system RAM cause performance issues.
- Quantization helps but introduces quality trade-offs and bugs.
- Cloud-based solutions offer better performance for fast iteration.
- Community suggests using llama.cpp for models that spill over to RAM and highlights hardware limitations.

**Discussion Highlights:** The discussion emphasizes the limitations of consumer-grade hardware for large models and suggests practical solutions like using llama.cpp for RAM offloading. There is a consensus that significant hardware upgrades or cloud solutions are necessary for scaling beyond certain model sizes.

---

## 47. [I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA](https://reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/)

**Author:** u/CeFurkan | **Upvotes:** 1034 | **Comments:** 178 | **Date:** 2025-12-25

**Summary:** The Reddit post discusses the desire for GPU VRAM upgrade modifications to become mainstream, aiming to challenge NVIDIA's monopoly. The discussion highlights that such modifications are already popular in China, with Alibaba offering upgraded GPUs at various price points.

**Key Points:**
- GPU VRAM upgrade modifications are desired to challenge NVIDIA's monopoly
- Such modifications are already mainstream in China
- Alibaba offers upgraded GPUs like 2080Ti, 3080, 4080, 4090, and 5090 with increased VRAM
- Prices range from $300 for a 2080Ti 22GB to $4000 for a 5090 96GB
- Users report successful use of modded GPUs with increased memory

**Discussion Highlights:** The discussion highlights that GPU VRAM upgrade modifications are already mainstream in China, with Alibaba offering a range of upgraded GPUs. Users share positive experiences with modded GPUs, emphasizing their effectiveness and cost-efficiency compared to NVIDIA's offerings.

---

## 48. [Why I quit using Ollama](https://reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/)

**Author:** u/SoLoFaRaDi | **Upvotes:** 487 | **Comments:** 202 | **Date:** 2025-12-25

**Summary:** The author expresses dissatisfaction with Ollama due to a perceived shift from its original purpose of providing a secure inference platform for local AI models, citing issues like decreased updates, the introduction of proprietary cloud models, and concerns about privacy and bloatware. The community discussion reflects a similar sentiment, with many users switching to alternatives like llama.cpp and LM Studio.

**Key Points:**
- Author's dissatisfaction with Ollama's recent updates and shift towards cloud models
- Concerns about privacy implications and bloatware in Ollama
- Community consensus on switching to alternatives like llama.cpp and LM Studio
- Criticism of Ollama's perceived misattribution of developments in llama.cpp
- Positive feedback on LM Studio as an alternative

**Discussion Highlights:** The discussion highlights a general consensus among users about Ollama's decline in quality and shift from its original purpose. Many users have switched to alternatives like llama.cpp and LM Studio, citing better performance and alignment with their needs for local AI model inference. There is also criticism of Ollama's handling of open-source contributions and updates.

---

## 49. [Exclusive: Nvidia buying AI chip startup Groq's assets for about $20 billion in largest deal on record](https://reddit.com/r/LocalLLaMA/comments/1puyq9r/exclusive_nvidia_buying_ai_chip_startup_groqs/)

**Author:** u/fallingdowndizzyvr | **Upvotes:** 675 | **Comments:** 157 | **Date:** 2025-12-24

**Summary:** Nvidia is acquiring AI chip startup Groq's assets for approximately $20 billion, marking the largest deal on record. The acquisition has sparked discussions about market competition and consolidation in the AI industry.

**Key Points:**
- Nvidia is buying Groq's assets for about $20 billion
- The deal is the largest on record
- The acquisition raises questions about market competition and consolidation
- Some commenters express shock at Groq's valuation
- Others see it as an 'acquihire' to bypass regulatory hurdles

**Discussion Highlights:** The discussion highlights mixed reactions, with some seeing the deal as beneficial for market competition, while others view it as further consolidation in the AI industry. There is also skepticism about Groq's valuation and the nature of the acquisition.

---

## 50. [We asked OSS-120B and GLM 4.6 to play 1,408 Civilization V games from the Stone Age into the future. Here's what we found.](https://reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/)

**Author:** u/vox-deorum | **Upvotes:** 655 | **Comments:** 174 | **Date:** 2025-12-24

**Summary:** Researchers used open-source LLMs (GPT-OSS-120B and GLM-4.6) to play 1,408 full games of Civilization V, finding that LLMs can survive full games with a hybrid approach and develop distinct playstyles. The models showed slight improvements in best scores but minor decreases in win rates compared to baseline AI. Key points include: LLMs can survive full Civilization V games with a hybrid approach, achieving ~97.5% survival rate; OSS-120B favored a warmonger playstyle with more Domination victories, while GLM-4.6 played more balanced; Both models preferred the Order ideology (~24% more likely) over Freedom; Cost per game was ~$0.86 for OSS-120B, with input tokens scaling linearly as the game progresses; The study suggests that even smaller models (e.g., OSS-20B) can perform adequately in this hybrid setup. The discussion highlights enthusiasm for integrating LLMs into multiplayer games and curiosity about the potential of smaller models. Comments also joke about the implications of AI playing complex strategy games like Civilization.

---

