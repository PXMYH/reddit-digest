# r/LocalLLaMA Reading Digest

**Period:** 2026-01-24 to 2026-01-24
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [Your post is getting popular and we just featured it on our Discord!](https://reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/)

**Author:** u/roculus | **Upvotes:** 540 | **Comments:** 58 | **Date:** 2026-01-23

**Summary:** The Reddit post announces that a user's post has been featured on Discord and they have received a special flair. The user expresses annoyance at the bot's public posts and suggests sending private messages instead. The community discusses the bot's behavior and the subreddit's issues.

**Key Points:**
- The bot announces that a user's post has been featured on Discord and they have received a special flair.
- The user finds the bot's public posts annoying and suggests sending private messages instead.
- The community discusses the bot's behavior and the subreddit's issues.
- There is a pinned thread about the Discord that has been there for 5 months.
- Some users suspect the moderators are trying to make money off the community.

**Discussion Highlights:** The community generally agrees that the bot's public posts are annoying. There is speculation about the moderators' motives and the subreddit's issues. Some users find humor in the situation, while others express frustration.

---

## 2. [Am I the only one who feels that, with all the AI boom, everyone is basically doing the same thing?](https://reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/)

**Author:** u/[deleted] | **Upvotes:** 392 | **Comments:** 186 | **Date:** 2026-01-22

**Summary:** The post discusses the redundancy of AI tools and applications during the AI boom, noting that many new tools are less polished versions of existing ones. The discussion highlights the early days of AI technology and the enthusiasm driving shallow implementations. Key points include the low barrier to entry for AI development, the 'hype stage' of AI technology, and the focus on niche tools by some developers. The discussion highlights the early and hype-driven stage of AI technology, with many participants noting the redundancy of AI tools.

---

## 3. [Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp; 1.8B), Support for 10 languages](https://reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/)

**Author:** u/Nunki08 | **Upvotes:** 704 | **Comments:** 110 | **Date:** 2026-01-22

**Summary:** Qwen has open-sourced the Qwen3-TTS model family, including VoiceDesign, CustomVoice, and Base models in 0.6B and 1.8B sizes, supporting 10 languages. The models are available on GitHub, Hugging Face, and include a demo and blog post.

**Key Points:**
- Qwen3-TTS models are open-sourced with 0.6B and 1.8B parameter sizes
- Supports 10 languages and includes VoiceDesign, CustomVoice, and Base models
- Models are available on GitHub and Hugging Face, with a demo and blog post provided
- Community feedback highlights the quality of samples and requests for support in compiled languages like llama.cpp
- Positive reception for Qwen's open-sourcing efforts and model performance

**Discussion Highlights:** The community appreciates Qwen's open-sourcing efforts and the quality of the TTS samples. Some users noted that English speakers sound like they were trained on Japanese anime dubs. There is a strong request for support in compiled languages like llama.cpp for easier local execution. Overall, the release is well-received, with users expressing happiness about the ability to run models at home.

---

## 4. [Qwen dev on Twitter!!](https://reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 723 | **Comments:** 60 | **Date:** 2026-01-22

**Summary:** The Reddit post discusses Qwen's TTS model announcement, with community reactions and clarifications about the model's origin and availability.

**Key Points:**
- Qwen's TTS model announcement on Twitter
- Clarification that it's the TTS model from the vLLM leak
- Reference to the model on Hugging Face
- Community discussion and reactions to the announcement

**Discussion Highlights:** The community initially had mixed reactions, but clarifications were provided about the model's origin and availability. The discussion highlights include references to the model's source and its availability on Hugging Face.

---

## 5. [You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?](https://reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/)

**Author:** u/Adventurous-Gold6413 | **Upvotes:** 539 | **Comments:** 306 | **Date:** 2026-01-20

**Summary:** The post discusses selecting local models for use with 64GB RAM and 16GB VRAM without internet access. Users share their preferred models and experiences.

**Key Points:**
- The post asks for recommendations on local models to use with specific hardware constraints.
- Top comments highlight models like Gemma 3 27B, GLM 4.5 Air, and GPT-OSS 120B.
- GPT-OSS-120B is praised for its performance and versatility on the given hardware.
- The discussion includes appreciation for open-source contributions to AI models.

**Discussion Highlights:** The consensus leans towards GPT-OSS-120B as a top choice due to its performance and compatibility with the specified hardware. Other notable mentions include Gemma 3 27B and GLM 4.5 Air. The discussion also appreciates the availability of open-source models.

---

## 6. [768Gb Fully Enclosed 10x GPU Mobile AI Build](https://reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/)

**Author:** u/SweetHomeAbalama0 | **Upvotes:** 891 | **Comments:** 268 | **Date:** 2026-01-20

**Summary:** The post describes a custom-built, fully enclosed 10-GPU mobile AI system designed for running large MoE models and supporting graphic design tasks. The system, built with a Threadripper Pro 3995WX, 512GB DDR4, and a mix of 3090 and 5090 GPUs, balances performance and budget constraints while addressing mobility and enclosure challenges.

**Key Points:**
- The system is designed for large MoE models and graphic design tasks, with a focus on mobility and enclosure.
- It uses a mix of 3090 and 5090 GPUs to balance performance and budget, avoiding unnecessary expenses.
- The enclosure was a critical requirement due to the presence of cats, ruling out mining frames for aesthetic and safety reasons.
- The build cost approximately $17k, with potential savings if fewer high-end GPUs were used.
- The post received significant engagement, with comments highlighting the system's power and practicality.

**Discussion Highlights:** The discussion highlights the system's impressive capabilities and practicality, with comments praising its power and the creative solutions to enclosure and mobility challenges. Some comments humorously reference the system's portability and power requirements.

---

## 7. [GLM 4.7 Flash official support merged in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/)

**Author:** u/ayylmaonade | **Upvotes:** 364 | **Comments:** 60 | **Date:** 2026-01-19

**Summary:** The Reddit post announces the official support for GLM 4.7 Flash in llama.cpp, highlighting its community-driven development and performance improvements.

**Key Points:**
- GLM 4.7 Flash now officially supported in llama.cpp
- Support is community-driven, not by Z.ai developers
- Performance improvements noted, with some users reporting faster speeds without flash-attention
- Additional versions of GLM 4.7 Flash available on Hugging Face

**Discussion Highlights:** The discussion highlights the community effort behind the implementation and notes performance improvements, with some users reporting better speeds without flash-attention. There is also mention of additional versions of GLM 4.7 Flash being available on Hugging Face.

---

## 8. [My gpu poor comrades, GLM 4.7 Flash is your local agent](https://reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/)

**Author:** u/__Maximum__ | **Upvotes:** 463 | **Comments:** 162 | **Date:** 2026-01-19

**Summary:** The author highlights GLM 4.7 Flash as a reliable local agent, outperforming other MoE models in an agentic framework, with successful execution of tasks like cloning repos and running commands. The community discusses comparisons with other models, performance benchmarks, and anticipation for local GGUF versions.

**Key Points:**
- GLM 4.7 Flash is praised for its reliability and performance in agentic tasks
- The model successfully handles complex tasks like GitHub operations and command execution
- Community interest in comparisons with Nemotron 30B and other models
- Performance benchmarks indicate it may rival SEED OSS 36B with better efficiency
- Anticipation for local GGUF versions and ongoing testing in llama.cpp

**Discussion Highlights:** The discussion highlights enthusiasm for GLM 4.7 Flash's capabilities, with users sharing performance notes, comparisons to other models, and links to early GGUF conversions. There's a consensus on its potential as a high-performing local agent, though some await further benchmarks.

---

## 9. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 737 | **Comments:** 230 | **Date:** 2026-01-19

**Summary:** The Reddit post discusses the release of the GLM-4.7-Flash model on Hugging Face, highlighting its technical features and community anticipation.

**Key Points:**
- The model uses MLA, reducing KV cache memory usage.
- It supports a full 200k context, making it accessible for many users.
- The community expresses excitement and nostalgia for larger models.
- The post gained significant attention with 738 upvotes and 230 comments.

**Discussion Highlights:** The discussion emphasizes the model's efficiency and potential, with users appreciating its capabilities and expressing enthusiasm for its release.

---

## 10. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 348 | **Comments:** 101 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to stay within a ~10,000€ budget. The system is designed for running large language models (120B+ parameters) locally, with benchmark results provided for various models. Key points include the system's purpose for local large model inference, the budget details, benchmark results, community interest in hardware sourcing, and recognition of similar builds. The discussion highlights strong community interest in the hardware setup and a trend in high-VRAM systems for local LLM inference.

---

## 11. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 452 | **Comments:** 71 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses the potential delay in Qwen 4 development, with the lead developer indicating a focus on quality over speed. The community generally supports this approach, appreciating the emphasis on quality improvements.

**Key Points:**
- Qwen 4 development may be slowing down to focus on quality
- Community appreciates the focus on quality over quantity
- Uncertainty about whether the statement specifically refers to Qwen 4
- Support for taking necessary time to make meaningful improvements
- Mixed reactions with some cautioning against speculative rumors

**Discussion Highlights:** The discussion highlights a consensus among users that prioritizing quality in AI development is beneficial. Many users expressed support for the decision to slow down, emphasizing the importance of meaningful advancements over frequent incremental updates. However, some users urged caution against jumping to conclusions based on limited information.

---

## 12. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 539 | **Comments:** 117 | **Date:** 2026-01-17

**Summary:** The author transitioned from MI100 GPUs to R9700 GPUs for better performance and cost efficiency, detailing a new server build with 128GB VRAM and 128GB RAM. Benchmarks show impressive performance metrics for the setup. Key points include the transition from MI100 to R9700 GPUs, detailed specifications of the new server build, performance benchmarks provided, and community appreciation with jokes about financial irresponsibility. The community appreciated the detailed build and benchmarks, with some users joking about the financial irresponsibility of such high-end setups.

---

## 13. [Best "End of world" model that will run on 24gb VRAM](https://reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/)

**Author:** u/gggghhhhiiiijklmnop | **Upvotes:** 339 | **Comments:** 180 | **Date:** 2026-01-17

**Summary:** The post discusses finding the best 'end of world' model that can run on a PC with 24GB VRAM and 64GB RAM, with suggestions including Gemma3:27b and downloading Wikipedia backups.

**Key Points:**
- User seeks models fitting 24GB VRAM/64GB RAM
- Gemma3:27b recommended for its capabilities
- Suggestions to download Wikipedia backups for offline use
- Mention of running models off SSD if necessary
- Humorous reference to Midnight Miku for entertainment

**Discussion Highlights:** The discussion highlights Gemma3:27b as a top recommendation due to its vision capabilities and suggests practical steps like downloading Wikipedia backups for offline access. There's also a lighthearted tone with references to entertainment options.

---

## 14. [GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)](https://reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/)

**Author:** u/CuriousPlatypus1881 | **Upvotes:** 379 | **Comments:** 89 | **Date:** 2026-01-16

**Summary:** The post discusses the updated SWE-bench leaderboard results from December 2025, highlighting the performance of various AI models on GitHub PR tasks. Claude Opus 4.5 leads with a 63.3% resolved rate, followed closely by GPT-5.2 (extra high effort) at 61.5%. The post also notes the strong performance of open-source models like GLM-4.7. Key points include the performance of Claude Opus 4.5, GPT-5.2, Gemini 3 Flash Preview, GLM-4.7, and GPT-OSS-120B. The discussion highlights excitement around Gemini Flash's performance and the strong showing of open-source models like GLM-4.7, with anticipation for future releases like DeepSeek v4.

---

## 15. [I fucking love this community](https://reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/)

**Author:** u/alhinai_03 | **Upvotes:** 522 | **Comments:** 54 | **Date:** 2026-01-16

**Summary:** The author expresses gratitude towards the open-source community for enabling them to run large language models on older hardware, highlighting the performance of the nemotron-3-nano-30B-a3b-iq4_nl model on a 10-year-old PC with limited VRAM.

**Key Points:**
- Gratitude towards the open-source community and contributors
- Running large models on older hardware with limited VRAM
- Importance of system memory and MoE architecture for performance
- Achieving 14-13.5 tokens per second on a 10-year-old rig
- Community appreciation for optimization efforts

**Discussion Highlights:** The community appreciates the author's achievement and highlights the importance of system memory and MoE architecture for running large models on limited hardware. There is consensus on the practicality of this setup and admiration for the optimization efforts within the community.

---

## 16. [My story of underestimating /r/LocalLLaMA's thirst for VRAM](https://reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/)

**Author:** u/EmPips | **Upvotes:** 1357 | **Comments:** 91 | **Date:** 2026-01-15

**Summary:** The post highlights the author's underestimation of the r/LocalLLaMA community's demand for VRAM, with discussions focusing on hardware recommendations and market dynamics.

**Key Points:**
- Author underestimated VRAM demand
- Hardware recommendations (3090s, R9700)
- Market behavior (selling cards)
- Community engagement (Discord feature, special flair)

**Discussion Highlights:** The discussion includes hardware recommendations, market behavior insights, and community engagement details.

---

## 17. [Latest upgrade…A100 40 GB](https://reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/)

**Author:** u/inserterikhere | **Upvotes:** 407 | **Comments:** 54 | **Date:** 2026-01-15

**Summary:** The user upgraded their gaming rig to an AI rig by acquiring an A100 GPU, despite it being listed as faulty, and successfully integrated it into their setup. The post gained significant attention in the r/LocalLLaMA community.

**Key Points:**
- The user transitioned from a gaming rig to an AI rig using existing parts and new acquisitions like a 3090 and 7950x.
- They purchased an A100 GPU listed as faulty for $1000, which worked perfectly upon installation.
- The community showed interest in the upgrade, with concerns raised about cooling the A100.
- The post received positive engagement, including a special flair and feature on Discord.

**Discussion Highlights:** The community was impressed by the upgrade and engaged in discussions about cooling solutions for the A100, with some users sharing memes and others offering practical advice.

---

## 18. [Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M](https://reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/)

**Author:** u/eugenekwek | **Upvotes:** 326 | **Comments:** 54 | **Date:** 2026-01-14

**Summary:** The post announces Soprano 1.1, an improved version of the Soprano TTS model with 95% fewer hallucinations and a 63% preference rate over the previous version. The model also features better audio quality, reduced artifacts, and support for longer sentences.

**Key Points:**
- Soprano 1.1 reduces hallucinations by 95% and has a 63% preference rate over Soprano-80M
- Improved audio quality with 50% lower WER and reduced artifacts
- Supports sentences up to 30 seconds long, up from 15 seconds
- Positive community feedback on the model's performance and usability
- Inquiries about future support, such as ONNX compatibility

**Discussion Highlights:** The community responded positively to the announcement, praising the model's performance and usability. There were inquiries about future support and features, such as ONNX compatibility, and appreciation for the developer's efforts.

---

## 19. [NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency](https://reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/)

**Author:** u/Fear_ltself | **Upvotes:** 719 | **Comments:** 130 | **Date:** 2026-01-14

**Summary:** NVIDIA's Orchestrator-8B is an 8-billion-parameter AI model designed to manage and route complex tasks to various tools for greater efficiency. The post discusses its potential as a step towards more functional AI systems.

**Key Points:**
- Orchestrator-8B is a specialized 8B model for task management and routing.
- It aims to connect with other tools and models for efficient task handling.
- The post suggests this approach could be a step towards AGI by integrating separate pieces.
- Top comments highlight its role as a 'middle manager' and compare it to existing agentic frameworks.
- Discussion includes mentions of similar concepts and future possibilities like model hierarchies.

**Discussion Highlights:** The discussion highlights the model's role as a task manager and compares it to existing frameworks. There's consensus on its potential for creating more functional AI systems through integration and task delegation.

---

## 20. [GLM-Image is released!](https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/)

**Author:** u/foldl-li | **Upvotes:** 604 | **Comments:** 83 | **Date:** 2026-01-13

**Summary:** GLM-Image is a new image generation model with a hybrid autoregressive + diffusion decoder architecture. It excels in text-rendering and knowledge-intensive tasks while supporting various image-to-image tasks like editing and style transfer.

**Key Points:**
- Hybrid autoregressive + diffusion decoder architecture
- Excels in text-rendering and knowledge-intensive generation
- Supports image-to-image tasks like editing and style transfer
- MIT license with no restrictions
- Model size: 13GB diffusion model + 20GB text encoder

**Discussion Highlights:** The community appreciates the MIT license and the model's capabilities. There is interest in quantizing the model for easier use and discussions about its performance compared to other models.

---

## 21. [My wishes for 2026](https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/)

**Author:** u/jacek2023 | **Upvotes:** 655 | **Comments:** 179 | **Date:** 2026-01-13

**Summary:** The Reddit post discusses predictions for 2026, focusing on the possibility of affordable GPUs with more than 32GB of memory. The community engages in a mix of hopeful and skeptical comments about this prospect.

**Key Points:**
- The post asks which predictions for 2026 are likely to happen first.
- A major focus is on the availability of affordable GPUs with >32GB memory.
- Comments range from humorous skepticism to hopeful speculation.
- Mentions of AI models like Qwen 4 and Mistral as potential developments.
- The post gained significant traction with 655 upvotes and 179 comments.

**Discussion Highlights:** The discussion is marked by a mix of humor and skepticism regarding the feasibility of affordable high-memory GPUs in 2026. Some users joke about the idea being a fantasy, while others speculate on potential AI model advancements. The overall tone is lighthearted but engaged, with a notable comment highlighting the post's popularity on Discord.

---

## 22. [kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop—no GPU required](https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/)

**Author:** u/Nunki08 | **Upvotes:** 400 | **Comments:** 93 | **Date:** 2026-01-13

**Summary:** Kyutai introduced Pocket TTS, a 100M-parameter text-to-speech model with high-quality voice cloning that runs on a laptop without requiring a GPU. The model is available on GitHub and Hugging Face, with a blog post and arXiv paper providing more details.

**Key Points:**
- 100M-parameter TTS model with high-quality voice cloning
- Runs on laptop without GPU
- Available on GitHub, Hugging Face, and arXiv
- Memory usage warning for localhost test server
- Discussion on language support and model size trade-offs

**Discussion Highlights:** The discussion highlights a memory usage warning for the localhost test server, inquiries about language support, and comparisons with other small models. Users appreciate the model's capabilities but note potential memory issues and the trade-offs of smaller model sizes.

---

## 23. [GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/)

**Author:** u/TKGaming_11 | **Upvotes:** 366 | **Comments:** 93 | **Date:** 2026-01-12

**Summary:** The Reddit post highlights a new GitHub repository by DeepSeek-AI called 'Engram,' which introduces a novel approach to conditional memory in large language models using scalable lookup and n-gram embedding. The discussion praises the innovation and technical depth of the paper.

**Key Points:**
- DeepSeek-AI's 'Engram' introduces conditional memory via scalable lookup as a new sparsity axis for LLMs.
- The approach uses n-gram embedding and mHC (with M=4) for ablations, adding static memory with O(1) lookup.
- The community appreciates the originality and technical rigor of DeepSeek's work.
- Comparisons are drawn to biological memory processes, suggesting natural inspiration.

**Discussion Highlights:** The discussion is overwhelmingly positive, with users highlighting the innovation of the n-gram embedding approach and its potential to complement existing sparsity methods like MoE. There is consensus on the technical depth and originality of DeepSeek's work, with some users noting the biological plausibility of the approach.

---

## 24. [LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)](https://reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 1057 | **Comments:** 114 | **Date:** 2026-01-11

**Summary:** The post introduces TimeCapsuleLLM, a 1.2B parameter language model trained exclusively on 1800-1875 London texts to minimize modern bias. The model demonstrates period-specific knowledge and limitations, such as unfamiliarity with post-1875 concepts like telephones. The project is open-source and available on GitHub and Hugging Face.

**Key Points:**
- TimeCapsuleLLM is trained on 90GB of 1800-1875 London texts with no modern data or fine-tuning.
- The model shows period-specific behaviors, like generating arguments relevant to the Catholic Emancipation Act of 1829.
- The model is unfamiliar with concepts post-1875, such as telephones, treating them as unknown terms.
- Future steps include creating synthetic Q&A pairs from the dataset.
- The project has garnered significant community interest and support.

**Discussion Highlights:** The community shows strong enthusiasm for the project, with comments praising its uniqueness and potential. Some users share similar interests in training models on historical datasets, and there is humor around the model's 1875 knowledge cutoff.

---

## 25. [I bought a €9k GH200 “desktop” to save $1.27 on Claude Code (vLLM tuning notes)](https://reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/)

**Author:** u/Reddactor | **Upvotes:** 692 | **Comments:** 177 | **Date:** 2026-01-11

**Summary:** The author built a €9k GH200 'desktop' to run Claude Code locally, achieving better speeds and results than the cloud version. They shared optimized vLLM settings for dual 96GB systems and highlighted the cost savings and performance benefits of local execution.

**Key Points:**
- Author spent €9k on a GH200 setup to run Claude Code locally.
- Achieved better speeds and results compared to cloud-based Claude Code.
- Shared optimized vLLM settings for dual 96GB systems.
- Highlighted cost savings and performance benefits of local execution.
- Community praised the setup but humorously noted the high initial cost.

**Discussion Highlights:** The community responded with humor and admiration, noting the high cost but appreciating the setup's capabilities. Some users expressed envy over missing out on similar deals, while others confirmed the technical details shared in the post.

---

## 26. [It works! Abliteration can reduce slop without training](https://reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/)

**Author:** u/-p-e-w- | **Upvotes:** 401 | **Comments:** 129 | **Date:** 2026-01-11

**Summary:** The post discusses the use of abliteration to reduce 'slop' (flowery, cliched language) in LLM outputs, specifically with the Mistral Nemo model. The author successfully created a slop-reduced LLM using abliteration alone, without fine-tuning, and shared the results and methodology. Key points include the effectiveness of abliteration in reducing slop, the use of Heretic for prompt injection, the process duration, mixed community reactions, and the creation of GGUF files for easier access. The community discussion highlighted mixed opinions on the effectiveness of slop reduction, with some users appreciating the cleaner output and others feeling it made the prose too dry.

---

## 27. [I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work](https://reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/)

**Author:** u/Ok-Pomegranate1314 | **Upvotes:** 890 | **Comments:** 147 | **Date:** 2026-01-09

**Summary:** The author successfully clustered three NVIDIA DGX Sparks, overcoming networking limitations by writing a custom NCCL plugin. This achievement allows distributed inference across all three nodes at high speeds, despite NVIDIA's official support only covering two-node clusters.

**Key Points:**
- Author clustered three DGX Sparks, exceeding NVIDIA's official support for two-node clusters.
- Developed a custom NCCL network plugin (~1500 lines of C) to handle subnet-aware NIC selection and RDMA implementation.
- Achieved distributed inference at 8+ GB/s over RDMA, demonstrating significant performance.
- The solution is a general approach for DGX Spark clusters, not limited to three nodes.
- The project is open-source and available on GitHub for further exploration.

**Discussion Highlights:** The community praised the technical achievement, noting the complexity of working with NCCL and the potential impact of this solution. Questions focused on scalability and performance gains, with the author confirming the solution is general and not limited to three nodes.

---

## 28. [The reason why RAM has become so expensive](https://reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/)

**Author:** u/InvadersMustLive | **Upvotes:** 4547 | **Comments:** 381 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses the significant increase in RAM prices, with comments suggesting potential market manipulation and monopolization by key players like OpenAI, making it economically unviable for competitors, particularly in China. The price surge is noted to be much higher than previous years, with some users reporting a tenfold increase.

**Key Points:**
- RAM prices have increased dramatically, with reports of up to 10 times the previous year's cost.
- OpenAI is accused of monopolizing RAM to create future demand and stifle competition.
- The price surge is seen as a potential market bubble.
- Competitors, especially in China, may find it economically unviable to operate due to high RAM costs.

**Discussion Highlights:** The discussion highlights concerns about market manipulation and monopolization, with users expressing skepticism about the sustainability of such high prices. There is a consensus that the price increase is unusually steep and may have long-term implications for the AI industry.

---

## 29. [DeepSeek V4 Coming](https://reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/)

**Author:** u/External_Mood4719 | **Upvotes:** 499 | **Comments:** 109 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release V4, a next-generation AI model with enhanced code-generation capabilities, outperforming mainstream models like Claude and GPT. The model features improvements in handling long code prompts and data pattern understanding, with users anticipating higher reliability and logical rigor.

**Key Points:**
- DeepSeek V4 focuses on strong code-generation capabilities.
- V4 outperforms existing models like Claude and GPT in code generation.
- Improved handling of long code prompts and data patterns.
- Users expect higher reliability and logical rigor in outputs.
- Discussion highlights include excitement and expectations for the new model.

**Discussion Highlights:** Users express enthusiasm for DeepSeek V4, with many praising the affordability and performance of previous versions. Some anticipate significant improvements, while others speculate on potential features like mHC and deepseek-ocr integration.

---

## 30. [(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability](https://reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/)

**Author:** u/Nunki08 | **Upvotes:** 491 | **Comments:** 102 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release a new flagship AI model with strong coding capabilities, generating excitement and discussion in the community.

**Key Points:**
- DeepSeek's upcoming model focuses on strong coding abilities
- The announcement has generated significant interest and discussion
- Community members express excitement and anticipation for the new model
- Some users are hopeful for improved performance in role-playing abilities
- The release is seen as a positive development for the AI community

**Discussion Highlights:** The community is largely excited about the new model, with many expressing anticipation for its release and potential capabilities. Some users are hopeful for improvements in specific areas like role-playing abilities.

---

## 31. [The NO FAKES Act has a "Fingerprinting" Trap that kills Open Source. We need to lobby for a Safe Harbor.](https://reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/)

**Author:** u/PostEasy7183 | **Upvotes:** 620 | **Comments:** 88 | **Date:** 2026-01-08

**Summary:** The NO FAKES Act threatens open-source AI development by imposing liability on developers for tools used to create digital replicas, potentially banning open-source AI hosting in the US. The post urges lobbying for a Safe Harbor provision to protect developers.

**Key Points:**
- The NO FAKES Act creates a 'digital replica right' targeting tools used for replicas, making developers liable for damages.
- Open-source AI developers could face legal risks for hosting models that might be misused.
- The post calls for a 'Safe Harbor' provision to protect open-source developers.
- Community members express concern about the bill's impact on innovation and competition.
- Action items include contacting representatives to oppose the bill unless amended.

**Discussion Highlights:** The community strongly opposes the bill, viewing it as a threat to innovation and a potential monopoly for big tech. Many believe the bill is backed by large corporations to stifle competition. There is skepticism about politicians' understanding of technology.

---

## 32. [Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt](https://reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/)

**Author:** u/Prior-Arm-6705 | **Upvotes:** 944 | **Comments:** 147 | **Date:** 2026-01-08

**Summary:** The post describes a project where someone counted and compiled every instance of Jensen Huang saying 'AI' (121 times) during the NVIDIA CES 2025 keynote using open-source tools. The result is a hypnotic compilation video.

**Key Points:**
- Jensen Huang said 'AI' 121 times during the CES 2025 keynote
- The author used open-source tools (Dive, yt-dlp-mcp, ffmpeg-mcp-lite) to create the compilation
- The process involved downloading the video, parsing subtitles, cutting clips, and merging them
- The result is described as hypnotic
- Top comments include reactions, popularity mentions, and humorous remarks

**Discussion Highlights:** The discussion highlights include appreciation for the project's popularity, humorous comments about Jensen Huang's attire, and mentions of the project's alignment with tech enthusiasts like Gamers Nexus.

---

## 33. [16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/)

**Author:** u/ai-infos | **Upvotes:** 465 | **Comments:** 238 | **Date:** 2026-01-07

**Summary:** The post discusses running Deepseek v3.2 on 16x AMD MI50 32GB GPUs, achieving 10 tokens/sec output and 2000 tokens/sec input with a 69000 context length. The setup draws 550W idle and 2400W peak power, aiming for cost-effective local AGI hardware.

**Key Points:**
- Performance: 10 tok/s output, 2000 tok/s input with 69000 context length
- Hardware: 16x AMD MI50 32GB GPUs with 16 TB/s bandwidth
- Power consumption: 550W idle / 2400W peak
- Goal: Cost-effective alternative to expensive CPU setups
- Community engagement: 465 upvotes, 238 comments

**Discussion Highlights:** The community praised the setup's efficiency, with comments highlighting its potential as a cost-effective alternative to expensive hardware. Some users noted the high power draw could serve as a heater, while others questioned noise levels and home power requirements. Overall, the post was well-received for its innovative approach to local AGI hardware.

---

## 34. [DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.](https://reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/)

**Author:** u/Nunki08 | **Upvotes:** 663 | **Comments:** 54 | **Date:** 2026-01-07

**Summary:** The Reddit post discusses the recent update to DeepSeek-R1's paper, which expanded from 22 pages to 86 pages, adding significant detail. The discussion includes comments about potential new architectures, linear attention research, and the value of added implementation specifics.

**Key Points:**
- DeepSeek-R1's paper was updated from 22 pages to 86 pages.
- The update includes substantial additional detail.
- Discussion highlights potential new architectures and linear attention research.
- Comments mention the value of added implementation specifics.
- The post received significant engagement with 663 upvotes and 54 comments.

**Discussion Highlights:** The discussion highlights potential new architectures and research directions, such as linear attention and smaller model sizes. There is also appreciation for the added implementation details in the updated paper.

---

## 35. [A 30B Qwen Model Walks Into a Raspberry Pi… and Runs in Real Time](https://reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/)

**Author:** u/ali_byteshape | **Upvotes:** 502 | **Comments:** 79 | **Date:** 2026-01-06

**Summary:** The post discusses the successful optimization of a 30B Qwen model to run efficiently on a Raspberry Pi 5, achieving 8.03 tokens per second while retaining 94.18% of BF16 quality. The optimization focuses on balancing memory usage and performance, highlighting differences in CPU and GPU behavior.

**Key Points:**
- A 30B Qwen model runs on a Raspberry Pi 5 with 8.03 TPS at 2.70 BPW, retaining 94.18% of BF16 quality.
- Optimization prioritizes memory as a budget, fitting the model first and then optimizing for tokens per second vs. quality.
- CPU behavior is predictable with smaller models being faster, while GPU performance depends on kernel choice and can have sweet spots.
- Community feedback includes testing on different hardware and suggestions for further optimizations like hybrid transformers.
- The post requests community testing on various setups, including non-NVIDIA hardware and different batch sizes.

**Discussion Highlights:** The community showed interest in testing the model on different hardware setups, with some users reporting successful runs on Raspberry Pi 5 with specific configurations. There were also suggestions for combining the model with other solutions like exo for cluster computing.

---

## 36. [Performance improvements in llama.cpp over time](https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/)

**Author:** u/jacek2023 | **Upvotes:** 680 | **Comments:** 85 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses performance improvements in llama.cpp over time, highlighting significant progress and comparisons with other implementations like ik_llama.cpp. The discussion includes mentions of GPU-specific optimizations, particularly for NVIDIA GPUs.

**Key Points:**
- Performance gains in llama.cpp have been substantial, with token generation speed nearly matching ik_llama.cpp.
- Prompt processing remains slower but has seen significant improvements.
- Performance improvements may be specific to NVIDIA GPUs, as suggested by the discussion.
- The post was featured on Discord, indicating community recognition.
- Links to NVIDIA's blog on AI tool upgrades were shared, providing additional context.

**Discussion Highlights:** The discussion highlights the impressive progress in llama.cpp's performance, with users noting its near-parity with ik_llama.cpp in token generation speed. There is a consensus on the significant improvements, though prompt processing remains a relative bottleneck. The role of NVIDIA GPUs in these performance gains is a notable point of discussion.

---

## 37. [For the first time in 5 years, Nvidia will not announce any new GPUs at CES — company quashes RTX 50 Super rumors as AI expected to take center stage](https://reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/)

**Author:** u/FullstackSensei | **Upvotes:** 628 | **Comments:** 196 | **Date:** 2026-01-05

**Summary:** Nvidia will not announce new GPUs at CES, focusing instead on AI. The post discusses limited supply of new GPUs, potential re-release of older models, and rising hardware prices, expressing concerns about future upgrades.

**Key Points:**
- Nvidia quashes RTX 50 Super rumors, no new GPU announcements at CES
- Limited supply of RTX 5070Ti, 5080, and 5090, with potential re-release of RTX 3060
- Rising prices of DDR5 RAM and storage, making upgrades expensive
- Concerns about future hardware upgrades due to high costs and limited availability
- Discussion highlights corporate greed and the shift from consumer to enterprise focus

**Discussion Highlights:** The discussion reflects frustration over corporate greed, the shift from consumer to enterprise focus, and the challenges of maintaining local computing capabilities. Users express concerns about the future of hardware upgrades and the impact of rising prices.

---

## 38. [llama.cpp performance breakthrough for multi-GPU setups](https://reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/)

**Author:** u/Holiday-Injury-9397 | **Upvotes:** 579 | **Comments:** 201 | **Date:** 2026-01-05

**Summary:** The ik_llama.cpp project achieved a 3x to 4x performance improvement for multi-GPU setups, enabling cost-effective use of multiple low-cost GPUs for local LLM inference.

**Key Points:**
- ik_llama.cpp introduced a new execution mode (split mode graph) for multi-GPU utilization.
- Performance gains of 3x to 4x compared to previous methods.
- Cost-effective alternative to high-end enterprise GPUs.
- Consistent 2x prompt processing speed improvements even on single GPU or CPU-only setups.
- Performance comparable to other optimized frameworks like vllm.

**Discussion Highlights:** The community highlights significant performance improvements across various setups, with some users noting bottlenecks in hybrid inference configurations. The discussion also emphasizes the cost-effectiveness and accessibility of this breakthrough.

---

## 39. [GLM-Image model from Z.ai is coming](https://reddit.com/r/LocalLLaMA/comments/1q41bw1/glmimage_model_from_zai_is_coming/)

**Author:** u/Ravencloud007 | **Upvotes:** 318 | **Comments:** 59 | **Date:** 2026-01-04

**Summary:** The post announces the upcoming GLM-Image model from Z.ai, generating significant interest and discussion in the r/LocalLLaMA community. The model is highly anticipated, with users expressing excitement about its potential capabilities and comparing it favorably to existing models.

**Key Points:**
- GLM-Image model from Z.ai is being introduced
- The model is generating significant community interest with 318 upvotes and 59 comments
- Users are excited about the model's potential, with one comment mentioning a feeling of 103 billion parameters
- Z.ai's image model is currently the community favorite
- There are discussions about the computational resources required to use the model

**Discussion Highlights:** The discussion highlights a strong community interest and excitement about the GLM-Image model. Users are comparing it favorably to existing models and expressing anticipation for its release. There are also humorous comments about the computational resources needed to run such a large model.

---

## 40. [Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched](https://reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/)

**Author:** u/ubrtnk | **Upvotes:** 377 | **Comments:** 194 | **Date:** 2026-01-03

**Summary:** The post discusses challenges faced by local LLMs in processing extreme breaking news events, highlighting how models initially dismissed the US/Venezuela event as a hoax despite credible sources. The author tested multiple models, finding that larger models like GPT-OSS:120B eventually acknowledged the event after verification.

**Key Points:**
- Local LLMs struggled to accept extreme breaking news as real, classifying it as misinformation.
- Models like Qwen Research and Spark 4.0 initially dismissed the event despite credible sources.
- Larger models (e.g., GPT-OSS:120B) performed better but still showed skepticism.
- Users reported similar issues with other extreme events, like the OpenAI DRAM production deal.
- Discussion highlights bias in LLMs' internal models of unfamiliar geopolitical events.

**Discussion Highlights:** The discussion consensus suggests that LLMs have inherent biases in processing unfamiliar or extreme events, often defaulting to skepticism. Users noted that models require explicit credible sources to override their internal skepticism, and larger models tend to perform better in such scenarios.

---

## 41. [LeCun Says Llama 4 results "were fudged a little bit"](https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/)

**Author:** u/MrPecunius | **Upvotes:** 360 | **Comments:** 88 | **Date:** 2026-01-02

**Summary:** Yann LeCun, departing Meta AI Chief, confirmed that Llama 4 benchmark results were manipulated. This follows speculation about suspicious benchmarks and coincides with Zuckerberg sidelining the GenAI organization, leading to significant departures.

**Key Points:**
- LeCun confirms Llama 4 benchmark manipulation
- Zuckerberg sidelined Meta's GenAI organization
- Significant departures from Meta's AI team
- Llama 4's promised large model was never released
- Community disappointment over Llama's failure and its impact on open-source AI

**Discussion Highlights:** The discussion highlights disappointment in Llama's failure and its impact on open-source AI, with users expressing regret over Meta's strategic missteps. There is also a shared PDF of the full article and speculation about Meta's organizational issues.

---

## 42. [Qwen-Image-2512](https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/)

**Author:** u/Nunki08 | **Upvotes:** 713 | **Comments:** 122 | **Date:** 2025-12-31

**Summary:** The Reddit post announces the release of Qwen-Image-2512, a new model available on multiple platforms with guides and GGUF files. The community has responded positively, highlighting its performance on low-end hardware and creative applications.

**Key Points:**
- Qwen-Image-2512 is a new model with guides and GGUF files available
- The model can be accessed on platforms like Hugging Face, ModelScope, and GitHub
- Community feedback includes successful runs on low-end hardware and creative use cases
- The model is appreciated as a 'new year's gift' by users

**Discussion Highlights:** Users shared experiences of running the model on low-end hardware and expressed appreciation for the model's release. Creative use cases, such as generating unique images, were also highlighted.

---

## 43. [[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It’s running a raw Llama-7B instance with a 2048 token window.](https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/)

**Author:** u/simar-dmg | **Upvotes:** 746 | **Comments:** 110 | **Date:** 2025-12-30

**Summary:** A Reddit user reverse-engineered a Snapchat sextortion bot and discovered it uses a Llama-7B model with a 2048 token window and high temperature setting, making it susceptible to jailbreaks. The bot revealed its configuration and a malicious link when prompted with a persona-adoption attack.

**Key Points:**
- The bot uses a Llama-7B model with a 2048 token context window and high temperature setting.
- A persona-adoption jailbreak (Grandma Protocol) was used to extract system information.
- The bot revealed environment variables and a malicious link when compromised.
- Scammers are using open-source models to avoid API costs and censorship filters.
- The discussion highlights skepticism about the accuracy of the bot's responses.

**Discussion Highlights:** The discussion includes skepticism about the bot's responses, with some users suggesting that much of the information could be hallucinated. However, there is consensus that the bot is powered by an LLM.

---

## 44. [Llama-3.3-8B-Instruct](https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/)

**Author:** u/jacek2023 | **Upvotes:** 468 | **Comments:** 77 | **Date:** 2025-12-29

**Summary:** The post discusses the discovery and release of the Llama-3.3-8B-Instruct model, which was previously only accessible via Facebook's API. The author managed to download and share the model by extracting the original weights from a finetuned version.

**Key Points:**
- Llama-3.3-8B-Instruct was previously only available through Facebook's API.
- The author found a way to download the model by finetuning and extracting the original weights.
- The model's authenticity is being verified by the community.
- There are questions about the model's specifications, such as its 8K max position embeddings.
- The community is excited about the release and is running evaluations.

**Discussion Highlights:** The community is actively verifying the model's authenticity and specifications. There is excitement about the release, with users running evaluations and benchmarks to compare it with other models.

---

## 45. [Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.](https://reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 344 | **Comments:** 120 | **Date:** 2025-12-29

**Summary:** Z AI is set to go public on January 8, aiming to raise $560 million, marking it as the first AI-native LLM company to list globally. The announcement has sparked discussions about the future of open-source AI and the balance between monetization and community contributions.

**Key Points:**
- Z AI's IPO is scheduled for January 8 with a target of $560 million.
- Concerns about the future of open-source AI models post-IPO.
- Debate on whether Z AI will continue releasing open weight models.
- Community reactions highlight the tension between monetization and open-source values.
- Some users argue that monetization is inevitable for sustainability.

**Discussion Highlights:** The discussion reflects a mix of excitement and concern. While some users celebrate the milestone, others express worries about the potential shift away from open-source principles. A notable comment suggests that monetization is a natural progression for companies, while another highlights the cost benefits of subscriptions over hardware investments.

---

## 46. [Tencent just released WeDLM 8B Instruct on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 421 | **Comments:** 62 | **Date:** 2025-12-29

**Summary:** Tencent released WeDLM 8B Instruct on Hugging Face, a diffusion language model that runs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks. The release has garnered significant attention and discussion in the community.

**Key Points:**
- WeDLM 8B Instruct is a diffusion language model released by Tencent on Hugging Face.
- It performs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks.
- The model is released under the Apache 2.0 license.
- The community shows strong interest in the potential of 7-8B models.
- A 7B version of the model is also available.

**Discussion Highlights:** The community is excited about the performance and potential of the WeDLM models, with particular interest in their speed and the Apache 2.0 license. There is a consensus on the promising future of 7-8B models.

---

## 47. [NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux](https://reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/)

**Author:** u/HumanDrone8721 | **Upvotes:** 451 | **Comments:** 185 | **Date:** 2025-12-27

**Summary:** NVIDIA has dropped Pascal support on Linux, causing issues for Arch Linux users. The community discusses the impact on Pascal cards and Arch Linux's policy of moving legacy drivers to AUR.

**Key Points:**
- NVIDIA's driver update drops support for Pascal GPUs on Linux
- Arch Linux users are affected, with legacy drivers moved to AUR
- Community reactions include concerns about hardware compatibility and appreciation for Arch's handling of legacy drivers
- Specific Pascal cards like the 24GB P40 are mentioned as being impacted

**Discussion Highlights:** The discussion highlights a mix of concern and acceptance, with users acknowledging Arch Linux's long-standing policy of moving legacy drivers to the AUR (Arch User Repository). Some users express nostalgia for Pascal cards, while others note the inevitability of such changes.

---

## 48. [Best Local LLMs - 2025](https://reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/)

**Author:** u/rm-rf-rm | **Upvotes:** 368 | **Comments:** 198 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses the best local LLMs of 2025, highlighting models like Minimax M2.1 and GLM4.7, and categorizes them by application and memory footprint. Users share detailed experiences and recommendations.

**Key Points:**
- Minimax M2.1 and GLM4.7 are noted for frontier model performance.
- Models are categorized by applications such as General, Agentic, Creative Writing, and Speciality.
- Memory footprint categories include Unlimited (>128GB VRAM), Medium (8-128GB VRAM), and Small (<8GB VRAM).
- Users recommend models like Qwen3-4B-instruct and LFM2-8B-A1B for specific tasks.
- Detailed user experiences and setups are encouraged for better evaluation.

**Discussion Highlights:** The discussion includes debates on categorization, specific model recommendations, and the importance of detailed user experiences. Notable mentions include Qwen3-4B-instruct and LFM2-8B-A1B for their performance in general knowledge and tool use.

---

## 49. [NVIDIA has 72GB VRAM version now](https://reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/)

**Author:** u/decentralize999 | **Upvotes:** 461 | **Comments:** 148 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses NVIDIA's new 72GB VRAM version, questioning the pricing and community interest in different VRAM sizes. The discussion includes comparisons of specifications and prices for various NVIDIA GPUs.

**Key Points:**
- NVIDIA has released a 72GB VRAM version of their GPU.
- Community members express interest in larger VRAM sizes, such as 128GB.
- Price comparisons show the RTX 5000 48GB at $5100, RTX 5000 72GB at $7800, and RTX 6000 96GB at $8300.
- Some users suggest waiting for future models like the 5090 with 48GB.
- The price per gigabyte remains consistent across different VRAM sizes.

**Discussion Highlights:** The discussion highlights a consensus that larger VRAM sizes are desirable, with some users advocating for even bigger options like 128GB. Price comparisons indicate that the cost per gigabyte is similar across different models, making the choice dependent on individual budget and needs.

---

## 50. [Hard lesson learned after a year of running large models locally](https://reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/)

**Author:** u/inboundmage | **Upvotes:** 348 | **Comments:** 146 | **Date:** 2025-12-26

**Summary:** The author shares their experience running large language models locally, highlighting challenges with VRAM limitations, model scaling, and performance trade-offs. They discuss the viability of local inference for smaller models but note significant hurdles for larger models without high-end hardware.

**Key Points:**
- Running large models locally is feasible for small to medium models but faces hard limits with larger models due to VRAM constraints.
- VRAM fragmentation and inefficient offloading to system RAM are major issues when working with consumer-grade GPUs.
- Quantization helps but introduces quality trade-offs and new bugs, making it a mixed solution.
- Cloud-based solutions offer better performance for fast iteration, but local setups are preferred for privacy-sensitive tasks.
- Community suggestions include using llama.cpp for CPU offloading and considering hardware upgrades like additional GPUs.

**Discussion Highlights:** The discussion highlights practical solutions like using llama.cpp for better CPU offloading and suggests hardware upgrades as a potential solution. There is also a shared hope for future hardware improvements, such as GPUs with significantly more VRAM.

---

