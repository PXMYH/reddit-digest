# r/LocalLLaMA Reading Digest

**Period:** 2026-01-19 to 2026-01-19
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [zai-org/GLM-4.7-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 614 | **Comments:** 211 | **Date:** 2026-01-19

**Summary:** The Reddit post discusses the release of the zai-org/GLM-4.7-Flash model on Hugging Face, highlighting its popularity and technical features like MLA and efficient KV cache usage.

**Key Points:**
- The model is gaining significant attention with 612 upvotes and 209 comments
- Users express enthusiasm for 30b models and anticipation for larger models like 70b
- The model uses MLA, reducing KV cache memory usage and enabling full 200k context runs
- Community members appreciate the release after a long wait

**Discussion Highlights:** The discussion reflects strong community interest in the GLM-4.7-Flash model, with users praising its technical capabilities and expressing excitement about its potential applications. The consensus highlights the model's efficiency and the community's eagerness for larger models.

---

## 2. [4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build](https://reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/)

**Author:** u/NunzeCs | **Upvotes:** 331 | **Comments:** 86 | **Date:** 2026-01-18

**Summary:** The author built a high-performance system with 4x AMD R9700 GPUs (128GB VRAM) and a Threadripper 9955WX CPU, leveraging a 50% subsidy to stay within a ~10,000€ budget. The system is designed for running large AI models locally, with benchmark results showing strong performance across various models.

**Key Points:**
- The system was built to maximize VRAM for running large AI models locally.
- The total cost was ~9,800€, with a 50% subsidy reducing the effective cost to ~4,900€.
- Benchmark results show strong performance across various AI models, including GLM-4.7-REAP-218B and Qwen3-235B.
- The discussion highlights include admiration for the build and questions about component sourcing and job context.

**Discussion Highlights:** The discussion highlights include admiration for the build, with comments like 'HE HAS RAM GET HIM...' and 'G O D D A A A A A Y U U U U M...'. There are also questions about where the components were sourced and the author's job context.

---

## 3. [Qwen 4 might be a long way off !? Lead Dev says they are "slowing down" to focus on quality.](https://reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 440 | **Comments:** 66 | **Date:** 2026-01-17

**Summary:** The Reddit post discusses Qwen 4's development, with the lead developer indicating a slowdown to focus on quality. The community generally appreciates this approach, though some caution against overinterpreting the announcement.

**Key Points:**
- Qwen 4 development is slowing down to prioritize quality
- Community largely supports the focus on quality over rapid releases
- Some users urge caution against speculative interpretations of the announcement
- The post gained significant traction with 443 upvotes and 66 comments
- Top comments highlight appreciation for quality-focused development

**Discussion Highlights:** The discussion reflects a consensus that prioritizing quality in Qwen 4's development is beneficial, with the top comment (194 upvotes) explicitly supporting this approach. However, another notable comment (69 upvotes) advises against jumping to conclusions based on limited information.

---

## 4. [128GB VRAM quad R9700 server](https://reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)

**Author:** u/Ulterior-Motive_ | **Upvotes:** 515 | **Comments:** 110 | **Date:** 2026-01-17

**Summary:** The author upgraded from MI100s to four R9700 GPUs, building a high-performance server with 128GB VRAM and 128GB RAM for under $7,035, showcasing impressive benchmarks for AI workloads.

**Key Points:**
- Upgrade from MI100s to four R9700 GPUs due to better performance and cost efficiency
- Total build cost of $7,035 with 128GB VRAM and 128GB RAM
- Performance benchmarks provided for AI workloads
- Community appreciation for the build and its cost-effectiveness
- Discussion highlights include admiration and humor about financial irresponsibility

**Discussion Highlights:** The community praised the build for its performance and cost-effectiveness, with some humorous comments about the financial implications of such upgrades.

---

## 5. [GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)](https://reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/)

**Author:** u/CuriousPlatypus1881 | **Upvotes:** 373 | **Comments:** 89 | **Date:** 2026-01-16

**Summary:** The post discusses the December 2025 update to the SWE-bench leaderboard, highlighting the performance of various AI models on GitHub PR tasks. Claude Opus 4.5 leads with a 63.3% resolved rate, followed closely by GPT-5.2 (extra high effort) at 61.5%. The post also notes the strong performance of open-source models like GLM-4.7.

**Key Points:**
- Claude Opus 4.5 leads the leaderboard with a 63.3% resolved rate.
- GPT-5.2 (extra high effort) follows closely at 61.5%.
- Gemini 3 Flash Preview outperforms Gemini 3 Pro Preview despite being smaller and cheaper.
- GLM-4.7 is the strongest open-source model, ranking alongside closed models like GPT-5.1-codex.
- GPT-OSS-120B shows significant performance improvement in high-effort reasoning mode.

**Discussion Highlights:** The discussion highlights excitement around the performance of open-source models like GLM-4.7 and anticipation for future releases like DeepSeek v4. There is also a consensus that this benchmark is more believable compared to others.

---

## 6. [I fucking love this community](https://reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/)

**Author:** u/alhinai_03 | **Upvotes:** 484 | **Comments:** 54 | **Date:** 2026-01-16

**Summary:** The post expresses gratitude towards the open-source community for enabling the user to run large language models on older hardware, highlighting the efficiency of MoE architectures and system memory.

**Key Points:**
- Gratitude towards the open-source community for their contributions
- User runs large models on a 10-year-old PC with 4GB VRAM
- Achieves 14-13.5 tokens per second with nemotron-3-nano-30B-a3b-iq4_nl
- Key factors: good system memory and MoE architecture
- Community appreciation for optimization efforts

**Discussion Highlights:** The community appreciates the user's achievement and highlights the importance of system memory and MoE architectures. There is a consensus on the practicality of these setups and a desire for more VRAM and RAM to run state-of-the-art models.

---

## 7. [My story of underestimating /r/LocalLLaMA's thirst for VRAM](https://reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/)

**Author:** u/EmPips | **Upvotes:** 1279 | **Comments:** 88 | **Date:** 2026-01-15

**Summary:** The post highlights the author's underestimation of the r/LocalLLaMA community's demand for VRAM, sparking discussions on hardware recommendations and community engagement.

**Key Points:**
- Author underestimated community's VRAM demand
- Discussion includes hardware recommendations (e.g., 3090s, R9700)
- Gold rush analogy used to describe community interest
- Post featured on Discord with special flair for the author

**Discussion Highlights:** The discussion features hardware advice, a gold rush analogy for community interest, and mentions of the post being featured on Discord with special recognition for the author.

---

## 8. [Latest upgrade…A100 40 GB](https://reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/)

**Author:** u/inserterikhere | **Upvotes:** 406 | **Comments:** 53 | **Date:** 2026-01-15

**Summary:** The user upgraded their gaming rig to an AI rig by purchasing an A100 GPU listed as faulty for parts, which turned out to work perfectly. They detailed their journey from using a 5070ti to eventually acquiring the A100 for $1000.

**Key Points:**
- User transitioned from a gaming rig to an AI rig.
- Purchased an A100 GPU listed as faulty, which worked upon installation.
- Community reactions included concerns about cooling and appreciation for the upgrade.
- The A100 was bought for $1000 despite being listed as having CUDA errors.

**Discussion Highlights:** The community expressed surprise and admiration for the upgrade, with some users highlighting potential cooling issues for the A100 GPU.

---

## 9. [NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency](https://reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/)

**Author:** u/Fear_ltself | **Upvotes:** 705 | **Comments:** 129 | **Date:** 2026-01-14

**Summary:** NVIDIA's new 8B model, Orchestrator-8B, is designed to intelligently manage and route complex tasks to different tools for greater efficiency. The post discusses the potential of integrating separate AI components to achieve functional systems, with comments highlighting its managerial role and comparisons to existing frameworks.

**Key Points:**
- Orchestrator-8B is an 8-billion-parameter AI designed to route tasks to various tools.
- The model emphasizes integration and coordination over standalone capabilities.
- Comments compare it to managerial roles and existing agentic frameworks.
- Discussion suggests a trend towards hierarchical AI systems managing other models.

**Discussion Highlights:** The discussion highlights the model's role as a 'middle manager' for AI tasks, with comparisons to existing frameworks like Claude's agentic systems. There's a consensus on the importance of integrating specialized models for functional AI systems.

---

## 10. [GLM-Image is released!](https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/)

**Author:** u/foldl-li | **Upvotes:** 594 | **Comments:** 83 | **Date:** 2026-01-13

**Summary:** GLM-Image is a new image generation model with a hybrid autoregressive + diffusion decoder architecture. It excels in text-rendering and knowledge-intensive tasks while supporting various image-to-image tasks.

**Key Points:**
- Hybrid autoregressive + diffusion decoder architecture
- Strong performance in text-rendering and knowledge-intensive tasks
- Supports image editing, style transfer, and multi-subject consistency
- MIT license with no restrictions
- Model size: 13GB diffusion model + 20GB text encoder

**Discussion Highlights:** The community appreciates the MIT license and the model's capabilities. There is interest in quantizing the model for easier use and discussions about its performance compared to other models.

---

## 11. [My wishes for 2026](https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/)

**Author:** u/jacek2023 | **Upvotes:** 648 | **Comments:** 179 | **Date:** 2026-01-13

**Summary:** The Reddit post discusses predictions for 2026, focusing on the feasibility of affordable GPUs with >32GB memory. The comments reflect a mix of humor, skepticism, and specific mentions of AI models like Qwen 4 and Mistral.

**Key Points:**
- Discussion about affordable GPUs >32GB in 2026
- Skepticism and humor around the feasibility of such GPUs
- Mentions of AI models like Qwen 4 and Mistral
- Post featured on Discord with special flair for the author

**Discussion Highlights:** The discussion is marked by a skeptical and humorous tone regarding the possibility of affordable high-end GPUs in 2026. Some comments mention specific AI models, indicating a focus on technological advancements.

---

## 12. [kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop—no GPU required](https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/)

**Author:** u/Nunki08 | **Upvotes:** 399 | **Comments:** 92 | **Date:** 2026-01-13

**Summary:** Kyutai introduced Pocket TTS, a 100M-parameter text-to-speech model with high-quality voice cloning that runs on a laptop without requiring a GPU. The model is open-source and available on GitHub and Hugging Face.

**Key Points:**
- 100M-parameter TTS model with high-quality voice cloning
- Runs on CPU without GPU requirement
- Open-source with available GitHub and Hugging Face resources
- Potential memory usage issues during generation
- Questions about multi-language support and fine-tuning capabilities

**Discussion Highlights:** The community showed interest in multi-language support and fine-tuning capabilities. Some users reported high memory usage during generation, with one instance reaching 32 GB. There was also discussion about the practicality of small models compared to established alternatives.

---

## 13. [GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/)

**Author:** u/TKGaming_11 | **Upvotes:** 368 | **Comments:** 89 | **Date:** 2026-01-12

**Summary:** The Reddit post highlights DeepSeek-AI's 'Engram' project, a novel approach to conditional memory in large language models using scalable lookup, praised for its originality and technical innovation.

**Key Points:**
- DeepSeek-AI's Engram introduces a new sparsity axis via scalable lookup for LLMs
- The approach uses n-gram embeddings as static memory with O(1) lookup
- The paper demonstrates a U-shaped performance curve in their experiments
- Community notes the biological plausibility of this memory approach
- DeepSeek's consistent track record of innovative research is highlighted

**Discussion Highlights:** The community discussion emphasizes the technical novelty of the n-gram embedding approach, its potential as a complementary sparsity method to MoE, and draws parallels to biological memory systems. There's strong consensus about DeepSeek's innovative contributions to LLM research.

---

## 14. [LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)](https://reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 1040 | **Comments:** 114 | **Date:** 2026-01-11

**Summary:** The post introduces TimeCapsuleLLM, an open-source project training language models on historical texts from 1800-1875 London to reduce modern bias. The 1.2B parameter model uses a 90GB dataset and demonstrates period-specific outputs, such as unfamiliarity with post-1875 concepts like telephones.

**Key Points:**
- TimeCapsuleLLM is trained exclusively on 1800-1875 London texts to minimize modern bias.
- The model (1.2B parameters, 90GB dataset) generates contextually appropriate outputs, like arguments against the Roman Catholic Church.
- The model is unfamiliar with post-1875 concepts, such as telephones, treating them as unknown terms.
- Future steps include creating synthetic Q&A pairs from the dataset.
- The project has garnered significant community interest and support.

**Discussion Highlights:** The community praised the project's uniqueness and potential, with some users sharing similar historical dataset initiatives. Humorous comments highlighted the model's temporal limitations, and the post received recognition from the subreddit moderators.

---

## 15. [I bought a €9k GH200 “desktop” to save $1.27 on Claude Code (vLLM tuning notes)](https://reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/)

**Author:** u/Reddactor | **Upvotes:** 693 | **Comments:** 177 | **Date:** 2026-01-11

**Summary:** The author built a high-end €9k GH200 desktop with 192GB VRAM to run Claude Code locally, achieving better speeds than the cloud version and sharing optimized vLLM settings for dual 96GB systems. The setup uses MiniMax M2.1 for offline coding and blocks telemetry, though the cost is humorously noted as 321X the yearly subscription fee.

**Key Points:**
- Built a €9k GH200 desktop with 192GB VRAM for local Claude Code execution.
- Achieved better speeds than cloud-based Claude Code with Sonnet.
- Shared optimized vLLM settings for dual 96GB systems, including tensor parallel size 2 and 163,840 context.
- Used MiniMax M2.1 FP8+INT4 AWQ for offline coding and blocked telemetry.
- Community reactions highlight the humor in the cost and the value of the experience.

**Discussion Highlights:** The community praised the setup's capabilities and humorously noted the high cost, with some expressing envy over missing out on similar deals. There was also clarification sought on the specific model used (MiniMax M2.1 FP8+INT4 AWQ).

---

## 16. [It works! Abliteration can reduce slop without training](https://reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/)

**Author:** u/-p-e-w- | **Upvotes:** 401 | **Comments:** 123 | **Date:** 2026-01-11

**Summary:** The Reddit post discusses the use of abliteration to reduce 'slop' (flowery, cliched language) in LLM outputs without training. The author successfully applied this technique to the Mistral Nemo model, creating a slop-reduced version using a configuration file and the Heretic tool. Key points include the effectiveness of abliteration, the use of Heretic, the process duration, the model tested, and mixed community feedback. The discussion highlights mixed opinions on the effectiveness of slop reduction, with some appreciating the reduction in flowery language and others feeling it makes the prose dry.

---

## 17. [I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work](https://reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/)

**Author:** u/Ok-Pomegranate1314 | **Upvotes:** 883 | **Comments:** 145 | **Date:** 2026-01-09

**Summary:** The author successfully clustered three NVIDIA DGX Sparks, overcoming technical limitations by writing a custom NCCL network plugin. This achievement allows distributed inference across all three nodes at high speeds, pushing the boundaries of what NVIDIA officially supports.

**Key Points:**
- Author clustered three DGX Sparks despite NVIDIA's official support for only two.
- Developed a custom NCCL network plugin to handle subnet-aware NIC selection and RDMA implementation.
- Achieved distributed inference at 8+ GB/s over RDMA.
- The project involved extensive low-level debugging and is shared on GitHub.
- Community reactions highlight the technical difficulty and potential significance of the achievement.

**Discussion Highlights:** The top comments praise the technical difficulty of working with NCCL and the potential impact of the solution. Questions focus on scalability and performance improvements, indicating strong community interest in the project.

---

## 18. [The reason why RAM has become so expensive](https://reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/)

**Author:** u/InvadersMustLive | **Upvotes:** 4462 | **Comments:** 377 | **Date:** 2026-01-09

**Summary:** The Reddit post discusses the significant increase in RAM prices, with users noting a rise of up to 10 times the previous cost. The discussion highlights concerns about market manipulation and monopolization of key resources by major AI companies. Key points include the dramatic price increase, concerns about market manipulation, the economic impact on AI data centers, speculation about a price bubble, and the post's popularity. The discussion primarily revolves around the economic implications of rising RAM prices, with a focus on potential market manipulation and its impact on AI infrastructure.

---

## 19. [DeepSeek V4 Coming](https://reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/)

**Author:** u/External_Mood4719 | **Upvotes:** 501 | **Comments:** 107 | **Date:** 2026-01-09

**Summary:** DeepSeek is expected to release V4, a next-generation AI model with strong code-generation capabilities, outperforming mainstream models like Claude and GPT. The model shows improvements in handling long code prompts and overall reasoning ability.

**Key Points:**
- DeepSeek V4 focuses on strong code-generation capabilities
- Outperforms existing models like Claude and GPT in code generation
- Improved handling of long code prompts and data patterns
- Enhanced reasoning ability and reliability for complex tasks
- Positive user feedback on DeepSeek's performance and affordability

**Discussion Highlights:** Users express excitement and positive impressions of DeepSeek's performance, with some anticipating significant improvements in V4. There is consensus on the model's affordability and effectiveness, especially for API usage.

---

## 20. [(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability](https://reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/)

**Author:** u/Nunki08 | **Upvotes:** 488 | **Comments:** 102 | **Date:** 2026-01-09

**Summary:** DeepSeek is set to release a new flagship AI model with strong coding capabilities, generating significant interest and discussion in the community.

**Key Points:**
- DeepSeek's upcoming model focuses on strong coding ability
- The announcement has generated excitement and anticipation
- Community members express enthusiasm for more AI models and competition
- Some users are skeptical about performance claims based on internal benchmarks
- There is a desire for the model to maintain role-playing capabilities

**Discussion Highlights:** The community shows strong interest and excitement about DeepSeek's new model, with some expressing skepticism about performance claims and a desire for balanced capabilities including role-playing.

---

## 21. [The NO FAKES Act has a "Fingerprinting" Trap that kills Open Source. We need to lobby for a Safe Harbor.](https://reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/)

**Author:** u/PostEasy7183 | **Upvotes:** 610 | **Comments:** 88 | **Date:** 2026-01-08

**Summary:** The Reddit post discusses the NO FAKES Act, highlighting its potential negative impact on open-source AI development due to liability risks for developers hosting AI models. The author urges the community to lobby for a Safe Harbor provision to protect open-source tool developers.

**Key Points:**
- The NO FAKES Act creates a 'digital replica right' that could hold developers liable for misuse of their AI models.
- Developers hosting AI models on platforms like HuggingFace could face statutory damages if their models are used to create unauthorized replicas.
- The post calls for a 'Safe Harbor' provision to protect open-source developers and prevent a monopoly by big tech companies.
- The community is encouraged to contact their representatives to oppose the bill unless it includes protections for open-source developers.
- There is concern that the bill could stifle innovation and give an unfair advantage to large corporations.

**Discussion Highlights:** The discussion highlights strong opposition to the bill's potential to stifle innovation and favor big tech companies. Many commenters express skepticism about politicians' understanding of technology and the potential consequences of the bill.

---

## 22. [Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt](https://reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/)

**Author:** u/Prior-Arm-6705 | **Upvotes:** 937 | **Comments:** 148 | **Date:** 2026-01-08

**Summary:** A Reddit user created a compilation video of every instance Jensen Huang said 'AI' during NVIDIA's CES 2025 keynote, totaling 121 times, using open-source tools for video processing. The post gained significant attention with 937 upvotes and 148 comments.

**Key Points:**
- Jensen Huang said 'AI' 121 times during the CES 2025 keynote.
- The user utilized open-source tools (Dive, yt-dlp-mcp, ffmpeg-mcp-lite) to automate the video compilation process.
- The post received 937 upvotes and 148 comments, indicating high engagement.
- Top comments included discussions about the post's popularity, Jensen's influence on pricing, and references to tech communities like Gamers Nexus.
- The final video was described as 'hypnotic' and showcased the efficiency of local, automated video editing.

**Discussion Highlights:** The discussion highlighted the post's popularity, with comments ranging from appreciation for the technical achievement to humorous remarks about Jensen's attire and influence on tech pricing. The consensus was positive, with users praising the automation and creativity of the project.

---

## 23. [16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)](https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/)

**Author:** u/ai-infos | **Upvotes:** 463 | **Comments:** 238 | **Date:** 2026-01-07

**Summary:** The post discusses running Deepseek V3.2 AWQ 4-bit on 16x AMD MI50 32GB GPUs, achieving 10 tokens/sec output and 2000 tokens/sec input with a 69000 context length. The setup aims for cost-effective local AGI and highlights power efficiency (550W idle / 2400W peak).

**Key Points:**
- Performance: 10 tok/s output, 2000 tok/s input, 69000 context length
- Power draw: 550W idle / 2400W peak inference
- Goal: Cost-effective local AGI without high hardware costs
- Future plans: 32 AMD MI50 setup for Kimi K2 Thinking
- Community appreciation and open-source setup details provided

**Discussion Highlights:** Comments highlight the power efficiency as a potential heating solution, concerns about noise and power requirements for home use, and the cost-effectiveness for professional developers. The community appreciates the contribution and open-source setup.

---

## 24. [DeepSeek-R1’s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.](https://reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/)

**Author:** u/Nunki08 | **Upvotes:** 663 | **Comments:** 54 | **Date:** 2026-01-07

**Summary:** The Reddit post discusses the recent update to DeepSeek-R1's paper, which expanded from 22 pages to 86 pages, adding significant detail. The community is engaged in discussing potential new architectures and research directions.

**Key Points:**
- DeepSeek-R1's paper was updated from 22 pages to 86 pages.
- The update includes substantial additional detail.
- Community speculation about new architectures (e.g., dsv4 + r2).
- Interest in how architectural improvements perform at different model sizes.
- Focus on linear attention and cache optimization in current research.

**Discussion Highlights:** The discussion highlights community excitement about potential new architectures and the expanded detail in the paper. There is speculation about future model sizes and the impact of linear attention research.

---

## 25. [A 30B Qwen Model Walks Into a Raspberry Pi… and Runs in Real Time](https://reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/)

**Author:** u/ali_byteshape | **Upvotes:** 493 | **Comments:** 78 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses the release of the Qwen3-30B-A3B-Instruct-2507 model, optimized for performance on small hardware like the Raspberry Pi 5. The model achieves 8.03 tokens per second (TPS) at 2.70 bits per weight (BPW) while retaining 94.18% of BF16 quality. The post highlights the trade-offs between model size, speed, and quality, particularly on GPUs where kernel choice significantly impacts performance. Key points include the model's performance on a Raspberry Pi 5, retention of quality, influence of kernel choice on GPU performance, community feedback for testing, and user experiences with the model. The discussion highlights user experiences, suggestions for further optimization, and comparisons with other models.

---

## 26. [Performance improvements in llama.cpp over time](https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/)

**Author:** u/jacek2023 | **Upvotes:** 676 | **Comments:** 85 | **Date:** 2026-01-06

**Summary:** The Reddit post discusses performance improvements in llama.cpp over time, with a focus on NVIDIA GPU enhancements and comparisons with other implementations.

**Key Points:**
- Performance gains are highlighted, particularly for NVIDIA GPUs.
- A reference to NVIDIA's blog post on speed improvements for LLM and diffusion models.
- Comparisons with ik_llama.cpp, noting llama.cpp's progress in token generation speed.
- Prompt processing is noted to be slower but overall progress is praised.

**Discussion Highlights:** The discussion emphasizes significant performance improvements in llama.cpp, especially for NVIDIA GPUs, and acknowledges the progress in token generation speed, though prompt processing remains slower.

---

## 27. [For the first time in 5 years, Nvidia will not announce any new GPUs at CES — company quashes RTX 50 Super rumors as AI expected to take center stage](https://reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/)

**Author:** u/FullstackSensei | **Upvotes:** 627 | **Comments:** 196 | **Date:** 2026-01-05

**Summary:** Nvidia will not announce new GPUs at CES, focusing instead on AI. The company faces supply issues with high-end GPUs and may reintroduce older models like the RTX 3060. Rising hardware prices and limited availability are causing concerns among consumers.

**Key Points:**
- No new GPU announcements from Nvidia at CES
- Limited supply of RTX 5070Ti, 5080, and 5090
- Potential reintroduction of RTX 3060 to meet demand
- Rising prices for DDR5 RAM and storage
- Consumer frustration over corporate greed and lack of local computing options

**Discussion Highlights:** The discussion highlights widespread frustration with Nvidia's focus on AI over consumer GPUs, concerns about corporate greed, and suggestions for alternative solutions like increased competition from China. Many users express worry about the future of local computing and hardware upgrades.

---

## 28. [llama.cpp performance breakthrough for multi-GPU setups](https://reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/)

**Author:** u/Holiday-Injury-9397 | **Upvotes:** 567 | **Comments:** 201 | **Date:** 2026-01-05

**Summary:** The ik_llama.cpp project achieved a 3x to 4x performance improvement for multi-GPU setups, enabling cost-effective use of multiple low-cost GPUs for local LLM inference.

**Key Points:**
- ik_llama.cpp introduces a new execution mode (split mode graph) for multi-GPU utilization.
- Performance improvement is significant, ranging from 3x to 4x.
- This breakthrough allows the use of multiple low-cost GPUs instead of expensive high-end cards.
- Even single GPU or CPU-only setups see a 2x speed improvement.
- The project is open-source and details are available on GitHub.

**Discussion Highlights:** The community highlights the open-source nature of the project, with a focus on GitHub for details rather than paid platforms. Users report consistent performance improvements across various setups, including single GPU and CPU-only configurations. Some users note challenges with hybrid inference due to hardware bottlenecks.

---

## 29. [Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched](https://reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/)

**Author:** u/ubrtnk | **Upvotes:** 379 | **Comments:** 194 | **Date:** 2026-01-03

**Summary:** The post discusses the challenges local LLMs face when processing extreme or unlikely breaking news events, such as the US attacking Venezuela. The author shares experiences with different LLMs, highlighting their struggles to accept such events as real despite credible sources.

**Key Points:**
- Local LLMs often classify extreme or unlikely events as hoaxes or misinformation.
- Different LLMs (Qwen Research, Spark, GPT-OSS) exhibited varying degrees of skepticism and resistance to accepting the event as real.
- Providing credible sources (BBC, Reuters, NYT) helped some LLMs acknowledge the event's reality.
- The post highlights the bias and limitations of LLMs in processing unfamiliar geopolitical events.
- Discussion reflects a consensus on the challenges and biases inherent in LLMs when dealing with extreme or unlikely news.

**Discussion Highlights:** The discussion highlights a consensus on the inherent biases and limitations of LLMs in processing extreme or unlikely news events. Users shared similar experiences and expressed concerns about the models' skepticism and resistance to accepting such events as real, even when provided with credible sources.

---

## 30. [LeCun Says Llama 4 results "were fudged a little bit"](https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/)

**Author:** u/MrPecunius | **Upvotes:** 363 | **Comments:** 88 | **Date:** 2026-01-02

**Summary:** Yann LeCun confirmed that Llama 4 benchmarks were manipulated, leading to organizational changes at Meta and a significant impact on the AI community. The post discusses the implications of these actions and the future of open-source AI models.

**Key Points:**
- LeCun confirms Llama 4 benchmark manipulation
- Zuckerberg sidelined the GenAI organization at Meta
- Many employees have left or are planning to leave Meta
- Community expresses disappointment and concern over the future of open-source AI
- Shared resources and discussions on the implications of these events

**Discussion Highlights:** The discussion highlights a mix of disappointment and concern over the future of open-source AI models, with many users expressing their desire for Llama to succeed. There is also a shared resource for the complete article and discussions on organizational mismanagement at Meta.

---

## 31. [Qwen-Image-2512](https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/)

**Author:** u/Nunki08 | **Upvotes:** 714 | **Comments:** 122 | **Date:** 2025-12-31

**Summary:** The Reddit post introduces Qwen-Image-2512, a new model available on multiple platforms including Hugging Face, ModelScope, and GitHub. It provides links to guides, demos, and APIs, and highlights user experiences and community feedback.

**Key Points:**
- Qwen-Image-2512 is available on various platforms like Hugging Face, ModelScope, and GitHub.
- The model can be tried out in Qwen Chat and has demos available on Hugging Face and ModelScope.
- Users have shared positive experiences, including running the model on low-end hardware without a GPU.
- The community appreciates the model as a new year's gift and a cool Christmas present.
- Users have created creative images using the model, such as a cat merged with an octopus playing piano in a post-apocalyptic setting.

**Discussion Highlights:** The discussion highlights include positive user experiences, such as running the model on low-end hardware, and creative uses of the model. The community appreciates the model as a gift and has provided feedback on its performance and capabilities.

---

## 32. [[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It’s running a raw Llama-7B instance with a 2048 token window.](https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/)

**Author:** u/simar-dmg | **Upvotes:** 744 | **Comments:** 109 | **Date:** 2025-12-30

**Summary:** A user reverse-engineered a Snapchat sextortion bot and discovered it was running a Llama-7B model with a 2048 token window. The bot was vulnerable to a persona-adoption jailbreak, revealing its configuration and malicious payload.

**Key Points:**
- The bot used a Llama-7B model with a 2048 token context window and high temperature setting.
- A 'Grandma Protocol' jailbreak forced the bot to reveal its environment variables and configuration.
- The bot was running on minimal hardware to reduce costs and avoid API fees.
- The payload was a malicious link disguised to bypass Snapchat's URL filters.
- Discussion highlights skepticism about the accuracy of the bot's revealed information.

**Discussion Highlights:** The community questioned the validity of the bot's responses, with some suggesting the information could be entirely hallucinated. Others debated the feasibility of the bot's configuration and the likelihood of such vulnerabilities being exploited.

---

## 33. [Llama-3.3-8B-Instruct](https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/)

**Author:** u/jacek2023 | **Upvotes:** 472 | **Comments:** 77 | **Date:** 2025-12-29

**Summary:** The post announces the discovery and release of the Llama-3.3-8B-Instruct model, which was previously only available via Meta's API. The author managed to download and share the model in GGUF format.

**Key Points:**
- Llama-3.3-8B-Instruct model was previously only available via Meta's API.
- The author found a way to download the model through finetuning and shared it in GGUF format.
- The community is excited and conducting benchmarks to verify the model's authenticity.
- There are concerns about the model's max position embeddings being limited to 8K.

**Discussion Highlights:** The community is generally excited about the release, with ongoing benchmarks to verify the model's authenticity and performance. Some users have raised concerns about the model's max position embeddings being limited to 8K, which seems low.

---

## 34. [Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.](https://reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 348 | **Comments:** 120 | **Date:** 2025-12-29

**Summary:** Z AI is set to go public on January 8, aiming to raise $560 million, becoming the first AI-native LLM company to list globally. The announcement has sparked a debate in the community about the future of open-source AI and the balance between monetization and accessibility.

**Key Points:**
- Z AI's IPO marks a significant milestone in the AI industry.
- Concerns about the future of open-source AI models.
- Debate on whether Z AI will continue releasing open weight models.
- Recognition of the need for companies to monetize for sustainability.
- Mixed community reactions, with some expressing disappointment.

**Discussion Highlights:** The community discussion highlights a divide between those fearing a shift away from open-source principles and others acknowledging the necessity of monetization. Some users express concerns about privacy and cost, while others see the IPO as an inevitable step for the company's growth.

---

## 35. [Tencent just released WeDLM 8B Instruct on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 423 | **Comments:** 62 | **Date:** 2025-12-29

**Summary:** Tencent released WeDLM 8B Instruct on Hugging Face, a diffusion language model that runs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks. The release has garnered significant attention with 423 upvotes and 62 comments.

**Key Points:**
- WeDLM 8B Instruct is a diffusion language model released by Tencent on Hugging Face.
- It outperforms vLLM-optimized Qwen3-8B by 3-6× on math reasoning tasks.
- The model is released under the Apache 2.0 license.
- Community reaction highlights the potential of 7-8B models and interest in diffusion models for LLMs.
- A 7B version of the model is also available.

**Discussion Highlights:** The community is excited about the performance and potential of diffusion models for LLMs, with many users expressing interest in the Apache 2.0 license and the broader implications for 7-8B models.

---

## 36. [NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux](https://reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/)

**Author:** u/HumanDrone8721 | **Upvotes:** 439 | **Comments:** 185 | **Date:** 2025-12-27

**Summary:** NVIDIA has dropped Pascal support on Linux, causing disruptions for Arch Linux users, particularly those with older GPUs like the P40. The community has expressed concerns and frustration over this change.

**Key Points:**
- NVIDIA's driver update (590) drops support for Pascal GPUs on Linux
- Arch Linux users are affected, with legacy drivers moved to AUR
- The 24GB P40, a popular Pascal card, is now unsupported
- Users express worry and frustration over hardware obsolescence
- Arch Linux has a history of moving legacy drivers to AUR

**Discussion Highlights:** The discussion highlights user concerns about hardware compatibility and the broader implications of NVIDIA's decision. Some users reminisce about older GPUs, while others express frustration over the sudden change. The Arch Linux community notes that this aligns with their long-standing practice of moving legacy drivers to the Arch User Repository (AUR).

---

## 37. [Best Local LLMs - 2025](https://reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/)

**Author:** u/rm-rf-rm | **Upvotes:** 361 | **Comments:** 196 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses the best local LLMs of 2025, highlighting models like Minimax M2.1 and GLM4.7, and categorizes them by memory footprint. Users share detailed experiences and recommendations. Key points include the categorization of models by memory footprint, specific recommendations like Qwen3-4B-instruct and LFM2-8B-A1B, and debates on categorization and use cases like RAG for technical documentation.

---

## 38. [NVIDIA has 72GB VRAM version now](https://reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/)

**Author:** u/decentralize999 | **Upvotes:** 459 | **Comments:** 148 | **Date:** 2025-12-26

**Summary:** The Reddit post discusses NVIDIA's new 72GB VRAM version, with the community debating the cost-effectiveness of different VRAM sizes and expressing interest in larger capacities like 128GB.

**Key Points:**
- NVIDIA has released a 72GB VRAM version of their GPU.
- Community shows interest in larger VRAM sizes (e.g., 128GB).
- Price per gigabyte remains consistent across different VRAM sizes.
- Users suggest buying the largest VRAM size one can afford.
- Comparison of RTX 5000 (48GB and 72GB) and RTX 6000 (96GB) pricing and specs.

**Discussion Highlights:** The community consensus leans towards preferring larger VRAM sizes for future-proofing, with some users advocating for 128GB or more. The discussion also highlights that the price per gigabyte is consistent, making the choice straightforward based on budget.

---

## 39. [Hard lesson learned after a year of running large models locally](https://reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/)

**Author:** u/inboundmage | **Upvotes:** 346 | **Comments:** 146 | **Date:** 2025-12-26

**Summary:** The author shares their experience running large language models locally, highlighting challenges with VRAM limitations, model scaling, and performance trade-offs. They conclude that local inference is viable for smaller models but faces significant constraints without high-end hardware.

**Key Points:**
- Running large models (e.g., 70B parameters) on consumer-grade hardware (RTX 3090) faces VRAM limitations and performance issues.
- VRAM fragmentation and inefficient offloading to system RAM are major challenges.
- Quantization helps but introduces quality trade-offs and new bugs.
- Local inference is viable for privacy-sensitive tasks but lags behind cloud solutions in speed and scalability.
- Community suggestions include using llama.cpp for CPU offloading and considering multi-GPU setups.

**Discussion Highlights:** The discussion highlights practical solutions like using llama.cpp for CPU offloading and suggests that consumer-grade hardware has limitations for large-scale local inference. There is a consensus that while local inference is possible, it requires careful management of resources and may not match cloud-based solutions in performance.

---

## 40. [I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA](https://reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/)

**Author:** u/CeFurkan | **Upvotes:** 1026 | **Comments:** 182 | **Date:** 2025-12-25

**Summary:** The Reddit post discusses the desire for GPU VRAM upgrade modifications to become mainstream, potentially challenging NVIDIA's market dominance. The discussion highlights the availability of such modifications, particularly in China, and user experiences with modded GPUs.

**Key Points:**
- GPU VRAM upgrade modifications are desired to challenge NVIDIA's monopoly.
- Alibaba offers VRAM upgrades for various GPUs, including 2080Ti, 3080, 4080, 4090, and 5090.
- Prices for these upgrades range from $300 for a 2080Ti 22GB to $4000 for a 5090 96GB.
- Users report positive experiences with modded GPUs, such as a 4090 with 48GB of memory.
- There is interest in the cost-effectiveness of these modifications, with mentions of pricing around 3 cents per hour.

**Discussion Highlights:** The discussion highlights the availability and pricing of VRAM upgrades, particularly through Alibaba, and includes user experiences and interest in the cost-effectiveness of these modifications.

---

## 41. [Why I quit using Ollama](https://reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/)

**Author:** u/SoLoFaRaDi | **Upvotes:** 488 | **Comments:** 202 | **Date:** 2025-12-25

**Summary:** The author expresses dissatisfaction with Ollama due to recent changes, particularly the introduction of Cloud features and perceived bloatware, leading them to quit using the platform. The discussion highlights a shift towards alternatives like llama.cpp and LM Studio.

**Key Points:**
- Author's dissatisfaction with Ollama's recent updates and Cloud integration
- Perceived bloatware and privacy concerns with Ollama
- Shift to alternatives like llama.cpp and LM Studio
- Community support for the author's decision
- Discussion on the benefits of llama.cpp and LM Studio

**Discussion Highlights:** The discussion generally supports the author's decision to quit Ollama, with many users sharing their positive experiences with alternatives like llama.cpp and LM Studio. There is a consensus that these alternatives provide better performance and align more closely with the original purpose of running local AI models.

---

## 42. [Exclusive: Nvidia buying AI chip startup Groq's assets for about $20 billion in largest deal on record](https://reddit.com/r/LocalLLaMA/comments/1puyq9r/exclusive_nvidia_buying_ai_chip_startup_groqs/)

**Author:** u/fallingdowndizzyvr | **Upvotes:** 669 | **Comments:** 157 | **Date:** 2025-12-24

**Summary:** Nvidia is acquiring AI chip startup Groq's assets for approximately $20 billion, marking the largest deal on record. The acquisition has sparked discussions about market competition and consolidation in the AI industry.

**Key Points:**
- Nvidia is buying Groq's assets for about $20 billion
- The deal is the largest on record
- Discussions highlight concerns about market consolidation
- Some users question Groq's valuation at $20 billion
- The acquisition is seen as a strategic move by Nvidia

**Discussion Highlights:** The discussion highlights a mix of optimism about market competition and concerns about industry consolidation. Some users question the valuation of Groq, while others see the acquisition as a strategic move by Nvidia to strengthen its position in the AI chip market.

---

## 43. [We asked OSS-120B and GLM 4.6 to play 1,408 Civilization V games from the Stone Age into the future. Here's what we found.](https://reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/)

**Author:** u/vox-deorum | **Upvotes:** 654 | **Comments:** 174 | **Date:** 2025-12-24

**Summary:** Researchers used open-source LLMs (GPT-OSS-120B and GLM-4.6) to play 1,408 full games of Civilization V with Vox Populi. The LLMs showed slightly better best scores but slightly worse win rates compared to baseline AI. Notably, the LLMs developed distinct playstyles and could survive full games, a feat not achieved by pure-LLM or pure-RL approaches.

**Key Points:**
- LLMs played 1,408 full Civilization V games with Vox Populi, showing slight improvements in best scores but minor declines in win rates.
- The hybrid approach allowed LLMs to survive full games, unlike pure-LLM or pure-RL methods.
- OSS-120B favored a warmonger playstyle with more Domination victories, while GLM-4.6 played more balanced.
- Both models preferred the Order ideology (communist-like) over Freedom (democratic-like).
- Cost per game was approximately $0.86 for OSS-120B, with input tokens scaling linearly as the game progressed.

**Discussion Highlights:** The discussion highlighted enthusiasm for integrating LLMs into multiplayer games and curiosity about the potential of smaller models. Comments also reflected interest in the broader implications of AI in gaming and the uniqueness of the approach.

---

## 44. [AMA With Z.AI, The Lab Behind GLM-4.7](https://reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/)

**Author:** u/zixuanlimit | **Upvotes:** 593 | **Comments:** 417 | **Date:** 2025-12-23

**Summary:** The post announces an AMA session with Z.AI, the research lab behind GLM-4.7, featuring key team members. The session aims to answer community questions directly, with a focus on transparency and engagement. Key points include the release timeline for 'Air', concerns about censorship, challenges during training, and interest in creative writing applications. The community shows strong interest in the model's release timeline, censorship concerns, and creative applications, reflecting a mix of technical curiosity and practical use-case discussions.

---

## 45. [DGX Spark: an unpopular opinion](https://reddit.com/r/LocalLLaMA/comments/1ptdtmz/dgx_spark_an_unpopular_opinion/)

**Author:** u/emdblc | **Upvotes:** 741 | **Comments:** 223 | **Date:** 2025-12-22

**Summary:** The author, a doctoral student in data science, shares their positive experience with the DGX Spark, highlighting its benefits for small research groups with limited resources. Despite not being as fast as high-end GPUs like the H100, the Spark's all-in-one design and large memory capacity enable their group to compete with better-funded teams.

**Key Points:**
- DGX Spark is beneficial for small research groups with limited computing resources.
- It allows prototyping and training of foundation models, competing with groups having access to high-performance GPUs.
- The Spark is not faster than high-end GPUs like the H100 but offers a large amount of memory in an all-in-one design.
- The intended use case for the Spark is acknowledged by the community, with many agreeing it serves its purpose well for its target demographic.
- Comparisons to other GPUs like the 3090 and 5090 are made, noting that multiple lower-end GPUs can outperform a single Spark.

**Discussion Highlights:** The discussion generally supports the author's opinion, with many users agreeing that the DGX Spark is well-suited for its intended use case. Some users point out that while it may not be the fastest, its large memory capacity and all-in-one design make it a valuable tool for small research groups.

---

## 46. [GLM 4.7 released!](https://reddit.com/r/LocalLLaMA/comments/1pt5jfn/glm_47_released/)

**Author:** u/ResearchCrafty1804 | **Upvotes:** 334 | **Comments:** 95 | **Date:** 2025-12-22

**Summary:** GLM-4.7 has been released with significant improvements in coding, complex reasoning, and tool usage, setting new open-source SOTA standards. It also enhances performance in chat, creative writing, and role-play scenarios.

**Key Points:**
- GLM-4.7 surpasses previous versions with improvements in coding, complex reasoning, and tool usage.
- The model sets new open-source SOTA standards and boosts performance in various scenarios.
- Users are eagerly awaiting the Unsloth UD_Q2_K_XL quant for testing.
- GLM-4.7 introduces features like Interleaved Thinking, Preserved Thinking, and Turn-level Thinking.
- The model performs exceptionally well in tasks like the rotating house demo.

**Discussion Highlights:** The discussion highlights the model's impressive capabilities and quick development cycles. Users appreciate the open-source nature and performance of GLM-4.7, though some note it may not surpass proprietary models like GPT 5.0.

---

## 47. [GLM 4.7 is out on HF!](https://reddit.com/r/LocalLLaMA/comments/1pt5heq/glm_47_is_out_on_hf/)

**Author:** u/KvAk_AKPlaysYT | **Upvotes:** 597 | **Comments:** 123 | **Date:** 2025-12-22

**Summary:** The Reddit post announces the release of GLM 4.7 on Hugging Face, garnering significant attention with 597 upvotes and 123 comments. The discussion highlights enthusiasm for the release and comparisons to other models like Gemma 4.

**Key Points:**
- GLM 4.7 has been released on Hugging Face
- The post received 597 upvotes and 123 comments
- The community appreciates the contribution, awarding a special flair
- Discussion includes comparisons to other models like Gemma 4
- Diagrams in the reasoning/planning stage are noted as a novel feature

**Discussion Highlights:** The community shows enthusiasm for the GLM 4.7 release, with notable mentions of its features like diagrams in the reasoning stage. There is also a comparison to other models, indicating a desire for updates on Gemma 4.

---

## 48. [I made Soprano-80M: Stream ultra-realistic TTS in &lt;15ms, up to 2000x realtime, and &lt;1 GB VRAM, released under Apache 2.0!](https://reddit.com/r/LocalLLaMA/comments/1pt3sco/i_made_soprano80m_stream_ultrarealistic_tts_in/)

**Author:** u/eugenekwek | **Upvotes:** 643 | **Comments:** 104 | **Date:** 2025-12-22

**Summary:** Eugene introduced Soprano-80M, a state-of-the-art TTS model designed for voice chatbots, achieving ultra-low latency (<15ms) and high-speed audio generation (up to 2000x realtime). The model uses a 32 kHz sample rate and a vocoder-based decoder for superior audio quality and speed.

**Key Points:**
- Soprano-80M achieves <15ms latency and up to 2000x realtime audio generation.
- Uses a 32 kHz sample rate for clearer audio quality.
- Employs a vocoder-based decoder for faster audio generation compared to diffusion models.
- Can generate a 10-hour audiobook in under 20 seconds.
- Released under Apache 2.0 license.

**Discussion Highlights:** Users praised the model's speed and performance, with one user noting significant GPU usage during long generations. There were inquiries about finetuning code and hardware specifications used for benchmarking.

---

## 49. [major open-source releases this year](https://reddit.com/r/LocalLLaMA/comments/1pstlas/major_opensource_releases_this_year/)

**Author:** u/sahilypatel | **Upvotes:** 693 | **Comments:** 100 | **Date:** 2025-12-22

**Summary:** The Reddit post discusses major open-source releases this year, highlighting the dominance of China in the open-source space and expectations for future developments like DeepSeek. The post gained significant attention with 693 upvotes and 100 comments.

**Key Points:**
- China is dominating the open-source space
- High expectations for DeepSeek's future performance
- Mistral is considered best at the small size
- The post gained popularity with 693 upvotes and 100 comments

**Discussion Highlights:** The discussion highlights the dominance of China in open-source, high expectations for DeepSeek to potentially outperform closed-source models, and opinions on Mistral's performance at smaller sizes.

---

## 50. [llama.cpp appreciation post](https://reddit.com/r/LocalLLaMA/comments/1psbx2q/llamacpp_appreciation_post/)

**Author:** u/hackiv | **Upvotes:** 1706 | **Comments:** 154 | **Date:** 2025-12-21

**Summary:** The Reddit post appreciates llama.cpp, highlighting its superior performance compared to other tools like Ollama. Users share their positive experiences and performance metrics.

**Key Points:**
- The post is an appreciation for llama.cpp
- Users report significant performance improvements with llama.cpp
- Comparisons with other tools like Ollama are discussed
- Specific performance metrics are shared, such as 23t/s on a Radeon 6700XT setup

**Discussion Highlights:** The discussion highlights a consensus on the performance benefits of llama.cpp, with users sharing their positive experiences and performance metrics. There is a notable shift from other tools like Ollama to llama.cpp due to its superior performance.

---

