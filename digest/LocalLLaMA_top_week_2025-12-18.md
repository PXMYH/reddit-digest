# r/LocalLLaMA Reading Digest

**Period:** 2025-12-18 to 2025-12-18
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster](https://reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/)

**Author:** u/geerlingguy | **Upvotes:** 250 | **Comments:** 74 | **Date:** 2025-12-18

**Summary:** The post discusses performance testing of Kimi K2 on a cluster of 4 Mac Studios using RDMA Tensor settings, highlighting the challenges in benchmarking and the potential for further testing.

**Key Points:**
- Performance testing of Kimi K2 on a 4x Mac Studio cluster with RDMA Tensor settings.
- Challenges in benchmarking due to lack of tools like llama-bench in Exo.
- Mention of additional data and resources in linked GitHub issue and blog post.
- Community appreciation for the contribution and testing efforts.
- Comparison with cost-effective alternatives like EPYC DDR5 systems.

**Discussion Highlights:** The discussion highlights community appreciation for the testing efforts, additional resources shared by the author, and a brief comparison with alternative hardware solutions.

---

## 2. [Google's Gemma models family](https://reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/)

**Author:** u/jacek2023 | **Upvotes:** 377 | **Comments:** 97 | **Date:** 2025-12-18

**Summary:** The Reddit post discusses Google's Gemma models family, focusing on FunctionGemma, which is designed for fine-tuning in function-calling tasks. The community is excited about this development and speculates about new models in the Gemma family.

**Key Points:**
- FunctionGemma is intended for fine-tuning in function-calling tasks, including multi-turn use cases.
- There is speculation about three new Gemma models based on the number of visible models in the collection.
- The community is highly enthusiastic about Google's advancements in the Gemma models family.

**Discussion Highlights:** The discussion highlights the community's excitement about FunctionGemma and its potential applications. There is also speculation about new models in the Gemma family, with some users calculating the possibility of three new models.

---

## 3. [MiraTTS: High quality and fast TTS model](https://reddit.com/r/LocalLLaMA/comments/1pper90/miratts_high_quality_and_fast_tts_model/)

**Author:** u/SplitNice1982 | **Upvotes:** 134 | **Comments:** 47 | **Date:** 2025-12-17

**Summary:** MiraTTS is a high-quality, fast TTS model capable of generating realistic 48khz speech at 100x realtime, optimized for efficiency and low latency. It supports multilingual versions and is memory-efficient, working with GPUs as low as 6GB VRAM.

**Key Points:**
- 100x realtime speed
- High-quality 48khz speech
- Memory efficient (6GB VRAM)
- Low latency (150ms)
- Multilingual and multispeaker support in progress

**Discussion Highlights:** Users inquired about multilingual support, voice cloning, and comparisons with KaniTTS. Some reported hardware compatibility issues with cheaper GPUs.

---

## 4. [Nvidia plans heavy cuts to GPU supply in early 2026](https://reddit.com/r/LocalLLaMA/comments/1pp8vo4/nvidia_plans_heavy_cuts_to_gpu_supply_in_early/)

**Author:** u/HumanDrone8721 | **Upvotes:** 336 | **Comments:** 171 | **Date:** 2025-12-17

**Summary:** Nvidia plans to significantly reduce GPU supply in early 2026, which, combined with similar cuts by Micron and Samsung, could make building gaming PCs challenging. The discussion highlights concerns about market competition and corporate spending priorities.

**Key Points:**
- Nvidia plans heavy cuts to GPU supply in early 2026
- Micron and Samsung are also cutting consumer RAM and SSD production
- 2026 may be a difficult year for building gaming PCs due to supply shortages
- Potential opportunity for new competitors in the market
- Criticism of corporate spending on stock buybacks instead of growth

**Discussion Highlights:** The discussion reflects concerns about the impact of supply cuts on gaming PC builds and frustration with corporate financial decisions. Some users see potential for new competition, while others criticize the focus on stock buybacks over innovation.

---

## 5. [Hey, LocalLLaMa. We need to talk...](https://reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/)

**Author:** u/Eisenstein | **Upvotes:** 380 | **Comments:** 124 | **Date:** 2025-12-17

**Summary:** The post highlights the importance of community engagement and support for contributors in the r/LocalLLaMA subreddit, emphasizing the need for upvotes and constructive feedback to encourage continued sharing and development.

**Key Points:**
- The author urges the community to engage with and support smaller posts and projects.
- Constructive feedback and upvotes are crucial for encouraging contributors.
- There is a mix of supportive and critical comments, with some users appreciating the sentiment while others express frustration with low-quality projects.
- The discussion highlights the tension between encouraging contributions and maintaining quality standards.
- The community values both positive reinforcement and honest critique.

**Discussion Highlights:** The discussion reveals a consensus on the importance of community engagement but also highlights differing opinions on the quality of contributions. Some users appreciate the call for support, while others criticize the prevalence of low-effort or AI-generated projects.

---

## 6. [Drummer's Cydonia and Magidonia 24B v4.3 - The best pair of Cydonia for RP yet!](https://reddit.com/r/LocalLLaMA/comments/1pp2j60/drummers_cydonia_and_magidonia_24b_v43_the_best/)

**Author:** u/TheLocalDrummer | **Upvotes:** 131 | **Comments:** 20 | **Date:** 2025-12-17

**Summary:** The post announces the release of Drummer's Cydonia and Magidonia 24B v4.3 models, described as the best pair for role-playing yet, with links to their respective repositories. The author expresses gratitude to patrons for their support and mentions a recent difficult choice made possible by their backing.

**Key Points:**
- Release of Cydonia-24B-v4.3 and Magidonia-24B-v4.3 models
- Models are praised for their quality, especially Magidonia
- Author thanks patrons for their support and freedom
- Links to Hugging Face repositories provided
- Top comments highlight appreciation and technical tips

**Discussion Highlights:** The discussion is largely positive, with users expressing gratitude and sharing technical tips, such as attaching a vision mmproj to the gguf. There is a consensus that Magidonia 4.3 is excellent and widely used.

---

## 7. [Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds.](https://reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/)

**Author:** u/themixtergames | **Upvotes:** 1110 | **Comments:** 127 | **Date:** 2025-12-17

**Summary:** Apple has introduced SHARP, a model capable of generating photorealistic 3D Gaussian representations from a single image in seconds. The model is highlighted for its speed and compatibility with Apple devices like the MacBook Pro M1 Max and Apple Vision Pro.

**Key Points:**
- SHARP generates photorealistic 3D Gaussian representations from a single image.
- The model operates in seconds and is optimized for Apple hardware.
- Examples were rendered in real-time on Apple Vision Pro and generated in 5–10 seconds on a MacBook Pro M1 Max.
- The model requires CUDA GPU for rendering trajectories.
- Community interest includes comparisons to cyberpunk's braindance and inquiries about content compatibility.

**Discussion Highlights:** The discussion highlights the model's speed and compatibility with Apple devices, with some users drawing comparisons to futuristic technologies like cyberpunk's braindance. There is also curiosity about the model's capabilities with different types of content.

---

## 8. [LangChain and LlamaIndex are in "steep decline" according to new ecosystem report. Anyone else quietly ditching agent frameworks?](https://reddit.com/r/LocalLLaMA/comments/1pox733/langchain_and_llamaindex_are_in_steep_decline/)

**Author:** u/Exact-Literature-395 | **Upvotes:** 205 | **Comments:** 57 | **Date:** 2025-12-17

**Summary:** The Reddit post discusses a report indicating a decline in community activity around LangChain, LlamaIndex, and AutoGen, attributing it to reduced community investment. The author shares their personal experience of moving away from LangChain due to its complexity and inefficiency, and questions whether agent frameworks are still necessary given the improvements in base models.

**Key Points:**
- LangChain, LlamaIndex, and AutoGen are experiencing a steep decline in community activity.
- The decline is attributed to reduced community investment and the complexity of these frameworks.
- The author found that removing LangChain and using APIs directly simplified their codebase and improved debugging.
- Some commenters criticize LangChain for being bloated, poorly designed, and non-Pythonic.
- There is a discussion about whether agent frameworks are still essential or if they add unnecessary complexity.

**Discussion Highlights:** The discussion highlights a general consensus that LangChain and similar frameworks are overly complex and may not be necessary for many use cases. Commenters express frustration with the frameworks' design and performance, while some defend their utility for complex workflows. The overall tone suggests a shift towards simpler, more direct approaches to working with LLMs.

---

## 9. [anthropic blog on code execution for agents. 98.7% token reduction sounds promising for local setups](https://reddit.com/r/LocalLLaMA/comments/1powhy6/anthropic_blog_on_code_execution_for_agents_987/)

**Author:** u/Zestyclose_Ring1123 | **Upvotes:** 134 | **Comments:** 33 | **Date:** 2025-12-17

**Summary:** Anthropic's blog discusses a new approach to code execution for agents, claiming a 98.7% token reduction, which could significantly benefit local setups by reducing context limits and improving privacy. The approach involves letting models explore tools on demand rather than preloading all tool definitions.

**Key Points:**
- Anthropic's approach reduces token usage by 98.7%, making it promising for local setups.
- The method involves model-generated code to orchestrate tools, reducing context limits and improving privacy.
- Sandboxing is a main challenge for running model-generated code locally.
- Similar patterns already exist in projects like HF's smolagents.
- The approach could make complex agents viable on consumer hardware with smaller context windows.

**Discussion Highlights:** The discussion highlights that similar patterns already exist in other projects like HF's smolagents, with some users expressing skepticism about Anthropic's originality. There is also mention of alternative approaches like generating a DAG of steps to reduce sandboxing needs.

---

## 10. [Peak LLM Wars: Xiaomi Blocks Kimi Employees on Twitter](https://reddit.com/r/LocalLLaMA/comments/1pow797/peak_llm_wars_xiaomi_blocks_kimi_employees_on/)

**Author:** u/nekofneko | **Upvotes:** 130 | **Comments:** 25 | **Date:** 2025-12-17

**Summary:** The Reddit post discusses the ongoing 'LLM wars' with a focus on Xiaomi blocking Kimi employees on Twitter, highlighting the competitive and dramatic nature of the AI industry.

**Key Points:**
- Xiaomi and Kimi are involved in a public dispute on Twitter.
- The post includes images that contribute to the narrative of 'LLM wars'.
- Top comments mention former DeepSeek members possibly being part of Xiaomi's team.
- Comparisons are drawn to other industry rivalries like Musk vs. Altman and Meta vs. Zuckerberg.
- The discussion humorously compares the situation to drama in the VTuber community.

**Discussion Highlights:** The discussion highlights the competitive nature of the AI industry, with users drawing parallels to other tech rivalries and humorously comparing the situation to online drama communities.

---

## 11. [Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model](https://reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 1127 | **Comments:** 119 | **Date:** 2025-12-17

**Summary:** Microsoft's TRELLIS 2-4B is an open-source image-to-3D model using Flow-Matching Transformers with a Sparse Voxel-based 3D VAE. It has 4 billion parameters and converts single images into 3D assets. The model has received mixed feedback, with some users praising its performance while others find it lacking in practical applications.

**Key Points:**
- Model Type: Flow-Matching Transformers with Sparse Voxel based 3D VAE
- Parameters: 4 Billion
- Input: Single Image
- Output: 3D Asset
- Mixed user feedback on performance and practicality

**Discussion Highlights:** Users have provided varied feedback, with some praising the model's performance and others criticizing its practicality. There are suggestions for improvements, such as the ability to upload a series of images for better results.

---

## 12. [QwenLong-L1.5: Revolutionizing Long-Context AI](https://reddit.com/r/LocalLLaMA/comments/1pokpha/qwenlongl15_revolutionizing_longcontext_ai/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 212 | **Comments:** 27 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses the QwenLong-L1.5 model, which achieves state-of-the-art long-context reasoning with novel data synthesis, stabilized RL, and memory management for contexts up to 4M tokens. The model is available on HuggingFace and has garnered significant interest and discussion.

**Key Points:**
- QwenLong-L1.5 achieves SOTA long-context reasoning with up to 4M tokens.
- The model uses novel data synthesis, stabilized RL, and memory management.
- Integration into llama.cpp may require additional work.
- The exact query template is crucial for optimal performance.
- The model has received positive feedback from the community.

**Discussion Highlights:** The discussion highlights the model's significant capabilities and potential challenges in integration. Users emphasize the importance of using the exact query template for best results and express overall enthusiasm for the model's performance.

---

## 13. [8x Radeon 7900 XTX Build for Longer Context Local Inference - Performance Results &amp; Build Details](https://reddit.com/r/LocalLLaMA/comments/1pogwb6/8x_radeon_7900_xtx_build_for_longer_context_local/)

**Author:** u/Beautiful_Trust_8151 | **Upvotes:** 718 | **Comments:** 210 | **Date:** 2025-12-16

**Summary:** The post details a custom multi-GPU setup using 8x AMD Radeon 7900 XTX cards for local AI inference, achieving 192 GB VRAM and stable performance with a 131072-token context window. The build cost around $6-7k and offers flexibility and long-context capability for specific work use cases.

**Key Points:**
- 8x AMD Radeon 7900 XTX GPUs provide 192 GB VRAM for local AI inference
- Performance testing shows stable results with a 131072-token context window
- Total build cost is around $6-7k, offering flexibility and long-context capability
- The system consumes about 900 watts during prompt processing and inferencing
- Discussion highlights appreciation for the build and suggestions for further optimization

**Discussion Highlights:** The discussion highlights appreciation for the build's capabilities and suggestions for further optimization, such as switching to Linux, ROCm, and vLLM for potentially better performance. The community also expressed admiration for the build's cost-effectiveness and performance.

---

## 14. [Nemotron 3 Nano 30B is Amazing! (TLDR)](https://reddit.com/r/LocalLLaMA/comments/1pocsdy/nemotron_3_nano_30b_is_amazing_tldr/)

**Author:** u/DonkeyBonked | **Upvotes:** 205 | **Comments:** 125 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses the user's experience with Nemotron 3 Nano 30B, highlighting its impressive token efficiency and performance on their hardware setup. The user compares it favorably to other models like Devstral 2 Small 24B and Qwen models, noting its ability to handle large context sizes efficiently.

**Key Points:**
- Nemotron 3 Nano 30B shows high token efficiency and fits well within the user's hardware constraints.
- The model outperforms Devstral 2 Small 24B and Qwen models in coding tasks and context handling.
- The user's setup involves a combination of GPUs and specific configurations to optimize performance.
- Discussion highlights include comparisons with other models like Qwen 3 30B A3B and IBM Granite 4 Hybrid Small.
- The model is praised for being truly open source and fast, though some users still prefer Qwen models for certain tasks.

**Discussion Highlights:** The discussion revolves around the performance and efficiency of Nemotron 3 Nano 30B compared to other models. Users share their experiences and preferences, with some highlighting the model's strengths in coding tasks and token efficiency, while others express a preference for models like Qwen 3 30B A3B.

---

## 15. [32GB Mi50's were getting so expensive that I ended up buying a 32GB w6800 for about the same price instead](https://reddit.com/r/LocalLLaMA/comments/1pob44f/32gb_mi50s_were_getting_so_expensive_that_i_ended/)

**Author:** u/EmPips | **Upvotes:** 233 | **Comments:** 42 | **Date:** 2025-12-16

**Summary:** The author opted for a 32GB w6800 GPU instead of a 32GB Mi50 due to similar pricing, highlighting the convenience and performance of the w6800. The discussion includes comparisons with other GPUs like the AMD Radeon AI PRO R9700 and Zotac 3090. Key points include the author's choice, the convenience of the w6800, and mentions of alternatives. The discussion revolves around cost-effectiveness and performance, with a consensus leaning towards the w6800 for its balance of price and performance.

---

## 16. [8 Million Users' AI Conversations Sold for Profit by "Privacy" Extensions | Koi Blog](https://reddit.com/r/LocalLLaMA/comments/1poal2a/8_million_users_ai_conversations_sold_for_profit/)

**Author:** u/ManThigh | **Upvotes:** 155 | **Comments:** 46 | **Date:** 2025-12-16

**Summary:** The Reddit post highlights privacy concerns regarding browser extensions selling AI conversation data of millions of users, emphasizing the importance of using local AI models and auditing extensions.

**Key Points:**
- Browser extensions like Urban VPN Proxy sold AI conversation data of millions of users
- Importance of running local AI models to avoid privacy breaches
- Need to audit browser extensions for data collection
- User interactions with AI are highly valuable and targeted by companies
- Consensus on punishing companies that buy such data

**Discussion Highlights:** The discussion emphasizes the value of local AI setups and the need for greater scrutiny of browser extensions. There is a strong consensus on the importance of privacy and the need to hold companies accountable for buying user data.

---

## 17. [Finally managed to run Qwen-2.5-7B on a 4GB GTX 1050 without CPU offloading (Surgical Memory Alignment)](https://reddit.com/r/LocalLLaMA/comments/1po97ad/finally_managed_to_run_qwen257b_on_a_4gb_gtx_1050/)

**Author:** u/HuseyinKama | **Upvotes:** 150 | **Comments:** 48 | **Date:** 2025-12-16

**Summary:** The post discusses a method called 'Surgical Memory Alignment' to run Qwen-2.5-7B on a 4GB GTX 1050 without CPU offloading, saving VRAM and improving speed. The author open-sourced the solution as QKV Core.

**Key Points:**
- Standard GGUF quantization tools add padding that wastes memory, causing OOM errors on low-end GPUs.
- Surgical Alignment trims and realigns memory blocks to save VRAM and improve I/O load times.
- The method saved 44MB per model, allowing Qwen-2.5-7B to run purely on GPU with a 34% improvement in load times.
- The solution is open-sourced as QKV Core, targeting users with 4GB/6GB GPUs.
- Discussion includes skepticism about the code and questions about the optimization process.

**Discussion Highlights:** The discussion includes skepticism about the code's effectiveness, questions about the optimization process, and praise for the work's potential impact on low-end GPU users.

---

## 18. [I was bored](https://reddit.com/r/LocalLLaMA/comments/1po8yt0/i_was_bored/)

**Author:** u/MyLovelyAngelKirino | **Upvotes:** 132 | **Comments:** 70 | **Date:** 2025-12-16

**Summary:** The author, who is unemployed with spare time and hardware, built a high-performance computer system. The post garnered significant attention, with comments praising the hardware specifications and expressing admiration.

**Key Points:**
- Author built a system with 3x 3090 GPUs, 512GB RAM, and an Epyc 7663 56-core CPU
- Post received 132 upvotes and 70 comments
- Community reactions included admiration, humor, and requests for details on water-cooling components
- Some users joked about the author's ability to acquire such hardware effortlessly

**Discussion Highlights:** The discussion highlighted admiration for the build's specifications and neatness, with some users humorously expressing envy. There was also a request for more details on the water-cooling setup.

---

## 19. [Meta announced a new SAM Audio Model for audio editing that can segment sound from complex audio mixtures using text, visual, and time span prompts.](https://reddit.com/r/LocalLLaMA/comments/1po7i0c/meta_announced_a_new_sam_audio_model_for_audio/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 502 | **Comments:** 84 | **Date:** 2025-12-16

**Summary:** Meta announced a new SAM Audio Model that can segment sound from complex audio mixtures using text, visual, and time span prompts, transforming audio processing.

**Key Points:**
- SAM Audio Model can isolate any sound from complex audio mixtures using text, visual, and time span prompts.
- Potential applications include isolating and subtracting unwanted sounds in Microsoft Teams meetings.
- The model can accurately pick out specific sounds from complex audio mixtures.
- Model sizes and specifications are available in the provided image link.
- The model can handle subtle sounds, such as a microphone tap, when prompted.

**Discussion Highlights:** The discussion highlights the potential applications of the SAM Audio Model, such as improving audio quality in virtual meetings by isolating and removing unwanted sounds. Users are impressed by the model's ability to accurately segment specific sounds from complex audio mixtures.

---

## 20. [Allen Institute for AI introduces Molmo 2](https://reddit.com/r/LocalLLaMA/comments/1po78bl/allen_institute_for_ai_introduces_molmo_2/)

**Author:** u/Agitated_Camel1886 | **Upvotes:** 239 | **Comments:** 22 | **Date:** 2025-12-16

**Summary:** Allen Institute for AI introduces Molmo 2, an 8B model capable of video analysis tasks like Video QA, counting, and dense captioning. The community is impressed by its performance and the public availability of datasets.

**Key Points:**
- Molmo 2 is an 8B model with advanced video analysis capabilities
- Allen AI releases datasets publicly, fostering community advancements
- An AMA was held to discuss Olmo 3 and Molmo 2
- The model's benchmarks are highly praised for its size
- Community reactions highlight excitement and curiosity about the model's capabilities

**Discussion Highlights:** The community is highly impressed by Molmo 2's capabilities and the public release of datasets. Key discussions include the AMA announcement, performance benchmarks, and general excitement about the model's potential.

---

## 21. [XiaomiMiMo/MiMo-V2-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1po3bn4/xiaomimimomimov2flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 233 | **Comments:** 51 | **Date:** 2025-12-16

**Summary:** The post discusses MiMo-V2-Flash, a Mixture-of-Experts (MoE) language model with 309B total parameters and 15B active parameters, designed for high-speed reasoning and agentic workflows. The model has shown impressive performance on multilingual SWE tasks, surpassing larger models like Sonnet 4.5 and Gemini 3. The discussion includes details about hardware requirements and skepticism about the performance claims.

**Key Points:**
- MiMo-V2-Flash is a MoE language model with 309B total parameters and 15B active parameters
- It outperforms larger models like Sonnet 4.5 and Gemini 3 on multilingual SWE tasks
- The model weights have been released
- Hardware requirements include 2 RTX 5060 Ti 16GB GPUs and 128 GB RAM for q4
- There is skepticism about the performance claims

**Discussion Highlights:** The discussion highlights the release of model weights and the impressive performance claims, but also includes skepticism about the model's performance given its size. Users also discuss hardware requirements and potential larger versions of the model.

---

## 22. [GLM-4.5V, GLM-4.6V and GLM_4.6V-Flash are now supported by llama.cpp (GGUFs)](https://reddit.com/r/LocalLLaMA/comments/1po18y9/glm45v_glm46v_and_glm_46vflash_are_now_supported/)

**Author:** u/jacek2023 | **Upvotes:** 167 | **Comments:** 34 | **Date:** 2025-12-16

**Summary:** The post announces support for GLM-4.5V, GLM-4.6V, and GLM-4.6V-Flash in llama.cpp with GGUFs, highlighting a recent merge of vision encoder support. The community expresses appreciation and discusses compatibility and comparisons with other models like Qwen3-VL-4B.

**Key Points:**
- Support for GLM-4.5V, GLM-4.6V, and GLM-4.6V-Flash has been added to llama.cpp with GGUFs.
- The merge includes support for vision encoders in these models.
- Community members express gratitude and excitement about the update.
- Some users report issues with vision support in GGUF repositories for GLM-4.6-Flash.
- Discussions include comparisons with other vision-language models like Qwen3-VL-4B.

**Discussion Highlights:** The community is generally positive about the update, with some users expressing gratitude and excitement. There are discussions about the compatibility of vision support in GGUF repositories and comparisons with other models like Qwen3-VL-4B. Some users report spending significant time setting up the new models.

---

## 23. [Qwen3 Next speed optimization has been merged into llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1pnz9xu/qwen3_next_speed_optimization_has_been_merged/)

**Author:** u/jacek2023 | **Upvotes:** 214 | **Comments:** 25 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses the speed optimization for Qwen3 Next in llama.cpp, highlighting significant performance improvements across different hardware configurations.

**Key Points:**
- Speed optimization for Qwen3 Next has been merged into llama.cpp
- Performance on M1 64GB improved from 12 t/s to 18 t/s
- Win11 + RTX5090 achieves 37.x t/s with Vulkan and 100+ t/s with UD-Q2_K_XL
- Qwen3-30B runs at around 58 t/s on M1 64GB
- Users report noticeable speed improvements and positive experiences

**Discussion Highlights:** Users report significant speed improvements, with specific performance metrics shared for different hardware setups. The consensus is positive, with users appreciating the optimization efforts.

---

## 24. [I may have over-quantized this little guy.](https://reddit.com/r/LocalLLaMA/comments/1pnz80z/i_may_have_overquantized_this_little_guy/)

**Author:** u/AllergicToTeeth | **Upvotes:** 137 | **Comments:** 30 | **Date:** 2025-12-16

**Summary:** The Reddit post humorously suggests the author may have excessively quantized a model, with comments joking about OpenAI and GPT versions.

**Key Points:**
- Author may have over-quantized a model
- Comments reference OpenAI and system prompts
- Jokes about GPT versions and leaks
- Mentions of quick loading with Q0 quantization

**Discussion Highlights:** The discussion is light-hearted with jokes about OpenAI and GPT versions, but lacks serious technical consensus.

---

## 25. [It was Ilya who "closed" OpenAI](https://reddit.com/r/LocalLLaMA/comments/1pnxekt/it_was_ilya_who_closed_openai/)

**Author:** u/licuphand | **Upvotes:** 513 | **Comments:** 229 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses Ilya's role in 'closing' OpenAI, sparking a debate on trust in AI governance and leadership dynamics among key figures like Elon Musk, Ilya Sutskever, and Sam Altman.

**Key Points:**
- Distrust in companies handling AI if the public cannot be trusted with it
- Historical context of oversight with the phrase 'Who will watch the watchmen?'
- Leadership struggles among Elon Musk, Ilya Sutskever, and Sam Altman
- Criticism of the philosophy behind restricting AI access
- Observation that multiple AI entities (SSI, xAI, OpenAI) are becoming 'CloseAI'

**Discussion Highlights:** The discussion highlights a consensus around the risks of centralized AI control, with many users expressing skepticism about corporate leadership in AI development. The historical reference to oversight and the leadership dynamics among key AI figures are recurring themes.

---

## 26. [Alibaba Open-Sources CosyVoice 3, a New TTS Model](https://reddit.com/r/LocalLLaMA/comments/1pnusp9/alibaba_opensources_cosyvoice_3_a_new_tts_model/)

**Author:** u/nekofneko | **Upvotes:** 215 | **Comments:** 31 | **Date:** 2025-12-16

**Summary:** Alibaba has open-sourced CosyVoice 3, a new TTS model with advanced features like multi-lingual support, high naturalness, and low latency. The model supports various languages, dialects, and emotions, and includes features like pronunciation inpainting and text normalization.

**Key Points:**
- Supports 9 common languages and 18+ Chinese dialects/accents
- Achieves state-of-the-art performance in content consistency and naturalness
- Features low latency (150ms) and supports both text-in and audio-out streaming
- Includes pronunciation inpainting and text normalization capabilities
- Supports various instructions like emotions, speed, and volume

**Discussion Highlights:** The discussion highlights comparisons with other models like Chatterbox and Microsoft VibeVoice, with users expressing interest in larger model versions and real-time voice cloning capabilities.

---

## 27. [New budget local AI rig](https://reddit.com/r/LocalLLaMA/comments/1pnllux/new_budget_local_ai_rig/)

**Author:** u/vucamille | **Upvotes:** 153 | **Comments:** 38 | **Date:** 2025-12-15

**Summary:** The user built a budget local AI rig with two 16GB MI50 GPUs, a Qiyida X99 motherboard, and a Xeon E5 2680 V4 CPU for around $650. The system works well with ROCm 7.0.2 and handles basic inference tasks, with plans for future upgrades.

**Key Points:**
- Budget build with two 16GB MI50 GPUs for ~$650
- ROCm 7.0.2 works, but multi-GPU had issues with the latest ROCm release
- System is expandable and can handle gaming alongside AI tasks
- Community praises the cost-effectiveness and potential of the build
- Requests for benchmarks and encouragement for multi-GPU optimization

**Discussion Highlights:** The community praised the build for its affordability and expandability, with many expressing interest in benchmarks and encouraging the OP to optimize the multi-GPU setup for full 32GB VRAM utilization.

---

## 28. [I'm strong enough to admit that this bugs the hell out of me](https://reddit.com/r/LocalLLaMA/comments/1pnfaqo/im_strong_enough_to_admit_that_this_bugs_the_hell/)

**Author:** u/ForsookComparison | **Upvotes:** 1693 | **Comments:** 353 | **Date:** 2025-12-15

**Summary:** The Reddit post expresses frustration with an unspecified issue, likely related to computing performance or workstation setups. The discussion includes humorous references to RAM and debates about the performance of different workstations.

**Key Points:**
- The post title indicates frustration with an unspecified issue
- Top comments include humorous references to RAM and workstation performance
- Discussion involves comparisons between Mac and GPU setups
- The post gained significant attention with 1693 upvotes and 353 comments

**Discussion Highlights:** The discussion highlights a mix of humor and technical debate, with some users joking about RAM and others seriously comparing the performance of different workstation setups, particularly Mac vs. GPU-based systems.

---

## 29. [They're finally here (Radeon 9700)](https://reddit.com/r/LocalLLaMA/comments/1pnd5uf/theyre_finally_here_radeon_9700/)

**Author:** u/Zeikos | **Upvotes:** 359 | **Comments:** 67 | **Date:** 2025-12-15

**Summary:** The post announces the arrival of Radeon 9700 GPUs, sparking community interest and requests for benchmarks. Users express nostalgia about the historic GPU name and eagerly await performance data.

**Key Points:**
- Radeon 9700 GPUs have arrived
- Community requests benchmarks and performance data
- Nostalgia about the historic Radeon 9700 name
- Interest in testing during holidays
- Specific requests for inference, training, and heat/noise benchmarks

**Discussion Highlights:** The community is enthusiastic about the new GPUs, with a strong focus on benchmarking and performance testing. There is a mix of nostalgia and practical interest in evaluating the new hardware's capabilities.

---

## 30. [status of Nemotron 3 Nano support in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1pnc045/status_of_nemotron_3_nano_support_in_llamacpp/)

**Author:** u/jacek2023 | **Upvotes:** 178 | **Comments:** 32 | **Date:** 2025-12-15

**Summary:** The Reddit post discusses the status of Nemotron 3 Nano support in llama.cpp, highlighting a GitHub pull request and community reactions. The discussion emphasizes the importance of collaboration between organizations and the llama.cpp project for new model architectures.

**Key Points:**
- Nemotron 3 Nano support is being added to llama.cpp via a GitHub pull request.
- The model sizes (Q4_K_M and Q4_K_XL) are noted to be around 24GB, which is a point of discussion.
- Community appreciation for Nvidia's approach and call for other labs to follow suit.
- Consensus that organizations should work with llama.cpp for early support of new models.

**Discussion Highlights:** The community positively views Nvidia's collaboration with llama.cpp and encourages other organizations to prioritize early integration of their models with widely-used tools like llama.cpp.

---

## 31. [NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model!](https://reddit.com/r/LocalLLaMA/comments/1pn8upp/nvidia_releases_nemotron_3_nano_a_new_30b_hybrid/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 841 | **Comments:** 178 | **Date:** 2025-12-15

**Summary:** NVIDIA has released Nemotron 3 Nano, a 30B hybrid reasoning model with a 1M context window, claiming best-in-class performance for SWE-Bench, reasoning, and chat. The model is available in GGUF format on Hugging Face.

**Key Points:**
- Nemotron 3 Nano is a 30B hybrid reasoning model with a 1M context window.
- It claims best-in-class performance for SWE-Bench, reasoning, and chat.
- The model is available in GGUF format on Hugging Face.
- It is part of the Nemotron 3 family of MoE models, which includes three sizes.
- Users report exceptionally fast generation speeds (110 t/s).

**Discussion Highlights:** The community is impressed by the model's speed and performance, with some expressing surprise at the 'nano' designation for a 30B model. Key discussion points include the model's speed, its place in the Nemotron 3 family, and its performance claims.

---

## 32. [NVIDIA Nemotron 3 Nano 30B A3B released](https://reddit.com/r/LocalLLaMA/comments/1pn8h5h/nvidia_nemotron_3_nano_30b_a3b_released/)

**Author:** u/rerri | **Upvotes:** 277 | **Comments:** 83 | **Date:** 2025-12-15

**Summary:** NVIDIA has released the Nemotron 3 Nano 30B A3B model, featuring a hybrid Mamba-Transformer architecture with 31.6B total parameters and exceptional inference efficiency. The model is fully open-source and designed for high throughput and low latency.

**Key Points:**
- Hybrid Mamba-Transformer MoE architecture with 31.6B total parameters
- Up to 4x faster inference than Nemotron Nano 2 and 3.3x faster than leading models in its size category
- 1M-token context window and best-in-class reasoning accuracy
- Fully open weights, datasets, training recipes, and framework
- Community discussion includes Llama.cpp integration and performance on consumer hardware

**Discussion Highlights:** The community is actively discussing integration with Llama.cpp, performance on consumer hardware like the 3090, and concerns about the use of synthetic data in training. Some users report high speed but mixed performance results.

---

## 33. [New Google model incoming!!!](https://reddit.com/r/LocalLLaMA/comments/1pn37mw/new_google_model_incoming/)

**Author:** u/R46H4V | **Upvotes:** 1250 | **Comments:** 262 | **Date:** 2025-12-15

**Summary:** The Reddit post discusses anticipation and speculation around an upcoming new Google model, with users expressing hopes for improvements over previous models like Gemma3-Math and expectations for multi-modal capabilities.

**Key Points:**
- Anticipation of a new Google model
- Hopes for improvements over previous models like Gemma3-Math
- Expectations for multi-modal capabilities
- High engagement with 1250 upvotes and 262 comments
- Community excitement and hype around the announcement

**Discussion Highlights:** The discussion highlights a strong community interest and excitement about the new Google model, with users expressing specific hopes for multi-modal capabilities and improvements over previous iterations like Gemma3-Math. The overall consensus is one of anticipation and optimism.

---

## 34. [llama.cpp: Automation for GPU layers, tensor split, tensor overrides, and context size (with MoE optimizations)](https://reddit.com/r/LocalLLaMA/comments/1pn2e1c/llamacpp_automation_for_gpu_layers_tensor_split/)

**Author:** u/Remove_Ayys | **Upvotes:** 190 | **Comments:** 61 | **Date:** 2025-12-15

**Summary:** The post discusses the implementation of automated memory allocation in llama.cpp, which optimizes GPU and CPU hybrid inference by iteratively reducing memory use and prioritizing dense tensors for better performance, especially in MoE models. This addresses previous issues with manual memory management and conservative heuristics used by downstream projects.

**Key Points:**
- Automated memory allocation for GPU layers and tensor splits in llama.cpp
- Iterative reduction of memory use to fit models across GPUs
- Prioritization of dense tensors for optimal MoE performance
- Generic implementation compatible with any ggml backend supporting hybrid inference
- Positive community feedback and suggestions for further improvements like caching and multi-GPU support

**Discussion Highlights:** The community appreciates the new feature, with suggestions for caching to eliminate fitting time and requests for better multi-GPU support. There is also interest in special handling for dense models and further optimizations.

---

## 35. [Aaaand... is gone...](https://reddit.com/r/LocalLLaMA/comments/1pmungj/aaaand_is_gone/)

**Author:** u/HumanDrone8721 | **Upvotes:** 924 | **Comments:** 204 | **Date:** 2025-12-14

**Summary:** The Reddit post discusses the discontinuation or scarcity of SATA drives, sparking a mix of humorous and dismissive reactions from the community.

**Key Points:**
- The post is about the end of SATA drives.
- Community reactions range from humor to dismissal.
- Some users mention buying additional storage (e.g., 2TB SSD).
- The discussion highlights differing opinions on the significance of the event.

**Discussion Highlights:** The community is divided, with some seeing the event as significant (e.g., buying more storage) and others dismissing it as a 'nothingburger.'

---

## 36. [Qwen3-Next-80B-A3B-Thinking-GGUF has just been released on HuggingFace](https://reddit.com/r/LocalLLaMA/comments/1pmo0dn/qwen3next80ba3bthinkinggguf_has_just_been/)

**Author:** u/LegacyRemaster | **Upvotes:** 124 | **Comments:** 54 | **Date:** 2025-12-14

**Summary:** The Reddit post announces the release of Qwen3-Next-80B-A3B-Thinking-GGUF on HuggingFace, highlighting its impressive performance in generating a Tetris game within a single HTML file, outperforming other models like Devstral. The discussion includes user impressions, confusion about the release timing, and technical questions about tool compatibility.

**Key Points:**
- Qwen3-Next-80B-A3B-Thinking-GGUF model released on HuggingFace
- Model excels in generating Tetris in a single HTML file
- Outperforms Devstral in accuracy
- Users express admiration for the model's capabilities
- Discussion includes queries about release timing and tool support

**Discussion Highlights:** Users are impressed by the model's performance, though there is some confusion about the release timing. Technical questions about tool compatibility, such as llamacpp support for native tool calling, are also raised.

---

## 37. [To Mistral and other lab employees: please test with community tools BEFORE releasing models](https://reddit.com/r/LocalLLaMA/comments/1pmgm2x/to_mistral_and_other_lab_employees_please_test/)

**Author:** u/dtdisapointingresult | **Upvotes:** 133 | **Comments:** 72 | **Date:** 2025-12-14

**Summary:** The Reddit post criticizes Mistral for releasing Devstral 2 without thorough testing with community tools, leading to issues like benchmark discrepancies and repetition loops. The author emphasizes the importance of testing with local tools to maintain reputation and user trust. Key points include: Devstral 2 release faced criticism due to issues like benchmark discrepancies and repetition loops; the author suggests that inadequate testing with community tools led to these problems; the post highlights the importance of local tools for AI geeks who influence tech recommendations; comments indicate mixed experiences, with some users reporting success with local tools and others facing issues; there is a discussion about the broader context of model releases and community expectations. The discussion highlights mixed user experiences with Devstral 2, with some praising its performance with local tools and others reporting issues. There is a consensus on the importance of thorough testing with community tools before release to avoid reputational damage.

---

## 38. [Understanding the new router mode in llama cpp server](https://reddit.com/r/LocalLLaMA/comments/1pmc7lk/understanding_the_new_router_mode_in_llama_cpp/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 167 | **Comments:** 44 | **Date:** 2025-12-14

**Summary:** Router mode in llama cpp server allows managing multiple AI models simultaneously without restarting the server, similar to Ollama-like functionality. It enables loading/unloading models on demand and routing requests to the appropriate model, saving memory and simplifying model switching.

**Key Points:**
- Router mode enables managing multiple AI models without restarting the server.
- It allows loading/unloading models on demand and routing requests to the appropriate model.
- Useful for testing multiple GGUF models, building local OpenAI-compatible APIs, and switching between models dynamically.
- Saves memory and simplifies model switching compared to previous methods.
- Discussion highlights include comparisons with llama-swap and requests for better VRAM management.

**Discussion Highlights:** The discussion includes comparisons with llama-swap, requests for better VRAM management, and questions about specifying which models stay in memory concurrently. Some users express interest in the new functionality but seek more details on its implementation and advantages over existing solutions.

---

## 39. [8x RTX Pro 6000 server complete](https://reddit.com/r/LocalLLaMA/comments/1plwgun/8x_rtx_pro_6000_server_complete/)

**Author:** u/koushd | **Upvotes:** 624 | **Comments:** 268 | **Date:** 2025-12-13

**Summary:** The Reddit post details the author's journey of upgrading their GPU server over several years, culminating in a powerful setup with 8x RTX Pro 6000 GPUs, a Threadripper PRO 9955WX CPU, and 384 GB RAM. The post highlights challenges faced during upgrades, including heat management and hardware compatibility issues.

**Key Points:**
- The author started with a single 3080 GPU and gradually upgraded to a powerful 8x RTX Pro 6000 setup.
- Heat management was a significant issue, leading to overheating and system crashes.
- Hardware compatibility issues arose, particularly with motherboard limitations and power requirements.
- The community discussion includes both admiration for the setup and criticism of the hardware management approach.

**Discussion Highlights:** The discussion highlights a mix of admiration for the powerful setup and criticism regarding the hardware management approach. Some users praised the setup as 'epyc,' while others criticized the use of a 'shitty aluminum frame' and the balancing of fans on GPUs.

---

## 40. [Mistral 3 Large is DeepSeek V3!?](https://reddit.com/r/LocalLLaMA/comments/1plpc6h/mistral_3_large_is_deepseek_v3/)

**Author:** u/seraschka | **Upvotes:** 174 | **Comments:** 33 | **Date:** 2025-12-13

**Summary:** The post discusses the architectural similarities between Mistral 3 Large and DeepSeek V3, noting that they share nearly identical sizes and architectures, with minor differences in expert configurations. The author highlights the open-source nature of these models and mentions that Mistral 3 was likely trained from scratch despite architectural similarities.

**Key Points:**
- Mistral 3 Large and DeepSeek V3 have almost identical sizes (671B vs. 673B) and share the same architecture.
- Mistral 3 adjusted the expert configuration by increasing expert size while decreasing the number of experts, aiming to improve latency.
- Mistral 3 was likely trained from scratch rather than fine-tuned from DeepSeek V3, as it uses a different tokenizer.
- The DeepSeek V3 architecture is being adopted by multiple models, including Kimi K2 and Gigachat, showcasing the spirit of open-source collaboration.
- Community discussions highlight the effectiveness of the DeepSeek V3 architecture, especially under resource constraints.

**Discussion Highlights:** The comments emphasize the open-source spirit, with multiple models adopting the DeepSeek V3 architecture. Users appreciate the innovation and efficiency of the architecture, noting its suitability for resource-constrained environments. There is also recognition of Mistral's multimodal capabilities as a form of innovation.

---

## 41. [OpenAI's flagship model, ChatGPT-5.2 Thinking, ranks  most censored AI on Sansa benchmark.](https://reddit.com/r/LocalLLaMA/comments/1plnuqu/openais_flagship_model_chatgpt52_thinking_ranks/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 615 | **Comments:** 112 | **Date:** 2025-12-13

**Summary:** The Reddit post discusses OpenAI's ChatGPT-5.2 model being ranked as the most censored AI on the Sansa benchmark, with users reporting issues in follow-up questions, research capabilities, and clinical note generation.

**Key Points:**
- ChatGPT-5.2 is ranked as the most censored AI on the Sansa benchmark.
- Users report issues with follow-up questions and research capabilities compared to previous versions.
- Difficulties in generating clinical notes for QA model evaluation.
- Curiosity about the testing criteria for the Sansa benchmark.
- Observations about Gemini being less censored than other models.

**Discussion Highlights:** Users express dissatisfaction with ChatGPT-5.2's performance in follow-up questions and research, as well as its increased censorship. There is curiosity about the benchmark testing criteria and observations about other models' censorship levels.

---

## 42. [Qwen3 Next generation optimization](https://reddit.com/r/LocalLLaMA/comments/1plng6f/qwen3_next_generation_optimization/)

**Author:** u/ilintar | **Upvotes:** 363 | **Comments:** 40 | **Date:** 2025-12-13

**Summary:** The post discusses optimizations made to Qwen3 Next generation, resulting in a 40% generation speed upgrade. The author invites others to test the improvements and share their results.

**Key Points:**
- Optimized autoregressive delta net computation for faster performance
- 40% generation speed upgrade reported by the author
- Community appreciation and engagement with the optimization efforts
- Questions about compatibility with ROCm/Vulkan
- Positive feedback and recognition from the community

**Discussion Highlights:** The community shows strong appreciation for the optimization efforts, with some users joking about the author's frequent contributions. There is interest in whether the speedup will work on ROCm/Vulkan, indicating a desire for broader compatibility.

---

## 43. [NVIDIA gpt-oss-120b Eagle Throughput model](https://reddit.com/r/LocalLLaMA/comments/1plewrk/nvidia_gptoss120b_eagle_throughput_model/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 242 | **Comments:** 53 | **Date:** 2025-12-13

**Summary:** The post discusses NVIDIA's GPT-OSS-120B-Eagle3-throughput model, an optimized speculative decoding module designed to improve text generation throughput using Eagle3 speculative decoding. It is licensed for both commercial and non-commercial use.

**Key Points:**
- Optimized speculative decoding module for improved throughput
- Uses NVIDIA’s Eagle3 speculative decoding approach
- Licensed under nvidia-open-model-license for commercial and non-commercial use
- Community interest in derestricted versions and CPU inference compatibility
- Not currently supported in llama.cpp

**Discussion Highlights:** The community shows strong interest in derestricted versions and potential CPU inference improvements. There is also discussion about compatibility issues, particularly with llama.cpp, and anticipation for future versions like REAP EAGLE3 HERETIC MOE GGUF.

---

## 44. [This is how open ai is advertising them selfs on reddit…. They are doomed](https://reddit.com/r/LocalLLaMA/comments/1plcrg8/this_is_how_open_ai_is_advertising_them_selfs_on/)

**Author:** u/ThinkExtension2328 | **Upvotes:** 233 | **Comments:** 77 | **Date:** 2025-12-12

**Summary:** The Reddit post criticizes OpenAI's advertising strategy, highlighting a shift from promoting advanced AI capabilities to using astrology-based ads, which is seen as a decline in their approach. Key points include the shift to astrology-based ads, the perception of this change as a decline, the community's view of the new strategy as less appealing and more profit-driven, the consensus that the new ads target a different audience, and the overall view of this shift as a significant fall from grace. The discussion highlights a general consensus that OpenAI's new advertising strategy is a significant departure from their previous focus on advanced AI and AGI, viewed as a decline and a move towards more profit-driven, less technical audiences.

---

## 45. [Running an LLM on a 3DS](https://reddit.com/r/LocalLLaMA/comments/1pl4njj/running_an_llm_on_a_3ds/)

**Author:** u/vreab | **Upvotes:** 296 | **Comments:** 35 | **Date:** 2025-12-12

**Summary:** The Reddit post discusses the feasibility and performance of running an LLM on a 3DS, with users expressing curiosity and admiration for the project. The discussion highlights the potential of running LLMs on unconventional hardware like gaming consoles.

**Key Points:**
- Running an LLM on a 3DS is a notable achievement.
- Users compare this to running LLMs on other unconventional hardware like the PS Vita and Wii.
- There is curiosity about the performance improvements on a 'new' 3DS.
- The project impresses users and sparks discussions about AI capabilities on older hardware.

**Discussion Highlights:** The discussion highlights the impressive nature of the project, with users comparing it to similar projects on other gaming consoles. There is a consensus that running LLMs on older hardware is a remarkable feat, and users are curious about potential performance improvements on newer versions of the 3DS.

---

## 46. [The new monster-server](https://reddit.com/r/LocalLLaMA/comments/1pl0ojb/the_new_monsterserver/)

**Author:** u/eribob | **Upvotes:** 585 | **Comments:** 125 | **Date:** 2025-12-12

**Summary:** The Reddit post by u/eribob details their upgraded 'Monster-server,' featuring a Ryzen 3950x CPU, three GPUs (including an RTX 4090), and extensive storage. The server runs local LLMs like GPT-OSS-120B for research and coding, with a 10GBe network connection. Key points include the hardware specifications, performance metrics, and notable comments. The discussion highlights nostalgia for early 2000s overclocking forums, curiosity about the user's location for affordable 10GBe internet, and technical discussions on GPU setup efficiency and heat management.

---

## 47. [Olmo 3.1 32B Think &amp; Instruct: New Additions to the Olmo Model Family](https://reddit.com/r/LocalLLaMA/comments/1pky5u4/olmo_31_32b_think_instruct_new_additions_to_the/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 177 | **Comments:** 28 | **Date:** 2025-12-12

**Summary:** The Olmo 3.1 32B Think and Instruct models are new additions to the Olmo family, each optimized for different use cases. The Think model specializes in deep reasoning, while the Instruct model focuses on instruction following and conversational fluency.

**Key Points:**
- Olmo 3.1 32B Think is optimized for deep reasoning, math, logic, and code generation.
- Olmo 3.1 32B Instruct is optimized for instruction following, conversational fluency, and tool-use capabilities.
- The models are fully open source and praised for their quality and improvements.
- The community anticipates future developments, such as MOE (Mixture of Experts).
- The models are available on HuggingFace.

**Discussion Highlights:** The community is positive about the new models, praising their open-source nature and improvements. There is anticipation for future developments like MOE, and the models are seen as valuable additions for various use cases.

---

## 48. [Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/)

**Author:** u/Nunki08 | **Upvotes:** 1329 | **Comments:** 155 | **Date:** 2025-12-12

**Summary:** NVIDIA accidentally uploaded the parent folder of their upcoming model on Hugging Face, sparking a discussion about the potential leak of sensitive information and the urgency to save the data before it gets taken down.

**Key Points:**
- NVIDIA's accidental upload of a parent folder on Hugging Face
- Potential leak of sensitive information related to an upcoming model
- Community urgency to save the data before it gets censored or removed
- Mentions of specific models like Nano and 30B-A3B
- Positive sentiment towards the Nemotron lineup and other projects

**Discussion Highlights:** The community expressed concern about the potential loss of valuable data and urged others to save the information before it gets taken down. There was also excitement about the mentioned models and projects, with some users highlighting the promise of the Nemotron lineup.

---

## 49. [Training an LLM only on 1800s London texts - 90GB dataset](https://reddit.com/r/LocalLLaMA/comments/1pkpsee/training_an_llm_only_on_1800s_london_texts_90gb/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 703 | **Comments:** 78 | **Date:** 2025-12-12

**Summary:** The post discusses an open-source project, TimeCapsuleLLM, focused on training LLMs using 1800-1875 London texts. The author has compiled a 90GB dataset with 135,000 documents and conducted bias analysis and preliminary model training.

**Key Points:**
- Project TimeCapsuleLLM aims to train LLMs on 1800-1875 London texts.
- Dataset size is 90GB with 135,000 documents.
- Bias report and preliminary model training have been conducted.
- Community appreciation for the detailed and methodical approach.
- Suggestions for using Mixture of Experts (MoE) for better compute efficiency.

**Discussion Highlights:** The community shows strong support for the project, appreciating the detailed approach and methodological rigor. There are suggestions for improving compute efficiency using MoE and questions about the inclusion criteria for texts.

---

## 50. [What is the smartest uncensored nsfw LLM you can run with 12GB VRAM and 32GB RAM?](https://reddit.com/r/LocalLLaMA/comments/1pkidf6/what_is_the_smartest_uncensored_nsfw_llm_you_can/)

**Author:** u/Dex921 | **Upvotes:** 401 | **Comments:** 144 | **Date:** 2025-12-11

**Summary:** The post asks for recommendations on the smartest uncensored NSFW LLM that can run with 12GB VRAM and 32GB RAM, including both local and closed-source options. The discussion highlights several models and their performance in NSFW contexts.

**Key Points:**
- The post allows for both local and closed-source LLM recommendations.
- TheDrummer_Cydonia-24B is mentioned as a truly uncensored local model.
- Qwen3 32B is noted for good NSFW roleplay results with appropriate prompting.
- The discussion emphasizes the importance of proper prompting for NSFW content.

**Discussion Highlights:** The discussion primarily focuses on local models like TheDrummer_Cydonia-24B and Qwen3 32B, with users sharing their experiences and the importance of effective prompting for NSFW content. There is a consensus on the effectiveness of these models for NSFW use cases.

---

