# r/LocalLLaMA Reading Digest

**Period:** 2025-12-18 to 2025-12-18
**Posts Summarized:** 48
**Total Posts Analyzed:** 48

---

## 1. [Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds.](https://reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/)

**Author:** u/themixtergames | **Upvotes:** 1051 | **Comments:** 124 | **Date:** 2025-12-17

**Summary:** Apple has introduced SHARP, a model capable of generating photorealistic 3D Gaussian representations from a single image in seconds. The model is highlighted for its speed and compatibility with Apple devices like the MacBook Pro M1 Max and Apple Vision Pro.

**Key Points:**
- SHARP generates 3D Gaussian representations from a single image in seconds.
- The model is optimized for Apple hardware, including the MacBook Pro M1 Max and Apple Vision Pro.
- The GitHub repository and research paper are available for further exploration.
- Community reactions include comparisons to cyberpunk's braindance and inquiries about content compatibility.

**Discussion Highlights:** The community showed enthusiasm for the technology, with comparisons to sci-fi concepts like cyberpunk's braindance. There were also humorous inquiries about the model's capabilities with adult content. The top comments highlighted the model's performance on Apple hardware and its real-time rendering capabilities.

---

## 2. [LangChain and LlamaIndex are in "steep decline" according to new ecosystem report. Anyone else quietly ditching agent frameworks?](https://reddit.com/r/LocalLLaMA/comments/1pox733/langchain_and_llamaindex_are_in_steep_decline/)

**Author:** u/Exact-Literature-395 | **Upvotes:** 200 | **Comments:** 56 | **Date:** 2025-12-17

**Summary:** The Reddit post discusses the decline of LangChain and LlamaIndex frameworks, citing reduced community activity and investment. Users share their experiences of moving away from these frameworks due to complexity and lack of necessity with improved base models.

**Key Points:**
- LangChain and LlamaIndex are listed as 'steepest declining' projects by community activity.
- Users report simplifying their codebases by removing these frameworks and calling APIs directly.
- Criticisms include bloated features, poor security/performance, and non-pythonic design choices.
- Some argue these frameworks solve problems that no longer exist with current model capabilities.
- Maintainers acknowledge the frameworks' initial popularity was due to ease of integration.

**Discussion Highlights:** The discussion reveals a consensus that these frameworks are becoming less relevant as base models improve. Users express frustration with complexity and prefer simpler, more direct approaches. There's a general sentiment that the future of software engineering may involve less reliance on such frameworks.

---

## 3. [Peak LLM Wars: Xiaomi Blocks Kimi Employees on Twitter](https://reddit.com/r/LocalLLaMA/comments/1pow797/peak_llm_wars_xiaomi_blocks_kimi_employees_on/)

**Author:** u/nekofneko | **Upvotes:** 128 | **Comments:** 25 | **Date:** 2025-12-17

**Summary:** The Reddit post discusses the ongoing LLM wars, highlighting a specific incident where Xiaomi blocks Kimi employees on Twitter. The post includes images and comments that reflect the competitive and dramatic nature of the LLM industry.

**Key Points:**
- Xiaomi and Kimi are involved in a public dispute on Twitter.
- The post includes images that illustrate the intensity of the LLM wars.
- Comments mention potential involvement of former DeepSeek members in Xiaomi's team.
- Comparisons are drawn to other industry rivalries like Musk vs. Altman and Meta vs. Zuckerberg.
- The discussion highlights the dramatic and competitive nature of the LLM industry.

**Discussion Highlights:** The discussion is marked by a mix of humor and serious commentary on industry rivalries. Users compare the situation to other tech industry feuds and express interest in further developments. The overall tone is one of amusement and curiosity about the ongoing LLM wars.

---

## 4. [Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model](https://reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 1106 | **Comments:** 118 | **Date:** 2025-12-17

**Summary:** Microsoft's TRELLIS 2-4B is an open-source image-to-3D model with 4 billion parameters, using Flow-Matching Transformers and a Sparse Voxel-based 3D VAE. It converts single images into 3D assets and has been well-received in the community.

**Key Points:**
- Model Type: Flow-Matching Transformers with Sparse Voxel based 3D VAE
- Parameters: 4 Billion
- Input: Single Image, Output: 3D Asset
- Model and demo available on Hugging Face
- Mixed community feedback on practical usability

**Discussion Highlights:** The community feedback is mixed, with some users praising the model's performance and others noting limitations in practical applications. There is also a suggestion to improve the model by allowing a series of images as input.

---

## 5. [QwenLong-L1.5: Revolutionizing Long-Context AI](https://reddit.com/r/LocalLLaMA/comments/1pokpha/qwenlongl15_revolutionizing_longcontext_ai/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 213 | **Comments:** 27 | **Date:** 2025-12-16

**Summary:** QwenLong-L1.5 is a new AI model achieving state-of-the-art long-context reasoning with novel data synthesis, stabilized RL, and memory management for contexts up to 4M tokens. The model is available on HuggingFace and has garnered significant attention for its capabilities.

**Key Points:**
- Achieves SOTA long-context reasoning with up to 4M tokens
- Utilizes novel data synthesis and stabilized RL techniques
- Available on HuggingFace for public use
- Integration with llama.cpp may require additional work
- Importance of using the exact query template for optimal performance

**Discussion Highlights:** The discussion highlights the model's significant advancements and potential challenges in integration. Users emphasize the need for visual improvements in graphs and the importance of adhering to the exact query template for best results.

---

## 6. [8x Radeon 7900 XTX Build for Longer Context Local Inference - Performance Results &amp; Build Details](https://reddit.com/r/LocalLLaMA/comments/1pogwb6/8x_radeon_7900_xtx_build_for_longer_context_local/)

**Author:** u/Beautiful_Trust_8151 | **Upvotes:** 710 | **Comments:** 210 | **Date:** 2025-12-16

**Summary:** The post details a custom multi-GPU setup using 8x AMD Radeon 7900 XTX cards for local AI inference, achieving 192 GB VRAM and stable performance with a 131k token context window. The build cost around $6-7k and offers flexibility and long-context capability for specific work use cases.

**Key Points:**
- 8x AMD Radeon 7900 XTX GPUs provide 192 GB VRAM for local AI inference
- Performance testing shows stable operation with 437 tokens/sec prompt processing and 27 tokens/sec generation at empty context
- Total build cost is around $6-7k, offering flexibility and long-context capability
- System consumes about 900 watts during operation
- Discussion highlights appreciation for the build and suggestions for further optimization with Linux and ROCm

**Discussion Highlights:** The discussion appreciates the build's capabilities and suggests potential improvements like switching to Linux and ROCm for better performance. There is also interest in testing other models like Qwen3-235B-A22B.

---

## 7. [Nemotron 3 Nano 30B is Amazing! (TLDR)](https://reddit.com/r/LocalLLaMA/comments/1pocsdy/nemotron_3_nano_30b_is_amazing_tldr/)

**Author:** u/DonkeyBonked | **Upvotes:** 200 | **Comments:** 117 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses the user's experience with Nemotron 3 Nano 30B, highlighting its token efficiency and performance on their hardware setup. The user compares it favorably to other models like Devstral 2 Small 24B and Qwen models, noting its ability to handle large context sizes efficiently.

**Key Points:**
- Nemotron 3 Nano 30B shows impressive token efficiency and performance on the user's hardware setup.
- The model can handle large context sizes, fitting 256k tokens in VRAM and up to 1M with spillover.
- Comparisons with other models like Devstral 2 Small 24B and Qwen models show Nemotron's superior performance in certain tasks.
- Users in the comments discuss the model's speed, open-source nature, and performance relative to other models like Qwen 30B.
- Some users suggest trying other models like IBM Granite 4 Hybrid Small for comparison.

**Discussion Highlights:** The discussion highlights the model's efficiency and performance, with some users comparing it favorably to other models. There is a consensus on its speed and open-source benefits, though some users still prefer other models like Qwen 30B for certain tasks.

---

## 8. [32GB Mi50's were getting so expensive that I ended up buying a 32GB w6800 for about the same price instead](https://reddit.com/r/LocalLLaMA/comments/1pob44f/32gb_mi50s_were_getting_so_expensive_that_i_ended/)

**Author:** u/EmPips | **Upvotes:** 231 | **Comments:** 42 | **Date:** 2025-12-16

**Summary:** The author opted for a 32GB w6800 GPU over a 32GB Mi50 due to similar pricing, highlighting pros like convenience and cooling, while discussing alternatives like the AMD Radeon AI PRO R9700 and Zotac 3090s.

**Key Points:**
- 32GB w6800 chosen over Mi50 for similar price
- Pros include convenience and effective blower-style cooling
- Alternatives mentioned: AMD Radeon AI PRO R9700 and Zotac 3090s
- Price comparison: w6800 at $500, Zotac 3090s at $540

**Discussion Highlights:** The discussion revolves around GPU choices, with users sharing pros/cons of the w6800 and suggesting alternatives like the R9700 and 3090s, indicating a focus on performance, price, and software support.

---

## 9. [8 Million Users' AI Conversations Sold for Profit by "Privacy" Extensions | Koi Blog](https://reddit.com/r/LocalLLaMA/comments/1poal2a/8_million_users_ai_conversations_sold_for_profit/)

**Author:** u/ManThigh | **Upvotes:** 153 | **Comments:** 46 | **Date:** 2025-12-16

**Summary:** The Reddit post highlights privacy concerns regarding browser extensions selling AI conversation data of millions of users for profit, emphasizing the importance of using local models and auditing extensions.

**Key Points:**
- Browser extensions like Urban VPN Proxy and 1ClickVPN Proxy sold AI conversation data of millions of users.
- The post emphasizes the importance of running local models to avoid data privacy issues.
- Users are advised to audit their extensions to prevent data leaks.
- The discussion highlights the value of user data and the need for stricter regulations.
- Many users express pride in using local setups to avoid such privacy breaches.

**Discussion Highlights:** The discussion consensus revolves around the need for better data privacy measures, with users expressing concern over the sale of their data and advocating for local setups to maintain privacy.

---

## 10. [Finally managed to run Qwen-2.5-7B on a 4GB GTX 1050 without CPU offloading (Surgical Memory Alignment)](https://reddit.com/r/LocalLLaMA/comments/1po97ad/finally_managed_to_run_qwen257b_on_a_4gb_gtx_1050/)

**Author:** u/HuseyinKama | **Upvotes:** 144 | **Comments:** 48 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses a custom framework called 'QKV Core' that enables running Qwen-2.5-7B on a 4GB GTX 1050 without CPU offloading by optimizing memory alignment and reducing padding overhead. The author achieved significant VRAM savings and performance improvements, making it feasible to run modern 7B models on low-end hardware.

**Key Points:**
- The author developed a custom framework called 'QKV Core' to optimize memory usage for running large language models on low-end GPUs.
- The framework reduces memory overhead by trimming and realigning memory blocks, saving about 44MB per model.
- Performance improvements include a ~34% reduction in I/O load times due to cache-aligned blocks.
- The solution is open-sourced and available on GitHub for others to use and provide feedback.
- The discussion highlights both appreciation for the work and skepticism about the claimed improvements.

**Discussion Highlights:** The discussion includes appreciation for the optimization work, skepticism about the claimed gains, and questions about the practical application of the framework. Some users expressed interest in testing the framework on their own low-end hardware.

---

## 11. [I was bored](https://reddit.com/r/LocalLLaMA/comments/1po8yt0/i_was_bored/)

**Author:** u/MyLovelyAngelKirino | **Upvotes:** 130 | **Comments:** 70 | **Date:** 2025-12-16

**Summary:** The author, who is unemployed with excess hardware and time, built a high-performance system featuring 3x 3090 GPUs, 512GB RAM, and an Epyc 7663 56-core processor. The post sparked discussions about hardware acquisition, system neatness, and water-cooling details.

**Key Points:**
- Author built a powerful system due to unemployment and excess hardware/time
- System specs: 3x 3090 GPUs, 512GB RAM, Epyc 7663 56-core CPU
- Top comment praises hardware specs
- Discussion about hardware acquisition and financial means
- Requests for details on water-cooling components

**Discussion Highlights:** The discussion highlights admiration for the hardware specs, curiosity about how the author acquired the hardware, and requests for more details on the water-cooling setup. Some users also joked about the author's identity and the neatness of the build.

---

## 12. [Meta announced a new SAM Audio Model for audio editing that can segment sound from complex audio mixtures using text, visual, and time span prompts.](https://reddit.com/r/LocalLLaMA/comments/1po7i0c/meta_announced_a_new_sam_audio_model_for_audio/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 498 | **Comments:** 80 | **Date:** 2025-12-16

**Summary:** Meta has introduced a new SAM Audio Model that revolutionizes audio editing by allowing users to isolate specific sounds from complex audio mixtures using text, visual, and time span prompts.

**Key Points:**
- SAM Audio Model enables easy isolation of sounds from complex audio mixtures.
- The model uses text, visual, and time span prompts for audio segmentation.
- Potential applications include filtering out unwanted noises in virtual meetings.
- The model demonstrates high accuracy in isolating specific sounds from videos.
- Model sizes and specifications are available for reference.

**Discussion Highlights:** The discussion highlights the potential of the SAM Audio Model in practical applications, such as improving audio quality in virtual meetings by filtering out unwanted noises. Users also expressed amazement at the model's ability to accurately isolate sounds from complex audio mixtures.

---

## 13. [Allen Institute for AI introduces Molmo 2](https://reddit.com/r/LocalLLaMA/comments/1po78bl/allen_institute_for_ai_introduces_molmo_2/)

**Author:** u/Agitated_Camel1886 | **Upvotes:** 235 | **Comments:** 22 | **Date:** 2025-12-16

**Summary:** Allen Institute for AI introduces Molmo 2, an 8B model with impressive video analysis capabilities, including Video QA, Counting and Pointing, and Dense Captioning. The community is highly enthusiastic, with an AMA scheduled to discuss the model and its datasets.

**Key Points:**
- Molmo 2 is an 8B model with advanced video analysis capabilities
- Allen AI releases datasets publicly, aiding community advancements
- An AMA is scheduled on r/LocalLLaMA to discuss Olmo 3 and Molmo 2
- Community reactions are overwhelmingly positive and impressed
- Benchmarks are strong for the model's size, with discussions on VRAM requirements

**Discussion Highlights:** The community is highly enthusiastic about Molmo 2's capabilities and the public release of datasets. Key discussions include the scheduled AMA, technical benchmarks, and practical considerations like VRAM requirements. Overall, the consensus is positive, with users praising the model's performance and the transparency of Allen AI.

---

## 14. [XiaomiMiMo/MiMo-V2-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1po3bn4/xiaomimimomimov2flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 231 | **Comments:** 51 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses MiMo-V2-Flash, a Mixture-of-Experts (MoE) language model with 309B total parameters and 15B active parameters, designed for high-speed reasoning and agentic workflows. Users highlight its impressive performance on multilingual SWE tasks and inquire about larger versions and hardware requirements.

**Key Points:**
- MiMo-V2-Flash is a MoE language model with 309B total parameters and 15B active parameters.
- It shows strong performance on multilingual SWE tasks, surpassing models like Sonnet 4.5 and Gemini 3.
- Users are interested in larger versions of the model and its hardware requirements.
- The model's weights have been released, making it accessible for further exploration.

**Discussion Highlights:** The discussion highlights the model's impressive performance and accessibility, with users expressing interest in its capabilities and potential larger versions. There is also curiosity about the hardware requirements for running the model efficiently.

---

## 15. [My professor lent me an A6000, so I tried to build a coding model. Here is Anni! (Qwen3-14B Fine-tune)](https://reddit.com/r/LocalLLaMA/comments/1po2slg/my_professor_lent_me_an_a6000_so_i_tried_to_build/)

**Author:** u/Outrageous-Yak8298 | **Upvotes:** 102 | **Comments:** 32 | **Date:** 2025-12-16

**Summary:** A 2nd year undergrad AI student trained a coding model named Anni using a single Nvidia A6000 GPU, achieving a benchmark score of 41.7% Pass@1 on LiveCodeBench, potentially matching Claude 3.5 Sonnet. The author discusses the training process, hardware constraints, and potential data contamination in the benchmark results.

**Key Points:**
- Trained a 14B Qwen3-based model named Anni on a single A6000 GPU.
- Achieved 41.7% Pass@1 on LiveCodeBench, potentially matching Claude 3.5 Sonnet.
- Training time reduced to ~2 weeks from an initial projection of ~1.6 months.
- Potential data contamination due to overlap between training and benchmark datasets.
- Discussion includes questions about training process, hardware, and congratulatory remarks.

**Discussion Highlights:** The discussion includes congratulatory comments, questions about the training process and hardware used, and appreciation for the transparency regarding potential data contamination in the benchmark results.

---

## 16. [GLM-4.5V, GLM-4.6V and GLM_4.6V-Flash are now supported by llama.cpp (GGUFs)](https://reddit.com/r/LocalLLaMA/comments/1po18y9/glm45v_glm46v_and_glm_46vflash_are_now_supported/)

**Author:** u/jacek2023 | **Upvotes:** 166 | **Comments:** 34 | **Date:** 2025-12-16

**Summary:** The Reddit post announces that GLM-4.5V, GLM-4.6V, and GLM_4.6V-Flash are now supported by llama.cpp with GGUFs, which is seen as a significant update by the community.

**Key Points:**
- Support for GLM-4.5V, GLM-4.6V, and GLM_4.6V-Flash has been added to llama.cpp.
- The update is celebrated as a great Christmas gift by the community.
- There are questions about whether the GGUFs support vision capabilities.
- Some users have faced challenges setting up the new models.
- Comparisons between Qwen3-VL-4B and GLM_4.6V are being discussed.

**Discussion Highlights:** The community is excited about the new support for GLM models in llama.cpp, though there are some concerns and questions about vision support and setup challenges. Comparisons with other models like Qwen3-VL-4B are also being explored.

---

## 17. [Qwen3 Next speed optimization has been merged into llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1pnz9xu/qwen3_next_speed_optimization_has_been_merged/)

**Author:** u/jacek2023 | **Upvotes:** 213 | **Comments:** 25 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses significant speed optimizations for Qwen3 Next in llama.cpp, with users reporting notable performance improvements across different hardware configurations.

**Key Points:**
- Speed optimization for Qwen3 Next has been merged into llama.cpp
- Performance on M1 64GB improved from 12 t/s to 18 t/s
- Win11 + RTX5090 setup achieves 37.x t/s with Vulkan and 100+ t/s with UD-Q2_K_XL
- Qwen3-30B runs at around 58 t/s on M1 64GB for comparison
- Users express appreciation for the optimization efforts

**Discussion Highlights:** The discussion highlights significant performance gains, with users sharing their benchmark results and expressing satisfaction with the optimizations. There is a consensus on the substantial improvement in speed, particularly on high-end hardware.

---

## 18. [I may have over-quantized this little guy.](https://reddit.com/r/LocalLLaMA/comments/1pnz80z/i_may_have_overquantized_this_little_guy/)

**Author:** u/AllergicToTeeth | **Upvotes:** 139 | **Comments:** 30 | **Date:** 2025-12-16

**Summary:** The post humorously suggests over-quantization of a model, sparking playful and technical discussions in the comments.

**Key Points:**
- Potential over-quantization of a model
- Community humor and banter
- Technical advice on system prompts
- Playful references to GPT versions

**Discussion Highlights:** The discussion includes a mix of humor, technical advice, and playful references to advanced AI models, with no clear consensus but a lighthearted tone.

---

## 19. [It was Ilya who "closed" OpenAI](https://reddit.com/r/LocalLLaMA/comments/1pnxekt/it_was_ilya_who_closed_openai/)

**Author:** u/licuphand | **Upvotes:** 505 | **Comments:** 229 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses Ilya Sutskever's role in OpenAI's direction, sparking a debate about trust in AI development and the implications of centralized control.

**Key Points:**
- Ilya Sutskever's influence on OpenAI's direction
- Skepticism about trusting companies with AI development
- Leadership struggles among key figures in AI (Elon, Ilya, Sam)
- Historical context of oversight and control in AI development

**Discussion Highlights:** The discussion highlights a consensus around skepticism of centralized control over AI, with references to historical oversight challenges and the ongoing power struggles among AI leaders.

---

## 20. [Alibaba Open-Sources CosyVoice 3, a New TTS Model](https://reddit.com/r/LocalLLaMA/comments/1pnusp9/alibaba_opensources_cosyvoice_3_a_new_tts_model/)

**Author:** u/nekofneko | **Upvotes:** 218 | **Comments:** 31 | **Date:** 2025-12-16

**Summary:** Alibaba has open-sourced CosyVoice 3, a new TTS model with advanced features like multi-lingual support, high naturalness, and low latency. The model supports various instructions and text normalization, making it suitable for production use.

**Key Points:**
- Supports 9 languages and 18+ Chinese dialects with zero-shot voice cloning
- Achieves state-of-the-art performance in consistency, similarity, and naturalness
- Features low latency (150ms) and supports both text-in and audio-out streaming
- Includes pronunciation inpainting and text normalization for production use
- Community interest in comparing it with other models like Chatterbox and Microsoft VibeVoice

**Discussion Highlights:** The community is excited about the release, with discussions focusing on comparisons with other TTS models like Chatterbox and Microsoft VibeVoice. Some users are eager for a larger model version (1.5B) and confirm the model's voice cloning capabilities.

---

## 21. [New budget local AI rig](https://reddit.com/r/LocalLLaMA/comments/1pnllux/new_budget_local_ai_rig/)

**Author:** u/vucamille | **Upvotes:** 155 | **Comments:** 38 | **Date:** 2025-12-15

**Summary:** The user built a budget AI rig using affordable components, including two MI50 16GB GPUs, achieving a total cost of around $650. The system performs well for inference tasks and is expandable for future upgrades.

**Key Points:**
- Budget build with MI50 16GB GPUs and Xeon E5 2680 V4 for $650
- ROCm 7.0.2 works well for inference tasks with dual GPUs
- Community praises the cost-effectiveness and expandability of the system
- User plans to add brackets and decorations, and may upgrade to 32GB GPUs later
- Top comment highlights the value of a 32GB VRAM pool for under $650

**Discussion Highlights:** The community consensus is highly positive, praising the cost-effectiveness and performance of the build. Users expressed interest in benchmarks and multi-GPU functionality, with one commenter noting they spent significantly more for similar performance.

---

## 22. [I'm strong enough to admit that this bugs the hell out of me](https://reddit.com/r/LocalLLaMA/comments/1pnfaqo/im_strong_enough_to_admit_that_this_bugs_the_hell/)

**Author:** u/ForsookComparison | **Upvotes:** 1690 | **Comments:** 345 | **Date:** 2025-12-15

**Summary:** The Reddit post expresses frustration with an unspecified issue, likely related to computing performance or workstation setups. The discussion includes humorous references to RAM and debates about the effectiveness of different workstation configurations.

**Key Points:**
- Post title indicates frustration with an unspecified issue
- Top comment references a Discord feature and special flair
- Meme about RAM doubling is a popular comment
- Discussion includes debates about Mac vs. GPU workstation performance
- Comments suggest the issue relates to computing or workstation setups

**Discussion Highlights:** The discussion highlights a mix of humor and technical debate, with some users joking about RAM and others seriously discussing the merits of different workstation configurations. There is no clear consensus, but the tone is generally lighthearted.

---

## 23. [Bolmo-the first family of competitive fully open byte-level language models (LMs) at the 1B and 7B parameter scales.](https://reddit.com/r/LocalLLaMA/comments/1pndzy7/bolmothe_first_family_of_competitive_fully_open/)

**Author:** u/BreakfastFriendly728 | **Upvotes:** 111 | **Comments:** 23 | **Date:** 2025-12-15

**Summary:** The post introduces Bolmo, a family of competitive fully open byte-level language models at 1B and 7B parameter scales, developed by AllenAI. These models use UTF-8 bytes for tokenization, offering a finer-grained approach compared to traditional subword tokenization.

**Key Points:**
- Bolmo models are fully open and competitive at 1B and 7B parameter scales.
- Byte-level language models process text using UTF-8 bytes instead of subword tokenization.
- The community is excited about the potential and advantages of byte-level models.
- Suggestions for future developments include making the models omnimodal.

**Discussion Highlights:** The discussion highlights excitement about the open-sourcing of byte-level models, with users expressing interest in their potential advantages and future developments like omnimodal capabilities.

---

## 24. [They're finally here (Radeon 9700)](https://reddit.com/r/LocalLLaMA/comments/1pnd5uf/theyre_finally_here_radeon_9700/)

**Author:** u/Zeikos | **Upvotes:** 356 | **Comments:** 67 | **Date:** 2025-12-15

**Summary:** The Reddit post announces the arrival of Radeon 9700 GPUs, sparking community interest and requests for benchmarks. Users express nostalgia about the historic GPU name and enthusiasm for testing performance.

**Key Points:**
- Community eagerly awaits benchmarks for the new Radeon 9700 GPUs
- Nostalgia about the Radeon 9700 name from the early 2000s
- Requests for inference, training, noise, and heat benchmarks
- Users plan to test and share performance data during holidays

**Discussion Highlights:** The discussion highlights strong community engagement, with users emphasizing the need for comprehensive benchmarks and sharing performance metrics. There is also a sense of nostalgia and excitement about the return of the Radeon 9700 name.

---

## 25. [status of Nemotron 3 Nano support in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1pnc045/status_of_nemotron_3_nano_support_in_llamacpp/)

**Author:** u/jacek2023 | **Upvotes:** 177 | **Comments:** 31 | **Date:** 2025-12-15

**Summary:** The Reddit post discusses the status of Nemotron 3 Nano support in llama.cpp, highlighting a GitHub pull request and community reactions. The discussion emphasizes the importance of collaboration between organizations and llama.cpp for new model architectures.

**Key Points:**
- Nemotron 3 Nano support is being integrated into llama.cpp via a GitHub pull request.
- The model sizes (Q4_K_M and Q4_K_XL) are noted to be around 24GB, which is a point of discussion.
- Community members appreciate Nvidia's approach and encourage other labs to follow suit.
- There is a consensus that collaboration with llama.cpp is beneficial for new model releases.

**Discussion Highlights:** The community generally supports the integration of Nemotron 3 Nano into llama.cpp and appreciates Nvidia's transparency. There is a call for other organizations to adopt similar practices for better compatibility and support.

---

## 26. [NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model!](https://reddit.com/r/LocalLLaMA/comments/1pn8upp/nvidia_releases_nemotron_3_nano_a_new_30b_hybrid/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 835 | **Comments:** 178 | **Date:** 2025-12-15

**Summary:** NVIDIA has released Nemotron 3 Nano, a 30B hybrid reasoning model with a 1M context window, claiming best-in-class performance for SWE-Bench, reasoning, and chat. The model is available in GGUF format via Hugging Face.

**Key Points:**
- Nemotron 3 Nano is a 30B hybrid reasoning model with a 1M context window.
- It claims best-in-class performance for SWE-Bench, reasoning, and chat.
- The model is available in GGUF format via Hugging Face.
- The Nemotron 3 family includes three sizes of MoE models.
- Users report exceptionally fast generation speeds (110 t/s).

**Discussion Highlights:** The community is excited about the model's speed and performance, with some confusion and humor around the 'Nano' designation for a 30B model. Key clarifications include the model family's MoE architecture and the availability of three sizes.

---

## 27. [NVIDIA Nemotron 3 Nano 30B A3B released](https://reddit.com/r/LocalLLaMA/comments/1pn8h5h/nvidia_nemotron_3_nano_30b_a3b_released/)

**Author:** u/rerri | **Upvotes:** 279 | **Comments:** 83 | **Date:** 2025-12-15

**Summary:** NVIDIA has released Nemotron 3 Nano 30B A3B, a highly efficient and accurate model with a hybrid Mamba-Transformer MoE architecture, 31.6B parameters, and a 1M-token context window. It offers exceptional inference speed and reasoning capabilities, with open weights, datasets, and training recipes.

**Key Points:**
- Hybrid Mamba-Transformer MoE architecture for high efficiency and accuracy
- 31.6B total parameters with ~3.6B active per token
- Up to 4x faster inference than Nemotron Nano 2
- 1M-token context window for long-horizon tasks
- Fully open data stack with 3T pre-training tokens and 13M post-training samples

**Discussion Highlights:** The discussion includes a Llama.cpp PR for integration, questions about offloading experts to system RAM, concerns about synthetic data training, and mixed feedback on model performance despite its speed.

---

## 28. [Alibaba Tongyi Open Sources Two Audio Models: Fun-CosyVoice 3.0 (TTS) and Fun-ASR-Nano-2512 (ASR)](https://reddit.com/r/LocalLLaMA/comments/1pn7c3f/alibaba_tongyi_open_sources_two_audio_models/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 110 | **Comments:** 25 | **Date:** 2025-12-15

**Summary:** Alibaba Tongyi has open-sourced two audio models: Fun-ASR-Nano-2512, a lightweight ASR model with lower inference costs, and Fun-CosyVoice 3.0, a TTS model with zero-shot voice cloning capabilities. Both models support local deployment and customization.

**Key Points:**
- Fun-ASR-Nano-2512 is a lightweight ASR model with lower inference costs.
- Fun-CosyVoice 3.0 supports zero-shot voice cloning.
- Both models are open-sourced and support local deployment.
- Community appreciates the release and sees it as a positive development in the field.
- Models are available on Hugging Face for further exploration.

**Discussion Highlights:** The community is positive about the release, with some users highlighting the potential to reduce reliance on Nvidia's frameworks. There is also enthusiasm for the smaller size and capabilities of the models, as well as anticipation for further developments.

---

## 29. [How to do a RTX Pro 6000 build right](https://reddit.com/r/LocalLLaMA/comments/1pn6ijr/how_to_do_a_rtx_pro_6000_build_right/)

**Author:** u/GPTrack_dot_ai | **Upvotes:** 114 | **Comments:** 174 | **Date:** 2025-12-15

**Summary:** The post discusses building a high-performance system using 8x Nvidia RTX Pro 6000 GPUs with integrated 400G networking, emphasizing ease of setup and key hardware choices like CPU, RAM, and storage. The discussion highlights the system's impressive specs and high cost.

**Key Points:**
- RTX Pro 6000 GPUs lack NVlink but feature integrated 400G networking for high-speed connectivity.
- The system requires careful selection of CPU, RAM, storage, and switch components.
- The build is described as straightforward with minimal setup complexity.
- Users express awe at the system's specs and humorously comment on its high cost.

**Discussion Highlights:** The discussion is marked by admiration for the system's capabilities, with users joking about its cost and comparing it to luxury items like Ferraris and private jets. There is no clear consensus beyond general awe and humor.

---

## 30. [New Google model incoming!!!](https://reddit.com/r/LocalLLaMA/comments/1pn37mw/new_google_model_incoming/)

**Author:** u/R46H4V | **Upvotes:** 1251 | **Comments:** 261 | **Date:** 2025-12-15

**Summary:** The Reddit post discusses anticipation and speculation around an upcoming new Google model, with users expressing hopes for improvements over previous models like Gemma3-Math and potential multi-modal capabilities.

**Key Points:**
- Anticipation of a new Google model announcement
- Hopes for improvements over previous models like Gemma3-Math
- Speculation about multi-modal capabilities
- Community excitement and hype around the announcement
- Mention of potential models like Gemma 4

**Discussion Highlights:** The discussion highlights a strong sense of anticipation and excitement within the community, with users expressing specific hopes for the new model's capabilities and improvements over previous iterations. There is a consensus of high expectations and curiosity about what the new model will offer.

---

## 31. [llama.cpp: Automation for GPU layers, tensor split, tensor overrides, and context size (with MoE optimizations)](https://reddit.com/r/LocalLLaMA/comments/1pn2e1c/llamacpp_automation_for_gpu_layers_tensor_split/)

**Author:** u/Remove_Ayys | **Upvotes:** 184 | **Comments:** 59 | **Date:** 2025-12-15

**Summary:** The post discusses the implementation of automation for GPU layers, tensor split, tensor overrides, and context size in llama.cpp, aiming to improve usability and performance, especially for MoE models. The author highlights the challenges of manual memory control and the benefits of the new automated approach.

**Key Points:**
- CPU + GPU hybrid inference is a core feature of llama.cpp.
- Manual memory control using parameters like --n-gpu-layers and --tensor-split is suboptimal.
- Automation for memory allocation has been implemented to improve usability and performance.
- The new functionality prioritizes dense tensors for better MoE performance.
- The implementation is generic and works for any ggml backend supporting CPU + GPU hybrid inference.

**Discussion Highlights:** The discussion highlights positive feedback on the implementation, suggestions for caching to reduce fitting time, and additional use cases like setting a 'leader' GPU for multi-GPU setups.

---

## 32. [I pitted GPT-5.2 against Opus 4.5 and Gemini 3 in a robot coding tournament](https://reddit.com/r/LocalLLaMA/comments/1pmx49s/i_pitted_gpt52_against_opus_45_and_gemini_3_in_a/)

**Author:** u/Inevitable_Can598 | **Upvotes:** 101 | **Comments:** 42 | **Date:** 2025-12-14

**Summary:** The post compares the performance of various LLMs in a Robocode tournament, highlighting Opus 4.5 as the top performer due to its reliability, while GPT-5.2 showed significant improvement over its predecessor but struggled with code optimization. DeepSeek 3.2 was noted as an outlier with its standard model outperforming the 'Thinking' version.

**Key Points:**
- Opus 4.5 achieved the highest ELO score, demonstrating reliability in coding tasks.
- GPT-5.2 showed a major upgrade over GPT-5.1, scoring nearly 400 ELO points higher.
- DeepSeek 3.2's standard model outperformed its 'Thinking' version and even beat GPT-5.2.
- OpenAI's models outperformed Google's Gemini models in this specific task.
- The author plans to automate the feedback process and test local LLMs next.

**Discussion Highlights:** The discussion emphasized interest in comparing closed models like Kimi K2 Thinking and DeepSeek 3.2 Speciale. Some users questioned the relevance of the post to the r/LocalLLaMA subreddit, while others praised Opus 4.5's performance.

---

## 33. [Aaaand... is gone...](https://reddit.com/r/LocalLLaMA/comments/1pmungj/aaaand_is_gone/)

**Author:** u/HumanDrone8721 | **Upvotes:** 924 | **Comments:** 196 | **Date:** 2025-12-14

**Summary:** The Reddit post discusses the discontinuation or scarcity of SATA drives, sparking a conversation about storage solutions and the implications for users.

**Key Points:**
- The post is about the discontinuation or scarcity of SATA drives.
- Users are discussing the need for alternative storage solutions like SSDs.
- Some comments downplay the significance, calling it a 'nothingburger'.
- The post has gained significant attention with 924 upvotes and 196 comments.

**Discussion Highlights:** The discussion highlights a mix of concern and indifference regarding the discontinuation of SATA drives. Some users see it as a significant issue requiring immediate action (e.g., buying more SSDs), while others view it as an overhyped non-issue. The overall consensus is divided, with no clear agreement on the impact of the situation.

---

## 34. [Qwen3-Next-80B-A3B-Thinking-GGUF has just been released on HuggingFace](https://reddit.com/r/LocalLLaMA/comments/1pmo0dn/qwen3next80ba3bthinkinggguf_has_just_been/)

**Author:** u/LegacyRemaster | **Upvotes:** 128 | **Comments:** 54 | **Date:** 2025-12-14

**Summary:** The Reddit post announces the release of Qwen3-Next-80B-A3B-Thinking-GGUF on HuggingFace, highlighting its impressive performance in a Tetris game implemented in a single HTML file. Users compare it favorably to other models like Devstral and discuss its capabilities and release timeline.

**Key Points:**
- Qwen3-Next-80B-A3B-Thinking-GGUF model released on HuggingFace
- Model excels in Tetris game implementation
- Compares favorably to Devstral in accuracy
- Discussion about its release timeline and capabilities
- Questions about native tool calling support in llamacpp

**Discussion Highlights:** The community is impressed with the model's performance, though there is some confusion about the release timeline. Key discussions include its potential for agentic coding, comparisons with other models, and technical questions about tool calling support.

---

## 35. [To Mistral and other lab employees: please test with community tools BEFORE releasing models](https://reddit.com/r/LocalLLaMA/comments/1pmgm2x/to_mistral_and_other_lab_employees_please_test/)

**Author:** u/dtdisapointingresult | **Upvotes:** 136 | **Comments:** 72 | **Date:** 2025-12-14

**Summary:** The Reddit post criticizes Mistral for releasing Devstral 2 without thorough testing with community tools, which led to issues like benchmark discrepancies and repetition loops. The author emphasizes the importance of testing with local tools to maintain reputation and user trust. Key points include the lack of testing with community tools, issues with benchmark discrepancies and repetition loops, and the importance of tech geeks' recommendations. The discussion highlights mixed experiences with the model in local tools and a consensus on the importance of thorough testing.

---

## 36. [Understanding the new router mode in llama cpp server](https://reddit.com/r/LocalLLaMA/comments/1pmc7lk/understanding_the_new_router_mode_in_llama_cpp/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 168 | **Comments:** 44 | **Date:** 2025-12-14

**Summary:** Router mode in llama cpp server allows managing multiple AI models simultaneously without restarting the server, similar to Ollama. It enables loading/unloading models on demand and routing requests to the appropriate model, saving memory and simplifying model switching.

**Key Points:**
- Router mode enables managing multiple AI models without restarting the server.
- It allows loading/unloading models on demand and routing requests to the appropriate model.
- Useful for testing multiple GGUF models, building local OpenAI-compatible APIs, and dynamic model switching.
- Saves memory and simplifies model management.
- Community discussion highlights comparisons with llama-swap and requests for better VRAM management.

**Discussion Highlights:** The community discussed comparisons with llama-swap, requested features like better VRAM management for multiple GPUs, and expressed interest in specifying which models stay in memory concurrently.

---

## 37. [8x RTX Pro 6000 server complete](https://reddit.com/r/LocalLLaMA/comments/1plwgun/8x_rtx_pro_6000_server_complete/)

**Author:** u/koushd | **Upvotes:** 625 | **Comments:** 268 | **Date:** 2025-12-13

**Summary:** The Reddit post details the author's journey of upgrading their GPU server over several years, culminating in a powerful setup with 8x RTX Pro 6000 GPUs, a Threadripper PRO 9955WX CPU, and 384 GB RAM. The post highlights challenges faced during upgrades, including heat management and hardware compatibility issues.

**Key Points:**
- The author started with a single 3080 GPU and gradually upgraded to a powerful 8x RTX Pro 6000 setup.
- Heat management was a significant issue, leading to overheating and system crashes.
- Hardware compatibility issues arose, particularly with the AM5 motherboard and IOMMU addressing.
- The author experimented with various solutions, including using multiple systems and different networking options to reduce latency.
- The post received significant engagement, with comments highlighting both admiration and criticism of the setup.

**Discussion Highlights:** The discussion highlights a mix of admiration for the powerful setup and criticism regarding the practicality and safety of the configuration. Some users praised the author's dedication and technical prowess, while others expressed concerns about the heat management and the use of a 'shitty aluminum frame' for such expensive hardware.

---

## 38. [Mistral 3 Large is DeepSeek V3!?](https://reddit.com/r/LocalLLaMA/comments/1plpc6h/mistral_3_large_is_deepseek_v3/)

**Author:** u/seraschka | **Upvotes:** 168 | **Comments:** 33 | **Date:** 2025-12-13

**Summary:** The post discusses the architectural similarities between Mistral 3 Large and DeepSeek V3, noting that they share nearly identical sizes and architectures, with minor differences in expert configurations. The community highlights the open-source nature of these models and their adoption by various teams.

**Key Points:**
- Mistral 3 Large and DeepSeek V3 have almost identical sizes (671B vs 673B).
- Mistral 3 Large uses the same architecture as DeepSeek V3 but with adjusted expert configurations.
- Mistral likely trained their model from scratch due to using a different tokenizer.
- Other models like Kimi K2 and Gigachat also use the DeepSeek V3 architecture.
- The community views this as a positive aspect of open-source collaboration.

**Discussion Highlights:** The discussion emphasizes the open-source spirit, with multiple teams adopting and adapting the DeepSeek V3 architecture. Some comments note that architectural similarities are expected due to limited ways to build decoder-only models, while others appreciate Mistral's multimodal innovations.

---

## 39. [OpenAI's flagship model, ChatGPT-5.2 Thinking, ranks  most censored AI on Sansa benchmark.](https://reddit.com/r/LocalLLaMA/comments/1plnuqu/openais_flagship_model_chatgpt52_thinking_ranks/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 612 | **Comments:** 112 | **Date:** 2025-12-13

**Summary:** OpenAI's ChatGPT-5.2 Thinking model is criticized for being the most censored AI on the Sansa benchmark, with users reporting poor performance in follow-up questions and research compared to previous versions.

**Key Points:**
- ChatGPT-5.2 is ranked as the most censored AI on the Sansa benchmark.
- Users report the model performs poorly in follow-up questions and research.
- The model frequently denies requests for evaluating QA models, a new issue not present in earlier versions.
- There is curiosity about the testing criteria used in the benchmark.
- Gemini is noted to be less censored than other models, including Mistral.

**Discussion Highlights:** The discussion highlights frustration with ChatGPT-5.2's increased censorship and limitations in follow-up questions and research capabilities, with users noting a decline in performance compared to previous versions.

---

## 40. [Qwen3 Next generation optimization](https://reddit.com/r/LocalLLaMA/comments/1plng6f/qwen3_next_generation_optimization/)

**Author:** u/ilintar | **Upvotes:** 357 | **Comments:** 40 | **Date:** 2025-12-13

**Summary:** The post discusses optimizations made to Qwen3, specifically an optimized autoregressive delta net computation that results in a 40% generation speed upgrade. The author invites others to test the improvements.

**Key Points:**
- Optimized autoregressive delta net computation for Qwen3
- 40% generation speed upgrade reported
- Community appreciation and engagement in comments
- Questions about compatibility with ROCm/Vulkan
- Author recognized for frequent contributions

**Discussion Highlights:** The community shows strong appreciation for the optimization work, with comments highlighting the author's frequent contributions and expressing interest in testing the speed improvements. There are questions about whether the speedup applies to ROCm/Vulkan in addition to CUDA.

---

## 41. [NVIDIA gpt-oss-120b Eagle Throughput model](https://reddit.com/r/LocalLLaMA/comments/1plewrk/nvidia_gptoss120b_eagle_throughput_model/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 239 | **Comments:** 53 | **Date:** 2025-12-13

**Summary:** The post discusses NVIDIA's GPT-OSS-120B-Eagle3-throughput model, an optimized speculative decoding module designed to improve text generation throughput using Eagle3 speculative decoding. It is licensed for both commercial and non-commercial use.

**Key Points:**
- Optimized speculative decoding module for improved throughput
- Uses NVIDIA’s Eagle3 speculative decoding approach
- Licensed under nvidia-open-model-license for commercial and non-commercial use
- Not supported in llama.cpp, limiting its accessibility
- Community interest in derestricted versions and CPU inference compatibility

**Discussion Highlights:** The discussion highlights community interest in derestricted versions of the model and its potential for CPU inference. There is also mention of its lack of support in llama.cpp, which limits its usability in certain environments.

---

## 42. [This is how open ai is advertising them selfs on reddit…. They are doomed](https://reddit.com/r/LocalLLaMA/comments/1plcrg8/this_is_how_open_ai_is_advertising_them_selfs_on/)

**Author:** u/ThinkExtension2328 | **Upvotes:** 238 | **Comments:** 76 | **Date:** 2025-12-12

**Summary:** The Reddit post criticizes OpenAI's advertising strategy, highlighting a shift from promoting advanced AI to using astrology ads, which is seen as a decline in their approach. Key points include the criticism of OpenAI's advertising shift, the suggestion of decline, discussions on profitability and irony, and a consensus on the fall from grace. The discussion highlights a consensus that OpenAI's new advertising strategy is seen as a significant decline from their previous stance on advanced AI and open models.

---

## 43. [Running an LLM on a 3DS](https://reddit.com/r/LocalLLaMA/comments/1pl4njj/running_an_llm_on_a_3ds/)

**Author:** u/vreab | **Upvotes:** 292 | **Comments:** 35 | **Date:** 2025-12-12

**Summary:** The Reddit post discusses the feasibility and novelty of running an LLM on a 3DS, drawing comparisons to similar projects on other devices like the PS Vita and Wii.

**Key Points:**
- Running an LLM on a 3DS is a novel and impressive project.
- Comparisons are made to similar projects on the PS Vita and Wii.
- Discussion includes curiosity about performance improvements on newer hardware like the 'new' 3DS.
- The project is seen as one of the most impressive in the subreddit.

**Discussion Highlights:** The discussion highlights the novelty and technical achievement of running an LLM on a 3DS, with users expressing admiration and curiosity about potential performance improvements on updated hardware.

---

## 44. [The new monster-server](https://reddit.com/r/LocalLLaMA/comments/1pl0ojb/the_new_monsterserver/)

**Author:** u/eribob | **Upvotes:** 591 | **Comments:** 125 | **Date:** 2025-12-12

**Summary:** The Reddit post details a user's upgraded 'Monster-server' setup, featuring a Ryzen 3950x CPU, 128GB RAM, and multiple GPUs including RTX 3090s and an RTX 4090. The server is used for running local LLMs like GPT-OSS-120B and other tasks, with a focus on performance and cost-effectiveness.

**Key Points:**
- The server uses a Ryzen 3950x CPU and 128GB RAM, with GPUs including RTX 3090s and an RTX 4090.
- The setup includes a 10GBe NIC and significant storage with an 8TB NVMe and 4x 18TB HDDs.
- The user runs GPT-OSS-120B fully in VRAM and uses the server for research, coding, and general tasks.
- Discussion highlights include nostalgia for early 2000s overclocking forums and questions about the user's location and internet setup.
- Some users noted potential performance issues with a 3-GPU setup compared to 2 or 4 GPUs.

**Discussion Highlights:** The discussion includes positive reactions to the setup, with users expressing nostalgia and curiosity about the user's location and internet setup. There are also technical comments about potential performance issues with the 3-GPU configuration.

---

## 45. [Olmo 3.1 32B Think &amp; Instruct: New Additions to the Olmo Model Family](https://reddit.com/r/LocalLLaMA/comments/1pky5u4/olmo_31_32b_think_instruct_new_additions_to_the/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 183 | **Comments:** 28 | **Date:** 2025-12-12

**Summary:** The post introduces Olmo 3.1 32B Think and Instruct models, two new 32-billion-parameter models optimized for deep reasoning and instruction following, respectively. The Think model excels in multi-step reasoning, math, logic, and code generation, while the Instruct model focuses on conversational fluency and tool-use capabilities.

**Key Points:**
- Olmo 3.1 32B Think and Instruct models are the newest additions to the Olmo family.
- The Think model is optimized for deep reasoning, trained with extended reinforcement learning on the Dolci-Think-RL dataset.
- The Instruct model is optimized for instruction following, conversational fluency, and tool-use capabilities.
- The models are fully open source and praised for their quality and continuous improvement.
- Community expectations include potential future developments like Mixture of Experts (MOE) models.

**Discussion Highlights:** The community appreciates the open-source nature of the Olmo models and their continuous improvement. There is enthusiasm for the new models and expectations for future developments like MOE models.

---

## 46. [Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/)

**Author:** u/Nunki08 | **Upvotes:** 1319 | **Comments:** 155 | **Date:** 2025-12-12

**Summary:** An NVIDIA employee accidentally uploaded the parent folder of their upcoming model on Hugging Face, sparking interest and urgency among users to save the files before potential removal.

**Key Points:**
- NVIDIA's upcoming model files were accidentally uploaded on Hugging Face.
- Users are urged to save the files before they might be taken down.
- The Nemotron lineup is mentioned as promising.
- There is concern about potential censoring of the uploaded content.

**Discussion Highlights:** The community is actively discussing the accidental upload, with a focus on preserving the files and expressing interest in the Nemotron lineup. There is a consensus on the urgency to save the content before any potential removal or censoring.

---

## 47. [Training an LLM only on 1800s London texts - 90GB dataset](https://reddit.com/r/LocalLLaMA/comments/1pkpsee/training_an_llm_only_on_1800s_london_texts_90gb/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 706 | **Comments:** 78 | **Date:** 2025-12-12

**Summary:** The post discusses the TimeCapsuleLLM project, which involves training an LLM on a 90GB dataset of 1800-1875 London texts. The author has conducted a bias report and trained a small evaluation model to assess the dataset before scaling up.

**Key Points:**
- The dataset consists of 90GB with 135,000 documents from 1800-1875 London texts.
- A bias report covering temporal, gender/pronoun, and geographic bias has been generated.
- A small evaluation model (300M parameters) was trained on a 15GB subset to evaluate the dataset.
- The community appreciates the detailed work and suggests considering MoE for better compute efficiency.
- The project aims to study historical texts despite inherent biases.

**Discussion Highlights:** The community shows strong support for the project, with suggestions for improvement such as using Mixture of Experts (MoE) for better compute efficiency. There is also interest in the methodology and progress of the project.

---

## 48. [Agentic Local AI on CPU = Mistral Vibe + Granite-4-h-1b](https://reddit.com/r/LocalLLaMA/comments/1pkdkjo/agentic_local_ai_on_cpu_mistral_vibe_granite4h1b/)

**Author:** u/PotentialFunny7143 | **Upvotes:** 235 | **Comments:** 40 | **Date:** 2025-12-11

**Summary:** The Reddit post discusses running local AI models like Mistral Vibe and Granite-4-h-1b on CPU hardware, highlighting their performance and efficiency. Users share experiences and ask about hardware requirements, performance metrics, and comparisons with other models.

**Key Points:**
- Mistral Vibe and Granite-4-h-1b are efficient for local CPU-based AI tasks.
- Users are interested in performance metrics like tokens per second and hardware requirements.
- Discussion includes comparisons with other models like Cline and Open Code.
- Questions about RAM and CPU consumption are raised.
- The upper capability boundaries of these models are explored.

**Discussion Highlights:** The discussion focuses on performance benchmarks, hardware efficiency, and comparisons with other models. Users are particularly interested in practical aspects like RAM usage, CPU consumption, and real-world performance metrics.

---

