# r/LocalLLaMA Reading Digest

**Period:** 2025-12-18 to 2025-12-18
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [Google's Gemma models family](https://reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/)

**Author:** u/jacek2023 | **Upvotes:** 288 | **Comments:** 65 | **Date:** 2025-12-18

**Summary:** The Reddit post discusses Google's Gemma models family, with a focus on the newly introduced FunctionGemma model intended for fine-tuning specific function-calling tasks. The community shows enthusiasm and speculative comments about potential new models.

**Key Points:**
- FunctionGemma is designed for fine-tuning specific function-calling tasks, including multi-turn use cases.
- The community expresses strong enthusiasm for Google's new models.
- Speculation about the release of three new Gemma models based on the number of visible models.
- The post gained significant attention with 288 upvotes and 65 comments.

**Discussion Highlights:** The discussion highlights include strong community enthusiasm for FunctionGemma, with comments praising Google's innovations. There is also speculation about the release of new Gemma models, based on the count of visible models in the collection.

---

## 2. [MiraTTS: High quality and fast TTS model](https://reddit.com/r/LocalLLaMA/comments/1pper90/miratts_high_quality_and_fast_tts_model/)

**Author:** u/SplitNice1982 | **Upvotes:** 131 | **Comments:** 39 | **Date:** 2025-12-17

**Summary:** MiraTTS is a high-quality, fast TTS model that generates realistic 48khz speech at 100x realtime, optimized for memory efficiency and low latency. It supports multilingual versions and is available on GitHub and Hugging Face.

**Key Points:**
- MiraTTS generates speech at 100x realtime with high quality and clarity.
- It is memory efficient, working with GPUs as low as 6GB VRAM.
- Low latency as low as 150ms, with streaming support planned.
- Multilingual versions are supported, with multispeaker in progress.
- Available on GitHub and Hugging Face with additional resources.

**Discussion Highlights:** The discussion highlights curiosity about multilingual support, voice cloning, and comparisons with other TTS models like KaniTTS. Users appreciate the frequent releases and quality of work.

---

## 3. [Nvidia plans heavy cuts to GPU supply in early 2026](https://reddit.com/r/LocalLLaMA/comments/1pp8vo4/nvidia_plans_heavy_cuts_to_gpu_supply_in_early/)

**Author:** u/HumanDrone8721 | **Upvotes:** 325 | **Comments:** 166 | **Date:** 2025-12-17

**Summary:** Nvidia plans to significantly reduce GPU supply in early 2026, which, combined with similar cuts by Micron and Samsung, could make building gaming PCs challenging. The move has sparked discussions about potential new competition and broader industry implications.

**Key Points:**
- Nvidia's GPU supply cuts in early 2026
- Micron and Samsung also reducing consumer RAM and SSD production
- Potential challenges for gaming PC builders in 2026
- Discussion about new competition entering the market
- Criticism of stock buybacks over investment in growth

**Discussion Highlights:** The discussion highlights concerns about the impact on gaming PC builders, with many users noting the broader trend of supply cuts across the industry. There is also speculation about new competition emerging and criticism of companies prioritizing stock buybacks over innovation.

---

## 4. [Hey, LocalLLaMa. We need to talk...](https://reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/)

**Author:** u/Eisenstein | **Upvotes:** 365 | **Comments:** 122 | **Date:** 2025-12-17

**Summary:** The post highlights the importance of community engagement and support for contributors in the r/LocalLLaMA subreddit, emphasizing the need for upvotes and constructive feedback to encourage continued sharing and development.

**Key Points:**
- The author urges the community to engage with and support smaller posts and projects.
- Constructive feedback and upvotes are crucial for encouraging contributors.
- There is a mix of supportive and critical comments, with some users expressing frustration over low-quality or overly ambitious projects.
- The community values genuine contributions and constructive engagement.
- The post has gained significant attention, as indicated by the high number of upvotes and comments.

**Discussion Highlights:** The discussion reveals a consensus on the importance of supporting genuine contributions, though there is debate over the quality of some projects. Many users appreciate the call for engagement but also express concerns about the prevalence of low-effort or overly ambitious projects.

---

## 5. [Drummer's Cydonia and Magidonia 24B v4.3 - The best pair of Cydonia for RP yet!](https://reddit.com/r/LocalLLaMA/comments/1pp2j60/drummers_cydonia_and_magidonia_24b_v43_the_best/)

**Author:** u/TheLocalDrummer | **Upvotes:** 125 | **Comments:** 20 | **Date:** 2025-12-17

**Summary:** The post announces the release of Drummer's Cydonia and Magidonia 24B v4.3 models, described as the best pair for role-playing yet, with links to their respective repositories. The author expresses gratitude to patrons for their support.

**Key Points:**
- Release of Cydonia-24B-v4.3 and Magidonia-24B-v4.3 models
- Models are praised for their quality, especially Magidonia
- Author thanks patrons for their support and freedom
- Links to Hugging Face repositories provided
- Community feedback highlights model excellence and appreciation for the author

**Discussion Highlights:** The community shows strong appreciation for the author's contributions, with comments praising the models and expressing gratitude. Some users share technical tips, like attaching a vision mmproj to the gguf, and others confirm the models' quality.

---

## 6. [Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds.](https://reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/)

**Author:** u/themixtergames | **Upvotes:** 1082 | **Comments:** 127 | **Date:** 2025-12-17

**Summary:** Apple has introduced SHARP, a model that can generate photorealistic 3D Gaussian representations from a single image in seconds. The model is showcased on GitHub and detailed in an arXiv paper, with examples rendered in real-time on Apple Vision Pro and generated quickly on a MacBook Pro M1 Max.

**Key Points:**
- SHARP generates photorealistic 3D Gaussian representations from a single image in seconds.
- The model is available on GitHub and detailed in an arXiv paper.
- Examples are rendered in real-time on Apple Vision Pro and generated in 5-10 seconds on a MacBook Pro M1 Max.
- The model requires CUDA GPU for rendering trajectories.
- Community interest includes potential applications and comparisons to cyberpunk's braindance.

**Discussion Highlights:** The community shows strong interest in SHARP's capabilities, with discussions ranging from technical requirements (CUDA GPU) to potential applications and comparisons to sci-fi concepts like cyberpunk's braindance. Some users also inquire about the model's applicability to adult content.

---

## 7. [LangChain and LlamaIndex are in "steep decline" according to new ecosystem report. Anyone else quietly ditching agent frameworks?](https://reddit.com/r/LocalLLaMA/comments/1pox733/langchain_and_llamaindex_are_in_steep_decline/)

**Author:** u/Exact-Literature-395 | **Upvotes:** 201 | **Comments:** 57 | **Date:** 2025-12-17

**Summary:** The Reddit post discusses the decline of LangChain and LlamaIndex frameworks, citing reduced community activity and investment. Users share experiences of simplifying their codebases by removing these frameworks and calling APIs directly, questioning the necessity of such tools with improved base models.

**Key Points:**
- LangChain, LlamaIndex, and AutoGen are listed as 'steepest declining' projects by community activity.
- Users report simplifying codebases and improving debugging by removing these frameworks.
- Criticism of LangChain includes bloated features, poor security/performance, and non-pythonic design.
- LlamaIndex maintainer acknowledges the shift but highlights the frameworks' initial ease of integration.
- Discussion suggests a potential shift towards simpler, more direct API usage.

**Discussion Highlights:** The discussion highlights a consensus around the decline of these frameworks, with users expressing frustration over complexity and lack of performance. There is a notable preference for simpler, more direct approaches to API usage, though some acknowledge the initial benefits these frameworks provided.

---

## 8. [anthropic blog on code execution for agents. 98.7% token reduction sounds promising for local setups](https://reddit.com/r/LocalLLaMA/comments/1powhy6/anthropic_blog_on_code_execution_for_agents_987/)

**Author:** u/Zestyclose_Ring1123 | **Upvotes:** 131 | **Comments:** 33 | **Date:** 2025-12-17

**Summary:** Anthropic's blog discusses a 'code execution' approach for agents, which reduces token usage significantly by letting models write code to orchestrate tools on demand. This could be beneficial for local setups with smaller models and offers privacy advantages by keeping sensitive data out of the model context.

**Key Points:**
- Anthropic's approach reduces token usage by 98.7%, making it feasible for local setups with smaller models.
- The method involves model-generated code to explore and use tools on demand, rather than preloading all tool definitions.
- Privacy is enhanced as sensitive data flows directly between tools without entering the model context.
- Sandboxing is a major challenge for running model-generated code locally.
- Similar patterns already exist in projects like HF's smolagents and other implementations.

**Discussion Highlights:** The discussion highlights that similar patterns have been independently discovered and implemented by others, such as HF's smolagents. There is also a focus on the security challenges of sandboxing model-generated code and alternative approaches like generating a DAG of steps to reduce sandboxing needs.

---

## 9. [Peak LLM Wars: Xiaomi Blocks Kimi Employees on Twitter](https://reddit.com/r/LocalLLaMA/comments/1pow797/peak_llm_wars_xiaomi_blocks_kimi_employees_on/)

**Author:** u/nekofneko | **Upvotes:** 130 | **Comments:** 25 | **Date:** 2025-12-17

**Summary:** The Reddit post discusses the ongoing 'LLM wars' with a focus on Xiaomi blocking Kimi employees on Twitter, highlighting the competitive and dramatic nature of the AI industry.

**Key Points:**
- Xiaomi is involved in a public dispute with Kimi on Twitter.
- There are speculations about former DeepSeek members joining Xiaomi.
- The post compares the situation to other tech industry rivalries like Musk vs. Altman and Meta vs. Zuckerberg.
- The discussion includes humorous comparisons to other online drama communities.

**Discussion Highlights:** The discussion highlights the competitive and sometimes dramatic nature of the AI industry, with users drawing parallels to other tech rivalries and online drama communities. The tone is largely humorous and speculative.

---

## 10. [Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model](https://reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 1124 | **Comments:** 119 | **Date:** 2025-12-17

**Summary:** Microsoft's TRELLIS 2-4B is an open-source image-to-3D model with 4 billion parameters, converting single images into 3D assets. The model uses Flow-Matching Transformers with Sparse Voxel based 3D VAE. Community feedback is mixed, with some users praising its quality while others find it lacking in practical applications.

**Key Points:**
- Model Type: Flow-Matching Transformers with Sparse Voxel based 3D VAE
- Parameters: 4 Billion
- Input: Single Image, Output: 3D Asset
- Mixed community feedback on practical usability
- Suggestions for improvement include using multiple images for better results

**Discussion Highlights:** The community discussion highlights mixed reactions, with some users finding the model excellent for certain use cases, while others criticize its practical usability. There are suggestions for improvements, such as using multiple images for better results.

---

## 11. [QwenLong-L1.5: Revolutionizing Long-Context AI](https://reddit.com/r/LocalLLaMA/comments/1pokpha/qwenlongl15_revolutionizing_longcontext_ai/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 213 | **Comments:** 27 | **Date:** 2025-12-16

**Summary:** QwenLong-L1.5 is a new AI model achieving state-of-the-art long-context reasoning with novel data synthesis, stabilized RL, and memory management for contexts up to 4M tokens. It is available on HuggingFace and has garnered significant attention for its capabilities.

**Key Points:**
- Achieves SOTA long-context reasoning with up to 4M tokens
- Uses novel data synthesis and stabilized RL techniques
- Available on HuggingFace under the name QwenLong-L1.5-30B-A3B
- Integration into llama.cpp may require additional work
- Specific query templates are recommended for optimal use

**Discussion Highlights:** The discussion highlights the model's significant potential and the need for integration work into existing frameworks like llama.cpp. Users also noted the importance of using specific query templates for best results. Some comments expressed enthusiasm for the model's capabilities.

---

## 12. [8x Radeon 7900 XTX Build for Longer Context Local Inference - Performance Results &amp; Build Details](https://reddit.com/r/LocalLLaMA/comments/1pogwb6/8x_radeon_7900_xtx_build_for_longer_context_local/)

**Author:** u/Beautiful_Trust_8151 | **Upvotes:** 713 | **Comments:** 210 | **Date:** 2025-12-16

**Summary:** The post details a custom multi-GPU setup using 8x AMD Radeon 7900 XTX cards for local AI inference, achieving 192 GB VRAM and stable performance with up to 200+ tokens per second during prompt processing. The build cost around $6-7k and is praised for its flexibility and long-context capability.

**Key Points:**
- 8x AMD Radeon 7900 XTX GPUs provide 192 GB VRAM for local AI inference
- Performance scales well with context, maintaining over 200 tokens per second during prompt processing
- Total build cost is around $6-7k, offering flexibility and long-context capability
- Community appreciates the build as a notable example of early AI era hardware
- Suggestions include switching to Linux and ROCm for potential performance improvements

**Discussion Highlights:** The community reacted positively, with comments highlighting the build's significance in the early AI era and suggesting potential optimizations like switching to Linux and ROCm for better performance.

---

## 13. [Nemotron 3 Nano 30B is Amazing! (TLDR)](https://reddit.com/r/LocalLLaMA/comments/1pocsdy/nemotron_3_nano_30b_is_amazing_tldr/)

**Author:** u/DonkeyBonked | **Upvotes:** 203 | **Comments:** 122 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses the user's experience with Nemotron 3 Nano 30B, highlighting its token efficiency and performance on their hardware setup. The user compares it favorably to other models like Devstral 2 Small 24B and Qwen models, noting its ability to handle large context sizes efficiently.

**Key Points:**
- Nemotron 3 Nano 30B shows impressive token efficiency and performance.
- The user's hardware setup includes an RTX 5000 and an RTX 3090 eGPU.
- Nemotron 3 Nano 30B fits 256k tokens in VRAM and can handle up to 1M context with spillover.
- Comparisons with other models like Devstral 2 Small 24B and Qwen models are made.
- Discussion highlights include performance benchmarks and use cases.

**Discussion Highlights:** The discussion highlights the model's performance and efficiency, with some users comparing it to other models like Qwen 30B. There is a general consensus that Nemotron 3 Nano 30B is a strong performer, though some users still prefer other models for specific tasks.

---

## 14. [32GB Mi50's were getting so expensive that I ended up buying a 32GB w6800 for about the same price instead](https://reddit.com/r/LocalLLaMA/comments/1pob44f/32gb_mi50s_were_getting_so_expensive_that_i_ended/)

**Author:** u/EmPips | **Upvotes:** 231 | **Comments:** 42 | **Date:** 2025-12-16

**Summary:** The author opted for a 32GB w6800 GPU instead of a 32GB Mi50 due to similar pricing, highlighting the pros and cons of their choice. The discussion includes comparisons with other GPUs like the AMD Radeon AI PRO R9700 and Zotac 3090.

**Key Points:**
- Author chose 32GB w6800 over 32GB Mi50 due to similar pricing
- Pros of w6800 include convenience and effective cooling
- Alternatives like AMD Radeon AI PRO R9700 and Zotac 3090 were discussed
- Price comparisons and performance considerations were key discussion points

**Discussion Highlights:** The discussion revolved around the cost-effectiveness and performance of different GPUs, with users sharing their experiences and recommendations. The consensus leaned towards considering alternatives based on specific needs and budget constraints.

---

## 15. [8 Million Users' AI Conversations Sold for Profit by "Privacy" Extensions | Koi Blog](https://reddit.com/r/LocalLLaMA/comments/1poal2a/8_million_users_ai_conversations_sold_for_profit/)

**Author:** u/ManThigh | **Upvotes:** 156 | **Comments:** 46 | **Date:** 2025-12-16

**Summary:** The Reddit post highlights privacy concerns regarding browser extensions selling AI conversation data of millions of users for profit, emphasizing the importance of using local models and auditing extensions.

**Key Points:**
- Browser extensions like Urban VPN Proxy and 1ClickVPN Proxy sold AI conversation data of millions of users.
- The post emphasizes the importance of running local models to avoid such privacy breaches.
- Users are advised to audit their extensions to prevent data leaks.
- The community expresses strong disapproval of companies buying and selling user data.
- Local setups are praised for their privacy benefits.

**Discussion Highlights:** The discussion highlights a strong consensus on the need for privacy protection, with users advocating for local models and expressing anger towards companies involved in data selling. The community also emphasizes the importance of auditing browser extensions to prevent data leaks.

---

## 16. [Finally managed to run Qwen-2.5-7B on a 4GB GTX 1050 without CPU offloading (Surgical Memory Alignment)](https://reddit.com/r/LocalLLaMA/comments/1po97ad/finally_managed_to_run_qwen257b_on_a_4gb_gtx_1050/)

**Author:** u/HuseyinKama | **Upvotes:** 146 | **Comments:** 48 | **Date:** 2025-12-16

**Summary:** The post describes a solution called 'Surgical Memory Alignment' to run Qwen-2.5-7B on a 4GB GTX 1050 without CPU offloading, saving VRAM and improving speed. The author open-sourced the framework as QKV Core.

**Key Points:**
- Standard GGUF quantization tools add padding that wastes memory, causing OOM errors on low-end GPUs.
- The solution involves analyzing layer entropy, switching storage methods, and trimming memory blocks to save VRAM.
- The method saved 44MB per model, allowing Qwen-2.5-7B to run entirely on GPU with a 34% improvement in I/O load times.
- The framework, QKV Core, is open-sourced for others with low-end GPUs to use.
- Community feedback includes praise, skepticism about the code, and questions about compatibility.

**Discussion Highlights:** The community responded with a mix of praise for the optimization, skepticism about the code's effectiveness, and questions about its practical application. Some users expressed gratitude for the work, while others questioned the validity of the benchmarks.

---

## 17. [I was bored](https://reddit.com/r/LocalLLaMA/comments/1po8yt0/i_was_bored/)

**Author:** u/MyLovelyAngelKirino | **Upvotes:** 130 | **Comments:** 70 | **Date:** 2025-12-16

**Summary:** The author, who is unemployed, built a high-performance computer setup with excess hardware, featuring 3x 3090 GPUs, 512GB RAM, and an Epyc 7663 56-core processor. The post garnered significant attention, with users expressing admiration and curiosity about the setup.

**Key Points:**
- Author built a powerful computer setup due to unemployment and excess hardware
- Hardware includes 3x 3090 GPUs, 512GB RAM, and an Epyc 7663 56-core processor
- Post received 130 upvotes and 70 comments, indicating high interest
- Users expressed admiration and curiosity, with some requesting details on water-cooling components
- One comment humorously referenced a character named Felix

**Discussion Highlights:** The discussion highlights the impressive hardware setup and the neatness of the build. Users expressed admiration and curiosity, with some requesting more details about the water-cooling components. There was also a humorous reference to a character named Felix.

---

## 18. [Meta announced a new SAM Audio Model for audio editing that can segment sound from complex audio mixtures using text, visual, and time span prompts.](https://reddit.com/r/LocalLLaMA/comments/1po7i0c/meta_announced_a_new_sam_audio_model_for_audio/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 505 | **Comments:** 81 | **Date:** 2025-12-16

**Summary:** Meta has introduced a new SAM Audio Model that revolutionizes audio editing by allowing users to isolate specific sounds from complex audio mixtures using text, visual, and time span prompts.

**Key Points:**
- SAM Audio Model enables easy isolation of sounds from complex audio mixtures.
- The model uses text, visual, and time span prompts for audio segmentation.
- Potential applications include filtering out unwanted noises in virtual meetings.
- The model demonstrates high precision in isolating specific sounds from videos.
- Model sizes and specifications are available for reference.

**Discussion Highlights:** The discussion highlights the potential of the SAM Audio Model in practical applications, such as improving audio quality in virtual meetings by filtering out unwanted noises. Users are impressed by the model's ability to accurately isolate sounds from complex audio mixtures.

---

## 19. [Allen Institute for AI introduces Molmo 2](https://reddit.com/r/LocalLLaMA/comments/1po78bl/allen_institute_for_ai_introduces_molmo_2/)

**Author:** u/Agitated_Camel1886 | **Upvotes:** 238 | **Comments:** 22 | **Date:** 2025-12-16

**Summary:** The Allen Institute for AI has introduced Molmo 2, an 8B model capable of advanced video analysis tasks like Video QA, counting, pointing, and dense captioning. The community is impressed with its capabilities and the public availability of datasets. Key points include Molmo 2's advanced video analysis capabilities, support for tasks like Video QA and dense captioning, an upcoming AMA to discuss Olmo 3 and Molmo 2, appreciation for the public release of datasets by Allen AI, and impressive benchmarks for its size. The discussion highlights the community's excitement and appreciation for the model's capabilities and transparency.

---

## 20. [XiaomiMiMo/MiMo-V2-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1po3bn4/xiaomimimomimov2flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 233 | **Comments:** 51 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses MiMo-V2-Flash, a Mixture-of-Experts (MoE) language model with 309B total parameters and 15B active parameters, designed for high-speed reasoning and agentic workflows. Users highlight its impressive performance on multilingual SWE tasks and discuss its technical specifications and potential applications.

**Key Points:**
- MiMo-V2-Flash is a Mixture-of-Experts (MoE) language model with 309B total parameters and 15B active parameters.
- It is designed for high-speed reasoning and agentic workflows.
- The model shows strong performance on multilingual SWE tasks, outperforming larger models like Sonnet 4.5 and Gemini 3.
- Users discuss the feasibility of running the model on specific hardware configurations.
- The release includes weights and technical documentation for further exploration.

**Discussion Highlights:** Users express excitement about the model's performance and the release of its weights. There is some skepticism about the model's performance claims, and discussions about hardware requirements and potential larger versions of the model.

---

## 21. [GLM-4.5V, GLM-4.6V and GLM_4.6V-Flash are now supported by llama.cpp (GGUFs)](https://reddit.com/r/LocalLLaMA/comments/1po18y9/glm45v_glm46v_and_glm_46vflash_are_now_supported/)

**Author:** u/jacek2023 | **Upvotes:** 168 | **Comments:** 34 | **Date:** 2025-12-16

**Summary:** The post announces that GLM-4.5V, GLM-4.6V, and GLM-4.6V-Flash models are now supported by llama.cpp with GGUFs, which is seen as a significant update by the community.

**Key Points:**
- Support for GLM-4.5V, GLM-4.6V, and GLM-4.6V-Flash has been added to llama.cpp.
- The update is celebrated as a great Christmas gift by the community.
- There is a question about whether GGUFs now support vision, with some users reporting issues.
- Comparisons between Qwen3-VL-4B and GLM-4.6V are being discussed.

**Discussion Highlights:** The community is excited about the new support for GLM models in llama.cpp, though there are some concerns and questions about vision support and compatibility with existing setups.

---

## 22. [Qwen3 Next speed optimization has been merged into llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1pnz9xu/qwen3_next_speed_optimization_has_been_merged/)

**Author:** u/jacek2023 | **Upvotes:** 214 | **Comments:** 25 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses the recent speed optimization for Qwen3 Next in llama.cpp, highlighting significant performance improvements across different hardware configurations.

**Key Points:**
- Speed optimization for Qwen3 Next has been merged into llama.cpp.
- Performance on M1 64GB improved from 12 t/s to 18 t/s.
- Other configurations show improvements, such as 37.x t/s on Win11 + RTX5090 + vulkan.
- Qwen3-30B achieves around 58 t/s on the same M1 64GB setup.
- Optimization is well-received by the community.

**Discussion Highlights:** The community consensus is positive, with users reporting significant speed improvements across various hardware setups, indicating a successful optimization effort.

---

## 23. [I may have over-quantized this little guy.](https://reddit.com/r/LocalLLaMA/comments/1pnz80z/i_may_have_overquantized_this_little_guy/)

**Author:** u/AllergicToTeeth | **Upvotes:** 136 | **Comments:** 30 | **Date:** 2025-12-16

**Summary:** The post discusses an over-quantized model, sparking humorous and technical comments about AI models and quantization levels.

**Key Points:**
- Mentions of ClosedAI and the open-source community
- Discussion on system prompts and their impact on model behavior
- References to quantization levels like Q0
- Humorous comments about GPT-5.4 and GPT-5.3 leaks

**Discussion Highlights:** The community engages in a mix of technical discussion about model quantization and playful banter about AI advancements.

---

## 24. [It was Ilya who "closed" OpenAI](https://reddit.com/r/LocalLLaMA/comments/1pnxekt/it_was_ilya_who_closed_openai/)

**Author:** u/licuphand | **Upvotes:** 511 | **Comments:** 229 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses Ilya Sutskever's role in OpenAI's shift towards closed models, sparking a debate on corporate control of AI and leadership dynamics.

**Key Points:**
- Distrust in corporate control of AI
- Historical parallels to governance issues
- Leadership struggles among Elon Musk, Ilya Sutskever, and Sam Altman
- Criticism of OpenAI's shift towards closed models

**Discussion Highlights:** The discussion highlights concerns about corporate control of AI, with many users drawing historical parallels and criticizing leadership dynamics at OpenAI, xAI, and SSI.

---

## 25. [Alibaba Open-Sources CosyVoice 3, a New TTS Model](https://reddit.com/r/LocalLLaMA/comments/1pnusp9/alibaba_opensources_cosyvoice_3_a_new_tts_model/)

**Author:** u/nekofneko | **Upvotes:** 214 | **Comments:** 31 | **Date:** 2025-12-16

**Summary:** Alibaba has open-sourced CosyVoice 3, a new TTS model with advanced features like multi-lingual support, high naturalness, and low latency. The model supports various languages, dialects, and emotions, and includes features like pronunciation inpainting and text normalization.

**Key Points:**
- Supports 9 common languages and 18+ Chinese dialects
- Achieves state-of-the-art performance in content consistency and naturalness
- Features low latency (150ms) and supports both text-in and audio-out streaming
- Supports pronunciation inpainting and text normalization
- Includes instruct support for languages, dialects, emotions, speed, and volume

**Discussion Highlights:** The discussion highlights comparisons with other models like Chatterbox and Microsoft VibeVoice, with users expressing interest in the model's capabilities and potential for voice cloning. Some users are eager for larger model versions and real-time applications.

---

## 26. [New budget local AI rig](https://reddit.com/r/LocalLLaMA/comments/1pnllux/new_budget_local_ai_rig/)

**Author:** u/vucamille | **Upvotes:** 154 | **Comments:** 38 | **Date:** 2025-12-15

**Summary:** The user built a budget-friendly local AI rig using cost-effective components like the Qiyida X99 mobo, Xeon E5 2680 V4, and two MI50 16GB GPUs, totaling around $650. The setup works well with ROCm 7.0.2 and supports multi-GPU inference, with plans for future upgrades.

**Key Points:**
- Budget build with a total cost of around $650
- Successful multi-GPU setup with MI50 16GB GPUs
- Positive user experience and community appreciation
- Future plans for upgrades and benchmarks
- Cost-effective components sourced from AliExpress and Alibaba

**Discussion Highlights:** The community praised the build for its affordability and performance, with requests for benchmarks and appreciation for the user's cost-effective approach.

---

## 27. [I'm strong enough to admit that this bugs the hell out of me](https://reddit.com/r/LocalLLaMA/comments/1pnfaqo/im_strong_enough_to_admit_that_this_bugs_the_hell/)

**Author:** u/ForsookComparison | **Upvotes:** 1691 | **Comments:** 351 | **Date:** 2025-12-15

**Summary:** The post expresses frustration about a tech-related issue, likely involving workstation performance or hardware limitations. The discussion includes humorous comments and technical debates about computing power.

**Key Points:**
- Post title indicates frustration with a tech issue
- Top comment references a Discord feature and special flair
- Meme about RAM doubling is shared
- Discussion includes debates about Mac vs. GPU workstations
- Humor and technical insights are mixed in the comments

**Discussion Highlights:** The discussion highlights a mix of humor, such as the RAM meme, and technical debates about the performance of different workstation setups, including Mac Mini M4 Pro and GPU-based systems.

---

## 28. [They're finally here (Radeon 9700)](https://reddit.com/r/LocalLLaMA/comments/1pnd5uf/theyre_finally_here_radeon_9700/)

**Author:** u/Zeikos | **Upvotes:** 358 | **Comments:** 67 | **Date:** 2025-12-15

**Summary:** The post announces the arrival of Radeon 9700 GPUs, sparking community interest and requests for benchmarks. Users express nostalgia for the historic GPU name and anticipation for performance data.

**Key Points:**
- Community eagerly awaits benchmarks for the new Radeon 9700 GPUs
- Nostalgia for the Radeon 9700 name from the early 2000s
- Requests for inference, training, and heat/noise benchmarks
- Users plan to test the GPUs during the holidays

**Discussion Highlights:** The community shows strong engagement, with a consensus on the need for comprehensive benchmarks to evaluate the new GPUs' performance and characteristics.

---

## 29. [status of Nemotron 3 Nano support in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1pnc045/status_of_nemotron_3_nano_support_in_llamacpp/)

**Author:** u/jacek2023 | **Upvotes:** 177 | **Comments:** 31 | **Date:** 2025-12-15

**Summary:** The Reddit post discusses the status of Nemotron 3 Nano support in llama.cpp, highlighting a GitHub pull request and community reactions. The discussion emphasizes the importance of collaboration between organizations and the llama.cpp project for new model architectures. Key points include: Nemotron 3 Nano support is being added to llama.cpp via a pull request; the model sizes (Q4_K_M and Q4_K_XL) are noted to be around 24GB, which is a point of discussion; community members praise Nvidia for their approach and encourage other labs to follow suit; there is a consensus that organizations releasing new models should work with llama.cpp for early support. The community appreciates Nvidia's proactive approach and hopes other organizations will prioritize early integration with widely-used tools like llama.cpp.

---

## 30. [NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model!](https://reddit.com/r/LocalLLaMA/comments/1pn8upp/nvidia_releases_nemotron_3_nano_a_new_30b_hybrid/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 841 | **Comments:** 178 | **Date:** 2025-12-15

**Summary:** NVIDIA has released Nemotron 3 Nano, a 30B hybrid reasoning model with a 1M context window and top performance in SWE-Bench, reasoning, and chat tasks. The model is available in GGUF format and is noted for its speed and efficiency.

**Key Points:**
- Nemotron 3 Nano is a 30B hybrid reasoning model with a 1M context window.
- It achieves best-in-class performance in SWE-Bench, reasoning, and chat tasks.
- The model is available in GGUF format via Hugging Face.
- It is part of the Nemotron 3 family of MoE models, which includes three sizes.
- Users report impressive speed, with 110 tokens per second generation on local hardware.

**Discussion Highlights:** The community discussion highlights the model's speed and efficiency, with users reporting high token generation rates. There is also clarification about the model family, which includes three sizes of MoE models. Some users expressed surprise at the 'nano' designation for a 30B model.

---

## 31. [NVIDIA Nemotron 3 Nano 30B A3B released](https://reddit.com/r/LocalLLaMA/comments/1pn8h5h/nvidia_nemotron_3_nano_30b_a3b_released/)

**Author:** u/rerri | **Upvotes:** 279 | **Comments:** 83 | **Date:** 2025-12-15

**Summary:** NVIDIA has released Nemotron 3 Nano 30B A3B, a new model featuring a hybrid Mamba-Transformer MoE architecture, exceptional inference efficiency, and a 1M-token context window. It is fully open with weights, datasets, and training recipes available.

**Key Points:**
- Hybrid Mamba-Transformer MoE architecture for efficient inference
- 31.6B total parameters with ~3.6B active per token
- Up to 4x faster than Nemotron Nano 2 and leading models in its size category
- 1M-token context window for long-horizon workflows
- Fully open with weights, datasets, and training recipes available

**Discussion Highlights:** The discussion includes a Llama.cpp PR for integration, questions about optimal Unsloth quant for a 3090 setup, concerns about synthetic data training, and performance feedback from users compiling the model.

---

## 32. [How to do a RTX Pro 6000 build right](https://reddit.com/r/LocalLLaMA/comments/1pn6ijr/how_to_do_a_rtx_pro_6000_build_right/)

**Author:** u/GPTrack_dot_ai | **Upvotes:** 116 | **Comments:** 174 | **Date:** 2025-12-15

**Summary:** The post discusses building a high-performance system using the RTX PRO 6000, highlighting its integration with high-speed networking and providing exemplary specs for a server setup. The discussion reflects awe at the system's capabilities and cost.

**Key Points:**
- The RTX PRO 6000 lacks NVlink but integrates high-speed networking directly at each GPU.
- An exemplary build includes 8x RTX PRO 6000 GPUs, high-end CPUs, and extensive memory and storage options.
- The system is described as powerful but expensive, with comments expressing admiration and humor about its cost.

**Discussion Highlights:** The discussion is marked by admiration for the system's power and humor about its high cost, with comments comparing it to luxury items like Ferraris and private jets.

---

## 33. [New Google model incoming!!!](https://reddit.com/r/LocalLLaMA/comments/1pn37mw/new_google_model_incoming/)

**Author:** u/R46H4V | **Upvotes:** 1249 | **Comments:** 262 | **Date:** 2025-12-15

**Summary:** The Reddit post discusses anticipation and speculation around an upcoming new Google model, with links to relevant sources. The community expresses hope for significant improvements and multi-modal capabilities.

**Key Points:**
- Anticipation of a new Google model
- Community hopes for improvements over previous models like Gemma3-Math
- Speculation about multi-modal capabilities
- High engagement with 1249 upvotes and 262 comments
- Links to relevant sources on X (Twitter) and Hugging Face

**Discussion Highlights:** The discussion highlights a strong sense of anticipation and hope within the community for a significant advancement in Google's model capabilities. There is a consensus on the desire for multi-modal features and improvements over previous iterations.

---

## 34. [llama.cpp: Automation for GPU layers, tensor split, tensor overrides, and context size (with MoE optimizations)](https://reddit.com/r/LocalLLaMA/comments/1pn2e1c/llamacpp_automation_for_gpu_layers_tensor_split/)

**Author:** u/Remove_Ayys | **Upvotes:** 189 | **Comments:** 59 | **Date:** 2025-12-15

**Summary:** The post discusses a new automation feature in llama.cpp for managing GPU layers, tensor splits, and context size, improving usability and performance for hybrid CPU-GPU inference. The implementation uses virtual test allocations to optimize memory use across GPUs, prioritizing dense tensors for better MoE performance.

**Key Points:**
- CPU + GPU hybrid inference is a core feature of llama.cpp, but manual memory allocation is suboptimal.
- New automation for memory allocation across GPUs has been implemented, using virtual test allocations.
- The implementation prioritizes dense tensors for better MoE performance.
- Positive feedback from the community, with suggestions for caching to reduce fitting time.
- Interest in multi-GPU setups and further optimizations.

**Discussion Highlights:** The community generally approves of the new automation feature, with suggestions for caching to reduce fitting time and interest in multi-GPU setups.

---

## 35. [Aaaand... is gone...](https://reddit.com/r/LocalLLaMA/comments/1pmungj/aaaand_is_gone/)

**Author:** u/HumanDrone8721 | **Upvotes:** 922 | **Comments:** 196 | **Date:** 2025-12-14

**Summary:** The Reddit post titled 'Aaaand... is gone...' in r/LocalLLaMA discusses the discontinuation or scarcity of a technology, likely related to storage drives, sparking a conversation about storage solutions and their implications.

**Key Points:**
- The post suggests the disappearance of a technology, possibly storage-related.
- Users discuss the impact on storage solutions, with one mentioning the purchase of a 2TB SSD.
- The discussion includes references to ownership and the relevance of SATA drives.
- Some users downplay the significance, calling it a 'nothingburger'.

**Discussion Highlights:** The discussion highlights a mix of concern and indifference regarding the discontinuation of a technology, with some users preparing for changes by purchasing new storage solutions, while others dismiss the significance of the event.

---

## 36. [Qwen3-Next-80B-A3B-Thinking-GGUF has just been released on HuggingFace](https://reddit.com/r/LocalLLaMA/comments/1pmo0dn/qwen3next80ba3bthinkinggguf_has_just_been/)

**Author:** u/LegacyRemaster | **Upvotes:** 125 | **Comments:** 54 | **Date:** 2025-12-14

**Summary:** The Reddit post announces the release of Qwen3-Next-80B-A3B-Thinking-GGUF on HuggingFace, highlighting its impressive performance in generating a Tetris game within a single HTML file. The community expresses strong positive reactions, with some noting its effectiveness in iterative agentic coding tasks.

**Key Points:**
- Qwen3-Next-80B-A3B-Thinking-GGUF model released on HuggingFace
- Model excels in generating a Tetris game in a single HTML file
- Community praises its performance in iterative agentic coding
- Some confusion about the release timeline
- Discussion about potential inclusion of classic games in training data

**Discussion Highlights:** The community is highly impressed with the model's capabilities, particularly in coding tasks. There is some debate about the release timeline and whether classic games like Tetris are part of the training dataset. Overall, the sentiment is overwhelmingly positive.

---

## 37. [To Mistral and other lab employees: please test with community tools BEFORE releasing models](https://reddit.com/r/LocalLLaMA/comments/1pmgm2x/to_mistral_and_other_lab_employees_please_test/)

**Author:** u/dtdisapointingresult | **Upvotes:** 135 | **Comments:** 72 | **Date:** 2025-12-14

**Summary:** The Reddit post criticizes Mistral for releasing Devstral 2 without thorough testing with community tools, which has negatively impacted their reputation. The author emphasizes the importance of testing with local tools to ensure smooth adoption by tech enthusiasts. Key points include issues with benchmark discrepancies and repetition loops, the importance of community tools, and mixed user experiences. The discussion highlights a divide in user experiences and a consensus on the need for better testing and documentation.

---

## 38. [Understanding the new router mode in llama cpp server](https://reddit.com/r/LocalLLaMA/comments/1pmc7lk/understanding_the_new_router_mode_in_llama_cpp/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 163 | **Comments:** 44 | **Date:** 2025-12-14

**Summary:** Router mode in llama cpp server allows managing multiple AI models simultaneously without restarting the server, enabling dynamic model switching and efficient memory usage.

**Key Points:**
- Router mode enables loading/unloading models on demand within a single server process.
- It saves memory and simplifies model switching by routing requests to the appropriate model.
- Useful for testing multiple GGUF models, building local APIs, and dynamic model switching.
- Comparisons with llama-swap and discussions on VRAM management were highlighted in the comments.

**Discussion Highlights:** The discussion included comparisons with llama-swap, questions about VRAM management for multiple GPUs, and requests for more detailed explanations on model memory management.

---

## 39. [8x RTX Pro 6000 server complete](https://reddit.com/r/LocalLLaMA/comments/1plwgun/8x_rtx_pro_6000_server_complete/)

**Author:** u/koushd | **Upvotes:** 624 | **Comments:** 268 | **Date:** 2025-12-13

**Summary:** The user detailed their journey upgrading a GPU server from a single 3080 to an 8x RTX Pro 6000 setup with a Threadripper PRO 9955WX and 384 GB RAM, facing challenges like overheating and power management. The post highlights the iterative upgrades, technical hurdles, and solutions attempted, including using multiple systems for pipeline parallelism.

**Key Points:**
- Upgraded from a single 3080 to 8x RTX Pro 6000 GPUs with a Threadripper PRO 9955WX and 384 GB RAM
- Faced overheating issues with dual 4090s, leading to a larger case and new host
- Encountered IOMMU addressing issues with 4x RTX Pro 6000, requiring a workaround with multiple systems
- Used 10Gb DAC SFP and Mellanox cards for RDMA to reduce latency, with minimal gains
- Power management was a significant challenge, requiring separate breakers for the 2400w total power draw

**Discussion Highlights:** The discussion included notable comments praising the setup as 'epyc' and comparing it to 'a Porsche in a trailer park.' There were concerns about the build quality and power management, with one user mentioning a Super Flower PSU blowing up. The consensus highlighted the impressive scale of the setup but also raised practical concerns about its implementation.

---

## 40. [Mistral 3 Large is DeepSeek V3!?](https://reddit.com/r/LocalLLaMA/comments/1plpc6h/mistral_3_large_is_deepseek_v3/)

**Author:** u/seraschka | **Upvotes:** 171 | **Comments:** 33 | **Date:** 2025-12-13

**Summary:** The post discusses the architectural similarities between Mistral 3 Large and DeepSeek V3, noting that they share nearly identical sizes and architectures, with minor differences in expert configurations. The community highlights the open-source spirit and the adoption of DeepSeek V3's architecture by multiple models.

**Key Points:**
- Mistral 3 Large and DeepSeek V3 have almost identical sizes (671B vs 673B).
- Mistral 3 Large uses the same architecture as DeepSeek V3 but with adjusted expert configurations.
- Mistral likely trained their model from scratch due to using their own tokenizer.
- The community notes that other models like Kimi K2 and Gigachat also use the DeepSeek V3 architecture.
- The adoption of DeepSeek V3's architecture is seen as a positive aspect of open-source collaboration.

**Discussion Highlights:** The discussion highlights the open-source spirit, with multiple models adopting the DeepSeek V3 architecture. Users appreciate the innovation and efficiency of the architecture, while also noting that Mistral's implementation includes multimodal capabilities.

---

## 41. [OpenAI's flagship model, ChatGPT-5.2 Thinking, ranks  most censored AI on Sansa benchmark.](https://reddit.com/r/LocalLLaMA/comments/1plnuqu/openais_flagship_model_chatgpt52_thinking_ranks/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 619 | **Comments:** 112 | **Date:** 2025-12-13

**Summary:** The Reddit post discusses OpenAI's ChatGPT-5.2 model being ranked as the most censored AI on the Sansa benchmark, with users expressing concerns about its performance and censorship levels compared to previous models and other AI models like Gemini and Mistral. Key points include performance issues with follow-up questions, increased denial of clinical note evaluations, and comparisons with other models. The discussion highlights user dissatisfaction and questions about the benchmark's testing criteria.

---

## 42. [Qwen3 Next generation optimization](https://reddit.com/r/LocalLLaMA/comments/1plng6f/qwen3_next_generation_optimization/)

**Author:** u/ilintar | **Upvotes:** 364 | **Comments:** 40 | **Date:** 2025-12-13

**Summary:** The post discusses optimizations made to Qwen3, specifically an autoregressive delta net computation that improves generation speed by 40%. The author invites others to test the optimizations and share their results.

**Key Points:**
- Optimized autoregressive delta net computation for Qwen3
- 40% generation speed improvement reported
- Optimizations remove unnecessary reshapes and computations
- Community encouraged to test and provide feedback
- Discussion includes questions about compatibility with ROCm/Vulkan

**Discussion Highlights:** The community shows strong appreciation for the optimization work, with comments highlighting the author's frequent contributions and expressing interest in further improvements. There is a question about whether the speedup applies to ROCm/Vulkan in addition to CUDA.

---

## 43. [NVIDIA gpt-oss-120b Eagle Throughput model](https://reddit.com/r/LocalLLaMA/comments/1plewrk/nvidia_gptoss120b_eagle_throughput_model/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 242 | **Comments:** 53 | **Date:** 2025-12-13

**Summary:** The post discusses NVIDIA's GPT-OSS-120B-Eagle3-throughput model, an optimized speculative decoding module designed to improve throughput during text generation using Eagle3 speculative decoding. It is licensed for both commercial and non-commercial use.

**Key Points:**
- GPT-OSS-120B-Eagle3-throughput is an optimized speculative decoding module built on the OpenAI gpt-oss-120b base model.
- It uses NVIDIA’s Eagle3 speculative decoding approach to predict a single draft token efficiently.
- The model is licensed under the nvidia-open-model-license for commercial and non-commercial use.
- It is intended for applications like AI agents, chatbots, and retrieval-augmented generation (RAG) systems.
- The model is not supported in llama.cpp, as indicated by a stale feature request.

**Discussion Highlights:** The discussion includes a request for a derestricted version of the model and mentions that it is not supported in llama.cpp. There is also a humorous comment about waiting for a REAP EAGLE3 HERETIC MOE GGUF version.

---

## 44. [This is how open ai is advertising them selfs on reddit…. They are doomed](https://reddit.com/r/LocalLLaMA/comments/1plcrg8/this_is_how_open_ai_is_advertising_them_selfs_on/)

**Author:** u/ThinkExtension2328 | **Upvotes:** 237 | **Comments:** 77 | **Date:** 2025-12-12

**Summary:** The Reddit post criticizes OpenAI's advertising strategy, highlighting a shift from promoting advanced AI to using astrology ads, which users find unappealing and indicative of a decline in the company's direction.

**Key Points:**
- Criticism of OpenAI's shift in advertising strategy
- Disappointment in the use of astrology ads
- Skepticism about the profitability and appeal of such ads
- Comparison to previous claims about AI advancements
- Discussion on the potential effectiveness of alternative advertising methods

**Discussion Highlights:** Users express disappointment and skepticism about OpenAI's new advertising approach, questioning its effectiveness and appeal to both programmers and general audiences. There is a consensus that the shift from advanced AI claims to astrology ads is a significant fall from grace.

---

## 45. [Running an LLM on a 3DS](https://reddit.com/r/LocalLLaMA/comments/1pl4njj/running_an_llm_on_a_3ds/)

**Author:** u/vreab | **Upvotes:** 294 | **Comments:** 35 | **Date:** 2025-12-12

**Summary:** The post discusses the feasibility and performance of running an LLM on a Nintendo 3DS, drawing comparisons to similar projects on platforms like the PS Vita and Wii. The community expresses admiration for the technical achievement and curiosity about potential performance improvements on newer hardware.

**Key Points:**
- Running an LLM on a 3DS is technically impressive and feasible
- Similar projects have been done on PS Vita and Wii
- Community is curious about performance improvements on newer 3DS models
- Discussion includes humor and admiration for the project
- Questions about potential AI applications in gaming

**Discussion Highlights:** The discussion highlights the technical achievement of running an LLM on a 3DS, with users expressing admiration and curiosity. There is a consensus that this is an impressive feat, and some users speculate about potential performance improvements on newer hardware. The conversation also includes humorous comparisons and reflections on the capabilities of modern AI.

---

## 46. [The new monster-server](https://reddit.com/r/LocalLLaMA/comments/1pl0ojb/the_new_monsterserver/)

**Author:** u/eribob | **Upvotes:** 591 | **Comments:** 125 | **Date:** 2025-12-12

**Summary:** The user shares their upgraded 'Monster Server' setup, featuring a Ryzen 3950x CPU, 128GB RAM, and three GPUs (2x RTX 3090 and 1x RTX 4090). The server runs local LLMs like GPT-OSS-120B and is used for research and coding. The post highlights the hardware configuration, performance, and user satisfaction.

**Key Points:**
- The server uses a Ryzen 3950x CPU and 128GB RAM, with three GPUs (2x RTX 3090 and 1x RTX 4090).
- The RTX 4090 is connected via an M.2 to PCIe adapter and a second PSU.
- The user runs GPT-OSS-120B fully in VRAM for research and coding.
- The setup includes 10GB fiber internet and a mix of NVMe and HDD storage.
- Discussion highlights include nostalgia for early 2000s overclocking and technical feedback on GPU setup efficiency.

**Discussion Highlights:** Users in the discussion expressed nostalgia for early 2000s overclocking forums and provided technical feedback, such as the inefficiency of a 3-GPU setup compared to 2 or 4 GPUs due to Tensor Parallel vs. Pipeline Parallel. Some users also asked about the heat management and the method used to connect the second PSU.

---

## 47. [Olmo 3.1 32B Think &amp; Instruct: New Additions to the Olmo Model Family](https://reddit.com/r/LocalLLaMA/comments/1pky5u4/olmo_31_32b_think_instruct_new_additions_to_the/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 183 | **Comments:** 28 | **Date:** 2025-12-12

**Summary:** The post introduces Olmo 3.1 32B Think and Instruct models, highlighting their specialized capabilities in deep reasoning and instruction following, respectively. The community response is positive, with appreciation for the open-source nature and quality of the models.

**Key Points:**
- Olmo 3.1 32B Think model excels in multi-step reasoning, math, logic, and code generation.
- Olmo 3.1 32B Instruct model is optimized for instruction following, conversational fluency, and tool-use.
- Both models are fully open-source and part of the Olmo family.
- Community feedback is largely positive, with anticipation for future developments like MOE.
- The models are available on HuggingFace.

**Discussion Highlights:** The discussion reflects enthusiasm for the new models, with users praising their open-source nature and the educational value of the accompanying paper. There is also anticipation for future advancements, such as Mixture of Experts (MOE) models.

---

## 48. [Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/)

**Author:** u/Nunki08 | **Upvotes:** 1324 | **Comments:** 155 | **Date:** 2025-12-12

**Summary:** An NVIDIA employee accidentally uploaded the parent folder of their upcoming model on Hugging Face, sparking a discussion about the potential leak of sensitive information and the urgency to save the data before it gets taken down.

**Key Points:**
- NVIDIA's upcoming model files were accidentally uploaded on Hugging Face.
- The post gained significant attention with 1324 upvotes and 155 comments.
- Users expressed concern about the data being taken down and urged others to save it.
- The Nemotron lineup was mentioned as promising.
- There was a sense of urgency to grab the data before full censoring.

**Discussion Highlights:** The discussion highlighted the importance of preserving the leaked data, with users expressing interest in the Nemotron lineup and the potential implications of the accidental upload.

---

## 49. [Training an LLM only on 1800s London texts - 90GB dataset](https://reddit.com/r/LocalLLaMA/comments/1pkpsee/training_an_llm_only_on_1800s_london_texts_90gb/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 707 | **Comments:** 78 | **Date:** 2025-12-12

**Summary:** The post discusses the TimeCapsuleLLM project, which involves training an LLM on a 90GB dataset of 1800-1875 London texts. The author has conducted a bias report and trained a small evaluation model to assess the dataset before scaling up.

**Key Points:**
- The dataset consists of 90GB with 135,000 documents from 1800-1875 London texts.
- A bias report covering temporal, gender/pronoun, and geographic bias has been generated.
- A small evaluation model (300M parameters) was trained on a 15GB subset to evaluate the dataset.
- The community appreciates the detailed work and suggests considering MoE for better compute efficiency.
- The project aims to study historical texts despite inherent biases.

**Discussion Highlights:** The community shows strong support for the project, with suggestions for improvement such as using Mixture of Experts (MoE) for better compute efficiency. There is also interest in the methodology and progress of the project.

---

## 50. [Agentic Local AI on CPU = Mistral Vibe + Granite-4-h-1b](https://reddit.com/r/LocalLLaMA/comments/1pkdkjo/agentic_local_ai_on_cpu_mistral_vibe_granite4h1b/)

**Author:** u/PotentialFunny7143 | **Upvotes:** 232 | **Comments:** 40 | **Date:** 2025-12-11

**Summary:** The post discusses running local AI models on CPU, highlighting Mistral Vibe and Granite-4-h-1b as viable options. Users are interested in performance comparisons, hardware requirements, and capability boundaries.

**Key Points:**
- Mistral Vibe and Granite-4-h-1b are mentioned as effective local AI models for CPU.
- Users are curious about performance comparisons with other models like Cline.
- Hardware requirements and token processing speeds are of interest.
- Discussion includes questions about RAM and CPU consumption.
- Users are inquiring about the upper capability limits of these models.

**Discussion Highlights:** The discussion highlights a strong interest in performance metrics, hardware efficiency, and comparative capabilities of local AI models running on CPU. Users are actively seeking practical insights and benchmarks.

---

