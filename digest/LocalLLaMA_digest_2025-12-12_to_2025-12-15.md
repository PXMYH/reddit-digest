# r/LocalLLaMA Reading Digest

**Period:** 2025-12-12 to 2025-12-15
**Posts Summarized:** 15
**Total Posts Analyzed:** 15

---

## 1. [Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/)

**Author:** u/Nunki08 | **Upvotes:** 1303 | **Comments:** 151 | **Date:** 2025-12-12

**Summary:** NVIDIA accidentally uploaded the parent folder of their upcoming model on Hugging Face, sparking discussions about the potential leak of sensitive information and the urgency to save the data before it gets taken down.

**Key Points:**
- NVIDIA made a mistake by uploading the parent folder of their upcoming model on Hugging Face.
- The post gained significant attention with 1303 upvotes and 151 comments.
- Users expressed concern about the potential removal of the uploaded content.
- The Nemotron lineup was mentioned as promising.
- There was a sense of urgency to grab the data before full censoring.

**Discussion Highlights:** The discussion highlighted concerns about data removal and the potential value of the leaked information. Users emphasized the importance of saving the data quickly and expressed interest in the Nemotron lineup.

---

## 2. [Aaaand... is gone...](https://reddit.com/r/LocalLLaMA/comments/1pmungj/aaaand_is_gone/)

**Author:** u/HumanDrone8721 | **Upvotes:** 856 | **Comments:** 174 | **Date:** 2025-12-14

**Summary:** The Reddit post titled 'Aaaand... is gone...' by u/HumanDrone8721 has gained significant attention with 856 upvotes and 174 comments. The post appears to be a link with no text content, sparking various reactions and discussions among users.

**Key Points:**
- The post has gained popularity with 856 upvotes and 174 comments.
- The author received a special flair for their contribution.
- Users are discussing the implications of the post, with some seeing it as a significant event while others downplay its importance.
- There is a mix of humorous and serious responses in the comments.
- Some users are preparing for potential data storage needs.

**Discussion Highlights:** The discussion highlights a mix of reactions, with some users preparing for increased data storage needs (e.g., buying a 2TB SSD) and others making light of the situation with humorous comments and GIFs. There is also a debate about the significance of the post, with some users seeing it as a major event and others dismissing it as a 'nothingburger.'

---

## 3. [Training an LLM only on 1800s London texts - 90GB dataset](https://reddit.com/r/LocalLLaMA/comments/1pkpsee/training_an_llm_only_on_1800s_london_texts_90gb/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 694 | **Comments:** 77 | **Date:** 2025-12-12

**Summary:** The post discusses the TimeCapsuleLLM project, which involves training an LLM on a 90GB dataset of 1800-1875 London texts. The author has conducted a bias report and trained a small evaluation model to assess the dataset before scaling up.

**Key Points:**
- The dataset consists of 90GB with 135,000 documents from 1800-1875 London texts.
- A bias report covering temporal, gender/pronoun, and geographic bias has been generated.
- A small evaluation model (300M parameters) was trained on a 15GB subset to evaluate the dataset.
- The community appreciates the detailed work and suggests considering MoE for better compute efficiency.
- The project aims to study historical texts despite inherent biases.

**Discussion Highlights:** The community shows strong support for the project, with suggestions for using Mixture of Experts (MoE) for better compute efficiency. There is also interest in the methodology and the potential for further exploration of historical texts.

---

## 4. [8x RTX Pro 6000 server complete](https://reddit.com/r/LocalLLaMA/comments/1plwgun/8x_rtx_pro_6000_server_complete/)

**Author:** u/koushd | **Upvotes:** 615 | **Comments:** 269 | **Date:** 2025-12-13

**Summary:** The Reddit post details a user's multi-year journey upgrading their GPU server for local LLM training, culminating in a system with 8x RTX Pro 6000 GPUs (768 GB VRAM), a Threadripper PRO 9955WX CPU, and 384 GB RAM. The user faced challenges with heat management, power consumption, and hardware compatibility, ultimately solving these issues through iterative upgrades and workarounds.

**Key Points:**
- The final setup includes 8x RTX Pro 6000 GPUs (4 Workstation, 4 Max-Q), providing 768 GB VRAM
- The system uses a Threadripper PRO 9955WX CPU and 384 GB RAM
- The user faced significant challenges with heat management, requiring a larger case and better cooling
- Hardware compatibility issues led to using two separate systems with pipeline parallelism
- The community reaction includes both admiration and criticism of the setup's practicality

**Discussion Highlights:** The discussion highlights a mix of admiration for the technical achievement and criticism of the practical implementation. Some commenters question the wisdom of using consumer hardware for such a high-end setup, while others express concern about power consumption and cooling solutions. There's also discussion about alternative approaches like using server-grade hardware in a rack setup.

---

## 5. [OpenAI's flagship model, ChatGPT-5.2 Thinking, ranks  most censored AI on Sansa benchmark.](https://reddit.com/r/LocalLLaMA/comments/1plnuqu/openais_flagship_model_chatgpt52_thinking_ranks/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 611 | **Comments:** 111 | **Date:** 2025-12-13

**Summary:** The Reddit post discusses OpenAI's ChatGPT-5.2 model, highlighting its high censorship level on the Sansa benchmark and criticisms regarding its performance in follow-up questions and research tasks.

**Key Points:**
- ChatGPT-5.2 is ranked as the most censored AI on the Sansa benchmark.
- Users report issues with follow-up questions and research capabilities compared to previous versions.
- The model frequently denies requests for evaluating QA models with made-up clinical notes.
- Discussion on what criteria are used for censorship benchmarks, with Grok scoring low.
- Observation that Gemini appears less censored than other open models, including Mistral.

**Discussion Highlights:** The discussion highlights user frustrations with ChatGPT-5.2's performance degradation and increased censorship. There is curiosity about the benchmark criteria and comparisons with other models like Gemini and Grok.

---

## 6. [The new monster-server](https://reddit.com/r/LocalLLaMA/comments/1pl0ojb/the_new_monsterserver/)

**Author:** u/eribob | **Upvotes:** 576 | **Comments:** 118 | **Date:** 2025-12-12

**Summary:** The user shares their upgraded 'Monster server' setup, featuring a Ryzen 3950x CPU, 128GB RAM, and three GPUs (2x RTX 3090 and 1x RTX 4090). The server runs local LLM models like GPT-OSS-120B and is used for research and coding. The post highlights the hardware configuration and performance details.

**Key Points:**
- Hardware setup includes Ryzen 3950x, 128GB RAM, and three GPUs (2x RTX 3090, 1x RTX 4090)
- Server runs local LLM models like GPT-OSS-120B for research and coding
- User has 10GB fiber internet for $50/month
- Discussion includes feedback on GPU setup efficiency and heat management
- Positive reactions and nostalgia for early 2000s overclocking forums

**Discussion Highlights:** The discussion includes positive reactions, nostalgia for early 2000s overclocking forums, questions about the user's location for affordable 10GB internet, feedback on GPU setup efficiency, and inquiries about heat management and the second PSU setup.

---

## 7. [Qwen3 Next generation optimization](https://reddit.com/r/LocalLLaMA/comments/1plng6f/qwen3_next_generation_optimization/)

**Author:** u/ilintar | **Upvotes:** 359 | **Comments:** 40 | **Date:** 2025-12-13

**Summary:** The post discusses optimizations made to Qwen3, specifically an autoregressive delta net computation that improves generation speed by 40%. The author invites others to test the optimizations and share their results.

**Key Points:**
- Optimized autoregressive delta net computation for Qwen3
- 40% generation speed improvement reported
- Optimizations include removing unnecessary reshapes and computations
- Author invites community testing and feedback
- Positive community response with appreciation for the contribution

**Discussion Highlights:** The community responded positively, with comments appreciating the contribution and asking about compatibility with ROCm/Vulkan. There was also humor about the author's frequent contributions and excitement about future optimizations.

---

## 8. [Running an LLM on a 3DS](https://reddit.com/r/LocalLLaMA/comments/1pl4njj/running_an_llm_on_a_3ds/)

**Author:** u/vreab | **Upvotes:** 294 | **Comments:** 34 | **Date:** 2025-12-12

**Summary:** The Reddit post discusses the feasibility and community reaction to running an LLM on a Nintendo 3DS, highlighting technical curiosity and comparisons to other platforms like the PS Vita and Wii.

**Key Points:**
- Running an LLM on a 3DS is technically feasible and impressive to the community.
- Comparisons to other platforms like the PS Vita and Wii are made, showing a trend of running LLMs on unconventional hardware.
- Community members express enthusiasm and curiosity about the performance and potential of such projects.
- Discussion includes humor and speculation about AI capabilities on older hardware.

**Discussion Highlights:** The community is highly impressed and engaged, with discussions ranging from technical feasibility to humorous comparisons. There is a consensus on the novelty and potential of running LLMs on older or unconventional hardware.

---

## 9. [NVIDIA gpt-oss-120b Eagle Throughput model](https://reddit.com/r/LocalLLaMA/comments/1plewrk/nvidia_gptoss120b_eagle_throughput_model/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 239 | **Comments:** 53 | **Date:** 2025-12-13

**Summary:** The post discusses NVIDIA's GPT-OSS-120B-Eagle3-throughput, an optimized speculative decoding module designed to improve text generation throughput using Eagle3 speculative decoding. It is licensed for both commercial and non-commercial use.

**Key Points:**
- Optimized speculative decoding module for improved throughput
- Uses NVIDIA’s Eagle3 speculative decoding approach
- Licensed under nvidia-open-model-license for commercial and non-commercial use
- Intended for AI agents, chatbots, and RAG systems
- Not supported in llama.cpp, limiting its usability in some contexts

**Discussion Highlights:** The discussion highlights interest in making the model derestricted, questions about its compatibility with CPU inference, and limitations due to lack of support in llama.cpp. There is also humor about waiting for a GGUF version.

---

## 10. [This is how open ai is advertising them selfs on reddit…. They are doomed](https://reddit.com/r/LocalLLaMA/comments/1plcrg8/this_is_how_open_ai_is_advertising_them_selfs_on/)

**Author:** u/ThinkExtension2328 | **Upvotes:** 235 | **Comments:** 77 | **Date:** 2025-12-12

**Summary:** The Reddit post criticizes OpenAI's advertising strategy, highlighting a shift from promoting advanced AI to using astrology ads, which users find inconsistent and unappealing.

**Key Points:**
- OpenAI's advertising shift from AI advancements to astrology ads
- Criticism of the inconsistency in OpenAI's messaging
- Discussion on the profitability of targeting non-technical audiences
- Suggestions for alternative advertising strategies
- Observations on the perceived decline in OpenAI's reputation

**Discussion Highlights:** Users express disappointment and humor at OpenAI's advertising shift, with a consensus that targeting non-technical audiences might be more profitable but damages their credibility among technical users.

---

## 11. [Olmo 3.1 32B Think &amp; Instruct: New Additions to the Olmo Model Family](https://reddit.com/r/LocalLLaMA/comments/1pky5u4/olmo_31_32b_think_instruct_new_additions_to_the/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 176 | **Comments:** 28 | **Date:** 2025-12-12

**Summary:** Olmo 3.1 32B Think and Instruct are new 32-billion-parameter models in the Olmo family, optimized for deep reasoning and instruction following, respectively. The Think model excels in multi-step reasoning and code generation, while the Instruct model focuses on conversational fluency and tool-use capabilities.

**Key Points:**
- Olmo 3.1 32B Think is optimized for deep reasoning, math, logic, and code generation.
- Olmo 3.1 32B Instruct is optimized for instruction following, conversational fluency, and tool-use capabilities.
- Both models are fully open source and part of the Olmo family.
- The community appreciates the openness and continuous improvement of Olmo models.
- There is anticipation for additional models, such as MOE (Mixture of Experts).

**Discussion Highlights:** The community is positive about the new models, appreciating their open-source nature and the educational value of the accompanying paper. There is also anticipation for future releases, including MOE models.

---

## 12. [Mistral 3 Large is DeepSeek V3!?](https://reddit.com/r/LocalLLaMA/comments/1plpc6h/mistral_3_large_is_deepseek_v3/)

**Author:** u/seraschka | **Upvotes:** 166 | **Comments:** 33 | **Date:** 2025-12-13

**Summary:** The post discusses the architectural similarities between Mistral 3 Large and DeepSeek V3, noting that they share nearly identical sizes and configurations, with Mistral 3 making adjustments to expert sizes for latency improvements. The community highlights the open-source spirit and the adoption of DeepSeek's architecture by multiple models.

**Key Points:**
- Mistral 3 Large and DeepSeek V3 have almost identical sizes (671B vs 673B).
- Mistral 3 uses the same architecture as DeepSeek V3 but adjusts expert sizes for better latency.
- Mistral likely trained their model from scratch due to using a different tokenizer.
- Other models like Kimi K2 and Gigachat also use the DeepSeek V3 architecture.
- The community views this as a positive example of open-source collaboration.

**Discussion Highlights:** The discussion highlights the open-source spirit, with multiple models adopting the DeepSeek V3 architecture. Users appreciate the innovation and efficiency of the architecture, while also noting that Mistral's adjustments for latency and multimodal capabilities add value.

---

## 13. [Understanding the new router mode in llama cpp server](https://reddit.com/r/LocalLLaMA/comments/1pmc7lk/understanding_the_new_router_mode_in_llama_cpp/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 161 | **Comments:** 44 | **Date:** 2025-12-14

**Summary:** Router mode in llama cpp server allows managing multiple AI models simultaneously without restarting the server, enabling dynamic model switching and efficient memory usage.

**Key Points:**
- Router mode enables loading/unloading models on demand within a single server process
- Eliminates need for separate server instances per model, saving memory and simplifying workflow
- Useful for testing multiple GGUF models, building APIs, and dynamic model switching
- Comparisons drawn to Ollama-like functionality and existing tools like llama-swap
- Users express interest in VRAM management and concurrent model specifications

**Discussion Highlights:** Users show strong interest in router mode's capabilities, with comparisons to existing tools like llama-swap. Key discussion points include VRAM management for multi-GPU setups and the ability to specify which models remain in memory concurrently. Some users find the explanatory image unhelpful while others appreciate the new functionality.

---

## 14. [To Mistral and other lab employees: please test with community tools BEFORE releasing models](https://reddit.com/r/LocalLLaMA/comments/1pmgm2x/to_mistral_and_other_lab_employees_please_test/)

**Author:** u/dtdisapointingresult | **Upvotes:** 137 | **Comments:** 71 | **Date:** 2025-12-14

**Summary:** The Reddit post criticizes Mistral for releasing Devstral 2 without thorough testing with community tools, which led to issues like benchmark discrepancies and repetition loops. The author emphasizes the importance of testing with local tools to maintain reputation and user trust. Key points include the lack of testing with community tools, issues with benchmark discrepancies and repetition loops, and the importance of tech geeks' recommendations. The discussion highlights mixed experiences with the model and a consensus on the need for thorough testing.

---

## 15. [Qwen3-Next-80B-A3B-Thinking-GGUF has just been released on HuggingFace](https://reddit.com/r/LocalLLaMA/comments/1pmo0dn/qwen3next80ba3bthinkinggguf_has_just_been/)

**Author:** u/LegacyRemaster | **Upvotes:** 120 | **Comments:** 52 | **Date:** 2025-12-14

**Summary:** The Reddit post announces the release of Qwen3-Next-80B-A3B-Thinking-GGUF on HuggingFace, highlighting its impressive performance in a Tetris game implemented in a single HTML file. Users compare it favorably to other models like Devstral and discuss its capabilities and release timing.

**Key Points:**
- Qwen3-Next-80B-A3B-Thinking-GGUF model released on HuggingFace
- Model excels in Tetris game implementation in a single HTML file
- Users report better performance compared to Devstral
- Discussion includes queries about release timing and technical capabilities
- Mixed reactions on whether classic games like Tetris are in training datasets

**Discussion Highlights:** The discussion highlights enthusiasm for the model's capabilities, with users praising its performance in iterative agentic coding tasks. Some users question the release timing, while others discuss technical aspects like native tool calling support in llamacpp. Overall, the consensus is positive, with users impressed by the model's performance.

---

