# r/LocalLLaMA Reading Digest

**Period:** 2025-12-12 to 2025-12-15
**Posts Summarized:** 16
**Total Posts Analyzed:** 16

---

## 1. [Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/)

**Author:** u/Nunki08 | **Upvotes:** 1292 | **Comments:** 149 | **Date:** 2025-12-12

**Summary:** An NVIDIA employee accidentally uploaded the parent folder of their upcoming model on Hugging Face, sparking a discussion about the potential leak of sensitive information and the urgency to save the data before it gets taken down.

**Key Points:**
- NVIDIA's upcoming model files were accidentally uploaded on Hugging Face.
- The community is concerned about the data being taken down and urges others to save it.
- The Nemotron lineup is mentioned as promising.
- There is a sense of urgency to grab the data before full censoring is implemented.

**Discussion Highlights:** The discussion highlights a mix of excitement about the potential leak and urgency to preserve the data. Many users express concern about the data being censored or removed, and there is a consensus on the importance of saving the information quickly.

---

## 2. [Aaaand... is gone...](https://reddit.com/r/LocalLLaMA/comments/1pmungj/aaaand_is_gone/)

**Author:** u/HumanDrone8721 | **Upvotes:** 771 | **Comments:** 154 | **Date:** 2025-12-14

**Summary:** The Reddit post discusses the discontinuation or scarcity of a technology, likely SATA drives, sparking a conversation about storage solutions and future implications.

**Key Points:**
- The post is about the discontinuation or scarcity of a technology, possibly SATA drives.
- Users are discussing storage solutions, such as buying additional SSDs.
- There is a mix of reactions, from humor to practical advice.
- Some users downplay the significance, calling it a 'nothingburger'.

**Discussion Highlights:** The discussion highlights a range of reactions, from humorous takes to practical advice on storage solutions. Some users see this as a significant event, while others downplay its importance, indicating a divide in the community's perception of the issue.

---

## 3. [Training an LLM only on 1800s London texts - 90GB dataset](https://reddit.com/r/LocalLLaMA/comments/1pkpsee/training_an_llm_only_on_1800s_london_texts_90gb/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 668 | **Comments:** 71 | **Date:** 2025-12-12

**Summary:** The post discusses the TimeCapsuleLLM project, which involves training an LLM on a 90GB dataset of 1800-1875 London texts. The author has conducted a bias report and trained a small evaluation model to assess the dataset before scaling up.

**Key Points:**
- The dataset consists of 90GB with 135,000 documents from 1800-1875 London texts.
- A bias report covering temporal, gender/pronoun, and geographic bias has been generated.
- A small evaluation model (300M parameters) was trained on a 15GB subset to evaluate the dataset.
- The community appreciates the detailed work and suggests considering Mixture of Experts (MoE) for better compute efficiency.
- The project aims to study historical texts despite inherent biases.

**Discussion Highlights:** The community shows strong support for the project, with suggestions for improving compute efficiency using MoE. There is also interest in the methodology and progress of the project.

---

## 4. [8x RTX Pro 6000 server complete](https://reddit.com/r/LocalLLaMA/comments/1plwgun/8x_rtx_pro_6000_server_complete/)

**Author:** u/koushd | **Upvotes:** 615 | **Comments:** 267 | **Date:** 2025-12-13

**Summary:** The author details their journey upgrading a GPU server from a single 3080 to an 8x RTX Pro 6000 setup with a Threadripper PRO 9955WX and 384 GB RAM, facing challenges like overheating and hardware limitations. The post highlights the iterative process of upgrading hardware and overcoming technical obstacles.

**Key Points:**
- Upgraded from a single 3080 to 8x RTX Pro 6000 GPUs with a Threadripper PRO 9955WX and 384 GB RAM
- Faced overheating issues and hardware limitations during upgrades
- Used a workaround with two systems in pipeline parallel to manage four GPUs
- Community reactions include admiration, criticism of the setup's physical implementation, and suggestions for server-grade hardware
- Discussion highlights the balance between performance and practicality in high-end computing setups

**Discussion Highlights:** The community expressed a mix of admiration for the setup's power and criticism of its physical implementation, with some suggesting the use of server-grade hardware for better reliability and efficiency. Notable comments include concerns about the setup's cooling and power management, as well as suggestions for alternative configurations.

---

## 5. [OpenAI's flagship model, ChatGPT-5.2 Thinking, ranks  most censored AI on Sansa benchmark.](https://reddit.com/r/LocalLLaMA/comments/1plnuqu/openais_flagship_model_chatgpt52_thinking_ranks/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 609 | **Comments:** 108 | **Date:** 2025-12-13

**Summary:** The Reddit post discusses OpenAI's ChatGPT-5.2 model being ranked as the most censored AI on the Sansa benchmark, with users reporting issues in follow-up questions, research capabilities, and clinical note generation compared to previous versions.

**Key Points:**
- ChatGPT-5.2 is ranked as the most censored AI on the Sansa benchmark.
- Users report issues with follow-up questions and research capabilities.
- Difficulties in generating clinical notes for QA model evaluation.
- Questions about the benchmark's testing criteria.
- Observations about Gemini being less censored than other models.

**Discussion Highlights:** Users express concerns about the model's performance degradation in specific tasks and question the benchmark's criteria, with some noting unexpected censorship levels in other models like Gemini.

---

## 6. [The new monster-server](https://reddit.com/r/LocalLLaMA/comments/1pl0ojb/the_new_monsterserver/)

**Author:** u/eribob | **Upvotes:** 576 | **Comments:** 118 | **Date:** 2025-12-12

**Summary:** The Reddit post details a powerful home server setup called the 'Monster Server,' built around a Ryzen 3950x CPU and three high-end GPUs (2x RTX 3090 and 1x RTX 4090). The user runs large language models locally and shares their satisfaction with the build's performance and cost-effectiveness.

**Key Points:**
- The server uses a Ryzen 3950x CPU and three GPUs (2x RTX 3090, 1x RTX 4090) for running local LLMs.
- The setup includes 128GB DDR4 RAM, 10GBe networking, and a mix of NVMe and HDD storage.
- The user runs GPT-OSS-120B fully in VRAM, achieving over 100 tokens per second.
- Discussion highlights include nostalgia for early 2000s overclocking forums and technical debates about GPU parallelism.
- The post received significant engagement, with 576 upvotes and 118 comments.

**Discussion Highlights:** The community reacted positively, with comments praising the build and discussing technical aspects like GPU parallelism and heat management. Some users expressed envy, while others shared insights on optimizing multi-GPU setups.

---

## 7. [Qwen3 Next generation optimization](https://reddit.com/r/LocalLLaMA/comments/1plng6f/qwen3_next_generation_optimization/)

**Author:** u/ilintar | **Upvotes:** 351 | **Comments:** 39 | **Date:** 2025-12-13

**Summary:** The post discusses optimizations made to Qwen3, specifically an optimized autoregressive delta net computation that results in a 40% generation speed upgrade. The author invites the community to test the improvements and share their results.

**Key Points:**
- Optimized autoregressive delta net computation for Qwen3
- 40% generation speed upgrade reported by the author
- Community encouraged to test and provide feedback
- Positive reactions from the community, including recognition and appreciation
- Questions about compatibility with ROCm/Vulkan in addition to CUDA

**Discussion Highlights:** The community responded positively to the optimization, with comments expressing appreciation and humor. There was a question about whether the speedup would work on ROCm/Vulkan as well as CUDA, indicating interest in broader compatibility.

---

## 8. [Running an LLM on a 3DS](https://reddit.com/r/LocalLLaMA/comments/1pl4njj/running_an_llm_on_a_3ds/)

**Author:** u/vreab | **Upvotes:** 290 | **Comments:** 34 | **Date:** 2025-12-12

**Summary:** The Reddit post discusses the feasibility and performance of running an LLM on a 3DS, drawing comparisons to similar projects on other devices like the PS Vita and Wii. The community expresses admiration for the project's technical achievement.

**Key Points:**
- Running an LLM on a 3DS is technically impressive and feasible.
- Similar projects have been attempted on other devices like the PS Vita and Wii.
- The community is highly impressed by the project's technical achievement.
- Discussions include potential performance improvements on newer hardware like the 'new' 3DS.

**Discussion Highlights:** The discussion highlights the technical feasibility and impressiveness of running an LLM on a 3DS, with comparisons to similar projects on other devices. There is also curiosity about potential performance improvements on newer hardware.

---

## 9. [First AI implosion: Oracle](https://reddit.com/r/LocalLLaMA/comments/1pmfglp/first_ai_implosion_oracle/)

**Author:** u/Terminator857 | **Upvotes:** 251 | **Comments:** 200 | **Date:** 2025-12-14

**Summary:** The Reddit post discusses Oracle's potential financial troubles, suggesting it might be the first major company to face an 'implosion' due to its debt-reliant expansion. Users speculate about the impact on the tech industry, particularly regarding memory prices and the duration of the current RAM shortage.

**Key Points:**
- Oracle is seen as a risky company due to its debt-reliant expansion strategy.
- The post suggests that Oracle's potential downfall could lead to cheaper memory prices.
- Users express frustration with LLM-generated social media posts.
- Oracle's financial troubles are contrasted with companies like Google, Microsoft, and Meta, which have more stable cash flows.
- The discussion highlights Oracle's significant presence in the tech industry, with users sharing their experiences with the company.

**Discussion Highlights:** The discussion highlights a mix of opinions, with some users expressing frustration with LLM-generated content and others sharing their experiences with Oracle. There is a consensus that Oracle's financial troubles could have significant implications for the tech industry, particularly in terms of memory prices and market dynamics.

---

## 10. [NVIDIA gpt-oss-120b Eagle Throughput model](https://reddit.com/r/LocalLLaMA/comments/1plewrk/nvidia_gptoss120b_eagle_throughput_model/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 241 | **Comments:** 53 | **Date:** 2025-12-13

**Summary:** The post discusses NVIDIA's GPT-OSS-120B-Eagle3-throughput model, an optimized speculative decoding module designed to improve throughput during text generation using Eagle3 speculative decoding. It is licensed for both commercial and non-commercial use.

**Key Points:**
- GPT-OSS-120B-Eagle3-throughput is an optimized speculative decoding module built on OpenAI's gpt-oss-120b base model.
- It uses NVIDIA’s Eagle3 speculative decoding approach to predict a single draft token efficiently.
- The model is licensed under the nvidia-open-model-license for commercial and non-commercial use.
- It is intended for applications like AI agents, chatbots, and retrieval-augmented generation (RAG) systems.
- The model is not supported in llama.cpp, as indicated by a stale feature request.

**Discussion Highlights:** The discussion highlights include a request for a derestricted version of the model, mentions of potential speed improvements with CPU inference, and the lack of support in llama.cpp. There is also a humorous comment about waiting for a REAP EAGLE3 HERETIC MOE GGUF version.

---

## 11. [This is how open ai is advertising them selfs on reddit…. They are doomed](https://reddit.com/r/LocalLLaMA/comments/1plcrg8/this_is_how_open_ai_is_advertising_them_selfs_on/)

**Author:** u/ThinkExtension2328 | **Upvotes:** 235 | **Comments:** 77 | **Date:** 2025-12-12

**Summary:** The Reddit post criticizes OpenAI's advertising strategy, highlighting a shift from promoting advanced AI to using astrology ads, which is seen as a decline in their approach.

**Key Points:**
- OpenAI's advertising strategy is criticized for shifting to astrology ads.
- The post suggests this shift indicates a decline in OpenAI's approach.
- Comments highlight the profitability of targeting horoscope believers over programmers.
- There is a consensus that this shift is a significant fall from grace for OpenAI.
- Some comments suggest OpenAI could leverage user data for a 'year in review' feature.

**Discussion Highlights:** The discussion highlights a consensus that OpenAI's shift to astrology ads is a significant decline from their previous stance on advanced AI. Comments suggest this strategy might be more profitable but is seen as a fall from grace.

---

## 12. [Olmo 3.1 32B Think &amp; Instruct: New Additions to the Olmo Model Family](https://reddit.com/r/LocalLLaMA/comments/1pky5u4/olmo_31_32b_think_instruct_new_additions_to_the/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 175 | **Comments:** 24 | **Date:** 2025-12-12

**Summary:** The post introduces Olmo 3.1 32B Think and Instruct models, highlighting their specialized capabilities in deep reasoning and instruction following, respectively. The community response is positive, with appreciation for the open-source nature and improvements in the models.

**Key Points:**
- Olmo 3.1 32B Think model excels in multi-step reasoning, math, logic, and code generation.
- Olmo 3.1 32B Instruct model is optimized for instruction following, conversational fluency, and tool-use.
- Models are fully open-source and part of the Olmo family.
- Community appreciates the continuous improvements and open-source nature.
- Expectations for future developments like MOE (Mixture of Experts).

**Discussion Highlights:** The community discussion is largely positive, with users praising the open-source nature of the Olmo models and their continuous improvements. There is also anticipation for future developments, such as the potential addition of Mixture of Experts (MOE) models.

---

## 13. [Mistral 3 Large is DeepSeek V3!?](https://reddit.com/r/LocalLLaMA/comments/1plpc6h/mistral_3_large_is_deepseek_v3/)

**Author:** u/seraschka | **Upvotes:** 169 | **Comments:** 33 | **Date:** 2025-12-13

**Summary:** The post discusses the architectural similarities between Mistral 3 Large and DeepSeek V3, noting that they share nearly identical sizes and architectures, with minor differences in expert configurations. The discussion highlights the open-source nature of these models and their adoption by other teams.

**Key Points:**
- Mistral 3 Large and DeepSeek V3 have almost identical sizes (671B vs 673B).
- Mistral 3 Large uses the same architecture as DeepSeek V3 but with adjusted expert configurations.
- The Mistral team likely trained their model from scratch rather than fine-tuning DeepSeek V3.
- Other models like Kimi K2 and Gigachat also use the DeepSeek V3 architecture.
- The open-source community appreciates the reuse and adaptation of proven architectures.

**Discussion Highlights:** The discussion emphasizes the collaborative spirit of open-source development, with multiple teams adopting and adapting the DeepSeek V3 architecture. Users appreciate the innovation and efficiency gains from reusing proven designs while acknowledging the importance of further advancements.

---

## 14. [Understanding the new router mode in llama cpp server](https://reddit.com/r/LocalLLaMA/comments/1pmc7lk/understanding_the_new_router_mode_in_llama_cpp/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 157 | **Comments:** 36 | **Date:** 2025-12-14

**Summary:** Router mode in llama cpp server allows managing multiple AI models simultaneously without restarting the server, similar to Ollama. It enables dynamic loading/unloading of models and routing requests to the appropriate model, saving memory and simplifying model switching.

**Key Points:**
- Router mode enables managing multiple AI models in a single server process.
- It allows loading/unloading models on demand and routing requests to the correct model.
- Benefits include memory savings and easier model switching compared to running separate servers.
- Useful for testing multiple GGUF models, building local APIs, and dynamic model switching.
- Discussion highlights differences and comparisons with llama-swap functionality.

**Discussion Highlights:** The discussion includes comparisons with llama-swap, questions about VRAM management for multiple GPUs, and requests for more detailed explanations. Some users express familiarity with similar functionality in llama-swap and seek clarification on how router mode differs or improves upon existing solutions.

---

## 15. [To Mistral and other lab employees: please test with community tools BEFORE releasing models](https://reddit.com/r/LocalLLaMA/comments/1pmgm2x/to_mistral_and_other_lab_employees_please_test/)

**Author:** u/dtdisapointingresult | **Upvotes:** 137 | **Comments:** 70 | **Date:** 2025-12-14

**Summary:** The Reddit post criticizes Mistral for releasing Devstral 2 without thorough testing with community tools, which has negatively impacted their reputation. The author emphasizes the importance of testing with local tools to ensure smooth adoption by AI enthusiasts and tech geeks.

**Key Points:**
- Devstral 2 release was marred by issues like benchmark discrepancies and repetition loops.
- The author suggests that inadequate testing with community tools led to the problematic release.
- The post highlights the influence of tech geeks in driving adoption and recommendations for tools.
- Comments indicate mixed experiences, with some users reporting success with local tools and others facing issues.
- There is a consensus on the need for better testing and documentation before model releases.

**Discussion Highlights:** The discussion highlights a divide in user experiences, with some praising the model's performance with local tools and others criticizing the lack of testing. There is a general agreement on the importance of thorough testing and documentation to ensure successful adoption by the community.

---

## 16. [Qwen3-Next-80B-A3B-Thinking-GGUF has just been released on HuggingFace](https://reddit.com/r/LocalLLaMA/comments/1pmo0dn/qwen3next80ba3bthinkinggguf_has_just_been/)

**Author:** u/LegacyRemaster | **Upvotes:** 116 | **Comments:** 48 | **Date:** 2025-12-14

**Summary:** The Reddit post announces the release of Qwen3-Next-80B-A3B-Thinking-GGUF on HuggingFace, highlighting its impressive performance in a Tetris game implemented in a single HTML file. Users compare it favorably to other models like Devstral and discuss its potential for agentic coding tasks.

**Key Points:**
- Qwen3-Next-80B-A3B-Thinking-GGUF model released on HuggingFace
- Model excels in Tetris game implementation in a single HTML file
- Users report better performance compared to Devstral
- Potential for iterative agentic coding tasks noted
- Discussion includes questions about release timing and tool support

**Discussion Highlights:** Users express enthusiasm for the model's capabilities, particularly in coding tasks, though there is some confusion about the release timing. Technical questions about tool support (e.g., llamacpp) are raised, and the discussion highlights the model's potential for local use.

---

