# r/LocalLLaMA Reading Digest

**Period:** 2025-12-19 to 2025-12-19
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster](https://reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/)

**Author:** u/geerlingguy | **Upvotes:** 442 | **Comments:** 121 | **Date:** 2025-12-18

**Summary:** The post discusses performance testing of Kimi K2 on a cluster of 4x Mac Studios using RDMA Tensor settings, highlighting challenges in benchmarking and comparisons due to lack of tools like llama-bench.

**Key Points:**
- Testing Kimi K2 on a 4x Mac Studio cluster with RDMA Tensor settings
- Challenges in benchmarking due to lack of tools like llama-bench
- RDMA support was recently stabilized, allowing for more testing
- Post gained significant attention with 442 upvotes and 121 comments
- Discussion includes questions about potential performance improvements with higher VRAM configurations

**Discussion Highlights:** The discussion highlights interest in the performance testing and potential improvements with higher VRAM configurations. There is also appreciation for the author's contributions and a mention of additional data available on GitHub.

---

## 2. [T5Gemma 2: The next generation of encoder-decoder models](https://reddit.com/r/LocalLLaMA/comments/1ppzhtq/t5gemma_2_the_next_generation_of_encoderdecoder/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 194 | **Comments:** 28 | **Date:** 2025-12-18

**Summary:** T5Gemma 2 models, based on Gemma 3, are multilingual and multimodal, handling text and image input with open weights for three pretrained sizes (270M, 1B, and 4B). They feature tied embeddings, merged attention, multimodality, extended long context, and support for over 140 languages.

**Key Points:**
- T5Gemma 2 models are multilingual and multimodal, supporting text and image input.
- They feature tied embeddings and merged attention mechanisms for efficiency.
- The models support over 140 languages and can handle context windows of up to 128K tokens.
- Community discussion highlights excitement about encoder-decoder models and potential for multimodal translation.
- Requests for GGUF format and larger models like Gemma 4 30-40B are noted.

**Discussion Highlights:** The community shows enthusiasm for the return of encoder-decoder models and their potential applications, particularly in multimodal translation. There are requests for additional formats and larger model sizes.

---

## 3. [Google's Gemma models family](https://reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/)

**Author:** u/jacek2023 | **Upvotes:** 469 | **Comments:** 112 | **Date:** 2025-12-18

**Summary:** The Reddit post discusses Google's Gemma models family, with a focus on FunctionGemma and community reactions. The post gained significant engagement with 469 upvotes and 112 comments.

**Key Points:**
- FunctionGemma is intended for fine-tuning specific function-calling tasks, including multi-turn use cases.
- The community humorously notes the introduction of FunctionGemma, referencing past jokes.
- There are 323 visible models in the collection, with speculation about three new Gemma models.
- The post received a special flair and was featured on Discord, indicating its popularity.

**Discussion Highlights:** The discussion highlights include enthusiasm for FunctionGemma, humorous references to past jokes, and speculation about new models. The community shows strong engagement and appreciation for the post.

---

## 4. [MiraTTS: High quality and fast TTS model](https://reddit.com/r/LocalLLaMA/comments/1pper90/miratts_high_quality_and_fast_tts_model/)

**Author:** u/SplitNice1982 | **Upvotes:** 137 | **Comments:** 52 | **Date:** 2025-12-17

**Summary:** MiraTTS is a high-quality, fast TTS model that generates realistic 48khz speech at 100x realtime, optimized for efficiency and low latency. It supports multilingual versions and is memory-efficient, working with GPUs as low as 6GB VRAM.

**Key Points:**
- Generates speech at 100x realtime
- High-quality 48khz speech
- Memory efficient with 6GB VRAM support
- Low latency as low as 150ms
- Multilingual and multispeaker support in progress

**Discussion Highlights:** The discussion highlights curiosity about multilingual support, voice cloning, and comparisons with other TTS models like KaniTTS. Users appreciate the work and express interest in trying the model, though some note hardware limitations.

---

## 5. [AMA with the Meta researchers behind SAM 3 + SAM 3D + SAM Audio](https://reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/)

**Author:** u/AIatMeta | **Upvotes:** 130 | **Comments:** 68 | **Date:** 2025-12-17

**Summary:** The post announces an AMA with Meta researchers behind SAM 3, SAM 3D, and SAM Audio, scheduled for December 18. The team introduces the new models and provides links for further exploration and a demo playground.

**Key Points:**
- AMA with Meta researchers on SAM 3, SAM 3D, and SAM Audio
- Researchers from different teams participating in the AMA
- Links provided for learning more about each model and a demo playground
- Top comments focus on model capabilities, architecture, and specific use cases
- AMA scheduled for December 18, 2-3pm PT

**Discussion Highlights:** The discussion highlights include questions about model capabilities, such as segmenting multiple objects, voice separation for home assistants, architectural similarities across models, and comparisons with existing tools like Demucs for audio stem creation.

---

## 6. [Nvidia plans heavy cuts to GPU supply in early 2026](https://reddit.com/r/LocalLLaMA/comments/1pp8vo4/nvidia_plans_heavy_cuts_to_gpu_supply_in_early/)

**Author:** u/HumanDrone8721 | **Upvotes:** 345 | **Comments:** 172 | **Date:** 2025-12-17

**Summary:** Nvidia plans to significantly reduce GPU supply in early 2026, which, combined with similar cuts by Micron and Samsung, could make building gaming PCs challenging. The discussion highlights concerns about market competition and the impact of corporate financial strategies on consumer access to technology.

**Key Points:**
- Nvidia's GPU supply cuts in early 2026
- Micron and Samsung also reducing consumer RAM and SSD production
- Potential challenges for gaming PC builders in 2026
- Concerns about reduced competition and innovation
- Criticism of corporate focus on stock buybacks over growth

**Discussion Highlights:** The discussion reflects concerns about the broader impact of supply cuts on the tech market, with users expressing frustration over potential limitations on consumer access to high-performance hardware. There is also speculation about new competitors entering the market due to reduced supply from established players.

---

## 7. [Hey, LocalLLaMa. We need to talk...](https://reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/)

**Author:** u/Eisenstein | **Upvotes:** 390 | **Comments:** 126 | **Date:** 2025-12-17

**Summary:** The post highlights the importance of community engagement and support for contributors in the r/LocalLLaMA subreddit, emphasizing the need for upvotes and constructive feedback to encourage continued sharing and development.

**Key Points:**
- Community engagement is crucial for the growth of local and open-source projects.
- Constructive feedback and upvotes are essential to encourage contributors.
- There is a concern about low-quality or AI-generated projects being shared.
- The community values honest and constructive feedback over mere entertainment.
- Engagement should focus on genuine contributions rather than superficial projects.

**Discussion Highlights:** The discussion reveals a consensus on the importance of engagement but also highlights skepticism towards low-quality or AI-generated projects. Many users agree that constructive feedback is valuable but are hesitant to support projects that lack substance or originality.

---

## 8. [Drummer's Cydonia and Magidonia 24B v4.3 - The best pair of Cydonia for RP yet!](https://reddit.com/r/LocalLLaMA/comments/1pp2j60/drummers_cydonia_and_magidonia_24b_v43_the_best/)

**Author:** u/TheLocalDrummer | **Upvotes:** 131 | **Comments:** 20 | **Date:** 2025-12-17

**Summary:** The post announces the release of Drummer's Cydonia and Magidonia 24B v4.3 models, described as the best pair for role-playing yet. The author expresses gratitude to patrons for their support and shares links to the models on Hugging Face. Key points include the release of the models, their praised quality, and community feedback highlighting their excellence. The discussion highlights community appreciation and additional resources shared by users.

---

## 9. [Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds.](https://reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/)

**Author:** u/themixtergames | **Upvotes:** 1135 | **Comments:** 129 | **Date:** 2025-12-17

**Summary:** Apple has introduced SHARP, a model that can generate photorealistic 3D Gaussian representations from a single image in seconds. The model is showcased with examples rendered in real-time on Apple Vision Pro and generated on a MacBook Pro M1 Max.

**Key Points:**
- SHARP generates 3D Gaussian representations from a single image quickly.
- Examples were rendered in real-time on Apple Vision Pro.
- Scenes were generated in 5–10 seconds on a MacBook Pro M1 Max.
- The model requires CUDA GPU for rendering trajectories.
- Community interest includes potential applications and performance on different hardware.

**Discussion Highlights:** The community showed significant interest in the model's capabilities, with discussions ranging from its performance on different hardware to potential applications like adult content and comparisons to cyberpunk's braindance. The top comments highlighted the model's speed and hardware requirements.

---

## 10. [LangChain and LlamaIndex are in "steep decline" according to new ecosystem report. Anyone else quietly ditching agent frameworks?](https://reddit.com/r/LocalLLaMA/comments/1pox733/langchain_and_llamaindex_are_in_steep_decline/)

**Author:** u/Exact-Literature-395 | **Upvotes:** 207 | **Comments:** 57 | **Date:** 2025-12-17

**Summary:** The Reddit post discusses the decline of LangChain and LlamaIndex frameworks, citing reduced community activity and investment. Users share experiences of simplifying their codebases by removing these frameworks and calling APIs directly, questioning the necessity of such tools with improved base models.

**Key Points:**
- LangChain, LlamaIndex, and AutoGen are listed as 'steepest declining' projects by community activity.
- Users report simplifying codebases and improving debugging by removing these frameworks.
- Criticism of LangChain includes bloated features, poor security/performance, and non-pythonic design.
- LlamaIndex maintainer acknowledges the shift but highlights the frameworks' initial ease of integration.
- Discussion suggests a trend towards simpler, more direct API usage.

**Discussion Highlights:** The discussion highlights a consensus that agent frameworks like LangChain and LlamaIndex may no longer be essential due to improvements in base models. Users express frustration with the complexity and abstractions of these frameworks, preferring simpler, more direct approaches. The LlamaIndex maintainer acknowledges the shift but notes the frameworks' initial value in ease of integration.

---

## 11. [anthropic blog on code execution for agents. 98.7% token reduction sounds promising for local setups](https://reddit.com/r/LocalLLaMA/comments/1powhy6/anthropic_blog_on_code_execution_for_agents_987/)

**Author:** u/Zestyclose_Ring1123 | **Upvotes:** 136 | **Comments:** 33 | **Date:** 2025-12-17

**Summary:** Anthropic's blog discusses a new approach to code execution for agents, claiming a 98.7% token reduction, which could significantly benefit local setups by reducing context limits and improving privacy. The approach involves letting models explore tools on demand rather than preloading all tool definitions. Key points include: Anthropic's method reduces tokens from 150k to 2k in their example; privacy is enhanced as sensitive data flows directly between tools without entering model context; sandboxing is a main challenge for running model-generated code locally; similar patterns already exist in projects like HF's smolagents; some users generate a DAG of steps instead of arbitrary code to reduce sandboxing needs. The discussion highlights that while Anthropic's approach is promising, similar patterns already exist in other projects like smolagents. Users also discuss alternative methods like generating a DAG of steps to reduce sandboxing needs and improve security.

---

## 12. [Peak LLM Wars: Xiaomi Blocks Kimi Employees on Twitter](https://reddit.com/r/LocalLLaMA/comments/1pow797/peak_llm_wars_xiaomi_blocks_kimi_employees_on/)

**Author:** u/nekofneko | **Upvotes:** 129 | **Comments:** 26 | **Date:** 2025-12-17

**Summary:** The Reddit post discusses the ongoing LLM wars, highlighting a specific incident where Xiaomi blocks Kimi employees on Twitter. The post includes images and comments that reflect the competitive and dramatic nature of the LLM industry. Key points include Xiaomi blocking Kimi employees, speculations about former DeepSeek members in Xiaomi, comparisons to other industry rivalries, and humorous references to online dramas. The discussion highlights the competitive and dramatic nature of the LLM industry, with users comparing it to other well-known industry rivalries and online dramas.

---

## 13. [Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model](https://reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 1139 | **Comments:** 120 | **Date:** 2025-12-17

**Summary:** Microsoft's TRELLIS 2-4B is an open-source image-to-3D model with 4 billion parameters, using Flow-Matching Transformers with Sparse Voxel based 3D VAE to convert single images into 3D assets. The model has received mixed reviews, with some users praising its quality while others find it lacking in practical applications.

**Key Points:**
- Model Type: Flow-Matching Transformers with Sparse Voxel based 3D VAE
- Parameters: 4 Billion
- Input: Single Image, Output: 3D Asset
- Mixed community feedback on model quality and practicality
- Suggestions for improvement include using multiple images for better results

**Discussion Highlights:** The community discussion highlights mixed opinions on the model's effectiveness, with some users finding the results impressive and others noting limitations in practical applications. There is a consensus that the model could benefit from using multiple images for better accuracy.

---

## 14. [QwenLong-L1.5: Revolutionizing Long-Context AI](https://reddit.com/r/LocalLLaMA/comments/1pokpha/qwenlongl15_revolutionizing_longcontext_ai/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 212 | **Comments:** 27 | **Date:** 2025-12-16

**Summary:** QwenLong-L1.5 is a new AI model that achieves state-of-the-art long-context reasoning with novel data synthesis, stabilized RL, and memory management for contexts up to 4M tokens. The model is available on HuggingFace and has garnered significant attention for its capabilities.

**Key Points:**
- Achieves SOTA long-context reasoning with up to 4M tokens
- Uses novel data synthesis and stabilized RL techniques
- Available on HuggingFace under the name QwenLong-L1.5-30B-A3B
- Integration into llama.cpp may require additional work
- Importance of using the exact query template for optimal performance

**Discussion Highlights:** The discussion highlights the model's significant advancements and potential challenges in integration. Users emphasize the importance of using the exact query template and express enthusiasm for the model's capabilities.

---

## 15. [8x Radeon 7900 XTX Build for Longer Context Local Inference - Performance Results &amp; Build Details](https://reddit.com/r/LocalLLaMA/comments/1pogwb6/8x_radeon_7900_xtx_build_for_longer_context_local/)

**Author:** u/Beautiful_Trust_8151 | **Upvotes:** 718 | **Comments:** 211 | **Date:** 2025-12-16

**Summary:** The post details a custom multi-GPU setup using 8x AMD Radeon 7900 XTX cards for local AI inference, highlighting performance metrics and build specifics. The author shares their experience with the system's stability and performance, noting its advantages in terms of upgradability and customizability for long-context AI tasks.

**Key Points:**
- The system uses 8x AMD Radeon 7900 XTX cards with 192 GB VRAM total, paired with an Intel Core i7-14700F and 192 GB system RAM.
- Performance testing with GLM4.5Air q6 shows 437 tokens per second for prompt processing and 27 tokens per second for generation with an empty context.
- The total build cost is around $6-7k, with a focus on upgradability and customizability.
- The system consumes about 900 watts during prompt processing and inferencing.
- The discussion highlights appreciation for the build and suggestions for potential improvements, such as switching to Linux and ROCm.

**Discussion Highlights:** The discussion highlights appreciation for the build's capabilities and its potential historical significance in the AI era. There are suggestions for further optimization, such as using Linux and ROCm, and requests for additional performance tests with other models.

---

## 16. [Nemotron 3 Nano 30B is Amazing! (TLDR)](https://reddit.com/r/LocalLLaMA/comments/1pocsdy/nemotron_3_nano_30b_is_amazing_tldr/)

**Author:** u/DonkeyBonked | **Upvotes:** 203 | **Comments:** 126 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses the user's experience with Nemotron 3 Nano 30B, highlighting its impressive token efficiency and performance on their hardware setup. The user compares it favorably to other models like Devstral 2 Small 24B and Qwen models, noting its ability to handle large contexts efficiently.

**Key Points:**
- Nemotron 3 Nano 30B shows high token efficiency and fits well within the user's hardware constraints.
- The model outperforms Devstral 2 Small 24B and Qwen models in coding tasks and context handling.
- Users discuss the model's speed, performance, and open-source nature, with some preferring Qwen 30B 2507 for certain tasks.
- The model's ability to handle large contexts (up to 1M) with minimal system RAM usage is highlighted.
- Comparisons with other models like IBM Granite 4 Hybrid Small are suggested for further testing.

**Discussion Highlights:** The discussion highlights the model's efficiency and performance, with users sharing their experiences and comparisons with other models. There is a general consensus on the model's capabilities, though some users prefer other models for specific tasks.

---

## 17. [32GB Mi50's were getting so expensive that I ended up buying a 32GB w6800 for about the same price instead](https://reddit.com/r/LocalLLaMA/comments/1pob44f/32gb_mi50s_were_getting_so_expensive_that_i_ended/)

**Author:** u/EmPips | **Upvotes:** 228 | **Comments:** 42 | **Date:** 2025-12-16

**Summary:** The author opted for a 32GB w6800 GPU instead of a 32GB Mi50 due to similar pricing, highlighting the pros and cons of the w6800. The discussion includes comparisons with other GPUs like the AMD Radeon AI PRO R9700 and Zotac 3090.

**Key Points:**
- The w6800 was chosen for its convenience and cooling performance.
- The AMD Radeon AI PRO R9700 was suggested as a more expensive but faster alternative.
- Zotac 3090s were available at a competitive price point.
- The w6800's blower-style cooler was noted for its effectiveness.
- Price comparisons were a central theme in the discussion.

**Discussion Highlights:** The discussion primarily revolved around price comparisons and performance trade-offs between different GPUs. The consensus leaned towards the w6800 for its balance of cost and convenience, though alternatives like the R9700 and 3090 were also considered viable depending on specific needs.

---

## 18. [8 Million Users' AI Conversations Sold for Profit by "Privacy" Extensions | Koi Blog](https://reddit.com/r/LocalLLaMA/comments/1poal2a/8_million_users_ai_conversations_sold_for_profit/)

**Author:** u/ManThigh | **Upvotes:** 161 | **Comments:** 47 | **Date:** 2025-12-16

**Summary:** The Reddit post highlights privacy concerns regarding browser extensions selling AI conversation data of millions of users for profit. It emphasizes the importance of using local AI models and auditing browser extensions to protect user data. Key points include the sale of data by extensions like Urban VPN Proxy, the advice to use local AI models, and the criticism of companies buying such data. The discussion consensus criticizes the sale of user data and emphasizes the importance of local AI setups for privacy.

---

## 19. [Finally managed to run Qwen-2.5-7B on a 4GB GTX 1050 without CPU offloading (Surgical Memory Alignment)](https://reddit.com/r/LocalLLaMA/comments/1po97ad/finally_managed_to_run_qwen257b_on_a_4gb_gtx_1050/)

**Author:** u/HuseyinKama | **Upvotes:** 151 | **Comments:** 48 | **Date:** 2025-12-16

**Summary:** The post describes a method called 'Surgical Memory Alignment' to run Qwen-2.5-7B on a 4GB GTX 1050 without CPU offloading, saving VRAM and improving speed. The author open-sourced the tool as QKV Core.

**Key Points:**
- Surgical Memory Alignment reduces VRAM usage by trimming and realigning memory blocks.
- The method saved about 44MB per model, allowing Qwen-2.5-7B to run purely on GPU.
- Speed improvements of ~34% in I/O load times were observed.
- The tool, QKV Core, is open-sourced and available on GitHub.
- Discussion includes skepticism about the gains and questions about the implementation.

**Discussion Highlights:** The discussion includes praise for the optimization, skepticism about the actual gains, and questions about the tool's functionality and implementation.

---

## 20. [I was bored](https://reddit.com/r/LocalLLaMA/comments/1po8yt0/i_was_bored/)

**Author:** u/MyLovelyAngelKirino | **Upvotes:** 129 | **Comments:** 70 | **Date:** 2025-12-16

**Summary:** The author, who is unemployed, built a high-performance computer setup with excess hardware, featuring 3x 3090 GPUs, 512GB RAM, and an Epyc 7663 56-core processor. The post sparked discussions about the hardware and requests for more details.

**Key Points:**
- Author built a powerful computer setup due to unemployment and excess hardware
- Hardware includes 3x 3090 GPUs, 512GB RAM, and an Epyc 7663 56-core processor
- Top comment highlights the impressive hardware specifications
- Users expressed interest in learning how the author acquired the hardware
- Requests for details on water-cooling components were made

**Discussion Highlights:** The discussion primarily focused on the impressive hardware setup, with users expressing admiration and curiosity about the specifications and components used. Some users humorously referenced the author's ability to acquire such hardware, while others requested more details about the build, particularly the water-cooling components.

---

## 21. [Meta announced a new SAM Audio Model for audio editing that can segment sound from complex audio mixtures using text, visual, and time span prompts.](https://reddit.com/r/LocalLLaMA/comments/1po7i0c/meta_announced_a_new_sam_audio_model_for_audio/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 504 | **Comments:** 85 | **Date:** 2025-12-16

**Summary:** Meta announced a new SAM Audio Model that can segment sound from complex audio mixtures using text, visual, and time span prompts, transforming audio processing.

**Key Points:**
- SAM Audio Model can isolate any sound from complex audio mixtures using text, visual, and time span prompts.
- Potential applications include isolating and subtracting unwanted sounds in Microsoft Teams meetings.
- The model can pick specific sounds from complex audio mixtures based on visual prompts.
- Model sizes and specifications are available in the provided image link.
- The model can handle accidental sounds, such as microphone taps, when prompted.

**Discussion Highlights:** The discussion highlights the potential applications of the SAM Audio Model, such as improving audio quality in virtual meetings by isolating unwanted sounds. Users are impressed by the model's ability to pick specific sounds from complex audio mixtures and its potential for practical use cases.

---

## 22. [Allen Institute for AI introduces Molmo 2](https://reddit.com/r/LocalLLaMA/comments/1po78bl/allen_institute_for_ai_introduces_molmo_2/)

**Author:** u/Agitated_Camel1886 | **Upvotes:** 244 | **Comments:** 22 | **Date:** 2025-12-16

**Summary:** Allen Institute for AI has introduced Molmo 2, an 8B model capable of advanced video analysis tasks like Video QA, counting, pointing, and dense captioning. The community is impressed by its capabilities and the public availability of datasets.

**Key Points:**
- Molmo 2 is an 8B model with advanced video analysis capabilities
- The model supports Video QA, counting, pointing, and dense captioning
- Allen AI releases datasets publicly, aiding community advancements
- An AMA was scheduled to discuss Olmo 3 and Molmo 2
- Community is highly impressed by the model's performance and size

**Discussion Highlights:** The community is highly impressed by Molmo 2's capabilities, especially given its 8B size. There is appreciation for Allen AI's practice of releasing datasets publicly, which aids in broader advancements. An AMA was scheduled to discuss the new models, indicating strong community interest.

---

## 23. [XiaomiMiMo/MiMo-V2-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1po3bn4/xiaomimimomimov2flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 236 | **Comments:** 52 | **Date:** 2025-12-16

**Summary:** The post discusses MiMo-V2-Flash, a Mixture-of-Experts (MoE) language model with 309B total parameters and 15B active parameters, designed for high-speed reasoning and agentic workflows. The model's performance on the SWE-Bench is noted to be exceptionally good, surpassing larger models like Sonnet 4.5 and Gemini 3. The discussion includes queries about larger versions and hardware requirements for running the model.

**Key Points:**
- MiMo-V2-Flash is a MoE language model with 309B total parameters and 15B active parameters.
- The model shows strong performance on the SWE-Bench, outperforming larger models.
- Users discuss the feasibility of running the model on specific hardware configurations.
- There is interest in whether larger versions of the model exist.
- Links to the tech report and blog are provided for further details.

**Discussion Highlights:** The discussion highlights the model's impressive performance and the community's interest in its capabilities and hardware requirements. Some users express skepticism about the performance claims, while others explore the practical aspects of running the model.

---

## 24. [GLM-4.5V, GLM-4.6V and GLM_4.6V-Flash are now supported by llama.cpp (GGUFs)](https://reddit.com/r/LocalLLaMA/comments/1po18y9/glm45v_glm46v_and_glm_46vflash_are_now_supported/)

**Author:** u/jacek2023 | **Upvotes:** 165 | **Comments:** 34 | **Date:** 2025-12-16

**Summary:** The Reddit post announces that GLM-4.5V, GLM-4.6V, and GLM_4.6V-Flash are now supported by llama.cpp with GGUFs, which is seen as a significant update by the community.

**Key Points:**
- Support for GLM-4.5V, GLM-4.6V, and GLM_4.6V-Flash has been added to llama.cpp.
- The update is celebrated as a great Christmas gift by the community.
- There are questions about whether the GGUFs support vision, with some users reporting issues.
- Comparisons between Qwen3-VL-4B and GLM_4.6V are being discussed.

**Discussion Highlights:** The community is excited about the new support for GLM models in llama.cpp, though there are some concerns and questions about vision support and compatibility with existing setups.

---

## 25. [Qwen3 Next speed optimization has been merged into llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1pnz9xu/qwen3_next_speed_optimization_has_been_merged/)

**Author:** u/jacek2023 | **Upvotes:** 216 | **Comments:** 25 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses the recent speed optimization for Qwen3 Next in llama.cpp, highlighting significant performance improvements across different hardware configurations.

**Key Points:**
- Speed optimization for Qwen3 Next has been merged into llama.cpp.
- Performance improvements reported: M1 64GB (12 t/s to 18 t/s), Win11 + RTX5090 + vulkan (37.x t/s), and UD-Q2_K_XL (100+ t/s).
- Comparison with Qwen3-30B shows 58 t/s on the same M1 64GB setup.
- Users express appreciation for the optimization and share their performance metrics.

**Discussion Highlights:** The discussion highlights a consensus on the significant performance gains achieved through the optimization, with users sharing their specific hardware setups and resulting speeds.

---

## 26. [I may have over-quantized this little guy.](https://reddit.com/r/LocalLLaMA/comments/1pnz80z/i_may_have_overquantized_this_little_guy/)

**Author:** u/AllergicToTeeth | **Upvotes:** 137 | **Comments:** 30 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses an over-quantized model, with comments highlighting its potential value to the open-source community and suggestions for improving its performance.

**Key Points:**
- The model is highly quantized, potentially making it valuable for open-source use.
- Suggestions include adding a system prompt to improve model behavior.
- Some users joke about the model being a leaked version of advanced AI models.
- The model is noted for its quick loading capabilities.
- There is humor and excitement about the model's potential.

**Discussion Highlights:** The discussion is largely positive, with users appreciating the model's potential and offering practical advice for its use. There is also a playful tone, with jokes about the model being a leaked advanced AI.

---

## 27. [It was Ilya who "closed" OpenAI](https://reddit.com/r/LocalLLaMA/comments/1pnxekt/it_was_ilya_who_closed_openai/)

**Author:** u/licuphand | **Upvotes:** 517 | **Comments:** 231 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses Ilya's role in 'closing' OpenAI, sparking a debate on AI governance and trust in companies versus the public. The discussion highlights concerns about centralized control of AI and the motivations of key figures like Ilya, Elon, and Sam.

**Key Points:**
- Ilya's actions are seen as pivotal in the perceived 'closing' of OpenAI.
- Public trust in AI is contrasted with trust in companies managing AI.
- The phrase 'Who will watch the watchmen' is referenced, emphasizing the age-old issue of oversight.
- Competition among key figures (Elon, Ilya, Sam) is noted as a driving factor in AI governance.
- The trend of AI companies becoming 'CloseAI' is criticized.

**Discussion Highlights:** The discussion centers on the tension between public and corporate control of AI, with many users expressing skepticism about centralized governance. A notable consensus is the concern over the lack of oversight and the potential for power struggles among AI leaders.

---

## 28. [Alibaba Open-Sources CosyVoice 3, a New TTS Model](https://reddit.com/r/LocalLLaMA/comments/1pnusp9/alibaba_opensources_cosyvoice_3_a_new_tts_model/)

**Author:** u/nekofneko | **Upvotes:** 215 | **Comments:** 31 | **Date:** 2025-12-16

**Summary:** Alibaba has open-sourced CosyVoice 3, a new TTS model with advanced features like multi-lingual support, high naturalness, and low latency. The model supports various languages, dialects, and emotions, making it suitable for production use.

**Key Points:**
- Supports 9 common languages and 18+ Chinese dialects/accents
- Achieves state-of-the-art performance in content consistency and naturalness
- Supports pronunciation inpainting and text normalization
- Features bi-streaming with low latency (150ms)
- Supports various instructions like emotions, speed, and volume

**Discussion Highlights:** Users are comparing CosyVoice 3 with other models like Chatterbox and Microsoft VibeVoice. There is interest in a larger model (1.5B) and positive feedback on its capabilities.

---

## 29. [New budget local AI rig](https://reddit.com/r/LocalLLaMA/comments/1pnllux/new_budget_local_ai_rig/)

**Author:** u/vucamille | **Upvotes:** 155 | **Comments:** 38 | **Date:** 2025-12-15

**Summary:** The user built a budget local AI rig for $650 using a Qiyida X99 mobo, 32GB RAM, Xeon E5 2680 V4, and two MI50 16GB GPUs. The system works well with ROCm 7.0.2 and can handle basic inference tests. The user is happy with the performance and plans to expand it in the future. Key points include the budget build costing $650 with expandable components, the use of two MI50 16GB GPUs with ROCm 7.0.2 for AI tasks, and the system's ability to handle basic inference tests and gaming. The community praised the cost-effective build, noting its expandability and performance.

---

## 30. [I'm strong enough to admit that this bugs the hell out of me](https://reddit.com/r/LocalLLaMA/comments/1pnfaqo/im_strong_enough_to_admit_that_this_bugs_the_hell/)

**Author:** u/ForsookComparison | **Upvotes:** 1702 | **Comments:** 355 | **Date:** 2025-12-15

**Summary:** The Reddit post discusses a user's frustration with a specific issue, sparking a lively discussion with various humorous and technical responses.

**Key Points:**
- The post is a link with no text content, sparking curiosity and engagement.
- Top comments include humorous references to RAM Doubler and technical discussions about workstations.
- The discussion highlights a mix of technical insights and playful banter.
- The post gained significant traction with 1702 upvotes and 355 comments.

**Discussion Highlights:** The discussion is a blend of technical insights about workstations and humorous comments, with a notable focus on RAM and GPU capabilities. The community engagement is high, as evidenced by the upvotes and comments.

---

## 31. [They're finally here (Radeon 9700)](https://reddit.com/r/LocalLLaMA/comments/1pnd5uf/theyre_finally_here_radeon_9700/)

**Author:** u/Zeikos | **Upvotes:** 358 | **Comments:** 67 | **Date:** 2025-12-15

**Summary:** The post announces the arrival of Radeon 9700 GPUs, sparking community interest and nostalgia. Users are eager for benchmarks and performance data.

**Key Points:**
- Radeon 9700 GPUs have been announced and are now available
- Community is highly interested in benchmarks and performance metrics
- Nostalgia about the Radeon 9700 name from the early 2000s
- Requests for specific benchmarks including inference, training, noise, and heat levels
- Users plan to test the GPUs during the holidays

**Discussion Highlights:** The discussion highlights a strong community interest in performance benchmarks, with users requesting detailed metrics on inference, training, noise, and heat levels. There is also a sense of nostalgia regarding the Radeon 9700 name, which was a top-tier GPU in the early 2000s. The consensus is focused on gathering comprehensive performance data to evaluate the new GPUs.

---

## 32. [status of Nemotron 3 Nano support in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1pnc045/status_of_nemotron_3_nano_support_in_llamacpp/)

**Author:** u/jacek2023 | **Upvotes:** 181 | **Comments:** 32 | **Date:** 2025-12-15

**Summary:** The Reddit post discusses the status of Nemotron 3 Nano support in llama.cpp, highlighting a GitHub pull request and community reactions. Users appreciate Nvidia's effort and emphasize the importance of collaboration with llama.cpp for new model architectures.

**Key Points:**
- Nemotron 3 Nano support is being added to llama.cpp via a GitHub pull request.
- Community praises Nvidia's approach and encourages other labs to follow suit.
- Discussion includes technical details about model sizes and memory requirements.
- Consensus that collaboration with llama.cpp is crucial for new model releases.

**Discussion Highlights:** The community consensus is positive, with users appreciating Nvidia's transparency and collaboration with llama.cpp. There's a strong emphasis on the importance of such partnerships for the broader AI ecosystem.

---

## 33. [NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model!](https://reddit.com/r/LocalLLaMA/comments/1pn8upp/nvidia_releases_nemotron_3_nano_a_new_30b_hybrid/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 845 | **Comments:** 178 | **Date:** 2025-12-15

**Summary:** NVIDIA has released Nemotron 3 Nano, a 30B hybrid reasoning model with a 1M context window and top performance in SWE-Bench, reasoning, and chat. The model is noted for its speed and is part of the Nemotron 3 family of MoE models.

**Key Points:**
- Nemotron 3 Nano is a 30B hybrid reasoning model
- It has a 1M context window and excels in SWE-Bench, reasoning, and chat
- The model is part of the Nemotron 3 family of MoE models
- Users report it is extremely fast, with 110 t/s generation speed
- The model was leaked a few days prior to the official release

**Discussion Highlights:** The discussion highlights the model's speed and performance, with users expressing surprise at the 'nano' designation for a 30B model. There is also clarification about the Nemotron 3 family of models and their sizes.

---

## 34. [NVIDIA Nemotron 3 Nano 30B A3B released](https://reddit.com/r/LocalLLaMA/comments/1pn8h5h/nvidia_nemotron_3_nano_30b_a3b_released/)

**Author:** u/rerri | **Upvotes:** 279 | **Comments:** 86 | **Date:** 2025-12-15

**Summary:** NVIDIA has released Nemotron 3 Nano 30B A3B, a new model featuring a hybrid Mamba-Transformer MoE architecture, exceptional inference efficiency, and a 1M-token context window. The model is fully open and designed for high throughput and low latency.

**Key Points:**
- Hybrid Mamba-Transformer MoE architecture for efficient inference
- 31.6B total parameters with ~3.6B active per token
- Up to 4x faster than Nemotron Nano 2 and leading models in its size category
- 1M-token context window for long-horizon workflows
- Fully open with open weights, datasets, and training recipes

**Discussion Highlights:** The discussion highlights include a Llama.cpp PR for integration, questions about optimal Unsloth quant for a 3090 GPU, concerns about synthetic data training, and performance feedback from users compiling the model.

---

## 35. [How to do a RTX Pro 6000 build right](https://reddit.com/r/LocalLLaMA/comments/1pn6ijr/how_to_do_a_rtx_pro_6000_build_right/)

**Author:** u/GPTrack_dot_ai | **Upvotes:** 114 | **Comments:** 174 | **Date:** 2025-12-15

**Summary:** The post discusses building a high-performance system using 8x Nvidia RTX Pro 6000 GPUs with integrated high-speed networking. It highlights the specifications and components needed for such a build, emphasizing its readiness for use with minimal setup required.

**Key Points:**
- The RTX Pro 6000 lacks NVlink but integrates high-speed networking directly at each GPU.
- The system requires a switch, CPU, RAM, and storage, with minimal setup complexity.
- Exemplary specs include 8x RTX Pro 6000 GPUs, 400G networking connections, and high-efficiency power supplies.
- The build is described as ready-to-use with a focus on performance and scalability.
- User reactions highlight the system's impressive specifications and high cost.

**Discussion Highlights:** Users expressed awe at the system's specifications, comparing it to luxury items like Ferraris and private jets. There was also humor about the cost, with comments joking about needing a mortgage to afford it.

---

## 36. [New Google model incoming!!!](https://reddit.com/r/LocalLLaMA/comments/1pn37mw/new_google_model_incoming/)

**Author:** u/R46H4V | **Upvotes:** 1255 | **Comments:** 263 | **Date:** 2025-12-15

**Summary:** The Reddit post discusses anticipation for a new Google model, with users expressing hope for improvements over previous models like Gemma3-Math and potential multi-modal capabilities.

**Key Points:**
- Anticipation for a new Google model
- Hope for improvements over Gemma3-Math
- Desire for multi-modal capabilities
- High engagement with 1255 upvotes and 263 comments

**Discussion Highlights:** Users are hopeful for significant improvements and new features in the upcoming model, with a focus on multi-modal capabilities and performance enhancements.

---

## 37. [llama.cpp: Automation for GPU layers, tensor split, tensor overrides, and context size (with MoE optimizations)](https://reddit.com/r/LocalLLaMA/comments/1pn2e1c/llamacpp_automation_for_gpu_layers_tensor_split/)

**Author:** u/Remove_Ayys | **Upvotes:** 192 | **Comments:** 62 | **Date:** 2025-12-15

**Summary:** The post discusses a new feature in llama.cpp that automates memory allocation for GPU layers, tensor splits, and context size, improving usability and performance, especially for MoE models. The implementation uses virtual test allocations to iteratively reduce memory use until the model fits across all GPUs.

**Key Points:**
- Automated memory allocation for GPU layers and tensor splits in llama.cpp
- Prioritization of dense tensors for better MoE performance
- Iterative reduction of memory use to fit models across GPUs
- Positive feedback on the implementation from the community
- Suggestions for caching to reduce fitting time and multi-GPU support

**Discussion Highlights:** The community appreciates the new feature, with suggestions for further improvements like caching to eliminate fitting time and better multi-GPU support. There is also interest in special handling for dense models and using weaker GPUs for non-AI tasks.

---

## 38. [Aaaand... is gone...](https://reddit.com/r/LocalLLaMA/comments/1pmungj/aaaand_is_gone/)

**Author:** u/HumanDrone8721 | **Upvotes:** 935 | **Comments:** 210 | **Date:** 2025-12-14

**Summary:** The Reddit post discusses the discontinuation or unavailability of a product or technology, likely related to storage drives, sparking a conversation about storage solutions and ownership.

**Key Points:**
- The post title suggests something is no longer available.
- Users discuss buying additional storage (2TB SSD).
- Mentions of SATA drives and their relevance.
- Discussion about ownership and technology changes.

**Discussion Highlights:** The discussion highlights a mix of humor, practical advice (buying more storage), and debate about the significance of the topic, with some users downplaying its impact while others see it as a notable change.

---

## 39. [Qwen3-Next-80B-A3B-Thinking-GGUF has just been released on HuggingFace](https://reddit.com/r/LocalLLaMA/comments/1pmo0dn/qwen3next80ba3bthinkinggguf_has_just_been/)

**Author:** u/LegacyRemaster | **Upvotes:** 129 | **Comments:** 54 | **Date:** 2025-12-14

**Summary:** The Reddit post announces the release of the Qwen3-Next-80B-A3B-Thinking-GGUF model on HuggingFace, highlighting its impressive performance in generating a Tetris game within a single HTML file. Users in the comments express amazement at the model's capabilities and discuss its potential for agentic coding tasks. Key points include the model's release, its performance in creating a Tetris game, user impressions, confusion about the release timeline, and questions about tool compatibility. The discussion highlights a strong positive reception of the model, with users praising its performance and potential applications.

---

## 40. [To Mistral and other lab employees: please test with community tools BEFORE releasing models](https://reddit.com/r/LocalLLaMA/comments/1pmgm2x/to_mistral_and_other_lab_employees_please_test/)

**Author:** u/dtdisapointingresult | **Upvotes:** 135 | **Comments:** 72 | **Date:** 2025-12-14

**Summary:** The Reddit post criticizes Mistral for releasing Devstral 2 without thorough testing with community tools, leading to issues like benchmark discrepancies and repetition loops. The author emphasizes the importance of testing with local tools to maintain reputation and user trust.

**Key Points:**
- Devstral 2 release faced criticism due to lack of testing with community tools.
- Issues included benchmark discrepancies and repetition loops.
- Author stresses the importance of testing with local tools for reputation and user trust.
- Community feedback highlights mixed experiences with the model in various tools.
- Discussion includes comparisons with other models and the importance of community adoption.

**Discussion Highlights:** The discussion highlights mixed experiences with Devstral 2 in various local tools, with some users reporting positive experiences while others face issues. There is a consensus on the importance of thorough testing with community tools before release to maintain reputation and user trust.

---

## 41. [Understanding the new router mode in llama cpp server](https://reddit.com/r/LocalLLaMA/comments/1pmc7lk/understanding_the_new_router_mode_in_llama_cpp/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 165 | **Comments:** 44 | **Date:** 2025-12-14

**Summary:** Router mode in llama cpp server allows managing multiple AI models simultaneously without restarting the server, enabling dynamic model switching and efficient memory usage.

**Key Points:**
- Router mode enables loading/unloading models on demand within a single server process
- Eliminates need for separate server instances per model, saving memory and simplifying workflow
- Useful for testing multiple GGUF models, building local APIs, and dynamic model switching
- Comparisons drawn to Ollama functionality and existing tools like llama-swap
- Users express interest in VRAM management and concurrent model loading features

**Discussion Highlights:** Users show strong interest in router mode's capabilities, with comparisons to existing tools like llama-swap. Key discussion points include VRAM management for multi-GPU setups and the ability to specify which models remain in memory concurrently. Some users note the explanatory image could be more informative.

---

## 42. [8x RTX Pro 6000 server complete](https://reddit.com/r/LocalLLaMA/comments/1plwgun/8x_rtx_pro_6000_server_complete/)

**Author:** u/koushd | **Upvotes:** 631 | **Comments:** 268 | **Date:** 2025-12-13

**Summary:** The Reddit post details a user's journey upgrading their GPU server, culminating in a setup with 8x RTX Pro 6000 GPUs, a Threadripper PRO 9955WX CPU, and 384 GB RAM, totaling 768 GB VRAM. The user faced challenges with heat management, power consumption, and hardware compatibility during the upgrades.

**Key Points:**
- The final setup includes 8x RTX Pro 6000 GPUs (4 Workstation, 4 Max-Q), a Threadripper PRO 9955WX CPU, and 384 GB RAM, providing 768 GB VRAM.
- The user encountered issues with overheating, power distribution, and hardware compatibility during upgrades.
- The post highlights the use of pipeline parallelism across two systems as a workaround for hardware limitations.
- The discussion includes comments on the impressive but unconventional setup, with some criticism of the hardware placement and cooling solutions.
- Notable comments mention concerns about power supply reliability and the cost of the setup.

**Discussion Highlights:** The discussion highlights a mix of admiration for the powerful setup and criticism of its practical implementation. Key points include concerns about cooling solutions, power supply reliability, and the overall cost-effectiveness of the build. Some users expressed awe at the sheer computational power, while others questioned the practicality of the setup.

---

## 43. [Mistral 3 Large is DeepSeek V3!?](https://reddit.com/r/LocalLLaMA/comments/1plpc6h/mistral_3_large_is_deepseek_v3/)

**Author:** u/seraschka | **Upvotes:** 169 | **Comments:** 33 | **Date:** 2025-12-13

**Summary:** The post discusses the architectural similarities between Mistral 3 Large and DeepSeek V3, noting that they share nearly identical sizes and architectures, with minor differences in expert configurations. The community highlights the open-source spirit and the adoption of DeepSeek V3's architecture by multiple models.

**Key Points:**
- Mistral 3 Large and DeepSeek V3 have almost identical sizes (671B vs 673B).
- Mistral 3 Large uses the same architecture as DeepSeek V3 but with adjusted expert configurations.
- The community views this as a positive example of open-source collaboration.
- Other models like Kimi K2 and Gigachat also use the DeepSeek V3 architecture.
- Mistral likely trained their model from scratch despite architectural similarities.

**Discussion Highlights:** The discussion highlights the open-source spirit, with users noting that multiple models are adopting the DeepSeek V3 architecture due to its proven effectiveness. There is consensus that architectural similarities are not surprising given the limited ways to build decoder-based models, and that Mistral's use of the architecture is fair within the open-source community.

---

## 44. [OpenAI's flagship model, ChatGPT-5.2 Thinking, ranks  most censored AI on Sansa benchmark.](https://reddit.com/r/LocalLLaMA/comments/1plnuqu/openais_flagship_model_chatgpt52_thinking_ranks/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 625 | **Comments:** 112 | **Date:** 2025-12-13

**Summary:** OpenAI's ChatGPT-5.2 model is criticized for being the most censored AI on the Sansa benchmark, with users reporting performance issues and excessive denial of requests compared to previous versions.

**Key Points:**
- ChatGPT-5.2 is ranked as the most censored AI on the Sansa benchmark
- Users report poor performance in follow-up questions and research tasks
- The model frequently denies requests, even for made-up clinical notes
- Comparisons with other models like Gemini and Mistral highlight differences in censorship levels
- Concerns about the testing criteria used in the benchmark

**Discussion Highlights:** The discussion highlights significant user dissatisfaction with ChatGPT-5.2's performance and censorship levels. Many users find it worse than previous versions and question the benchmark's testing criteria, especially given the low ranking of models like Grok. There is also surprise at Gemini being less censored than some open models.

---

## 45. [Qwen3 Next generation optimization](https://reddit.com/r/LocalLLaMA/comments/1plng6f/qwen3_next_generation_optimization/)

**Author:** u/ilintar | **Upvotes:** 361 | **Comments:** 40 | **Date:** 2025-12-13

**Summary:** The post discusses optimizations for Qwen3, specifically an optimized autoregressive delta net computation that results in a 40% generation speed upgrade. The author invites the community to test the improvements.

**Key Points:**
- Optimized autoregressive delta net computation for Qwen3
- 40% generation speed upgrade reported
- Community encouraged to test and provide feedback
- Positive reception with comments praising the contribution
- Questions about compatibility with ROCm/Vulkan

**Discussion Highlights:** The community responded positively, with comments praising the author's work and expressing interest in further optimizations. There was a question about whether the speedup would apply to ROCm/Vulkan in addition to CUDA.

---

## 46. [NVIDIA gpt-oss-120b Eagle Throughput model](https://reddit.com/r/LocalLLaMA/comments/1plewrk/nvidia_gptoss120b_eagle_throughput_model/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 241 | **Comments:** 53 | **Date:** 2025-12-13

**Summary:** The post discusses NVIDIA's GPT-OSS-120B-Eagle3-throughput model, an optimized speculative decoding module designed to improve throughput during text generation using Eagle3 speculative decoding. It is licensed for commercial and non-commercial use in various AI applications.

**Key Points:**
- Optimized speculative decoding module for improved throughput
- Uses NVIDIA’s Eagle3 speculative decoding approach
- Licensed under nvidia-open-model-license for commercial and non-commercial use
- Intended for AI agents, chatbots, RAG systems, and instruction-following tasks
- Not supported in llama.cpp, limiting its use in some environments

**Discussion Highlights:** The discussion highlights interest in making the model derestricted, questions about its compatibility with CPU inference, and the lack of support in llama.cpp. There is also humor about waiting for a REAP EAGLE3 HERETIC MOE GGUF version.

---

## 47. [This is how open ai is advertising them selfs on reddit…. They are doomed](https://reddit.com/r/LocalLLaMA/comments/1plcrg8/this_is_how_open_ai_is_advertising_them_selfs_on/)

**Author:** u/ThinkExtension2328 | **Upvotes:** 239 | **Comments:** 77 | **Date:** 2025-12-12

**Summary:** The Reddit post criticizes OpenAI's advertising strategy, highlighting a shift from promoting advanced AI to using astrology ads, which is seen as a decline in their approach. Key points include the criticism of OpenAI's focus on normies rather than programmers, the irony of their shift from warning about open models to using astrology ads, and the consensus that this approach may be more profitable but is seen as a fall from grace. The discussion highlights a consensus that OpenAI's shift in advertising strategy is seen as a decline in their approach, with humor and criticism about their potential use of personal data.

---

## 48. [Running an LLM on a 3DS](https://reddit.com/r/LocalLLaMA/comments/1pl4njj/running_an_llm_on_a_3ds/)

**Author:** u/vreab | **Upvotes:** 297 | **Comments:** 35 | **Date:** 2025-12-12

**Summary:** The post discusses the feasibility and performance of running an LLM on a Nintendo 3DS, drawing comparisons to similar projects on other platforms like the PS Vita and Wii.

**Key Points:**
- Running an LLM on a 3DS is technically feasible, as demonstrated by similar projects on other platforms.
- Performance improvements might be possible on a 'new' 3DS model.
- The project is seen as impressive and innovative within the community.
- Comparisons are made to other unconventional platforms like the PS Vita and Wii.

**Discussion Highlights:** The discussion highlights the technical achievement of running an LLM on a 3DS, with users expressing admiration for the project and curiosity about potential performance improvements on newer hardware.

---

## 49. [The new monster-server](https://reddit.com/r/LocalLLaMA/comments/1pl0ojb/the_new_monsterserver/)

**Author:** u/eribob | **Upvotes:** 590 | **Comments:** 125 | **Date:** 2025-12-12

**Summary:** The user shares their upgraded 'Monster server' setup, featuring a Ryzen 3950x CPU, 128GB RAM, and three GPUs (2x RTX 3090 and 1x RTX 4090). The server runs local LLMs like GPT-OSS-120B and is used for research and coding. The post highlights the hardware configuration, performance, and user satisfaction. Key points include the server's hardware specifications, the RTX 4090's connection via an M.2 to PCIe adapter and a second PSU, and the user's experience with GPT-OSS-120B. The discussion includes positive feedback on the server setup, with users expressing envy and nostalgia, and some raising concerns about the performance of a 3-GPU setup compared to 2 or 4 GPUs.

---

## 50. [Olmo 3.1 32B Think &amp; Instruct: New Additions to the Olmo Model Family](https://reddit.com/r/LocalLLaMA/comments/1pky5u4/olmo_31_32b_think_instruct_new_additions_to_the/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 179 | **Comments:** 28 | **Date:** 2025-12-12

**Summary:** The post introduces Olmo 3.1 32B Think and Instruct models, highlighting their specialized capabilities in deep reasoning and instruction following, respectively. The models are praised for being fully open-source and improving in quality.

**Key Points:**
- Olmo 3.1 32B Think is optimized for deep reasoning, math, logic, and code generation.
- Olmo 3.1 32B Instruct is focused on instruction following, conversational fluency, and tool-use capabilities.
- The models are fully open-source and part of the Olmo family.
- Community feedback highlights the models' quality and openness.
- Expectations for future developments like MOE (Mixture of Experts).

**Discussion Highlights:** The community appreciates the open-source nature of the models and their continuous improvement. There is anticipation for future advancements like MOE.

---

