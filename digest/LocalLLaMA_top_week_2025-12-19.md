# r/LocalLLaMA Reading Digest

**Period:** 2025-12-19 to 2025-12-19
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster](https://reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/)

**Author:** u/geerlingguy | **Upvotes:** 498 | **Comments:** 134 | **Date:** 2025-12-18

**Summary:** The post discusses performance testing of Kimi K2 on a cluster of 4x Mac Studios, highlighting challenges with benchmarking tools and plans for further testing before returning the hardware.

**Key Points:**
- Testing Kimi K2 on a 4x Mac Studio cluster with varying RAM configurations
- Challenges with benchmarking tools like the lack of a direct comparison tool similar to llama-bench
- Future testing plans before returning the hardware in February
- Community engagement and recognition for the contribution
- Expectations for improved performance with upcoming Apple Silicon ultra chips featuring MATMUL instructions

**Discussion Highlights:** The discussion highlights community appreciation for the testing efforts, additional resources shared by the author, and anticipation for future improvements with new Apple Silicon chips.

---

## 2. [T5Gemma 2: The next generation of encoder-decoder models](https://reddit.com/r/LocalLLaMA/comments/1ppzhtq/t5gemma_2_the_next_generation_of_encoderdecoder/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 209 | **Comments:** 31 | **Date:** 2025-12-18

**Summary:** T5Gemma 2 models, based on Gemma 3, are multilingual and multimodal, handling text and image input with open weights for three pretrained sizes (270M, 1B, and 4B). They feature tied embeddings, merged attention, multimodality, extended long context, and support for over 140 languages.

**Key Points:**
- Tied embeddings reduce parameter count and improve memory efficiency
- Merged attention mechanism simplifies architecture and improves inference
- Multimodal capabilities for text and image processing
- Extended context window of up to 128K tokens
- Support for over 140 languages

**Discussion Highlights:** The community is excited about the new encoder-decoder model, with comments highlighting its potential for multimodal translation and expressing anticipation for future developments like Gemma 4.

---

## 3. [Google's Gemma models family](https://reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/)

**Author:** u/jacek2023 | **Upvotes:** 478 | **Comments:** 116 | **Date:** 2025-12-18

**Summary:** The Reddit post discusses Google's Gemma models family, highlighting the introduction of FunctionGemma and community reactions. The discussion includes technical details and enthusiasm from the community.

**Key Points:**
- Introduction of FunctionGemma for fine-tuning
- Community excitement and jokes about Gemma models
- Technical details and model counts discussed
- Positive reception and special recognition for the post

**Discussion Highlights:** The community shows strong interest in FunctionGemma, with jokes about its naming and technical discussions about model counts. There is also appreciation for the post's recognition and popularity.

---

## 4. [MiraTTS: High quality and fast TTS model](https://reddit.com/r/LocalLLaMA/comments/1pper90/miratts_high_quality_and_fast_tts_model/)

**Author:** u/SplitNice1982 | **Upvotes:** 140 | **Comments:** 53 | **Date:** 2025-12-17

**Summary:** MiraTTS is a high-quality, fast TTS model that generates realistic 48khz speech at 100x realtime, optimized for efficiency and low latency. It supports multilingual versions and is memory-efficient, working with GPUs as low as 6GB VRAM.

**Key Points:**
- MiraTTS generates speech at 100x realtime with high quality and clarity.
- It is memory-efficient and works with GPUs having 6GB VRAM.
- Supports multilingual versions and aims for multispeaker functionality.
- Low latency as low as 150ms, with streaming code to be released soon.
- Optimized using Lmdeploy and FlashSR for audio enhancement.

**Discussion Highlights:** The discussion highlights curiosity about multilingual support, voice cloning, and comparisons with other TTS models like KaniTTS. Users appreciate the work and express interest in trying the model, though some face hardware limitations.

---

## 5. [AMA with the Meta researchers behind SAM 3 + SAM 3D + SAM Audio](https://reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/)

**Author:** u/AIatMeta | **Upvotes:** 133 | **Comments:** 74 | **Date:** 2025-12-17

**Summary:** The Reddit post is an AMA with Meta researchers introducing SAM 3, SAM 3D, and SAM Audio, new models in the Segment Anything collection. The team shared details about the models and answered questions from the community.

**Key Points:**
- SAM 3, SAM 3D, and SAM Audio are new models in the Segment Anything collection.
- The AMA included researchers from across the team, providing insights into the models.
- Users discussed capabilities and limitations, such as segmenting multiple objects and voice separation.
- The models can be tested in the Segment Anything Playground.
- The AMA concluded with thanks to participants and anticipation for future sessions.

**Discussion Highlights:** Users inquired about the models' capabilities, such as segmenting multiple objects simultaneously and voice separation for home assistants. There was also interest in the architectural similarities across the models and their potential applications, like stem creation for music.

---

## 6. [Nvidia plans heavy cuts to GPU supply in early 2026](https://reddit.com/r/LocalLLaMA/comments/1pp8vo4/nvidia_plans_heavy_cuts_to_gpu_supply_in_early/)

**Author:** u/HumanDrone8721 | **Upvotes:** 348 | **Comments:** 174 | **Date:** 2025-12-17

**Summary:** Nvidia plans to significantly reduce GPU supply in early 2026, which, combined with similar cuts by Micron and Samsung, may impact gaming PC builds and market competition.

**Key Points:**
- Nvidia's GPU supply cuts in early 2026
- Micron and Samsung also reducing consumer RAM and SSD production
- Potential impact on gaming PC builds in 2026
- Concerns about market competition and stock buybacks
- Speculation about restricting access to advanced hardware for local use

**Discussion Highlights:** The discussion highlights concerns about the impact on gaming PC builds, potential market competition, and criticism of stock buybacks over investment in growth. Users speculate about the motives behind the cuts and their long-term effects on the industry.

---

## 7. [Hey, LocalLLaMa. We need to talk...](https://reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/)

**Author:** u/Eisenstein | **Upvotes:** 398 | **Comments:** 131 | **Date:** 2025-12-17

**Summary:** The post highlights the importance of community engagement and feedback for open-source projects, emphasizing the need to support and encourage contributors by providing constructive feedback and upvotes.

**Key Points:**
- Community engagement is crucial for the success of open-source projects.
- Constructive feedback and upvotes are essential for encouraging contributors.
- The post urges users to engage with smaller projects and provide honest feedback.
- Top comments reflect a mix of support for the post's message and skepticism about the quality of some projects.
- There is a consensus on the need for genuine engagement and feedback.

**Discussion Highlights:** The discussion highlights a consensus on the importance of community engagement and feedback, with some users expressing concerns about the quality of certain projects and the need for genuine, constructive feedback.

---

## 8. [Drummer's Cydonia and Magidonia 24B v4.3 - The best pair of Cydonia for RP yet!](https://reddit.com/r/LocalLLaMA/comments/1pp2j60/drummers_cydonia_and_magidonia_24b_v43_the_best/)

**Author:** u/TheLocalDrummer | **Upvotes:** 130 | **Comments:** 20 | **Date:** 2025-12-17

**Summary:** The post announces the release of Drummer's Cydonia and Magidonia 24B v4.3 models, described as the best pair for role-playing yet. The author expresses gratitude to patrons for their support and shares links to the models on Hugging Face. Key points include the release of the models, their praised quality, and community feedback highlighting their excellence. The discussion highlights community appreciation and technical details shared by users.

---

## 9. [Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds.](https://reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/)

**Author:** u/themixtergames | **Upvotes:** 1145 | **Comments:** 129 | **Date:** 2025-12-17

**Summary:** Apple has introduced SHARP, a model that can generate photorealistic 3D Gaussian representations from a single image in seconds. The model is showcased on GitHub and detailed in an arXiv paper, with examples rendered in real-time on Apple Vision Pro and generated on a MacBook Pro M1 Max.

**Key Points:**
- SHARP generates 3D Gaussian representations from a single image in seconds.
- The model is demonstrated on Apple Vision Pro and MacBook Pro M1 Max.
- The GitHub repository and arXiv paper provide technical details.
- Community discussion includes comparisons to cyberpunk's braindance and inquiries about content compatibility.
- The post received significant engagement with 1145 upvotes and 129 comments.

**Discussion Highlights:** The community showed enthusiasm for the technology, with comparisons to cyberpunk's braindance and questions about its capabilities. The top comments highlighted the real-time rendering on Apple Vision Pro and the quick generation times on a MacBook Pro M1 Max.

---

## 10. [LangChain and LlamaIndex are in "steep decline" according to new ecosystem report. Anyone else quietly ditching agent frameworks?](https://reddit.com/r/LocalLLaMA/comments/1pox733/langchain_and_llamaindex_are_in_steep_decline/)

**Author:** u/Exact-Literature-395 | **Upvotes:** 205 | **Comments:** 57 | **Date:** 2025-12-17

**Summary:** The Reddit post discusses the decline of LangChain and LlamaIndex frameworks, citing reduced community activity and investment. Users share their experiences of moving away from these frameworks due to complexity and lack of necessity with improved base models.

**Key Points:**
- LangChain and LlamaIndex are in steep decline according to a recent report.
- Users report better results by calling APIs directly instead of using these frameworks.
- Criticisms include bloated features, poor security/performance, and non-pythonic design.
- Some argue these frameworks solve problems that no longer exist with current model capabilities.
- Maintainers acknowledge the shift but highlight the frameworks' historical role in integration ease.

**Discussion Highlights:** The discussion reveals a consensus that these frameworks are becoming less relevant as base models improve. Many users express frustration with the complexity and lack of transparency in these tools, preferring direct API calls. However, there's acknowledgment of their past utility in simplifying integrations.

---

## 11. [anthropic blog on code execution for agents. 98.7% token reduction sounds promising for local setups](https://reddit.com/r/LocalLLaMA/comments/1powhy6/anthropic_blog_on_code_execution_for_agents_987/)

**Author:** u/Zestyclose_Ring1123 | **Upvotes:** 134 | **Comments:** 33 | **Date:** 2025-12-17

**Summary:** Anthropic's blog discusses a new approach to code execution for agents, which significantly reduces token usage by letting models explore tools on demand. This could be a game-changer for local setups, addressing context limits and privacy concerns.

**Key Points:**
- Anthropic's approach reduces token usage by 98.7%, making it feasible for local models with smaller context limits.
- The method involves model-generated code that orchestrates tools, with data flowing through variables rather than context.
- Privacy is enhanced as sensitive data never enters the model context, flowing directly between tools.
- Sandboxing is a major challenge for running model-generated code locally.
- Similar patterns already exist in projects like HF's smolagents and other implementations.

**Discussion Highlights:** The discussion highlights that while Anthropic's approach is promising, similar patterns have been explored by others, such as HF's smolagents. There is consensus on the potential benefits for local setups and the importance of addressing sandboxing challenges.

---

## 12. [Peak LLM Wars: Xiaomi Blocks Kimi Employees on Twitter](https://reddit.com/r/LocalLLaMA/comments/1pow797/peak_llm_wars_xiaomi_blocks_kimi_employees_on/)

**Author:** u/nekofneko | **Upvotes:** 133 | **Comments:** 30 | **Date:** 2025-12-17

**Summary:** The Reddit post discusses the ongoing LLM wars, highlighting a specific incident where Xiaomi blocks Kimi employees on Twitter. The post includes images and comments that reflect the competitive and dramatic nature of the LLM industry. Key points include Xiaomi blocking Kimi employees, rumors about former DeepSeek members joining Xiaomi's team, comparisons to other industry rivalries, and humorous comparisons to other online dramas. The discussion highlights the competitive and dramatic nature of the LLM industry, with users comparing it to other online dramas and industry rivalries, and speculating about team movements and company dynamics.

---

## 13. [Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model](https://reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 1141 | **Comments:** 120 | **Date:** 2025-12-17

**Summary:** Microsoft's TRELLIS 2-4B is an open-source image-to-3D model using Flow-Matching Transformers with a Sparse Voxel-based 3D VAE, featuring 4 billion parameters. It converts single images into 3D assets and has received mixed reactions from the community regarding its practical utility.

**Key Points:**
- Model Type: Flow-Matching Transformers with Sparse Voxel-based 3D VAE
- Parameters: 4 Billion
- Input: Single Image, Output: 3D Asset
- Mixed community reactions on practical utility
- Suggestions for improvement include using multiple images

**Discussion Highlights:** The community has mixed opinions, with some praising the model's capabilities and others criticizing its practical utility. Suggestions for improvement include the ability to upload multiple images for better results.

---

## 14. [QwenLong-L1.5: Revolutionizing Long-Context AI](https://reddit.com/r/LocalLLaMA/comments/1pokpha/qwenlongl15_revolutionizing_longcontext_ai/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 213 | **Comments:** 27 | **Date:** 2025-12-16

**Summary:** QwenLong-L1.5 is a new AI model that achieves state-of-the-art long-context reasoning with novel data synthesis, stabilized RL, and memory management for contexts up to 4M tokens. The model is available on HuggingFace and has garnered significant attention for its capabilities.

**Key Points:**
- Achieves SOTA long-context reasoning with novel data synthesis and stabilized RL
- Supports contexts up to 4M tokens
- Available on HuggingFace
- Integration into llama.cpp may require additional work
- Specific query template is recommended for optimal use

**Discussion Highlights:** The discussion highlights the model's significant capabilities and potential challenges in integration. Users appreciate the model's performance but note the need for specific query templates and potential difficulties in integrating it with existing systems like llama.cpp.

---

## 15. [8x Radeon 7900 XTX Build for Longer Context Local Inference - Performance Results &amp; Build Details](https://reddit.com/r/LocalLLaMA/comments/1pogwb6/8x_radeon_7900_xtx_build_for_longer_context_local/)

**Author:** u/Beautiful_Trust_8151 | **Upvotes:** 716 | **Comments:** 211 | **Date:** 2025-12-16

**Summary:** The post details a custom multi-GPU setup using 8x AMD Radeon 7900 XTX cards for local AI inference, highlighting performance metrics and build specifics. The system demonstrates stable performance with significant context lengths and offers flexibility for future upgrades.

**Key Points:**
- 8x AMD Radeon 7900 XTX GPUs with 192 GB VRAM total
- Performance metrics: 437 tokens/sec prompt processing, 27 tokens/sec generation with empty context
- Total build cost around $6-7k, offering customizability and long-context capability
- System consumes about 900 watts during operation
- Discussion highlights the uniqueness and cost-effectiveness of the build

**Discussion Highlights:** The discussion appreciates the innovative GPU build, comparing it to historical technological advancements. Comments highlight the cost-effectiveness and performance of the setup, with suggestions for further testing with different models.

---

## 16. [Nemotron 3 Nano 30B is Amazing! (TLDR)](https://reddit.com/r/LocalLLaMA/comments/1pocsdy/nemotron_3_nano_30b_is_amazing_tldr/)

**Author:** u/DonkeyBonked | **Upvotes:** 206 | **Comments:** 136 | **Date:** 2025-12-16

**Summary:** The post discusses the author's experience with Nemotron 3 Nano 30B, highlighting its token efficiency and performance on their hardware setup. The model fits well within their VRAM constraints and outperforms other models they've tried.

**Key Points:**
- Nemotron 3 Nano 30B shows impressive token efficiency, fitting 256k tokens in VRAM.
- The model performs well on the author's hardware setup, which includes an RTX 5000 and an RTX 3090.
- The author uses llama.cpp to split layers between GPUs, avoiding slow communication over TB3.
- Nemotron 3 Nano 30B is praised for its speed and performance, though some users prefer Qwen 30B for certain tasks.
- The model is noted for being truly open source, which is a significant advantage.

**Discussion Highlights:** The discussion highlights the model's efficiency and performance, with some users comparing it favorably to Qwen 30B. There is a consensus that Nemotron 3 Nano 30B is a strong performer, though preferences vary based on specific use cases.

---

## 17. [32GB Mi50's were getting so expensive that I ended up buying a 32GB w6800 for about the same price instead](https://reddit.com/r/LocalLLaMA/comments/1pob44f/32gb_mi50s_were_getting_so_expensive_that_i_ended/)

**Author:** u/EmPips | **Upvotes:** 230 | **Comments:** 42 | **Date:** 2025-12-16

**Summary:** The author opted for a 32GB w6800 GPU instead of a 32GB Mi50 due to similar pricing, citing convenience and cooling performance as key factors. The discussion includes comparisons with other GPUs like the AMD Radeon AI PRO R9700 and Zotac 3090.

**Key Points:**
- Author chose 32GB w6800 over 32GB Mi50 due to similar pricing
- Pros of w6800 include convenience and effective blower-style cooling
- Alternatives mentioned: AMD Radeon AI PRO R9700 and Zotac 3090
- Price comparison: w6800 at $500 vs. Zotac 3090 at $540

**Discussion Highlights:** The discussion highlights the trade-offs between different GPUs, with some users suggesting alternatives like the AMD Radeon AI PRO R9700 for better performance and software support, while others noted competitive pricing on the Zotac 3090.

---

## 18. [8 Million Users' AI Conversations Sold for Profit by "Privacy" Extensions | Koi Blog](https://reddit.com/r/LocalLLaMA/comments/1poal2a/8_million_users_ai_conversations_sold_for_profit/)

**Author:** u/ManThigh | **Upvotes:** 159 | **Comments:** 47 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses privacy concerns regarding browser extensions selling AI conversation data of millions of users for profit, highlighting the importance of using local models and auditing extensions.

**Key Points:**
- Browser extensions like Urban VPN Proxy and 1ClickVPN Proxy sold AI conversation data of millions of users.
- The post emphasizes the need for auditing extensions and using local models to protect privacy.
- Community reactions include calls for punishing companies that buy such data and pride in using local setups.
- The discussion highlights the value of data in the current digital landscape.

**Discussion Highlights:** The community consensus is critical of the practice, with calls for accountability and a preference for local setups to ensure privacy.

---

## 19. [Finally managed to run Qwen-2.5-7B on a 4GB GTX 1050 without CPU offloading (Surgical Memory Alignment)](https://reddit.com/r/LocalLLaMA/comments/1po97ad/finally_managed_to_run_qwen257b_on_a_4gb_gtx_1050/)

**Author:** u/HuseyinKama | **Upvotes:** 149 | **Comments:** 48 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses a custom framework called 'QKV Core' that optimizes memory usage for running large language models like Qwen-2.5-7B on low-end GPUs such as the GTX 1050 with 4GB VRAM. The framework uses 'Surgical Alignment' to reduce memory overhead and improve performance, achieving significant VRAM savings and faster I/O load times.

**Key Points:**
- The author developed a custom framework called 'QKV Core' to optimize memory usage for large language models on low-end GPUs.
- The framework uses 'Surgical Alignment' to reduce memory overhead by trimming and realigning memory blocks.
- The optimization saved about 44MB of VRAM, allowing the entire Qwen-2.5-7B model to run purely on GPU without CPU offloading.
- The optimization also improved I/O load times by approximately 34%.
- The project is open-sourced and available on GitHub for others to use and provide feedback.

**Discussion Highlights:** The discussion highlights include praise for the optimization work, skepticism about the code and its effectiveness, questions about the practical application of the framework, and appreciation for the focus on memory efficiency.

---

## 20. [I was bored](https://reddit.com/r/LocalLLaMA/comments/1po8yt0/i_was_bored/)

**Author:** u/MyLovelyAngelKirino | **Upvotes:** 130 | **Comments:** 70 | **Date:** 2025-12-16

**Summary:** The author, who is unemployed with spare time and hardware, built a high-performance computer setup. The post garnered significant attention, with comments focusing on the hardware specifications and playful reactions.

**Key Points:**
- Author built a high-performance computer setup
- Hardware includes 3x 3090 GPUs, 512GB RAM, and an Epyc 7663 56-core CPU
- Community reactions include admiration, humor, and requests for hardware details

**Discussion Highlights:** The discussion highlights the community's interest in the hardware specifications, with some users jokingly expressing envy and others requesting more details about the setup, particularly the water-cooling components.

---

## 21. [Meta announced a new SAM Audio Model for audio editing that can segment sound from complex audio mixtures using text, visual, and time span prompts.](https://reddit.com/r/LocalLLaMA/comments/1po7i0c/meta_announced_a_new_sam_audio_model_for_audio/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 510 | **Comments:** 85 | **Date:** 2025-12-16

**Summary:** Meta introduced SAM Audio Model, a tool for isolating sounds from complex audio mixtures using text, visual, and time span prompts, revolutionizing audio editing.

**Key Points:**
- SAM Audio Model can segment sounds from complex audio mixtures using multiple prompt types.
- Potential applications include filtering out unwanted noises in virtual meetings.
- The model's ability to isolate sounds from visual cues is highly advanced.
- Community interest in model sizes and specific use cases like music instruments.

**Discussion Highlights:** The community is excited about practical applications like noise reduction in meetings and the model's advanced capabilities in sound isolation. There is also interest in technical details like model sizes and specific use cases.

---

## 22. [Allen Institute for AI introduces Molmo 2](https://reddit.com/r/LocalLLaMA/comments/1po78bl/allen_institute_for_ai_introduces_molmo_2/)

**Author:** u/Agitated_Camel1886 | **Upvotes:** 246 | **Comments:** 22 | **Date:** 2025-12-16

**Summary:** Allen Institute for AI has introduced Molmo 2, an 8B model capable of advanced video analysis tasks like Video QA, counting, pointing, and dense captioning. The community is impressed by its capabilities and the public availability of datasets.

**Key Points:**
- Molmo 2 is an 8B model from Allen Institute for AI with advanced video analysis capabilities.
- The model supports tasks like Video QA, counting, pointing, and dense captioning.
- An AMA session was held on r/LocalLLaMA to discuss Olmo 3 and Molmo 2.
- The community appreciates the public release of datasets by Allen AI.
- The model's benchmarks are impressive for its size.

**Discussion Highlights:** The community is highly impressed with Molmo 2's capabilities and the transparency of Allen AI in releasing datasets publicly. There is also excitement about the AMA session and the model's performance benchmarks.

---

## 23. [XiaomiMiMo/MiMo-V2-Flash Â· Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1po3bn4/xiaomimimomimov2flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 239 | **Comments:** 54 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses MiMo-V2-Flash, a Mixture-of-Experts (MoE) language model by XiaomiMiMo with 309B total parameters and 15B active parameters, designed for high-speed reasoning and agentic workflows. Users highlight its impressive performance on multilingual SWE tasks and discuss its technical specifications and potential use cases.

**Key Points:**
- MiMo-V2-Flash is a MoE language model with 309B total parameters and 15B active parameters.
- It is designed for high-speed reasoning and agentic workflows.
- The model shows strong performance on multilingual SWE tasks, surpassing larger models like Sonnet 4.5 and Gemini 3.
- Users discuss the feasibility of running the model on specific hardware configurations.
- The release includes weights and technical documentation for further exploration.

**Discussion Highlights:** Users express excitement about the model's performance and the release of its weights. There is some skepticism about the model's performance claims, but overall, the discussion is positive and focuses on technical details and potential applications.

---

## 24. [GLM-4.5V, GLM-4.6V and GLM_4.6V-Flash are now supported by llama.cpp (GGUFs)](https://reddit.com/r/LocalLLaMA/comments/1po18y9/glm45v_glm46v_and_glm_46vflash_are_now_supported/)

**Author:** u/jacek2023 | **Upvotes:** 168 | **Comments:** 34 | **Date:** 2025-12-16

**Summary:** The post announces that GLM-4.5V, GLM-4.6V, and GLM_4.6V-Flash are now supported by llama.cpp with GGUFs, which is seen as a significant update by the community.

**Key Points:**
- Support for GLM-4.5V, GLM-4.6V, and GLM_4.6V-Flash has been added to llama.cpp.
- The update is celebrated as a great Christmas gift by the community.
- There are questions about whether the GGUFs support vision capabilities.
- Comparisons between Qwen3-VL-4B and GLM_4.6V are being discussed.

**Discussion Highlights:** The community is excited about the new support for GLM models in llama.cpp, with some users expressing gratitude and others discussing technical details and comparisons with other models.

---

## 25. [Qwen3 Next speed optimization has been merged into llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1pnz9xu/qwen3_next_speed_optimization_has_been_merged/)

**Author:** u/jacek2023 | **Upvotes:** 212 | **Comments:** 25 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses the recent speed optimization for Qwen3 Next in llama.cpp, highlighting significant performance improvements across different hardware configurations.

**Key Points:**
- Speed optimization for Qwen3 Next has been merged into llama.cpp.
- Performance on M1 64GB improved from 12 t/s to 18 t/s.
- Other configurations show notable speed gains, such as 37.x t/s on Win11 + RTX5090 + vulkan.
- Qwen3-30B achieves around 58 t/s on the same M1 64GB setup.
- Users report substantial improvements in processing speed.

**Discussion Highlights:** The discussion highlights a consensus on the significant performance improvements, with users reporting notable speed gains across various hardware setups.

---

## 26. [I may have over-quantized this little guy.](https://reddit.com/r/LocalLLaMA/comments/1pnz80z/i_may_have_overquantized_this_little_guy/)

**Author:** u/AllergicToTeeth | **Upvotes:** 141 | **Comments:** 33 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses an over-quantized model, with comments humorously suggesting it surpasses OpenAI's efforts and highlighting the importance of system prompts for model behavior.

**Key Points:**
- The post is about an over-quantized model.
- Comments joke about surpassing OpenAI's models.
- System prompts are mentioned as crucial for model behavior.
- Humorous references to GPT-5 leaks and versions.

**Discussion Highlights:** The discussion is lighthearted, with users joking about the model's capabilities and comparing it to OpenAI's efforts. There's a consensus on the importance of system prompts for proper model functioning.

---

## 27. [It was Ilya who "closed" OpenAI](https://reddit.com/r/LocalLLaMA/comments/1pnxekt/it_was_ilya_who_closed_openai/)

**Author:** u/licuphand | **Upvotes:** 521 | **Comments:** 236 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses Ilya's role in 'closing' OpenAI, sparking a debate on AI governance and trust in companies versus the public. The comments highlight skepticism about corporate control of AI and reference historical concerns about oversight.

**Key Points:**
- Ilya's involvement in OpenAI's direction is a central topic
- Public trust in AI governance is questioned
- Historical references to oversight concerns are made
- Competition among AI leaders (Elon, Ilya, Sam) is noted
- Criticism of corporate control over AI development

**Discussion Highlights:** The discussion emphasizes distrust in corporate AI governance, with comments referencing historical oversight concerns and highlighting competition among AI leaders. There is a consensus that public involvement in AI governance is crucial.

---

## 28. [Alibaba Open-Sources CosyVoice 3, a New TTS Model](https://reddit.com/r/LocalLLaMA/comments/1pnusp9/alibaba_opensources_cosyvoice_3_a_new_tts_model/)

**Author:** u/nekofneko | **Upvotes:** 220 | **Comments:** 31 | **Date:** 2025-12-16

**Summary:** Alibaba has open-sourced CosyVoice 3, a new TTS model with advanced features like multi-lingual support, high naturalness, and low-latency streaming. The model supports various instructions and pronunciation inpainting, making it suitable for production use.

**Key Points:**
- Supports 9 languages and 18+ Chinese dialects with zero-shot voice cloning
- Achieves state-of-the-art performance in consistency, similarity, and naturalness
- Offers low-latency bi-streaming and supports various instructions like emotions and speed
- Users are comparing it to other models like Chatterbox and Microsoft VibeVoice
- There is interest in a potential 1.5B version of the model

**Discussion Highlights:** The discussion highlights comparisons with other TTS models like Chatterbox and Microsoft VibeVoice, with users expressing interest in a larger model version and praising the current release.

---

## 29. [New budget local AI rig](https://reddit.com/r/LocalLLaMA/comments/1pnllux/new_budget_local_ai_rig/)

**Author:** u/vucamille | **Upvotes:** 156 | **Comments:** 38 | **Date:** 2025-12-15

**Summary:** The user built a budget-friendly local AI rig using a Qiyida X99 motherboard, Xeon E5 2680 V4 CPU, and two MI50 16GB GPUs for around $650. The system performs well for AI inference tasks and is easily expandable, with plans to add more GPUs or decorations in the future.

**Key Points:**
- The total cost of the build was approximately $650, with the PSU being the most expensive component.
- The system uses ROCm 7.0.2 and has been tested successfully for basic inference tasks with llama.cpp.
- The community praised the build for its cost-effectiveness and expandability, noting the 32GB VRAM pool and quad-channel DDR4 support.
- Some users requested benchmarks, and the OP mentioned potential future upgrades or modifications.
- The build is also capable of gaming, adding versatility to the setup.

**Discussion Highlights:** The discussion highlights the cost-effectiveness and performance of the build, with users expressing admiration for the value achieved. There is interest in benchmarks and future upgrades, and the community consensus is positive, emphasizing the potential of the system for AI tasks and gaming.

---

## 30. [I'm strong enough to admit that this bugs the hell out of me](https://reddit.com/r/LocalLLaMA/comments/1pnfaqo/im_strong_enough_to_admit_that_this_bugs_the_hell/)

**Author:** u/ForsookComparison | **Upvotes:** 1711 | **Comments:** 359 | **Date:** 2025-12-15

**Summary:** The Reddit post expresses frustration with an unspecified issue, likely related to workstation performance. The discussion includes comparisons between Mac and GPU setups, with some users highlighting the limitations of Macs for certain tasks.

**Key Points:**
- The post title indicates frustration with an unspecified issue.
- The discussion includes comparisons between Mac and GPU workstations.
- Some users argue that Macs are not ideal for tasks requiring full GPU capabilities.
- The post has gained significant attention with 1711 upvotes and 359 comments.

**Discussion Highlights:** The discussion highlights differing opinions on the suitability of Macs versus GPU setups for a 'perfect workstation,' with some users emphasizing the advantages of full GPU setups for certain tasks.

---

## 31. [Bolmo-the first family of competitive fully open byte-level language models (LMs) at the 1B and 7B parameter scales.](https://reddit.com/r/LocalLLaMA/comments/1pndzy7/bolmothe_first_family_of_competitive_fully_open/)

**Author:** u/BreakfastFriendly728 | **Upvotes:** 109 | **Comments:** 23 | **Date:** 2025-12-15

**Summary:** The post introduces Bolmo, the first family of competitive fully open byte-level language models at 1B and 7B parameter scales, which use UTF-8 bytes for tokenization. The community is excited about the open-sourcing of these models and discusses potential future developments.

**Key Points:**
- Bolmo is a family of fully open byte-level language models at 1B and 7B parameter scales.
- Byte-level language models tokenize text into UTF-8 bytes instead of subwords.
- The community is enthusiastic about the open-sourcing of these models.
- Potential future developments include omnimodal capabilities.

**Discussion Highlights:** The community is excited about the open-sourcing of byte-level models and discusses potential future developments like omnimodal capabilities. Some users are curious about the advantages of byte-level models and the availability of GGUF formats.

---

## 32. [They're finally here (Radeon 9700)](https://reddit.com/r/LocalLLaMA/comments/1pnd5uf/theyre_finally_here_radeon_9700/)

**Author:** u/Zeikos | **Upvotes:** 363 | **Comments:** 67 | **Date:** 2025-12-15

**Summary:** The Reddit post announces the arrival of the Radeon 9700 GPUs, sparking community interest and requests for benchmarks and performance data.

**Key Points:**
- Community eagerly awaits benchmarks for the new Radeon 9700 GPUs
- Nostalgia expressed over the Radeon 9700 name from the early 2000s
- Requests for specific benchmarks including inference, training, noise, and heat levels
- Enthusiasm for testing and sharing performance data during the holidays

**Discussion Highlights:** The community is highly engaged and focused on gathering performance data, with a consensus on the need for comprehensive benchmarks to evaluate the new GPUs.

---

## 33. [status of Nemotron 3 Nano support in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1pnc045/status_of_nemotron_3_nano_support_in_llamacpp/)

**Author:** u/jacek2023 | **Upvotes:** 177 | **Comments:** 32 | **Date:** 2025-12-15

**Summary:** The Reddit post discusses the status of Nemotron 3 Nano support in llama.cpp, highlighting a GitHub pull request. The community appreciates Nvidia's effort and emphasizes the importance of collaboration with llama.cpp for new model architectures.

**Key Points:**
- Nemotron 3 Nano support is being added to llama.cpp via a GitHub pull request.
- The community praises Nvidia for their collaboration with llama.cpp.
- There is a discussion about the model sizes and their RAM/VRAM requirements.
- The community encourages other labs to follow Nvidia's example in supporting llama.cpp.
- The consensus is that collaboration with llama.cpp is crucial for new model releases.

**Discussion Highlights:** The discussion highlights the importance of collaboration between model developers and llama.cpp. The community appreciates Nvidia's effort and encourages other labs to follow suit. There is also a focus on the practical aspects of model sizes and their hardware requirements.

---

## 34. [NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model!](https://reddit.com/r/LocalLLaMA/comments/1pn8upp/nvidia_releases_nemotron_3_nano_a_new_30b_hybrid/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 845 | **Comments:** 178 | **Date:** 2025-12-15

**Summary:** NVIDIA has released Nemotron 3 Nano, a 30B hybrid reasoning model with a 1M context window and top performance in SWE-Bench, reasoning, and chat. The model is available for download via Hugging Face.

**Key Points:**
- Nemotron 3 Nano is a 30B hybrid reasoning model with a 1M context window.
- It offers best-in-class performance for SWE-Bench, reasoning, and chat.
- The model is available for download via Hugging Face.
- It is part of the Nemotron 3 family of MoE models, which includes three sizes.
- Users report exceptional speed, with 110 tokens per second generation on local machines.

**Discussion Highlights:** The community is excited about the model's speed and performance. Some users noted that the model was leaked a few days prior to the official release. There is also discussion about the model family, which includes three sizes, and the surprising classification of a 30B model as 'nano'.

---

## 35. [NVIDIA Nemotron 3 Nano 30B A3B released](https://reddit.com/r/LocalLLaMA/comments/1pn8h5h/nvidia_nemotron_3_nano_30b_a3b_released/)

**Author:** u/rerri | **Upvotes:** 281 | **Comments:** 88 | **Date:** 2025-12-15

**Summary:** NVIDIA has released Nemotron 3 Nano 30B A3B, a new model featuring a hybrid Mamba-Transformer MoE architecture, exceptional inference efficiency, and a 1M-token context window. The model is fully open and designed for high throughput and low latency.

**Key Points:**
- Hybrid Mamba-Transformer MoE architecture for efficient inference
- 31.6B total parameters with ~3.6B active per token
- Up to 4x faster than Nemotron Nano 2 and 3.3x faster than leading models in its size category
- 1M-token context window for long-horizon workflows
- Fully open with open weights, datasets, and training recipes

**Discussion Highlights:** The discussion highlights include a Llama.cpp PR for integration, questions about optimal Unsloth quant for specific hardware, concerns about synthetic data training, and performance feedback from users compiling the model.

---

## 36. [Alibaba Tongyi Open Sources Two Audio Models: Fun-CosyVoice 3.0 (TTS) and Fun-ASR-Nano-2512 (ASR)](https://reddit.com/r/LocalLLaMA/comments/1pn7c3f/alibaba_tongyi_open_sources_two_audio_models/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 112 | **Comments:** 25 | **Date:** 2025-12-15

**Summary:** Alibaba Tongyi has open-sourced two audio models: Fun-CosyVoice 3.0 (TTS) and Fun-ASR-Nano-2512 (ASR). These models are lightweight, support local deployment, and offer features like zero-shot voice cloning.

**Key Points:**
- Fun-ASR-Nano is a lightweight ASR model with lower inference costs and support for local deployment.
- Fun-CosyVoice 3.0 offers zero-shot voice cloning and is ready for local deployment.
- The models are open-sourced and available for custom fine-tuning.
- Community feedback highlights the potential to compete with Nvidia's Parakeet and the quality of the models for their size.
- The Italian demo was noted for its quality, despite an odd music intro.

**Discussion Highlights:** The community appreciates the open-sourcing of these models, noting their potential to compete with existing frameworks like Nvidia's Nemo. Users also highlighted the quality of the models, especially for specific languages like Italian, and expressed interest in further developments.

---

## 37. [How to do a RTX Pro 6000 build right](https://reddit.com/r/LocalLLaMA/comments/1pn6ijr/how_to_do_a_rtx_pro_6000_build_right/)

**Author:** u/GPTrack_dot_ai | **Upvotes:** 118 | **Comments:** 175 | **Date:** 2025-12-15

**Summary:** The post discusses building a high-performance system using RTX Pro 6000 GPUs, highlighting the RTX PRO server setup with 8 GPUs, each featuring a 400G networking connection. The system requires additional components like a switch, CPU, RAM, and storage, and is described as ready to use with minimal setup complexity.

**Key Points:**
- The RTX PRO 6000 lacks NVlink, so Nvidia integrated high-speed networking directly into each GPU.
- The RTX PRO server setup includes 8 PCIe slots for RTX Pro 6000 server edition cards, each with a 400G networking connection.
- The system requires additional components like a switch, CPU, RAM, and storage, but is otherwise ready to use.
- The exemplary specs include high-end components like Intel Xeon processors, large amounts of RDIMM or MRDIMM, and multiple storage and networking options.
- User reactions highlight the system's impressive specifications and high cost.

**Discussion Highlights:** Users expressed awe at the system's specifications, comparing it to luxury items like Ferraris and private jets. There was also humor about the cost, with comments suggesting the need for a mortgage to afford it.

---

## 38. [New Google model incoming!!!](https://reddit.com/r/LocalLLaMA/comments/1pn37mw/new_google_model_incoming/)

**Author:** u/R46H4V | **Upvotes:** 1254 | **Comments:** 263 | **Date:** 2025-12-15

**Summary:** The Reddit post discusses anticipation for a new Google model, with users expressing hope for improvements over previous models like Gemma3-Math and potential multi-modal capabilities.

**Key Points:**
- Anticipation for a new Google model
- Hope for improvements over Gemma3-Math
- Desire for multi-modal capabilities
- High engagement with 1254 upvotes and 263 comments

**Discussion Highlights:** Users are hopeful for significant improvements and new features in the upcoming model, with a focus on multi-modal capabilities and performance enhancements.

---

## 39. [llama.cpp: Automation for GPU layers, tensor split, tensor overrides, and context size (with MoE optimizations)](https://reddit.com/r/LocalLLaMA/comments/1pn2e1c/llamacpp_automation_for_gpu_layers_tensor_split/)

**Author:** u/Remove_Ayys | **Upvotes:** 193 | **Comments:** 62 | **Date:** 2025-12-15

**Summary:** The post discusses a new feature in llama.cpp that automates memory allocation for GPU layers, tensor splits, and context size, improving usability and performance, especially for MoE models. The implementation uses virtual test allocations to iteratively reduce memory use and prioritizes dense tensors for better performance.

**Key Points:**
- Automated memory allocation for GPU layers and tensor splits in llama.cpp
- Iterative reduction of memory use using virtual test allocations
- Prioritization of dense tensors for optimal MoE performance
- Generic implementation compatible with any ggml backend supporting CPU + GPU hybrid inference
- Positive community feedback and suggestions for further improvements like caching and multi-GPU support

**Discussion Highlights:** The community appreciates the new feature, with suggestions for caching to eliminate fitting time and requests for better multi-GPU support. There is also interest in special handling for dense models and further optimizations.

---

## 40. [Aaaand... is gone...](https://reddit.com/r/LocalLLaMA/comments/1pmungj/aaaand_is_gone/)

**Author:** u/HumanDrone8721 | **Upvotes:** 940 | **Comments:** 212 | **Date:** 2025-12-14

**Summary:** The Reddit post discusses the discontinuation or scarcity of SATA drives, sparking a conversation about storage solutions and their implications.

**Key Points:**
- The post title suggests the disappearance of something significant.
- Comments indicate a focus on storage drives, particularly SATA drives.
- Users discuss the impact and alternatives, such as buying additional SSDs.
- There is a mix of humor and serious discussion about the topic.

**Discussion Highlights:** The discussion highlights a mix of reactions, from humorous takes to practical advice about storage solutions. Some users downplay the significance, while others see it as a notable event.

---

## 41. [Qwen3-Next-80B-A3B-Thinking-GGUF has just been released on HuggingFace](https://reddit.com/r/LocalLLaMA/comments/1pmo0dn/qwen3next80ba3bthinkinggguf_has_just_been/)

**Author:** u/LegacyRemaster | **Upvotes:** 124 | **Comments:** 54 | **Date:** 2025-12-14

**Summary:** The Reddit post announces the release of Qwen3-Next-80B-A3B-Thinking-GGUF on HuggingFace, highlighting its impressive performance in generating a Tetris game within a single HTML file, outperforming other models like Devstral. The discussion includes user impressions, confusion about the release timing, and technical questions about tool compatibility.

**Key Points:**
- Qwen3-Next-80B-A3B-Thinking-GGUF model released on HuggingFace
- Model excels in generating Tetris in a single HTML file
- Performs better than Devstral in accuracy
- Users express amazement at the model's capabilities
- Discussion includes queries about release timing and tool support

**Discussion Highlights:** Users are highly impressed with the model's performance, though there is some confusion about the release timing. Technical questions about tool compatibility, such as whether llamacpp supports native tool calling with Qwen3-Next, are also raised.

---

## 42. [To Mistral and other lab employees: please test with community tools BEFORE releasing models](https://reddit.com/r/LocalLLaMA/comments/1pmgm2x/to_mistral_and_other_lab_employees_please_test/)

**Author:** u/dtdisapointingresult | **Upvotes:** 138 | **Comments:** 72 | **Date:** 2025-12-14

**Summary:** The Reddit post criticizes Mistral for releasing Devstral 2 without thorough testing with community tools, which has negatively impacted their reputation. The author emphasizes the importance of testing with local tools to ensure smooth adoption by AI geeks and tech enthusiasts.

**Key Points:**
- Devstral 2 release was marred by issues like benchmark discrepancies and repetition loops.
- The author attributes these issues to poor testing, documentation, and embedded templates.
- The post highlights the importance of community tools and the influence of tech geeks in adoption.
- Comments show mixed experiences, with some users praising the model's performance and others noting persistent issues.
- There is a consensus that proper testing with community tools is crucial for successful model releases.

**Discussion Highlights:** The discussion highlights a divide in user experiences, with some praising the model's performance in various applications and others pointing out ongoing issues. There is a general agreement on the importance of thorough testing with community tools to ensure smooth adoption and positive reception.

---

## 43. [Understanding the new router mode in llama cpp server](https://reddit.com/r/LocalLLaMA/comments/1pmc7lk/understanding_the_new_router_mode_in_llama_cpp/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 166 | **Comments:** 44 | **Date:** 2025-12-14

**Summary:** Router mode in llama cpp server allows managing multiple AI models simultaneously without restarting the server, enabling dynamic model switching and efficient memory usage.

**Key Points:**
- Router mode enables loading/unloading models on demand within a single server process.
- It saves memory and simplifies model switching compared to running separate servers for each model.
- Useful for testing multiple GGUF models, building local APIs, and dynamic model switching.
- Discussion highlights include comparisons with llama-swap and requests for better VRAM management.

**Discussion Highlights:** The discussion includes comparisons with llama-swap, requests for better VRAM management, and questions about specifying which models stay in memory concurrently.

---

## 44. [8x RTX Pro 6000 server complete](https://reddit.com/r/LocalLLaMA/comments/1plwgun/8x_rtx_pro_6000_server_complete/)

**Author:** u/koushd | **Upvotes:** 633 | **Comments:** 268 | **Date:** 2025-12-13

**Summary:** The Reddit post details a user's journey upgrading their GPU server to a final configuration of 8x RTX Pro 6000 GPUs (768 GB VRAM), a Threadripper PRO 9955WX CPU, and 384 GB RAM. The user faced challenges with heat management, power consumption, and hardware compatibility during the upgrades.

**Key Points:**
- Final configuration: 8x RTX Pro 6000 GPUs, Threadripper PRO 9955WX CPU, 384 GB RAM
- Challenges included overheating, power management, and hardware compatibility
- User initially used a single 3080 GPU, upgraded to 4090s, and eventually to RTX Pro 6000s
- Discussion highlights include concerns about the setup's physical stability and power requirements
- Some comments praised the setup while others criticized the implementation

**Discussion Highlights:** The discussion includes a mix of praise for the powerful setup and criticism regarding the physical implementation and power management. Notable comments highlight concerns about the setup's stability and the high power consumption.

---

## 45. [Mistral 3 Large is DeepSeek V3!?](https://reddit.com/r/LocalLLaMA/comments/1plpc6h/mistral_3_large_is_deepseek_v3/)

**Author:** u/seraschka | **Upvotes:** 172 | **Comments:** 33 | **Date:** 2025-12-13

**Summary:** The post discusses the architectural similarities between Mistral 3 Large and DeepSeek V3, noting that Mistral 3 Large uses the same architecture as DeepSeek V3 but with adjusted expert configurations. The discussion highlights the open-source spirit and the adoption of DeepSeek V3's architecture by multiple models.

**Key Points:**
- Mistral 3 Large and DeepSeek V3 have nearly identical sizes (671B vs. 673B).
- Mistral 3 Large uses the same architecture as DeepSeek V3 but with larger experts and fewer in number.
- The Mistral team likely trained Mistral 3 from scratch rather than fine-tuning DeepSeek V3.
- Multiple models, including Kimi K2 and Gigachat, have adopted the DeepSeek V3 architecture.
- The open-source community appreciates the sharing and adoption of successful architectures.

**Discussion Highlights:** The discussion emphasizes the spirit of open source, with multiple models adopting the DeepSeek V3 architecture. Users appreciate the innovation and sharing within the community, noting that while Mistral 3 Large uses a similar architecture, it introduces some modifications and innovations like multimodal capabilities.

---

## 46. [OpenAI's flagship model, ChatGPT-5.2 Thinking, ranks  most censored AI on Sansa benchmark.](https://reddit.com/r/LocalLLaMA/comments/1plnuqu/openais_flagship_model_chatgpt52_thinking_ranks/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 623 | **Comments:** 112 | **Date:** 2025-12-13

**Summary:** The Reddit post discusses OpenAI's ChatGPT-5.2 model being ranked as the most censored AI on the Sansa benchmark, with users expressing dissatisfaction over its performance in follow-up questions and clinical note evaluations. Key points include: ChatGPT-5.2 is ranked as the most censored AI on the Sansa benchmark; users report poor performance with follow-up questions and research tasks; the model frequently denies requests for evaluating clinical notes; there is curiosity about the testing criteria used in the benchmark; and Gemini is noted to be less censored than other open models. The discussion highlights user frustration with ChatGPT-5.2's performance and censorship, with comparisons to other models like Gemini and Grok.

---

## 47. [Qwen3 Next generation optimization](https://reddit.com/r/LocalLLaMA/comments/1plng6f/qwen3_next_generation_optimization/)

**Author:** u/ilintar | **Upvotes:** 365 | **Comments:** 40 | **Date:** 2025-12-13

**Summary:** The post discusses optimizations for Qwen3, specifically an autoregressive delta net computation that improves generation speed by 40%. The author invites others to test the optimizations and share their results.

**Key Points:**
- Optimized autoregressive delta net computation for Qwen3
- 40% generation speed improvement reported
- Optimizations include removing unnecessary reshapes and computations
- Author invites community testing and feedback
- Discussion highlights appreciation and curiosity about broader compatibility (e.g., ROCm/Vulkan)

**Discussion Highlights:** The community shows strong appreciation for the optimization work, with comments highlighting the author's frequent contributions and expressing interest in broader compatibility beyond CUDA. There is a consensus of excitement and gratitude for the performance improvements.

---

## 48. [NVIDIA gpt-oss-120b Eagle Throughput model](https://reddit.com/r/LocalLLaMA/comments/1plewrk/nvidia_gptoss120b_eagle_throughput_model/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 245 | **Comments:** 53 | **Date:** 2025-12-13

**Summary:** The post discusses NVIDIA's GPT-OSS-120B-Eagle3-throughput model, an optimized speculative decoding module designed to improve throughput during text generation using Eagle3 speculative decoding. It is licensed for both commercial and non-commercial use.

**Key Points:**
- Optimized speculative decoding module for improved throughput
- Uses NVIDIAâs Eagle3 speculative decoding approach
- Licensed under nvidia-open-model-license for commercial and non-commercial use
- Not supported in llama.cpp, as per community discussion
- Community interest in derestricted versions and CPU inference capabilities

**Discussion Highlights:** The community shows interest in derestricted versions and CPU inference capabilities. There is also a noted lack of support in llama.cpp, which some users find disappointing. The post gained significant attention, as indicated by upvotes and comments.

---

## 49. [This is how open ai is advertising them selfs on redditâ¦. They are doomed](https://reddit.com/r/LocalLLaMA/comments/1plcrg8/this_is_how_open_ai_is_advertising_them_selfs_on/)

**Author:** u/ThinkExtension2328 | **Upvotes:** 236 | **Comments:** 77 | **Date:** 2025-12-12

**Summary:** The Reddit post criticizes OpenAI's advertising strategy, highlighting a shift from promoting advanced AI to using astrology ads, which users find inconsistent and unappealing.

**Key Points:**
- OpenAI's advertising shift from AI advancements to astrology ads
- Criticism of the inconsistency in OpenAI's messaging
- Discussion on the profitability of targeting horoscope believers
- Suggestions for alternative advertising strategies
- Observations on OpenAI's perceived decline in reputation

**Discussion Highlights:** Users express disappointment in OpenAI's advertising approach, suggesting it may be more profitable but damages their credibility. There is a consensus that the shift from technical advancements to astrology ads is a significant fall from grace.

---

## 50. [Running an LLM on a 3DS](https://reddit.com/r/LocalLLaMA/comments/1pl4njj/running_an_llm_on_a_3ds/)

**Author:** u/vreab | **Upvotes:** 297 | **Comments:** 35 | **Date:** 2025-12-12

**Summary:** The Reddit post discusses the feasibility and performance of running an LLM on a 3DS, drawing comparisons to similar projects on platforms like the PS Vita and Wii. The community expresses admiration for the technical achievement and curiosity about potential performance improvements on newer hardware.

**Key Points:**
- Running an LLM on a 3DS is technically impressive and feasible.
- Similar projects have been done on platforms like the PS Vita and Wii.
- Community members are curious about performance improvements on newer hardware like the 'new' 3DS.
- There is admiration for the technical achievement and potential applications in gaming.

**Discussion Highlights:** The discussion highlights the technical impressiveness of running an LLM on a 3DS, with comparisons to similar projects on other platforms. There is curiosity about potential performance improvements and admiration for the technical achievement.

---

