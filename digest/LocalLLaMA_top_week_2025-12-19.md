# r/LocalLLaMA Reading Digest

**Period:** 2025-12-19 to 2025-12-19
**Posts Summarized:** 50
**Total Posts Analyzed:** 50

---

## 1. [Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster](https://reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/)

**Author:** u/geerlingguy | **Upvotes:** 400 | **Comments:** 111 | **Date:** 2025-12-18

**Summary:** The post discusses performance testing of Kimi K2 on a cluster of 4x Mac Studios, highlighting the use of RDMA Tensor settings and the challenges in benchmarking. The author, u/geerlingguy, mentions ongoing testing and the lack of straightforward benchmarking tools like llama-bench in Exo.

**Key Points:**
- Performance testing of Kimi K2 on a 4x Mac Studio cluster with RDMA Tensor settings.
- Challenges in benchmarking due to the lack of tools like llama-bench in Exo.
- Ongoing testing and debugging of RDMA support.
- Mention of upcoming Apple Silicon ultra chips with MATMUL instructions for potential performance improvements.
- Recognition of the author's contribution with a special flair and feature on Discord.

**Discussion Highlights:** The discussion highlights the author's significant contribution and the potential for future performance improvements with new Apple Silicon chips. There is also appreciation for the detailed testing and data shared by the author.

---

## 2. [T5Gemma 2: The next generation of encoder-decoder models](https://reddit.com/r/LocalLLaMA/comments/1ppzhtq/t5gemma_2_the_next_generation_of_encoderdecoder/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 178 | **Comments:** 23 | **Date:** 2025-12-18

**Summary:** T5Gemma 2 models, based on Gemma 3, are multilingual and multimodal, handling text and image input with open weights for three pretrained sizes (270M, 1B, and 4B). They feature tied embeddings, merged attention, multimodality, extended long context, and support for over 140 languages.

**Key Points:**
- T5Gemma 2 models are based on Gemma 3 and are multilingual and multimodal.
- Key features include tied embeddings, merged attention, multimodality, extended long context, and support for over 140 languages.
- Models are available in three sizes: 270M, 1B, and 4B.
- The community is excited about the return of encoder-decoder models and potential applications in multimodal translation.
- There is anticipation for GGUF format availability.

**Discussion Highlights:** The community is enthusiastic about the new encoder-decoder model, with comments highlighting its potential for multimodal translation and expressing anticipation for future developments like GGUF format availability.

---

## 3. [Google's Gemma models family](https://reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/)

**Author:** u/jacek2023 | **Upvotes:** 448 | **Comments:** 110 | **Date:** 2025-12-18

**Summary:** The Reddit post discusses Google's Gemma models family, highlighting the introduction of FunctionGemma and community reactions. The discussion includes insights on new models and community engagement.

**Key Points:**
- Introduction of FunctionGemma for fine-tuning specific tasks
- Community excitement and engagement with the new models
- Speculation about the number of new Gemma models
- Positive sentiment towards Google's contributions

**Discussion Highlights:** The discussion highlights the community's enthusiasm for FunctionGemma and the potential for new Gemma models. There is a consensus on the positive impact of Google's contributions to the field.

---

## 4. [MiraTTS: High quality and fast TTS model](https://reddit.com/r/LocalLLaMA/comments/1pper90/miratts_high_quality_and_fast_tts_model/)

**Author:** u/SplitNice1982 | **Upvotes:** 137 | **Comments:** 52 | **Date:** 2025-12-17

**Summary:** MiraTTS is a high-quality, fast TTS model that generates realistic 48khz speech at 100x realtime, optimized for efficiency and low latency. It supports multilingual versions and is memory-efficient, working with GPUs as low as 6GB VRAM.

**Key Points:**
- 100x realtime speed
- High-quality 48khz speech
- Memory efficient (6GB VRAM)
- Low latency (150ms)
- Multilingual support

**Discussion Highlights:** The discussion highlights curiosity about multilingual support, voice cloning, and comparisons with other TTS models like KaniTTS. Users appreciate the work and express interest in trying the model.

---

## 5. [AMA with the Meta researchers behind SAM 3 + SAM 3D + SAM Audio](https://reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/)

**Author:** u/AIatMeta | **Upvotes:** 125 | **Comments:** 68 | **Date:** 2025-12-17

**Summary:** The post announces an AMA with Meta researchers behind SAM 3, SAM 3D, and SAM Audio, highlighting their capabilities and providing links to learn more. The discussion includes questions about model performance, architecture, and practical applications.

**Key Points:**
- AMA session with Meta researchers for SAM 3, SAM 3D, and SAM Audio
- Models are part of the Segment Anything collection
- Discussion includes questions on segmentation capabilities, voice separation, and model architecture
- Links provided for further learning and a playground for testing the models
- AMA scheduled for December 18, 2-3pm PT

**Discussion Highlights:** Users are interested in the practical applications and limitations of the models, such as segmenting multiple objects, voice separation for home assistants, and stem creation for music. There is also curiosity about the architectural similarities between the models.

---

## 6. [Nvidia plans heavy cuts to GPU supply in early 2026](https://reddit.com/r/LocalLLaMA/comments/1pp8vo4/nvidia_plans_heavy_cuts_to_gpu_supply_in_early/)

**Author:** u/HumanDrone8721 | **Upvotes:** 346 | **Comments:** 171 | **Date:** 2025-12-17

**Summary:** Nvidia plans to significantly reduce GPU supply in early 2026, which, combined with similar cuts by Micron and Samsung, could make building gaming PCs challenging. The discussion highlights concerns about market competition and corporate spending priorities.

**Key Points:**
- Nvidia's GPU supply cuts in early 2026
- Micron and Samsung also reducing consumer RAM and SSD production
- Potential challenges for gaming PC builders in 2026
- Concerns about corporate spending on stock buybacks instead of growth
- Opportunities for new competition in the market

**Discussion Highlights:** The discussion reflects a consensus that 2026 will be a difficult year for PC builders due to supply cuts. There is also speculation about the impact on market competition and criticism of corporate financial strategies.

---

## 7. [Hey, LocalLLaMa. We need to talk...](https://reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/)

**Author:** u/Eisenstein | **Upvotes:** 388 | **Comments:** 124 | **Date:** 2025-12-17

**Summary:** The post encourages the r/LocalLLaMA community to engage more with smaller projects by providing feedback and upvotes, emphasizing the importance of supporting open-source contributions. The discussion highlights mixed reactions, with some agreeing on the need for engagement while others criticize the quality of certain projects. Key points include the encouragement to engage with and support smaller projects, the importance of providing feedback and upvotes, mixed reactions in the comments, the need for constructive feedback, and the effort required to create and share projects. The discussion reveals a divide in the community, with some members supporting the call for engagement and others expressing frustration with the quality of certain projects, but there is a consensus on the importance of constructive feedback and recognition for contributors.

---

## 8. [Drummer's Cydonia and Magidonia 24B v4.3 - The best pair of Cydonia for RP yet!](https://reddit.com/r/LocalLLaMA/comments/1pp2j60/drummers_cydonia_and_magidonia_24b_v43_the_best/)

**Author:** u/TheLocalDrummer | **Upvotes:** 130 | **Comments:** 20 | **Date:** 2025-12-17

**Summary:** The post announces the release of Drummer's Cydonia and Magidonia 24B v4.3 models, praised as the best pair for role-playing yet. The author expresses gratitude to patrons for their support and shares links to the models on Hugging Face. Key points include the release of the models, their high praise for role-playing, the author's gratitude, and positive user feedback. The discussion highlights appreciation for the models and technical tips shared by users.

---

## 9. [Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds.](https://reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/)

**Author:** u/themixtergames | **Upvotes:** 1124 | **Comments:** 129 | **Date:** 2025-12-17

**Summary:** Apple has introduced SHARP, a model that can generate photorealistic 3D Gaussian representations from a single image in seconds. The model is showcased with examples rendered in real-time on Apple Vision Pro and generated on a MacBook Pro M1 Max.

**Key Points:**
- SHARP generates 3D Gaussian representations from a single image.
- Examples were rendered in real-time on Apple Vision Pro.
- Scenes were generated in 5–10 seconds on a MacBook Pro M1 Max.
- The model requires CUDA GPU for rendering trajectories.
- Community reactions include comparisons to cyberpunk's braindance and inquiries about content compatibility.

**Discussion Highlights:** The community showed enthusiasm for the technology, with comparisons to cyberpunk's braindance and questions about its capabilities and limitations. The top comments highlighted the real-time rendering capabilities and the speed of scene generation.

---

## 10. [LangChain and LlamaIndex are in "steep decline" according to new ecosystem report. Anyone else quietly ditching agent frameworks?](https://reddit.com/r/LocalLLaMA/comments/1pox733/langchain_and_llamaindex_are_in_steep_decline/)

**Author:** u/Exact-Literature-395 | **Upvotes:** 204 | **Comments:** 57 | **Date:** 2025-12-17

**Summary:** The Reddit post discusses the decline of LangChain and LlamaIndex frameworks, citing reduced community activity and investment. Users share their experiences of moving away from these frameworks due to complexity and lack of necessity with improved base models.

**Key Points:**
- LangChain, LlamaIndex, and AutoGen are listed as 'steepest declining' projects by community activity.
- Users report better results by calling APIs directly instead of using these frameworks.
- Criticisms include bloated features, poor security/performance, and non-pythonic code.
- Maintainers acknowledge the shift but highlight the frameworks' initial ease of integration.
- Discussion suggests a trend towards simpler, more direct approaches in LLM development.

**Discussion Highlights:** The discussion highlights a consensus that agent frameworks like LangChain and LlamaIndex are becoming less essential as base models improve. Users express frustration with the complexity and lack of transparency in these frameworks, preferring direct API calls and simpler code structures. The maintainer of LlamaIndex acknowledges the shift but emphasizes the frameworks' initial role in facilitating community contributions.

---

## 11. [anthropic blog on code execution for agents. 98.7% token reduction sounds promising for local setups](https://reddit.com/r/LocalLLaMA/comments/1powhy6/anthropic_blog_on_code_execution_for_agents_987/)

**Author:** u/Zestyclose_Ring1123 | **Upvotes:** 132 | **Comments:** 33 | **Date:** 2025-12-17

**Summary:** Anthropic's blog discusses a new approach to code execution for agents, claiming a 98.7% token reduction, which could significantly benefit local setups by reducing context limits and improving privacy. The approach involves letting models explore tools on demand rather than preloading all tool definitions.

**Key Points:**
- Anthropic's approach reduces token usage by 98.7%, making it promising for local setups.
- The method involves model-generated code to orchestrate tools, reducing context limits.
- Privacy is enhanced as sensitive data flows directly between tools without entering the model context.
- Sandboxing is a main challenge for running model-generated code locally.
- Similar patterns already exist in projects like HF's smolagents and other implementations.

**Discussion Highlights:** The discussion highlights that similar patterns already exist in other projects like HF's smolagents, with some users pointing out that Anthropic might be presenting existing ideas as their own. There is also mention of using DAGs (Directed Acyclic Graphs) for tool orchestration to reduce sandboxing needs and avoid non-terminating constructs.

---

## 12. [Peak LLM Wars: Xiaomi Blocks Kimi Employees on Twitter](https://reddit.com/r/LocalLLaMA/comments/1pow797/peak_llm_wars_xiaomi_blocks_kimi_employees_on/)

**Author:** u/nekofneko | **Upvotes:** 130 | **Comments:** 26 | **Date:** 2025-12-17

**Summary:** The Reddit post discusses the ongoing LLM wars, highlighting Xiaomi blocking Kimi employees on Twitter. The post includes images and comments that add context to the situation.

**Key Points:**
- Xiaomi blocking Kimi employees on Twitter
- Mention of former DeepSeek members in Xiaomi team
- Comparison to other tech industry beefs
- Reference to r/vtuberdrama but for LLMs

**Discussion Highlights:** The discussion includes comments about the meme format, speculation about team members, comparisons to other industry conflicts, and humorous references to other drama communities.

---

## 13. [Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model](https://reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 1136 | **Comments:** 120 | **Date:** 2025-12-17

**Summary:** Microsoft's TRELLIS 2-4B is an open-source image-to-3D model with 4 billion parameters, capable of generating 3D assets from single images. The model uses Flow-Matching Transformers with Sparse Voxel based 3D VAE and has received significant attention on Reddit with 1136 upvotes and 120 comments.

**Key Points:**
- Model Type: Flow-Matching Transformers with Sparse Voxel based 3D VAE
- Parameters: 4 Billion
- Input: Single Image, Output: 3D Asset
- Model, demo, and blog post links provided
- Mixed community reactions with some praising the results and others finding limitations

**Discussion Highlights:** The community discussion highlights mixed reactions, with some users praising the model's results and others pointing out limitations in practical applications. There is also a suggestion to improve the model by allowing a series of images as input.

---

## 14. [QwenLong-L1.5: Revolutionizing Long-Context AI](https://reddit.com/r/LocalLLaMA/comments/1pokpha/qwenlongl15_revolutionizing_longcontext_ai/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 209 | **Comments:** 27 | **Date:** 2025-12-16

**Summary:** QwenLong-L1.5 is a new AI model that achieves state-of-the-art long-context reasoning with novel data synthesis, stabilized RL, and memory management for contexts up to 4M tokens. The model is available on HuggingFace and has generated significant interest in the community.

**Key Points:**
- QwenLong-L1.5 achieves SOTA long-context reasoning with novel data synthesis, stabilized RL, and memory management for contexts up to 4M tokens.
- The model is available on HuggingFace under the name QwenLong-L1.5-30B-A3B.
- Integration into llama.cpp may require some work.
- The model uses a specific query template for optimal performance.
- Community feedback highlights the model's significance and potential for improvement in visual representation.

**Discussion Highlights:** The community is excited about the model's capabilities and potential applications. Some users have noted the need for better visual representation in graphs and the requirement for additional work to integrate the model into existing frameworks like llama.cpp. There is also a mention of the importance of using the exact query template provided by the model's creators for optimal performance.

---

## 15. [8x Radeon 7900 XTX Build for Longer Context Local Inference - Performance Results &amp; Build Details](https://reddit.com/r/LocalLLaMA/comments/1pogwb6/8x_radeon_7900_xtx_build_for_longer_context_local/)

**Author:** u/Beautiful_Trust_8151 | **Upvotes:** 717 | **Comments:** 210 | **Date:** 2025-12-16

**Summary:** The post describes a custom multi-GPU setup using 8x AMD Radeon 7900 XTX cards for local AI inference, highlighting performance results and build details. The system demonstrates stable performance with long-context capabilities and is praised for its customizability and cost-effectiveness.

**Key Points:**
- The system uses 8x AMD Radeon 7900 XTX cards with 192 GB VRAM total, paired with an Intel Core i7-14700F and 192 GB system RAM.
- Performance testing shows stable results with GLM4.5Air q6, maintaining over 200 tokens per second for prompt processing even with a filled context.
- The total build cost is around $6-7k, offering a cost-effective solution for long-context AI inference.
- The setup is praised for its upgradability, customizability, and genuine long-context capability.
- Suggestions from the discussion include switching to Linux, ROCm, and vLLM for potentially better performance.

**Discussion Highlights:** The discussion highlights appreciation for the innovative GPU build, with suggestions for further optimization using Linux and ROCm. There is consensus on the cost-effectiveness and performance of the setup, with interest in additional benchmarking with other models like Qwen3-235B-A22B.

---

## 16. [Nemotron 3 Nano 30B is Amazing! (TLDR)](https://reddit.com/r/LocalLLaMA/comments/1pocsdy/nemotron_3_nano_30b_is_amazing_tldr/)

**Author:** u/DonkeyBonked | **Upvotes:** 203 | **Comments:** 126 | **Date:** 2025-12-16

**Summary:** The post discusses the user's experience with Nemotron 3 Nano 30B, highlighting its token efficiency and performance on their hardware setup. The user compares it favorably to other models like Devstral 2 Small 24B and Qwen models, noting its ability to handle large contexts efficiently.

**Key Points:**
- Nemotron 3 Nano 30B shows impressive token efficiency, fitting 256k tokens in VRAM and handling up to 1M context with spillover.
- The model performs well on the user's hardware setup, which includes an RTX 5000 and an RTX 3090 eGPU.
- Comparisons with other models like Devstral 2 Small 24B and Qwen models show Nemotron's superior performance in coding tasks.
- Users in the comments discuss the model's speed, performance, and open-source nature, with some preferring Qwen models for certain tasks.
- The model's hybrid architecture (Mamba2 hybrid MoE) is noted, with suggestions to compare it with similar models like IBM Granite 4 Hybrid Small.

**Discussion Highlights:** The discussion highlights the model's efficiency and performance, with users sharing their experiences and comparisons with other models. There is a consensus on the model's strengths, though some users prefer other models for specific tasks. The open-source nature of Nemotron is also praised.

---

## 17. [32GB Mi50's were getting so expensive that I ended up buying a 32GB w6800 for about the same price instead](https://reddit.com/r/LocalLLaMA/comments/1pob44f/32gb_mi50s_were_getting_so_expensive_that_i_ended/)

**Author:** u/EmPips | **Upvotes:** 229 | **Comments:** 42 | **Date:** 2025-12-16

**Summary:** The author opted for a 32GB w6800 GPU instead of a 32GB Mi50 due to similar pricing, highlighting the pros and cons of the w6800. The discussion includes comparisons with other GPUs like the AMD Radeon AI PRO R9700 and Zotac 3090.

**Key Points:**
- The w6800 was chosen for its convenience and cooling performance.
- The AMD Radeon AI PRO R9700 was suggested as a more expensive but faster alternative.
- Zotac 3090s were available at a competitive price point.
- The w6800's blower-style cooler was noted for its effectiveness.
- The decision was influenced by cost and performance considerations.

**Discussion Highlights:** The discussion revolves around the trade-offs between different GPUs, with a focus on price, performance, and cooling solutions. The consensus leans towards the w6800 for its balance of cost and convenience, though alternatives like the R9700 and 3090 are also considered viable.

---

## 18. [8 Million Users' AI Conversations Sold for Profit by "Privacy" Extensions | Koi Blog](https://reddit.com/r/LocalLLaMA/comments/1poal2a/8_million_users_ai_conversations_sold_for_profit/)

**Author:** u/ManThigh | **Upvotes:** 159 | **Comments:** 47 | **Date:** 2025-12-16

**Summary:** The Reddit post highlights privacy concerns regarding browser extensions selling AI conversations of millions of users for profit, emphasizing the importance of using local models and auditing extensions.

**Key Points:**
- Browser extensions like Urban VPN Proxy and 1ClickVPN Proxy sold user AI conversations.
- Over 6 million users were affected by these extensions.
- The post advocates for using local models to avoid such privacy breaches.
- Discussion includes calls for punishing companies that buy such data.
- Users express pride in their local setups and caution against browser-based interfaces.

**Discussion Highlights:** The discussion consensus emphasizes the importance of privacy, with users advocating for local setups and expressing concern over data being sold by extensions. There is a strong sentiment against companies that buy such data.

---

## 19. [Finally managed to run Qwen-2.5-7B on a 4GB GTX 1050 without CPU offloading (Surgical Memory Alignment)](https://reddit.com/r/LocalLLaMA/comments/1po97ad/finally_managed_to_run_qwen257b_on_a_4gb_gtx_1050/)

**Author:** u/HuseyinKama | **Upvotes:** 150 | **Comments:** 48 | **Date:** 2025-12-16

**Summary:** The post discusses a method called 'Surgical Memory Alignment' that allows running Qwen-2.5-7B on a 4GB GTX 1050 without CPU offloading by optimizing memory usage and reducing padding overhead, resulting in significant VRAM savings and speed improvements.

**Key Points:**
- Standard GGUF quantization tools add padding that causes memory issues on low-end GPUs.
- Surgical Alignment trims and realigns memory blocks to reduce waste and improve efficiency.
- The method saved about 44MB per model, allowing Qwen-2.5-7B to run purely on GPU.
- Speed improvements of ~34% in I/O load times were observed.
- The project, QKV Core, is open-sourced for community feedback.

**Discussion Highlights:** The discussion includes skepticism about the code's effectiveness, appreciation for the optimization efforts, and questions about the practical application of the method for users with limited VRAM.

---

## 20. [I was bored](https://reddit.com/r/LocalLLaMA/comments/1po8yt0/i_was_bored/)

**Author:** u/MyLovelyAngelKirino | **Upvotes:** 132 | **Comments:** 70 | **Date:** 2025-12-16

**Summary:** The author, who is unemployed, built a high-performance computer setup with excess hardware, sparking community interest and playful reactions.

**Key Points:**
- Author built a powerful computer setup due to unemployment and excess hardware
- Setup includes 3x 3090 GPUs, 512GB RAM, and an Epyc 7663 56-core CPU
- Community reactions range from admiration to playful envy
- Interest in hardware details, particularly water-cooling components
- Discussion about the neatness and potential for additional GPUs

**Discussion Highlights:** The community showed strong interest in the hardware specifications and setup details, with some users expressing playful envy. There was a focus on the neatness of the build and requests for more information on components like water-cooling.

---

## 21. [Meta announced a new SAM Audio Model for audio editing that can segment sound from complex audio mixtures using text, visual, and time span prompts.](https://reddit.com/r/LocalLLaMA/comments/1po7i0c/meta_announced_a_new_sam_audio_model_for_audio/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 501 | **Comments:** 85 | **Date:** 2025-12-16

**Summary:** Meta announced a new SAM Audio Model that revolutionizes audio editing by allowing users to isolate any sound from complex audio mixtures using text, visual, and time span prompts. The model has garnered significant attention, with 501 upvotes and 85 comments on the Reddit post.

**Key Points:**
- SAM Audio Model enables easy isolation of sounds from complex audio mixtures using various prompts.
- The model has potential applications like filtering out unwanted noises in virtual meetings.
- Users are impressed by the model's ability to accurately pick out specific sounds from complex audio.
- Model sizes and specifications are available for reference.
- The model can handle intricate audio details, such as isolating a microphone tap.

**Discussion Highlights:** The discussion highlights the model's potential for practical applications, such as improving audio quality in virtual meetings by filtering out unwanted noises. Users also express amazement at the model's precision in isolating specific sounds from complex audio mixtures. There is a general consensus on the model's impressive capabilities and its potential impact on audio editing.

---

## 22. [Allen Institute for AI introduces Molmo 2](https://reddit.com/r/LocalLLaMA/comments/1po78bl/allen_institute_for_ai_introduces_molmo_2/)

**Author:** u/Agitated_Camel1886 | **Upvotes:** 241 | **Comments:** 22 | **Date:** 2025-12-16

**Summary:** The Allen Institute for AI has introduced Molmo 2, an 8B model capable of advanced video analysis tasks like Video QA, counting, pointing, and dense captioning. The community is impressed by its capabilities and the public availability of datasets.

**Key Points:**
- Molmo 2 is an 8B model from Allen Institute for AI with advanced video analysis capabilities.
- The model supports tasks like Video QA, counting, pointing, and dense captioning.
- An AMA was held on r/LocalLLaMA to discuss Olmo 3 and Molmo 2.
- The community appreciates the public release of datasets by Allen AI.
- The model's benchmarks are impressive for its size.

**Discussion Highlights:** The community is highly impressed with Molmo 2's capabilities, especially its video analysis features. There is also appreciation for the public release of datasets, which aids in further advancements. An AMA was conducted to discuss the model and related topics.

---

## 23. [XiaomiMiMo/MiMo-V2-Flash · Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1po3bn4/xiaomimimomimov2flash_hugging_face/)

**Author:** u/Dark_Fire_12 | **Upvotes:** 234 | **Comments:** 51 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses MiMo-V2-Flash, a Mixture-of-Experts (MoE) language model with 309B total parameters and 15B active parameters, designed for high-speed reasoning and agentic workflows. The model's performance on the SWE-Bench is notably strong, surpassing larger models like Sonnet 4.5 and Gemini 3. The discussion includes queries about larger versions and hardware requirements for running the model.

**Key Points:**
- MiMo-V2-Flash is a MoE model with 309B total parameters and 15B active parameters.
- It shows strong performance on the SWE-Bench, outperforming larger models.
- The model's weights have been released, allowing for public use.
- Discussion includes questions about larger versions and hardware feasibility.
- Links to the tech report and blog are provided for further details.

**Discussion Highlights:** The discussion highlights the model's impressive performance and the feasibility of running it on specific hardware configurations. There is also curiosity about potential larger versions of the model.

---

## 24. [GLM-4.5V, GLM-4.6V and GLM_4.6V-Flash are now supported by llama.cpp (GGUFs)](https://reddit.com/r/LocalLLaMA/comments/1po18y9/glm45v_glm46v_and_glm_46vflash_are_now_supported/)

**Author:** u/jacek2023 | **Upvotes:** 167 | **Comments:** 34 | **Date:** 2025-12-16

**Summary:** The post announces support for GLM-4.5V, GLM-4.6V, and GLM_4.6V-Flash in llama.cpp with GGUFs, highlighting a significant update for the community.

**Key Points:**
- Support for GLM-4.5V, GLM-4.6V, and GLM_4.6V-Flash has been added to llama.cpp.
- The update is seen as a valuable Christmas gift by the community.
- There are questions about whether the GGUFs support vision capabilities.
- Comparisons between Qwen3-VL-4B and GLM_4.6V are being discussed.

**Discussion Highlights:** The community is excited about the new support, with some users expressing gratitude and others discussing technical details and comparisons with other models.

---

## 25. [Qwen3 Next speed optimization has been merged into llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1pnz9xu/qwen3_next_speed_optimization_has_been_merged/)

**Author:** u/jacek2023 | **Upvotes:** 213 | **Comments:** 25 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses the recent speed optimization for Qwen3 Next in llama.cpp, highlighting significant performance improvements across different hardware configurations.

**Key Points:**
- Speed optimization for Qwen3 Next has been merged into llama.cpp
- Performance on M1 64GB improved from 12 t/s to 18 t/s
- Qwen3-30B achieves around 58 t/s on the same hardware
- Win11 + RTX5090 + vulkan setup achieves 37.x t/s without CUDA
- Over 100 t/s achievable with UD-Q2_K_XL without CPU offloading

**Discussion Highlights:** Users report significant performance gains, with notable improvements on various hardware setups, indicating a successful optimization effort.

---

## 26. [I may have over-quantized this little guy.](https://reddit.com/r/LocalLLaMA/comments/1pnz80z/i_may_have_overquantized_this_little_guy/)

**Author:** u/AllergicToTeeth | **Upvotes:** 139 | **Comments:** 30 | **Date:** 2025-12-16

**Summary:** The Reddit post humorously discusses the potential over-quantization of a model, with comments suggesting it might be a significant achievement for the open-source community. The discussion includes technical details about system prompts and quantization levels, along with playful comparisons to advanced AI models.

**Key Points:**
- The post title suggests the author may have over-quantized a model.
- Comments humorously compare the model to advanced AI models like GPT-5.
- Technical details include the use of system prompts and quantization levels like Q0.
- The discussion highlights the potential significance of the model for the open-source community.

**Discussion Highlights:** The discussion is lighthearted and technical, with users joking about the model's capabilities while also providing insights into the importance of system prompts and quantization in model behavior.

---

## 27. [It was Ilya who "closed" OpenAI](https://reddit.com/r/LocalLLaMA/comments/1pnxekt/it_was_ilya_who_closed_openai/)

**Author:** u/licuphand | **Upvotes:** 511 | **Comments:** 231 | **Date:** 2025-12-16

**Summary:** The Reddit post discusses Ilya's role in 'closing' OpenAI, sparking a debate on trust in AI governance and leadership conflicts among key figures like Elon, Ilya, and Sam.

**Key Points:**
- Distrust in companies handling AI if the public cannot be trusted with it
- Historical context of oversight with the phrase 'Who will watch the watchmen' being nearly 2000 years old
- Leadership conflicts among Elon, Ilya, and Sam, each vying for control and glory
- Criticism of the philosophy behind closing AI development to the public
- Mention of SSI, xAI, and OpenAI all moving towards being 'CloseAI'

**Discussion Highlights:** The discussion highlights a consensus on the dangers of centralized control over AI, with many users expressing skepticism about the motives of key figures in the AI industry and the historical context of oversight and trust.

---

## 28. [Alibaba Open-Sources CosyVoice 3, a New TTS Model](https://reddit.com/r/LocalLLaMA/comments/1pnusp9/alibaba_opensources_cosyvoice_3_a_new_tts_model/)

**Author:** u/nekofneko | **Upvotes:** 215 | **Comments:** 31 | **Date:** 2025-12-16

**Summary:** Alibaba has open-sourced CosyVoice 3, a new TTS model with advanced features like multi-language support, high naturalness, and low latency. The model supports various instructions and text normalization, making it suitable for production use.

**Key Points:**
- Supports 9 common languages and 18+ Chinese dialects/accents
- Achieves state-of-the-art performance in content consistency and naturalness
- Features bi-streaming with latency as low as 150ms
- Supports pronunciation inpainting and text normalization
- Discussion highlights include comparisons with other models like Chatterbox and Microsoft VibeVoice

**Discussion Highlights:** The discussion focuses on comparisons with other TTS models, inquiries about larger model releases, and the model's voice cloning capabilities. Users express enthusiasm and interest in the model's performance and features.

---

## 29. [New budget local AI rig](https://reddit.com/r/LocalLLaMA/comments/1pnllux/new_budget_local_ai_rig/)

**Author:** u/vucamille | **Upvotes:** 154 | **Comments:** 38 | **Date:** 2025-12-15

**Summary:** The author built a budget-friendly local AI rig using a Qiyida X99 motherboard, Xeon E5 2680 V4 CPU, 32GB RAM, and two MI50 16GB GPUs for around $650. The system performs well with ROCm 7.0.2 and supports multi-GPU inference, with plans for future upgrades.

**Key Points:**
- Budget build with high performance: $650 for a system with 32GB RAM and dual MI50 GPUs.
- ROCm 7.0.2 enables multi-GPU functionality, though initial attempts with newer ROCm versions failed.
- Community praise for cost-effectiveness and expandability, with benchmarks showing strong performance for models like gpt-oss-20b.
- Future plans include adding brackets for GPU sag and potential upgrades to 32GB GPUs when prices drop.
- The rig doubles as a gaming PC, adding versatility.

**Discussion Highlights:** The community highlighted the build's cost-effectiveness and expandability, with benchmarks confirming strong performance for AI tasks. Users expressed interest in further benchmarks and multi-GPU optimization, while praising the author's achievement compared to more expensive alternatives.

---

## 30. [I'm strong enough to admit that this bugs the hell out of me](https://reddit.com/r/LocalLLaMA/comments/1pnfaqo/im_strong_enough_to_admit_that_this_bugs_the_hell/)

**Author:** u/ForsookComparison | **Upvotes:** 1697 | **Comments:** 356 | **Date:** 2025-12-15

**Summary:** The post expresses frustration about a computing-related issue, with comments discussing RAM, workstation performance, and humor around technical setups.

**Key Points:**
- Post title indicates frustration with a specific issue
- Comments include humor about RAM and workstation performance
- Discussion highlights differences between Mac and GPU setups
- Meme references to RAM and computing power

**Discussion Highlights:** The discussion is a mix of humor and technical debate, with some users joking about RAM and others comparing Mac and GPU workstation performance.

---

## 31. [They're finally here (Radeon 9700)](https://reddit.com/r/LocalLLaMA/comments/1pnd5uf/theyre_finally_here_radeon_9700/)

**Author:** u/Zeikos | **Upvotes:** 364 | **Comments:** 67 | **Date:** 2025-12-15

**Summary:** The post announces the arrival of Radeon 9700 GPUs, sparking community excitement and requests for benchmarks. Users express nostalgia about the historic GPU name and eagerly await performance data.

**Key Points:**
- Radeon 9700 GPUs have arrived, generating community interest
- Users are requesting comprehensive benchmarks (inference, training, noise/heat levels)
- Nostalgia expressed over the historic Radeon 9700 name from the 2000s
- Community wants performance comparisons and data for evaluation
- Specific benchmark types requested: inference, training/fine-tuning, and thermal performance

**Discussion Highlights:** The community shows strong enthusiasm for the new GPUs, with a consensus on the need for detailed benchmarks. There's a mix of excitement about new hardware and nostalgia for the classic Radeon 9700 name. Users are particularly interested in performance metrics and thermal characteristics.

---

## 32. [status of Nemotron 3 Nano support in llama.cpp](https://reddit.com/r/LocalLLaMA/comments/1pnc045/status_of_nemotron_3_nano_support_in_llamacpp/)

**Author:** u/jacek2023 | **Upvotes:** 183 | **Comments:** 32 | **Date:** 2025-12-15

**Summary:** The Reddit post discusses the status of Nemotron 3 Nano support in llama.cpp, highlighting a GitHub pull request and community reactions. The discussion emphasizes the importance of collaboration between organizations and llama.cpp for new model architectures.

**Key Points:**
- Nemotron 3 Nano support is being added to llama.cpp via a GitHub pull request.
- Community appreciates Nvidia's effort and encourages other labs to follow suit.
- Discussion includes technical details about model sizes and memory requirements.
- Consensus that collaboration with llama.cpp is beneficial for new model releases.

**Discussion Highlights:** The community positively reacts to Nvidia's support for llama.cpp, emphasizing the importance of such collaborations. Technical details about model sizes and memory usage are discussed, and there is a general consensus that this approach should be adopted by other organizations releasing new models.

---

## 33. [NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model!](https://reddit.com/r/LocalLLaMA/comments/1pn8upp/nvidia_releases_nemotron_3_nano_a_new_30b_hybrid/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 837 | **Comments:** 178 | **Date:** 2025-12-15

**Summary:** NVIDIA has released Nemotron 3 Nano, a 30B hybrid reasoning model with a 1M context window and top performance in SWE-Bench, reasoning, and chat. The model is noted for its speed and is part of the Nemotron 3 family of MoE models.

**Key Points:**
- Nemotron 3 Nano is a 30B hybrid reasoning model with a 1M context window.
- It offers best-in-class performance for SWE-Bench, reasoning, and chat.
- The model is part of the Nemotron 3 family, which includes MoE models of varying sizes.
- Users report exceptional speed, with one achieving 110 tokens per second locally.
- The model's size (30B) is considered 'nano' in the context of the Nemotron 3 family.

**Discussion Highlights:** The discussion highlights the model's speed and performance, with users expressing surprise at the 'nano' designation for a 30B model. There is also clarification about the Nemotron 3 family, which includes models of different sizes.

---

## 34. [NVIDIA Nemotron 3 Nano 30B A3B released](https://reddit.com/r/LocalLLaMA/comments/1pn8h5h/nvidia_nemotron_3_nano_30b_a3b_released/)

**Author:** u/rerri | **Upvotes:** 280 | **Comments:** 85 | **Date:** 2025-12-15

**Summary:** NVIDIA has released Nemotron 3 Nano 30B A3B, a new model featuring a hybrid Mamba-Transformer MoE architecture, exceptional inference efficiency, and a 1M-token context window. The model is fully open and designed for high throughput and low latency.

**Key Points:**
- Hybrid Mamba-Transformer MoE architecture for efficient inference
- 31.6B total parameters with ~3.6B active per token
- Up to 4x faster than Nemotron Nano 2 and 3.3x faster than leading models in its size category
- 1M-token context window for long-horizon workflows
- Fully open with open weights, datasets, training recipes, and framework

**Discussion Highlights:** The discussion highlights include a Llama.cpp PR for integration, questions about optimal Unsloth quant for specific hardware, concerns about synthetic data training, and performance feedback from users who have tested the model.

---

## 35. [New Google model incoming!!!](https://reddit.com/r/LocalLLaMA/comments/1pn37mw/new_google_model_incoming/)

**Author:** u/R46H4V | **Upvotes:** 1257 | **Comments:** 263 | **Date:** 2025-12-15

**Summary:** The Reddit post discusses anticipation and speculation around an upcoming Google model, with users expressing hope for improvements over previous models like Gemma3-Math and potential multi-modal capabilities.

**Key Points:**
- Anticipation of a new Google model
- Hope for improvements over Gemma3-Math
- Speculation about multi-modal capabilities
- High engagement with 1257 upvotes and 263 comments

**Discussion Highlights:** The community is highly engaged and hopeful for significant advancements in the new model, with a focus on multi-modal capabilities and overall performance improvements.

---

## 36. [llama.cpp: Automation for GPU layers, tensor split, tensor overrides, and context size (with MoE optimizations)](https://reddit.com/r/LocalLLaMA/comments/1pn2e1c/llamacpp_automation_for_gpu_layers_tensor_split/)

**Author:** u/Remove_Ayys | **Upvotes:** 188 | **Comments:** 62 | **Date:** 2025-12-15

**Summary:** The post discusses a new automation feature in llama.cpp for managing GPU memory allocation, which prioritizes dense tensors for optimal performance, especially in MoE models. This addresses previous manual and heuristic-based methods that were suboptimal.

**Key Points:**
- CPU + GPU hybrid inference is a core feature of llama.cpp
- Manual memory allocation (e.g., --n-gpu-layers) is being replaced by automation
- New automation uses virtual test allocations to iteratively reduce memory use
- Dense tensors are prioritized for better MoE performance
- Users appreciate the feature and suggest caching for efficiency

**Discussion Highlights:** The discussion highlights appreciation for the new automation feature, with suggestions for caching to reduce fitting time and interest in multi-GPU setups with prioritization.

---

## 37. [Aaaand... is gone...](https://reddit.com/r/LocalLLaMA/comments/1pmungj/aaaand_is_gone/)

**Author:** u/HumanDrone8721 | **Upvotes:** 926 | **Comments:** 210 | **Date:** 2025-12-14

**Summary:** The Reddit post discusses the discontinuation or disappearance of a technology or product, likely related to storage devices, sparking a conversation about storage solutions and their relevance.

**Key Points:**
- The post title suggests something significant is no longer available.
- Comments mention buying additional storage (2TB SSD) and reference a GIF.
- Discussion includes perspectives on ownership and the relevance of SATA drives.
- Some users downplay the significance, calling it a 'nothingburger'.

**Discussion Highlights:** The discussion highlights a mix of reactions, with some users preparing for the change by purchasing additional storage, while others debate the significance of the event, particularly in relation to SATA drives and broader technological trends.

---

## 38. [To Mistral and other lab employees: please test with community tools BEFORE releasing models](https://reddit.com/r/LocalLLaMA/comments/1pmgm2x/to_mistral_and_other_lab_employees_please_test/)

**Author:** u/dtdisapointingresult | **Upvotes:** 137 | **Comments:** 72 | **Date:** 2025-12-14

**Summary:** The Reddit post criticizes Mistral for releasing Devstral 2 without thorough testing with community tools, which led to issues like benchmark discrepancies and repetition loops. The author emphasizes the importance of testing with local tools to maintain reputation and user trust.

**Key Points:**
- Devstral 2 release faced criticism due to lack of testing with community tools.
- Issues included benchmark discrepancies and repetition loops.
- The author stresses the importance of testing with local tools for reputation and user trust.
- Community feedback highlights mixed experiences with the model across different tools.
- The post underscores the influence of tech geeks in driving adoption and recommendations.

**Discussion Highlights:** The discussion reveals a mix of experiences with Devstral 2, with some users reporting positive outcomes and others facing issues. There is a consensus on the need for better testing and documentation to ensure smooth integration with community tools.

---

## 39. [Understanding the new router mode in llama cpp server](https://reddit.com/r/LocalLLaMA/comments/1pmc7lk/understanding_the_new_router_mode_in_llama_cpp/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 164 | **Comments:** 44 | **Date:** 2025-12-14

**Summary:** Router mode in llama cpp server allows managing multiple AI models simultaneously without restarting the server, enabling dynamic model switching and efficient memory usage.

**Key Points:**
- Router mode enables loading/unloading models on demand within a single server process.
- It saves memory and simplifies model switching compared to running separate servers.
- Useful for testing multiple GGUF models, building local APIs, and dynamic model switching.
- Discussion highlights include comparisons with llama-swap and requests for VRAM management features.

**Discussion Highlights:** The discussion includes comparisons with llama-swap, requests for better VRAM management, and general enthusiasm for the new functionality, though some users find the provided image unhelpful.

---

## 40. [8x RTX Pro 6000 server complete](https://reddit.com/r/LocalLLaMA/comments/1plwgun/8x_rtx_pro_6000_server_complete/)

**Author:** u/koushd | **Upvotes:** 623 | **Comments:** 268 | **Date:** 2025-12-13

**Summary:** The Reddit post details the author's journey of upgrading their GPU server over several years, culminating in a powerful setup with 8x RTX Pro 6000 GPUs, a Threadripper PRO 9955WX CPU, and 384 GB RAM. The post highlights challenges faced during upgrades, including heat management and hardware compatibility issues.

**Key Points:**
- The author started with a single 3080 GPU and gradually upgraded to a powerful 8x RTX Pro 6000 setup.
- Heat management was a significant issue, leading to overheating and system crashes.
- Hardware compatibility issues arose, particularly with motherboard limitations and power requirements.
- The community discussion includes both admiration for the setup and criticism of the implementation.

**Discussion Highlights:** The discussion highlights a mix of admiration for the powerful setup and criticism regarding the implementation, such as the use of a shoddy aluminum frame and unconventional cooling methods. Some users also shared their own experiences with similar hardware challenges.

---

## 41. [Mistral 3 Large is DeepSeek V3!?](https://reddit.com/r/LocalLLaMA/comments/1plpc6h/mistral_3_large_is_deepseek_v3/)

**Author:** u/seraschka | **Upvotes:** 172 | **Comments:** 33 | **Date:** 2025-12-13

**Summary:** The post discusses the architectural similarities between Mistral 3 Large and DeepSeek V3, noting that they share nearly identical sizes and architectures, with minor differences in expert configurations. The community highlights the open-source nature of these models and their adoption by various teams.

**Key Points:**
- Mistral 3 Large and DeepSeek V3 have almost identical sizes (671B vs 673B) and share the same architecture.
- Mistral 3 adjusted the expert configuration by increasing expert size while decreasing their number, maintaining the same number of expert parameters.
- The Mistral team likely trained their model from scratch rather than fine-tuning DeepSeek V3, as they use a different tokenizer.
- Other models like Kimi K2 and Gigachat also use the DeepSeek V3 architecture, showcasing its popularity in the open-source community.
- The community views this architectural reuse as a positive aspect of open-source collaboration.

**Discussion Highlights:** The discussion highlights the open-source spirit, with multiple teams adopting the DeepSeek V3 architecture. Users appreciate the innovation and efficiency of the architecture, while also noting that Mistral added multimodal capabilities as a form of innovation.

---

## 42. [OpenAI's flagship model, ChatGPT-5.2 Thinking, ranks  most censored AI on Sansa benchmark.](https://reddit.com/r/LocalLLaMA/comments/1plnuqu/openais_flagship_model_chatgpt52_thinking_ranks/)

**Author:** u/Difficult-Cap-7527 | **Upvotes:** 623 | **Comments:** 112 | **Date:** 2025-12-13

**Summary:** The Reddit post discusses OpenAI's ChatGPT-5.2 model, highlighting its high censorship levels on the Sansa benchmark and perceived performance issues compared to previous versions.

**Key Points:**
- ChatGPT-5.2 is ranked as the most censored AI on the Sansa benchmark.
- Users report that the model struggles with follow-up questions and research tasks, performing worse than version 5.1.
- The model frequently denies requests for evaluating QA models, a behavior not observed in previous versions.
- There is curiosity about the testing criteria used in the benchmark, especially given Grok's low ranking.
- Gemini is noted to be less censored than other open models, including Mistral.

**Discussion Highlights:** The discussion highlights user dissatisfaction with ChatGPT-5.2's performance and censorship levels, with comparisons to previous models and other AI systems like Gemini and Grok.

---

## 43. [Qwen3 Next generation optimization](https://reddit.com/r/LocalLLaMA/comments/1plng6f/qwen3_next_generation_optimization/)

**Author:** u/ilintar | **Upvotes:** 358 | **Comments:** 40 | **Date:** 2025-12-13

**Summary:** The post discusses optimizations for Qwen3, specifically an autoregressive delta net computation that improves generation speed by 40%. The author invites feedback on the optimization's performance.

**Key Points:**
- Optimized autoregressive delta net computation
- 40% generation speed improvement reported
- Optimizations include removing unnecessary reshapes
- Community appreciation and engagement
- Query about compatibility with ROCm/Vulkan

**Discussion Highlights:** The community shows strong appreciation for the optimization work, with comments highlighting the author's frequent contributions and expressing interest in further improvements. There is also a question about whether the speedup applies to ROCm/Vulkan in addition to CUDA.

---

## 44. [NVIDIA gpt-oss-120b Eagle Throughput model](https://reddit.com/r/LocalLLaMA/comments/1plewrk/nvidia_gptoss120b_eagle_throughput_model/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 243 | **Comments:** 53 | **Date:** 2025-12-13

**Summary:** The post discusses NVIDIA's GPT-OSS-120B-Eagle3-throughput model, an optimized speculative decoding module designed to improve text generation throughput using Eagle3 speculative decoding. It is licensed for both commercial and non-commercial use.

**Key Points:**
- GPT-OSS-120B-Eagle3-throughput is an optimized speculative decoding module built on OpenAI's gpt-oss-120b base model.
- It uses NVIDIA’s Eagle3 speculative decoding approach to predict a single draft token efficiently.
- The model is licensed under the nvidia-open-model-license for commercial and non-commercial use.
- It is intended for applications like AI agents, chatbots, and retrieval-augmented generation (RAG) systems.
- The model is not supported in llama.cpp, as indicated by a stale feature request.

**Discussion Highlights:** The discussion includes a request for a derestricted version of the model, mentions of potential CPU inference benefits, and a note about the lack of support in llama.cpp. There is also a humorous comment about waiting for a REAP EAGLE3 HERETIC MOE GGUF version.

---

## 45. [This is how open ai is advertising them selfs on reddit…. They are doomed](https://reddit.com/r/LocalLLaMA/comments/1plcrg8/this_is_how_open_ai_is_advertising_them_selfs_on/)

**Author:** u/ThinkExtension2328 | **Upvotes:** 234 | **Comments:** 77 | **Date:** 2025-12-12

**Summary:** The Reddit post criticizes OpenAI's advertising strategy, highlighting a shift from promoting advanced AI to using astrology ads, which is seen as a decline in their approach.

**Key Points:**
- OpenAI's advertising strategy is criticized for shifting from advanced AI to astrology ads.
- The post suggests that OpenAI's focus on normies rather than programmers is a misstep.
- Comments highlight the irony of OpenAI's previous stance on the dangers of open models.
- There is a consensus that the new advertising approach is less impressive and potentially more profitable.
- The discussion includes humor about OpenAI's data collection capabilities.

**Discussion Highlights:** The discussion highlights a consensus that OpenAI's shift in advertising strategy is seen as a decline from their previous focus on advanced AI. Comments emphasize the irony of their previous stance on open models and the potential profitability of targeting a broader audience.

---

## 46. [Running an LLM on a 3DS](https://reddit.com/r/LocalLLaMA/comments/1pl4njj/running_an_llm_on_a_3ds/)

**Author:** u/vreab | **Upvotes:** 294 | **Comments:** 35 | **Date:** 2025-12-12

**Summary:** The Reddit post discusses the feasibility and performance of running an LLM on a Nintendo 3DS, with users expressing curiosity and admiration for the project. The discussion includes comparisons to similar projects on other devices like the PS Vita and Wii.

**Key Points:**
- Running an LLM on a 3DS is technically feasible and impressive.
- Similar projects have been done on devices like the PS Vita and Wii.
- Users are curious about performance improvements on a 'new' 3DS.
- There is admiration for the technical achievement of running an LLM on such hardware.

**Discussion Highlights:** The discussion highlights the technical curiosity and admiration for running an LLM on unconventional hardware like the 3DS. Users compare it to similar projects on other devices and express interest in potential performance improvements on newer versions of the 3DS.

---

## 47. [The new monster-server](https://reddit.com/r/LocalLLaMA/comments/1pl0ojb/the_new_monsterserver/)

**Author:** u/eribob | **Upvotes:** 588 | **Comments:** 125 | **Date:** 2025-12-12

**Summary:** The author shares their upgraded 'Monster-server,' a powerful homelab setup featuring a Ryzen 3950x CPU, 128GB RAM, and three GPUs (2x RTX 3090 and 1x RTX 4090). The server runs local LLMs like GPT-OSS-120B and is used for research and coding.

**Key Points:**
- The server uses a Ryzen 3950x CPU and 128GB RAM, with three GPUs (2x RTX 3090 and 1x RTX 4090).
- The RTX 4090 is connected via an M.2 to PCIe adapter and a second PSU.
- The author runs GPT-OSS-120B fully in VRAM, achieving over 100 tokens per second.
- The setup includes 10GB fiber internet and a mix of NVMe and HDD storage.
- Discussion highlights include nostalgia for early 2000s overclocking forums and questions about the 3-GPU setup's efficiency.

**Discussion Highlights:** The discussion includes nostalgia for early 2000s overclocking forums, questions about the author's location due to affordable 10GB internet, and debates on the efficiency of a 3-GPU setup compared to 2 or 4 GPUs. Some users also expressed envy and curiosity about the heat management and PSU setup.

---

## 48. [Olmo 3.1 32B Think &amp; Instruct: New Additions to the Olmo Model Family](https://reddit.com/r/LocalLLaMA/comments/1pky5u4/olmo_31_32b_think_instruct_new_additions_to_the/)

**Author:** u/Dear-Success-1441 | **Upvotes:** 182 | **Comments:** 28 | **Date:** 2025-12-12

**Summary:** Olmo 3.1 32B Think and Instruct are new 32-billion-parameter models in the Olmo family, optimized for deep reasoning and instruction following, respectively. The Think model excels in multi-step reasoning and code generation, while the Instruct model focuses on conversational fluency and tool-use capabilities.

**Key Points:**
- Olmo 3.1 32B Think is optimized for deep reasoning, math, logic, and code generation.
- Olmo 3.1 32B Instruct is optimized for instruction following, conversational fluency, and tool-use capabilities.
- Both models are fully open-source and part of the Olmo family.
- The community appreciates the models' openness and continuous improvement.
- There is anticipation for potential future developments like MOE (Mixture of Experts).

**Discussion Highlights:** The community discussion highlights appreciation for the open-source nature of the Olmo models and their continuous improvement. There is also anticipation for future developments, such as the potential addition of Mixture of Experts (MOE) models.

---

## 49. [Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face](https://reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/)

**Author:** u/Nunki08 | **Upvotes:** 1326 | **Comments:** 156 | **Date:** 2025-12-12

**Summary:** An NVIDIA employee accidentally uploaded the parent folder of their upcoming model on Hugging Face, sparking interest and urgency among users to save the files before potential removal.

**Key Points:**
- NVIDIA's upcoming model files were accidentally uploaded on Hugging Face.
- Users are urged to save the files before they might be taken down.
- The Nemotron lineup is mentioned as promising.
- There is concern about potential censoring of the uploaded content.

**Discussion Highlights:** The community is actively discussing the accidental upload, with many users emphasizing the importance of preserving the files and expressing interest in the Nemotron projects.

---

## 50. [Training an LLM only on 1800s London texts - 90GB dataset](https://reddit.com/r/LocalLLaMA/comments/1pkpsee/training_an_llm_only_on_1800s_london_texts_90gb/)

**Author:** u/Remarkable-Trick-177 | **Upvotes:** 708 | **Comments:** 79 | **Date:** 2025-12-12

**Summary:** The post discusses the TimeCapsuleLLM project, which involves training an LLM on a 90GB dataset of 1800-1875 London texts. The author has conducted a bias report and trained a small evaluation model to assess the dataset before scaling up.

**Key Points:**
- The dataset consists of 90GB with 135,000 documents from 1800-1875 London texts.
- A bias report covering temporal, gender/pronoun, and geographic bias has been generated.
- A small evaluation model (300M parameters) was trained on a 15GB subset to evaluate the dataset.
- The community appreciates the detailed work and suggests considering MoE for better compute efficiency.
- The project aims to study historical biases and train an LLM specific to the 1800s London context.

**Discussion Highlights:** The community shows strong support for the project, with suggestions to consider Mixture of Experts (MoE) for better compute efficiency. There is also interest in the methodology and the potential for further exploration of historical texts.

---

